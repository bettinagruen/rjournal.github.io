# Generated by `rjournal_article()` using `knitr::purl()`: do not edit by hand
# Please edit RJ-2022-019.Rmd to modify this file

## ----setup, include=FALSE-----------------------------------------------------
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library("ggplot2")


## ----fairnessTable1, fig.align='center', out.width  = "100%", fig.cap="Summary of possible model outcomes for subpopulation $A = a$. We assume that outcome $Y = 1$ is favourable."----
knitr::include_graphics("table1.png")


## ---- echo = TRUE-------------------------------------------------------------
library("fairmodels")
data("german")

lm_model <- glm(Risk~., data = german, family = binomial(link = "logit"))


## ---- echo = TRUE, results='hide'---------------------------------------------
library("DALEX")

y_numeric <- as.numeric(german$Risk) -1
explainer_lm <- DALEX::explain(lm_model, data = german[,-1], y = y_numeric)


## ---- echo = TRUE, results='hide',  out.width  = "75%", fig.align='center'----
fobject <- fairness_check(explainer_lm,
                protected = german$Sex, privileged = "male",
                verbose = FALSE)


## ----fairness-plot-1, echo = TRUE, results='hide',  out.width  = "75%", fig.align='center', fig.cap="The Fairness Check plot summarises the ratio of fairness measures between unprivileged and privileged subgroups. The light green areas correspond to values within $(\\varepsilon, \\frac{1}{\\varepsilon})$ and signify an acceptable difference in fairness metrics. They are bounded by red rectangles indicating values that do not meet the 4/5 rule. Fairness metrics names are given along the formulas used to calculate the score in some subgroups to facilitate interpretation. For example, the ratio here means that after metric scores were calculated, the values for unprivileged groups (female) were divided by values for the privileged subgroup (male). In this example, except for the predictive equality ratio, the other measures are $\\varepsilon$-non-discriminatory. "----
plot(fobject)


## ---- echo = TRUE,  out.width  = "75%", fig.align='center'--------------------
print(fobject, colorize = FALSE)


## ----fairness-plot-2, echo = TRUE, results='hide',  out.width  = "75%", fig.align='center', fig.cap="The Metric Scores plot summarises raw fairness metrics scores for subgroups. The dots stand for unprivileged subgroups (female) while vertical lines stans for the privileged subgroup (male). The horizontal lines act as a visual aid for measuring the difference between the scores of the metrics between the privileged and unprivileged subgroups."----
plot(metric_scores(fobject))


## ---- echo = TRUE, results='hide'---------------------------------------------
discriminative_lm_model <- glm(Risk~.,
         data   = german[c("Risk", "Sex","Age",
                "Checking.account", "Credit.amount")],
         family = binomial(link = "logit"))

library("ranger")
rf_model <- ranger::ranger(Risk ~.,
         data = german, probability = TRUE,
         max.depth = 4, seed = 123)


## ---- echo = TRUE, results='hide'---------------------------------------------
explainer_dlm <- DALEX::explain(discriminative_lm_model,
        data = german[c("Sex", "Age", "Checking.account", "Credit.amount")],
        y = y_numeric,
        label = "discriminative_lm") 

explainer_rf <- DALEX::explain(rf_model, 
        data = german[,-1], y = y_numeric)


## ----fairness-plot-3, echo = TRUE, results='hide',  out.width  = "75%", fig.align='center', fig.cap="The Fairness Check plot for multiple models. It helps to compare models based on five selected fairness measures. "----
fobject <- fairness_check(explainer_rf, explainer_dlm, fobject)
plot(fobject)


## ---- echo = TRUE,  out.width  = "75%", fig.align='center'--------------------
print(fobject, colorize = FALSE)


## ----classdiagram, fig.cap="Class diagram for objects created by functions from the fairmodels package. Each rectangle corresponds to one class, the name of this class is in the header of the rectangle. Each of these classes is a list containing a certain list of objects. The top slot lists the names and types of each object the list. The bottom slot contains a list of functions that can be performed on objects of the specified class. If two classes are connected by a line ending in a diamond it means that one class contains objects of the other class. If two rectangles are connected by a dashed line, it means that on the basis of one object, an object of another class can be produced. In this case, more detailed fairness statistics can be produced from the central object of the fairness check class. See the full resolution at https://bit.ly/3HNbNvo", fig.align = 'center', out.width  = "100%"----
knitr::include_graphics("class_diagram.png")


## ----flowchart, fig.cap="Flowchart for the fairness assessment with the fairmodels package. The arrows describe typical sequences of actions when exploring the fairness of the models. For ease of use, the names of the functions that can be used in a given step are indicated. Note that this procedure is intended to look at the model from multiple perspectives in order to track down potential problems in the model. Merely satisfying the fairness criteria does not automatically mean that the model is free of any errors", fig.align = 'center', out.width  = "100%"----
knitr::include_graphics("flow.png")


## ----table-html, eval = knitr::is_html_output(), results='asis'---------------
#> cat('<table>
#> <caption>Fairness metrics implemented in the <strong>fairmodels</strong>
#> package. </caption>
#> <colgroup>
#> <col style="width: 19%" />
#> <col style="width: 21%" />
#> <col style="width: 14%" />
#> <col style="width: 45%" />
#> </colgroup>
#> <thead>
#> <tr class="header">
#> <th>Metric</th>
#> <th>Formula</th>
#> <th>Name</th>
#> <th>Fairness criteria</th>
#> </tr>
#> </thead>
#> <tbody>
#> <tr class="odd">
#> <td>TPR</td>
#> <td><span class="math inline">\\(\\frac{TP}{TP + FN}\\)</span></td>
#> <td>True positive rate</td>
#> <td>Equal opportunity <span class="citation"
#> data-cites="NIPS20166374">(<a href="#ref-NIPS20166374"
#> role="doc-biblioref">Hardt et al. 2016</a>)</span></td>
#> </tr>
#> <tr class="even">
#> <td>TNR</td>
#> <td><span class="math inline">\\(\\frac{TN}{TN + FP}\\)</span></td>
#> <td>True negative rate</td>
#> <td></td>
#> </tr>
#> <tr class="odd">
#> <td>PPV</td>
#> <td><span class="math inline">\\(\\frac{TP}{TP + FP}\\)</span></td>
#> <td>Positive predictive value</td>
#> <td>Predictive parity <span class="citation" data-cites="ppv">(<a
#> href="#ref-ppv" role="doc-biblioref">Chouldechova 2016</a>)</span></td>
#> </tr>
#> <tr class="even">
#> <td>NPV</td>
#> <td><span class="math inline">\\(\\frac{TN}{TN + FN}\\)</span></td>
#> <td>Negative predictive value</td>
#> <td></td>
#> </tr>
#> <tr class="odd">
#> <td>FNR</td>
#> <td><span class="math inline">\\(\\frac{FN}{FN + TP}\\)</span></td>
#> <td>False negative rate</td>
#> <td></td>
#> </tr>
#> <tr class="even">
#> <td>FPR</td>
#> <td><span class="math inline">\\(\\frac{FP}{FP + TN}\\)</span></td>
#> <td>False positive rate</td>
#> <td>Predictive equality <span class="citation" data-cites="ppe">(<a
#> href="#ref-ppe" role="doc-biblioref">Corbett-Davies et al.
#> 2017</a>)</span></td>
#> </tr>
#> <tr class="odd">
#> <td>FDR</td>
#> <td><span class="math inline">\\(\\frac{FP}{FP + TP}\\)</span></td>
#> <td>False discovery rate</td>
#> <td></td>
#> </tr>
#> <tr class="even">
#> <td>FOR</td>
#> <td><span class="math inline">\\(\\frac{FN}{FN + TN}\\)</span></td>
#> <td>False omission rate</td>
#> <td></td>
#> </tr>
#> <tr class="odd">
#> <td>TS</td>
#> <td><span class="math inline">\\(\\frac{TP}{TP + FN + FP}\\)</span></td>
#> <td>Threat score</td>
#> <td></td>
#> </tr>
#> <tr class="even">
#> <td>F1</td>
#> <td><span class="math inline">\\(\\frac{2 \\cdot PPV * TPR}{PPV +
#> TPR}\\)</span></td>
#> <td>F1 score</td>
#> <td></td>
#> </tr>
#> <tr class="odd">
#> <td>STP</td>
#> <td><span class="math inline">\\(\\frac{TP + FP}{TP + FP + TN +
#> FN}\\)</span></td>
#> <td>Positive rate</td>
#> <td>Statistical parity</td>
#> </tr>
#> <tr class="even">
#> <td>ACC</td>
#> <td><span class="math inline">\\(\\frac{TP + TN}{TP + TN + FP +
#> FN}\\)</span></td>
#> <td>Accuracy</td>
#> <td>Overall accuracy equality</td>
#> </tr>
#> </tbody>
#> </table>')


## ----table-latex, eval = knitr::is_latex_output(), results='asis'-------------
cat('\\begin{table}[h!]
\\begin{center}
\\footnotesize
    \\begin{tabular}{p{1cm}p{2.6cm}p{3.8cm}p{4cm}}
    \\hline
Metric & Formula & Name & Fairness criteria \\\\ \\hline    
TPR & $\\frac{TP}{TP + FN}$ & True positive rate & Equal opportunity \\newline\\citep{NIPS20166374} \\\\ \\hline
TNR & $\\frac{TN}{TN + FP}$ & True negative rate & \\\\ \\hline
PPV & $\\frac{TP}{TP + FP}$ & Positive predictive value & Predictive parity \\newline\\citep{ppv}\\\\ \\hline
NPV & $\\frac{TN}{TN + FN}$ & Negative predictive value & \\\\ \\hline
FNR & $\\frac{FN}{FN + TP}$ & False negative rate & \\\\ \\hline
FPR & $\\frac{FP}{FP + TN}$ & False positive rate & Predictive equality \\newline\\citep{ppe} \\\\ \\hline
FDR & $\\frac{FP}{FP + TP}$ & False discovery rate &\\\\ \\hline
FOR & $\\frac{FN}{FN + TN}$ & False omission rate & \\\\ \\hline
TS & $\\frac{TP}{TP + FN + FP}$  & Threat score &\\\\ \\hline
STP & $\\frac{TP + FP}{TP + FP + TN + FN}$ & Positive rate & Statistical parity \\newline \\citep{statisticalparity}\\\\ \\hline
ACC & $\\frac{TP + TN}{TP + TN + FP + FN}$ & Accuracy & Overall accuracy equality \\newline\\citep{accuracy} \\\\ \\hline
F1 &  $\\frac{2 \\cdot PPV * TPR}{PPV + TPR}$ & F1 score &\\\\ \\hline
    \\end{tabular}
    \\caption{Fairness metrics implemented in the \\textbf{fairmodels} package}
\\label{tab:Metrics-table}
\\end{center}
\\end{table}')


## ---- echo = TRUE, results='hide', fig.keep='none'----------------------------
fp1  <- plot(ceteris_paribus_cutoff(fobject, "male", cumulated=TRUE))
fp2  <- plot(fairness_heatmap(fobject))
fp3  <- plot(stack_metrics(fobject))
fp4  <- plot(plot_density(fobject))


## ----all, echo = TRUE, results='hide', fig.width=10, fig.height=10,  out.width  = "100%", fig.align='center', fig.cap="Four examples of additional graphical functions are available in the fairmodels package that facilitates model and bias exploration. The Ceteris Paribus Cuttoff plot helps select the cutoff values for each model to maximize a particular measure of fairness. In this case, the suggested cutoff point for both linear models is similar. However, the ranger model does not have calibrated probabilities and thus requires a different cutoff. The Heatmap plot is very helpful when comparing large numbers of models. It shows profiles of selected fairness measures for each of the models under consideration. In this case, the fairness profiles for both linear models are similar. The Stacked Metric plot helps you compare models by summing five different fairness measures. The different layers of this plot allow you to compare individual measures, but if you don't know which one to focus on, it is useful to look at the sum of the measures. In this case, the ranger model has the highest fairness values. Finally, the Density plot helps to compare the score distributions of the models between the advantaged and disadvantaged groups. In this case, we find that for females the distributions of the scores are lower in all models, with the largest difference for the lm model. "----
library("patchwork")
fp1 + fp2 + fp3 + fp4 + 
  plot_layout(ncol = 2)


## ---- echo = TRUE-------------------------------------------------------------
resampled_german   <- german |> pre_process_data(protected = german$Sex,
                y_numeric, type = 'resample_uniform')

lm_model_resample  <- glm(Risk~.,
                data   = resampled_german,
                family = binomial(link = "logit"))

explainer_lm_resample <- DALEX::explain(lm_model_resample,
                data = german[,-1], y = y_numeric, verbose = FALSE)


## ---- echo = TRUE-------------------------------------------------------------
new_explainer <- explainer_lm |> roc_pivot(protected = german$Sex,
                privileged = "male", theta = 0.05)


## ----mitigation, echo = TRUE, fig.cap="Graphical summary of a base model (blue bars) and model after applying two bias mitigation techniques (red and green bars). By comparing adjacent rectangles one can read how the respective technique affected the corresponding fairness measure", fig.align = 'center', out.width  = "80%"----
fobject <- fairness_check(explainer_lm_resample, new_explainer, explainer_lm,
                protected = german$Sex, privileged = "male",
                label = c("resample", "roc", "base"),
                verbose = FALSE)

fobject |> plot()

