% !TeX root = RJwrapper.tex
\title{OTrecod: An R Package for Data Fusion using Optimal Transportation Theory}
\author{by Gregory Guernec, Valerie Gares, Jeremy Omer, Philippe Saint-Pierre, and Nicolas Savy}

\maketitle

\abstract{%
The advances of information technologies often confront users with a large amount of data which is essential to integrate easily. In this context, creating a single database from multiple separate data sources can appear as an attractive but complex issue when same information of interest is stored in at least two distinct encodings. In this situation, merging the data sources consists in finding a common recoding scale to fill the incomplete information in a synthetic database. The \CRANpkg{OTrecod} package provides R-users two functions dedicated to solve this recoding problem using optimal transportation theory. Specific arguments of these functions enrich the algorithms by relaxing distributional constraints or adding a regularization term to make the data fusion more flexible.
The \CRANpkg{OTrecod} package also provides a set of support functions dedicated to the harmonization of separate data sources, the handling of incomplete information and the selection of matching variables. This paper gives all the keys to quickly understand and master the original algorithms implemented in the \CRANpkg{OTrecod} package, assisting step by step the user in its data fusion project.
}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The large amount of data produced by information technology requires flexible tools to facilitate its handling. Among them, the field of data fusion (Hall and Llinas 1997; Klein 2004; Castanedo 2013) also known as statistical matching (Adamek 1994; D'Orazio, Di Zio, and Scanu 2006; Vantaggi 2008) aims to integrate the overall information from multiple data sources for a better understanding of the phenomena that interact in the population.

Assuming that two heterogeneous databases \emph{A} and \emph{B} share a set of common variables \emph{X} while an information of interest is encoded in two distinct scales respectively: \emph{Y} in \emph{A} and \emph{Z} in \emph{B}. If \emph{Y} and \emph{Z} are never jointly observed, a basic data fusion objective consists in the recoding of \emph{Y} in the same scale of \emph{Z} (or conversely), to allow the fusion between the databases as illustrated in Table \ref{tab:fig1}.

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|cc|c|c|c|c|c|}
\multicolumn{5}{c}{Initial}  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{5}{c}{Final}\\
\multicolumn{5}{l}{}  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{5}{c}{} \\
\cline{1-5} ${DB}$ & ${ID}$ & ${Y}$ & ${Z}$ & ${X}$ & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{5}{c}{} \\

\cline{1-5} \cline{8-12} ${A}$ & 1 &  \multirow{4}{*}{\rotatebox[origin=c]{90}{observed}} &
\multirow{4}{*}{\rotatebox[origin=c]{90}{\strong{???}}} & \multirow{4}{*}{\rotatebox[origin=c]{90}{observed}} &  &  &  ${DB}$ & ${ID}$ & ${Y}$ & ${Z}$ & ${X}$ \\
\cline{8-12} $A$ & 2 &  &  &  &  &  & $A$ &  1  & \multirow{4}{*}{\rotatebox[origin=c]{90}{observed}} &
\multirow{4}{*}{\rotatebox[origin=c]{90}{\strong{predicted}}} & \multirow{4}{*}{\rotatebox[origin=c]{90}{observed}} \\
$\dots$ & $\dots$ &  &  &  & $\Rightarrow$ &  & $A$ &  2 &  &  & \\
$A$ & $n_A$ &  &  &  &  &  & $\dots$ &  $\dots$ &  &  & \\
\cline{1-5} 
\multicolumn{5}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & $A$ & $n_A$ &  &  & \\
\cline{1-5} \cline{8-12} ${DB}$ & ${ID}$ & ${Y}$ & ${Z}$ & ${X}$ &  & & $B$ & $n_A$+1 & \multirow{4}{*}{\rotatebox[origin=c]{90}{\strong{predicted}}} &
\multirow{4}{*}{\rotatebox[origin=c]{90}{observed}} & \multirow{4}{*}{\rotatebox[origin=c]{90}{observed}} \\
\cline{1-5} $B$ & $n_A$+1 &  
\multirow{4}{*}{\rotatebox[origin=c]{90}{\strong{???}}} &
\multirow{4}{*}{\rotatebox[origin=c]{90}{observed}} & \multirow{4}{*}{\rotatebox[origin=c]{90}{observed}} & $\Rightarrow$ & &  $B$ & $n_A$+2 &  &  & \\
$B$ & $n_A$+2 &  &  &  & &  & $\dots$ &  $\dots$ &  &  & \\
$\dots$ & $\dots$ &  &  &  & &  & $B$ &  $n_A$+$n_B$ &  &  & \\
\cline{8-12} $B$ & $n_A$+$n_B$ &  &  &  & & \multicolumn{1}{l}{} & \multicolumn{5}{l}{} \\
\cline{1-5}
\end{tabular}
\end{center}
\caption{This package provides algorithms for merging two databases $A$ and $B$ where two variables of interest, $Y$ and $Z$, are never jointly observed: the final result is a unique and synthetic database where $Y$ and $Z$ are fully completed.}
\label{tab:fig1}
\end{table}

Providing a solution to this recoding problem is often very attractive because it aims at giving access to more accurate and consistent information with no additional costs in a unique and bigger database. Despite this, if we exclude all \texttt{R} data integration packages applied in the context of genetic area like the \BIOpkg{MultiDataSet} package (Hernandez-Ferrer et al. 2017), \BIOpkg{OMICsPCA} package (Das and Tripathy 2022), or the \BIOpkg{mixOmics} package (F et al. 2017) which are often only effective for quantitative data integration. To our knowledge, the \CRANpkg{StatMatch} package (D'Orazio 2022) is actually the only one that provide a concrete solution to the problem using hot deck imputation procedures. The main reason for this relative deficiency is that this problem is, in fact, often assimilated and solved like a missing data imputation problem. According to this idea, a very large amount of works and reference books now exist about the handling of missing data (Zhu, Wang, and Samworth 2019; Little and Rubin 2019). Moreover, we can enumerate several \texttt{R} packages that we can sort by types of imputation methods (Mayer et al. 2019): \CRANpkg{mice} (van Buuren and Groothuis-Oudshoorn 2011) and \CRANpkg{missForest} (Stekhoven and Bühlmann 2012; Stekhoven 2022) which use conditional models, \CRANpkg{softImpute} (Hastie and Mazumder 2021) and \CRANpkg{missMDA} (Josse and Husson 2016) which apply low-rank based models. For all these packages, imputation performances can sometimes fluctuate a lot according to the structure and the proportion of non-response encountered. Regressions and non parametric imputation approaches (like hot-deck methods from donor based family) seem to use partially, or not at all, the available information of the two databases to provide the individual predictions. Contrary to our approach, all these methods only use the set of shared variables \emph{X} for the prediction of \emph{Z} in \emph{A} (or conversely \emph{Y} in \emph{B}) without really taking into account \emph{Y} and its interrelations with \emph{X} in their process of predictions.

The purpose of this paper is to present a new package to the \texttt{R} community called \CRANpkg{OTrecod} which provides a simple and intuitive access to two original algorithms (Garès et al. 2020; Garès and Omer 2022) dedicated to solve these recoding problems in the data fusion context by considering them as applications of Optimal Transportation theory (OT). In fact, this theory was already applied in many areas: for example, the \CRANpkg{transport} package (Schuhmacher et al. 2022) solves optimal transport problems in the field of image processing. A specific package called \emph{POT: Python Optimal Transport} (Flamary et al. 2021) also exists in the \emph{Python} software (Van Rossum and Drake Jr 1995) to solve optimization problems using optimal transportation theory in the fields of signal theory, image processing and domain adaptation. Nevertheless, all these available tools were still not really adapted to our recoding problem and the performances established by OT-based approaches to predict missing information compared to more standard processes (Garès et al. 2020; Garès and Omer 2022; Muzellec et al. 2020) have finished to confirm our decision.

In \CRANpkg{OTrecod} the first provided algorithm, called \texttt{OUTCOME} and integrated in the \texttt{OT\_outcome} function consists in finding a map that pushes the distribution of \emph{Y} forward to the distribution of \emph{Z} (Garès et al. 2020) while the second one, called \texttt{JOINT} and integrated in the \texttt{OT\_joint} function, pushes the distribution of \emph{(Y,X)} forward to the distribution of \emph{(Z,X)}. Consequently, by building, these two algorithms take advantage of all the potential relationships between \emph{Y}, \emph{Z}, and \emph{X} for the prediction of the incomplete information of \emph{Y} and/or \emph{Z} in \emph{A} and \emph{B}. Enrichments related to these algorithms and described in (Garès and Omer 2022; Cuturi 2013) are also available via the optional arguments of these two functions. In its current version, these algorithms are accompanied by original preparation (\texttt{merge\_dbs}, \texttt{select\_pred}) and validation (\texttt{verif\_OT}) functions which are key steps of any standard statistical matching project and can be used independently of the \texttt{OUTCOME} and \texttt{JOINT} algorithms.

\hypertarget{solving-recoding-problems-using-optimal-transportation-theory}{%
\section{Solving recoding problems using optimal transportation theory}\label{solving-recoding-problems-using-optimal-transportation-theory}}

\hypertarget{the-optimal-transportation-problem}{%
\subsubsection{The optimal transportation problem}\label{the-optimal-transportation-problem}}

The optimal transportation (\textbf{OT}) problem was originally stated by Monge (1781) and consists in finding the cheapest way to transport a pile of sand to fill a hole. Formally the problem writes as follows.

Consider two (Radon) spaces \(\mathbb{X}\) and \(\mathbb{Y}\), \(\mu^X\) a probability measure on \(\mathbb{X}\), and \(\mu^Y\) a probability measure on \(\mathbb{Y}\) and \(c\) a Borel-measurable function from \(\mathbb{X} \times \mathbb{Y}\) to \(\left[0,\infty\right]\). The Kantorovich's formulation of the optimal transportation problem (Kantorovich 1942) consists in finding a measure \(\gamma \in \Gamma(\mu^X,\mu^Y)\) that realizes the infimum:

\begin{equation}
  \inf\left\{\left. \int_{{\mathbb{X}} \times {\mathbb{Y}}} c(x, y) \, \mathrm{d} \gamma (x,y) \right| \gamma \in \Gamma(\mu^X,\mu^Y) \right\},
\label{eq:kanto1}
\end{equation}

where \(\Gamma(\mu^X,\mu^Y)\) is the set of measures on \(\mathbb{X} \times \mathbb{Y}\) with marginals \(\mu^X\) on \(\mathbb{X}\) and \(\mu^Y\) on \(\mathbb{Y}\).

This theory is applied here to solve a recoding problem of missing distributions in a data fusion area. To do so, we make use of Kantovorich's formulation adapted to the discrete case, known as Hitchcock's problem (Hitchcock 1941). Therefore, by construction, the proposed algorithms are usable for specific target variables only: categorical variables, ordinal or nominal, and discrete variables with finite number of values.

\hypertarget{optt}{%
\subsubsection{Optimal transportation of outcomes applied to data recoding}\label{optt}}

Let \(A\) and \(B\) be two databases corresponding to two independent sets of subjects.
We assume without loss of generality that the two databases have equal sizes, so that they can be written as \(A=\{i_1,\dots,i_{n}\}\) and \(B=\{j_1,\dots,j_{n}\}\). Let \(\big((X_i,Y_i,Z_i)\big)_{i\in A}\) and \(\big((X_j,Y_j,Z_j)\big)_{j\in B}\) be two sequences of i.i.d. discrete random variables with values in \(\mathcal{X} \times \mathcal{Y} \times \mathcal{Z}\), where \(\mathcal{X}\) is a finite subset of \(\mathbb{R}^P\), and \(\mathcal{Y}\) and \(\mathcal{Z}\) are finite subsets of \(\mathbb{R}\). Variables \((X_i,Y_i,Z_i), i\in A\), are i.i.d copies of \((X^A,Y^A,Z^A)\) and \((X_j,Y_j,Z_j), j\in B\), are i.i.d copies of \((X^B,Y^B,Z^B)\). Moreover assume that \(\big\{(X_i,Y_i,Z_i),i\in A\big\}\) are independent of \(\big\{(X_j,Y_j,Z_j),j\in B\big\}\).
The first version using the optimal transportation algorithm approach, described in (Garès et al. 2020), assumes that:

\textbf{Assumption 1.} \emph{\(Y^A\) and \(Z^A\) respectively follow the same distribution as \(Y^B\) and \(Z^B\)}.

\textbf{Assumption 2.} \emph{For all \(x \in \mathcal{X}\) the probability distributions of \(Y^A\) and \(Z^A\) given that \(X^A= x\) are respectively equal to those of \(Y^B\) and \(Z^B\) given that \(X^B= x\)}.

In this setting, the aim is to solve the recoding problem given by equation \eqref{eq:kanto1} that pushes \(\mu^{Y^A}\) forward to \(\mu^{Z^A}\).
The variable \(\gamma\) of \eqref{eq:kanto1} is a discrete measure with marginals \(\mu^{Y^A}\) and \(\mu^{Z^A}\), represented by a \(|\mathcal{Y}| \times |\mathcal{Z}|\) matrix.
The cost function denoted as \(c\) is a \(|\mathcal{Y}| \times |\mathcal{Z}|\) matrix, \((c_{y,z})_{y\in \mathcal{Y},z\in \mathcal{Z}}\).
The goal is in the identification of:

\begin{equation}
\gamma^*\in argmin_{\gamma \in \mathbb{R}_+^{|\mathcal{Y}| \times |\mathcal{Z}|}}\left\{\langle \gamma  \; ,\; c \rangle : \gamma \mathbf{1}_{|\mathcal{Z}|} = \mu^{Y^A},\gamma^T \mathbf{1}_{|\mathcal{Y}|} = \mu^{Z^A}\right\},
\label{eq:modeldim}
\end{equation}

where \(\langle \cdot \;,\; \cdot \rangle\)
is the dot product, \(\mathbf{1}\) is a vector of ones with appropriate dimension and \(M^T\) is the transpose of matrix \(M\). The cost function considered by Garès et al. (2020), \(c_{y,z}\), measures the average distance between the profiles of shared variables of \(A\) satisfying \(Y=y\) and subjects of \(B\) satisfying \(Z=z\), that is:

\begin{equation}
c_{y,z} = \mathbb{E} \left[d(X^A,X^B) \mid Y^A= y, Z^B= z\right],
\label{eq:costgroup}
\end{equation}

where \(d\) is a given distance function to choose on \(\mathcal{X} \times \mathcal{X}\).

In fact, the above situation cannot be solved in reality, since the distributions of \(X^A\), \(X^B\), \(Y^A\) and \(Z^A\) are never jointly observed. As a consequence, the following unbiased empirical estimators are used: \(\hat{\mu}^{X^A}_n\) of \({\mu}^{X^A}\) and \(\hat{\mu}^{X^B}_n\) of \({\mu}^{X^B}\). Because \(Y\) and \(Z\) are only available in \(A\) and \(B\) respectively, two distinct empirical estimators have to be defined:

\begin{equation}
    \begin{aligned}
        \hat{\mu}^{Y^A}_{n,y} & = & \frac{1}{n}\sum_{i\in A} ~\mathbf{1}_{\{Y_i = y\}},\: \forall y\in\mathcal{Y},\\
        \hat{\mu}^{Z^A}_{n,z} & = & \frac{1}{n}\sum_{j\in B} ~\mathbf{1}_{\{Z_j = z\}},\: \forall z\in \mathcal{Z},
    \end{aligned}
    \label{eq:estimatoroutcomes}
 \end{equation}

where \(\mathbf{1}_{\{Y = y\}}=1\) if \(Y=y\) and \(0\) otherwise. The \protect\hyperlink{optt}{\textbf{assumption 1}} gives: \({\mu}^{Z^A}={\mu}^{Z^B}\) from which we can conclude that \(\hat{\mu}^{Z^B}_{n,z}=\hat{\mu}^{Z^A}_{n,z}\).
Finally, denoting: \[\kappa_{n,y,z}\equiv \sum_{i\in A} \sum_{j\in B}~ \mathbf{1}_{\left\{Y_i=y,Z_j=z\right\}},\]

the number of pairs \((i,j)\in A\times B\) such that \(Y_i=y\) and \(Z_j=z\), the cost matrix \(c\) is estimated by:

\begin{equation}
\hat{c}_{n,y,z}=\left\{ \begin{array}{ll} \frac{1}{\kappa_{n,y,z}}\sum_{i\in A} \sum_{j\in B}~ \mathbf{1}_{\left\{Y_i=y,Z_j=z\right\}} \times d(X_i,X_j), & \: \forall y\in \mathcal{Y}, z\in\mathcal{Z}:\kappa_{n,y,z}\neq 0,\\
 0, & \:\forall y \in \mathcal{Y}, z \in \mathcal{Z}:\kappa_{n,y,z} = 0.
\end{array}\right.
\label{eq:cost}
\end{equation}

Plugging the values observed for these estimators in \eqref{eq:modeldim} yields to a linear programming model denoted:

\begin{equation}
\hat{\mathcal{P}}^0_n: \left\{
    \begin{aligned}
         \min\: & <\widehat{c}_n,\gamma>\\
                \text{s.t.}\:& \sum_{z\in Z} \gamma_{y,z} = \mu^{Y^A}_{n,y}, \:\forall y\in \mathcal{Y}, \\
                & \sum_{y\in Y} \gamma_{y,z} = \mu^{Z^A}_{n,z}, \:\forall z\in \mathcal{Z}, \\
                & \gamma_{y,z} \geq 0, \: \forall y\in \mathcal{Y}, \forall z\in \mathcal{Z}.
    \end{aligned}\right.
    \label{eq:outco}
\end{equation}

The solution \(\hat{\gamma}_n\) can then be interpreted as an estimator \(\hat{\mu}^{(Y^A,Z^A)}_n\) of the joint distribution of \(Y^A\) and \(Z^A\), \(\mu^{(Y^A,Z^A)}\). If this estimate is necessary, it is nevertheless insufficient here to provide the individual predictions on \(Z\) in \(A\). These predictions are done in a second step using a nearest neighbor algorithm from which we deduce an estimation of \(\mu^{Z^A\mid X^A=x,Y^A=y}\) (see Garès and Omer (2022) for details). In the remainder, the overall algorithm described in this section is referred to as \texttt{OUTCOME}. To improve the few drawbacks of this algorithm described in Garès et al. (2020), derived algorithms from \texttt{OUTCOME} have been developed (Garès and Omer 2022) and described in the following part.

\hypertarget{optimal-transportation-of-outcomes-and-covariates}{%
\subsubsection{Optimal transportation of outcomes and covariates}\label{optimal-transportation-of-outcomes-and-covariates}}

Using the same notations, Garès et al. (2020) propose to search for an optimal transportation map between the two joint distributions of \((X^A,Y^A)\) and \((X^A,Z^A)\) with marginals \(\mu^{(X^A,Y^A)}\) and \(\mu^{(X^A,Z^A)}\) respectively. Under Kantorovich's formulation in a discrete setting, they search for:
\[\gamma^*\in \operatorname{argmin}_{\gamma\in \mathcal{D}} <c,\gamma>,\]
where \(c\) is a given cost matrix and \(\mathcal{D}\) is the set of joint distributions with marginals \(\mu^{(X^A,Y^A)}\) and \(\mu^{(X^A,Z^A)}\). It is natural to see any element \(\gamma\in \mathcal{D}\) as the vector of joint probabilities \(\mathbb{P}((X^A=x,Y^A=y),(X^A=x',Z^A=z))\) for any \(x,x'\in\mathcal{X}^2\), \(y\in\mathcal{Y}\) and \(z\in\mathcal{Z}\). Since this probability nullifies for all \(x\neq x'\), \(\gamma\in \mathcal{D}\) is defined as a vector of \(\mathbb{R}^{\left\lvert X \right\rvert\times \left\lvert \mathcal{Y} \right\rvert \times\left\lvert\mathcal{Z} \right\rvert }\), where \(\gamma_{x,y,z}\) stands for an estimation of the joint probability \(\mathbb{P}(X^A=x,Y^A=y,Z^A=z)\).
These notations lead to the more detailed model:

\begin{equation}
    \mathcal{P}:\left\{
    \begin{aligned}
        \min\: & <c,\gamma> \\
                \text{s.t.}\:& \sum_{z\in Z} \gamma_{x,y,z} = \mu^{(X^A,Y^A)}_{x,y}, \:\forall x\in\mathcal{X}, \forall y\in \mathcal{Y},\\
                & \sum_{y\in Y} \gamma_{x,y,z} = \mu^{(X^A,Z^A)}_{x,z}, \:\forall x\in\mathcal{X}, \forall z\in \mathcal{Z},\\
                & \gamma_{x,y,z} \geq 0,  \:\forall x\in\mathcal{X}, \forall y\in \mathcal{Y}, \forall z\in \mathcal{Z}.
    \end{aligned}\right.
\label{eq:jointkanto}
\end{equation}

The above algorithm can be solved only if the marginals \(\mu^{(X^A,Y^A)}\) and \(\mu^{(X^A,Z^A)}\) are known, but, based on \protect\hyperlink{optt}{assumption 2}, unbiased estimators \(\hat{\mu}^{X^A,Y^A}_n\) and \(\hat{\mu}^{X^A,Z^A}_n\) can be built according to the \protect\hyperlink{optt}{previous subsection}. For the first one it gives:

\begin{equation}
     \begin{aligned}
         \hat{\mu}^{X^A,Y^A}_{n} & = & \frac{1}{n}\sum_{i\in A} ~\mathbf{1}_{\left\{Y_i = y,X_i = x\right\}}, \:\forall x\in\mathcal{X}, \: \forall y\in\mathcal{Y}.
     \end{aligned}
     \label{eq:estimatoutcov}
 \end{equation}

The cost matrix introduced in the \texttt{OUTCOME} algorithm is used \eqref{eq:costgroup} and estimated by \eqref{eq:cost}. Formally we can write:

\begin{equation} 
  c_{x,y,z} = c_{y,z}, \:\forall x\in\mathcal{X},\forall y\in\mathcal{Y},\forall z\in\mathcal{Z},
  \label{eq:truecostjoint}
\end{equation}

\noindent which does not depend on the value of \(x\).

Plugging the values observed for these estimators in \eqref{eq:jointkanto} yield a linear programming model denoted as \(\widehat{\mathcal{P}}_n\). In contrast to \textsc{outcome}, the algorithm that consists in solving \(\widehat{\mathcal{P}}_n\) to solve the recoding problem is referred to as \texttt{JOINT} in what follows.

An estimation of the distribution of \(Z^A\) given the values of \(X^A\) and \(Y^A\) is then given by:

\begin{equation}
 \tilde{\mu}^{Z^A\mid X^A=x,Y^A=y}_{n,z}=
 \left\{\begin{aligned}
 &\frac{\hat{\gamma}_{n,x,y,z}}{\hat{\mu}^{(X^A,Y^A)}_{n,x,y}}, & \:\forall x\in\mathcal{X},y\in\mathcal{Y},z\in\mathcal{Z}: \hat{\mu}^{(X^A,Y^A)}_{n,x,y}\neq 0,\\
 &0, & \:\forall x\in\mathcal{X},y\in\mathcal{Y},z\in\mathcal{Z}: \hat{\mu}^{(X^A,Y^A)}_{n,x,y}= 0.
 \end{aligned}\right.
\label{eq:condprob}
\end{equation}

and an individual prediction of \(Z^A\) is then deduced using the maximum a posterior rule:
\[
\widehat{z}_i^A= \operatorname{argmax}_{z\in\mathcal{Z}} \tilde{\mu}^{Z^A\mid X^A=x_i,Y^A=y_i}_{n,z}.
\]

Due to potential errors in the estimations of \(\mathcal{P}\), the constraints of \(\widehat{\mathcal{P}}_n\) may derive from the true values of the marginals of \(\mu^{(X^A,Y^A,Z^A)}\). To deal with this situation, small violations of the constraints of \(\widehat{\mathcal{P}}_n\) are allowed by enriching the initial algorithm as described in Garès and Omer (2022).

The equality constraints of \(\widehat{\mathcal{P}}_n\) are then relaxed as follows:

\begin{align}
            & \sum_{z\in Z} \gamma_{x,y,z} = \hat{\mu}^{(X^A,Y^A)}_{n,x,y} + e^{X,Y}_{x,y}, \:\forall x\in\mathcal{X}, \forall y\in \mathcal{Y}    \label{eq:relaxationxy}\\
            & \sum_{y\in Y} \gamma_{x,y,z} = \tilde{\mu}^{(X^A,Z^A)}_{n,x,z} + e^{X,Z}_{x,z}, \:\forall x\in\mathcal{X}, \forall z\in \mathcal{Z}    \label{eq:relaxationxz}\\
            & \sum_{x\in \mathcal{X},y\in \mathcal{Y}} e^{X,Y}_{x,y} = 0,\: \sum_{x\in \mathcal{X},z\in \mathcal{Z}} e^{X,Z}_{x,z} = 0     \label{eq:relaxsum}\\
            & -e^{X,Y,+}_{x,y}\leq e^{X,Y}_{x,y} \leq e^{X,Y,+}_{x,y}, \:\forall x\in\mathcal{X}, \forall y\in \mathcal{Y}    \label{eq:ctrelaxabsxy}\\
            & -e^{X,Z,+}_{x,z}\leq e^{X,Z}_{x,z} \leq e^{X,Z,+}_{x,z}, \:\forall x\in\mathcal{X}, \forall z\in \mathcal{Z}    \label{eq:ctrelaxabsxz}\\
            & \sum_{x\in \mathcal{X},y\in \mathcal{Y}} e^{X,Y,+}_{x,y} \leq \alpha_n,\: \sum_{x\in \mathcal{X},z\in \mathcal{Z}} e^{X,Z,+}_{x,z} \leq \alpha_n.  \label{eq:ctrelaxnorme}
\end{align}

This relaxation is possible by introducing extra-variables \(e^{X,Y,+}\) and \(e^{X,Z,+}\) as additional constraints \eqref{eq:ctrelaxabsxy}--\eqref{eq:ctrelaxabsxz}.
Garès and Omer (2022) suggests to consider \(\alpha_n:=\frac{\alpha}{\sqrt{n}}\) from \eqref{eq:ctrelaxnorme}, with a parameter \(\alpha\) to calibrate numerically but proposes also a default value fixed to \(0.4\).

A regularization term \(\lambda\) given by \((\frac{\pi_{x,y,z}}{\hat{\mu}^{X^A}_{n,x}})_{x\in\mathcal{X},y\in\mathcal{Y},z\in\mathcal{Z}}\) can also be added to improve regularity in the variations of the conditional distribution \(\mu^{Y^A,Z^A\mid X^A=x}\) with respect to \(x\). The corresponding regularized algorithm is:

\begin{equation}
 \widehat{\mathcal{P}}^R_n:    \left\{
     \begin{aligned}
          \min\: & <\widehat{c}_n,\gamma> + \lambda \sum_{(x_i,x_j)\in E_\mathcal{X}}      w_{i,j}\sum_{y\in\mathcal{Y},z\in\mathcal{Z}} r^+_{i,j,y,z}\\
            &\text{s.t.}\: \text{constraints }  \text{(11)--(16)} \\
            & \frac{\gamma_{x_i,y,z}}{\hat{\mu}^{X^A}_{n,x_i}}-\frac{\gamma_{x_j,y,z}}{\hat{\mu}^{X^A}_{n,x_j}} \leq r^+_{i,j,y,z}, \:\forall \{x_i,x_j\}\in E_\mathcal{X}, y\in\mathcal{Y}, z\in \mathcal{Z}\\
            &  \frac{\gamma_{x_i,y,z}}{\hat{\mu}^{X^A}_{n,x_i}}-\frac{\gamma_{x_j,y,z}}{\hat{\mu}^{X^A}_{n,x_j}} \geq -r^+_{i,j,y,z}, \:\forall \{x_i,x_j\}\in E_\mathcal{X}, y\in\mathcal{Y}, z\in \mathcal{Z}\\
             & \gamma_{x,y,z} \geq 0,  \:\forall x\in\mathcal{X}, \forall y\in \mathcal{Y}, \forall z\in \mathcal{Z}.
     \end{aligned}\right.
 \label{eq:estimatemodelregularization}
 \end{equation}

The constant \(\lambda\in\mathbb{R}^+\) is a regularization parameter to be calibrated numerically (\(0.1\) can be considered as default value) and \(E_\mathcal{X}\subset \mathcal{X}^2\) includes the pairs of elements of X defined as neighbors: \(\left\{x_i, x_j\right\}\in E_\mathcal{X}\) if \(x_j\) is among the k nearest neighbors of \(x_i\) for some parameter \(k \geq 1\). The method that computes a solution to the recoding problem with regularization and relaxation is called \texttt{R-JOINT}.

In a same way, a relaxation of the \protect\hyperlink{optt}{\textbf{assumption 1}} is also proposed and added to the \texttt{OUTCOME} algorithm: this resulting method is denoted \texttt{R-OUTCOME} and is related to the following program:

\begin{equation}
\widehat{\mathcal{P}}^{0-R}_n:    \left\{
 \begin{aligned}
      \min\: & <\widehat{c}_n,\gamma>\\
         & \sum_{z\in \mathcal{Z}} \gamma_{y,z} = \hat{\mu}^{Y^A}_{n,y} + e^{Y}_{y}, \:\forall y\in \mathcal{Y}\\
         & \sum_{y\in \mathcal{Y}} \gamma_{y,z} = \tilde{\mu}^{Z^A}_{n,z} + e^{Z}_{z}, \:\forall z\in \mathcal{Z}\\
         & \sum_{y\in \mathcal{Y}} e^{Y}_{y} = 0,\: \sum_{z\in \mathcal{Z}} e^{Z}_z = 0\\
         & -e^{Y,+}_{y}\leq e^{Y}_{y} \leq e^{Y,+}_{y}, \:\forall y\in \mathcal{Y}\\
         & -e^{Z,+}_{z}\leq e^{Z}_{z} \leq e^{Z,+}_{z}, \:\forall z\in \mathcal{Z}\\
         & \sum_{y\in \mathcal{Y}} e^{Y,+}_{y} \leq \alpha_n,\: \sum_{z\in \mathcal{Z}} e^{Z,+}_{z} \leq \alpha_n\\
         & \gamma_{y,z} \geq 0,  \:\forall y\in \mathcal{Y}, \forall z\in \mathcal{Z}.
 \end{aligned}\right.
\label{eq:modeloutcomerelaxed}
\end{equation}

Note that algorithms \texttt{JOINT} and \texttt{R-JOINT} do not require \protect\hyperlink{optt}{\textbf{assumption 1}}. The relaxation in \texttt{R-OUTCOME} alleviates its dependence to the satisfaction of this assumption. However, algorithms \texttt{JOINT} and \texttt{R-JOINT} require that \(X\) is a set of discrete variables (factors ordered or not are obviously allowed) while the absence of \(X\) in the linear algorithms \texttt{OUTCOME} and \texttt{R-OUTCOME} allow \(X\) to be a set of discrete and/or continuous variables. In this case, the nature of the variables \(X\) need to be considered when choosing the distance \(d\).

\hypertarget{package-installation-and-description}{%
\section{Package installation and description}\label{package-installation-and-description}}

\hypertarget{installation}{%
\subsubsection{Installation}\label{installation}}

The \CRANpkg{OTrecod} package can be installed from the Comprehensive R Archive Network (CRAN) by using the following template:

\begin{verbatim}
install.packages("OTrecod")
\end{verbatim}

The \href{https://github.com/otrecoding/OTrecod}{development version} of \pkg{OTrecod} is also available and can be directly installed from \textbf{GitHub} by loading the \CRANpkg{devtools} (Wickham et al. 2022) package (\texttt{R\textgreater{}\ install\_github("otrecoding/OTrecod")}).

\hypertarget{main-functions}{%
\subsubsection{Main functions}\label{main-functions}}

The two types of optimal transportation algorithms previously introduced (\texttt{OUTCOME} and \texttt{JOINT}) and their respective enrichments (\texttt{R-OUTCOME} and \texttt{R-JOINT}) are available in the \pkg{OTrecod} package via two core functions denoted \texttt{OT\_outcome} and \texttt{OT\_joint}. Details about their implementations in \texttt{R} are described in the following section. In this package, these algorithms of recoding are seen as fundamental steps of data fusion projects that also require often adapted preparation and validation functions. In this way, the Table \ref{tab:tab2} introduces the main functions proposed by \pkg{OTrecod} to handle a data fusion project.

\begin{table}[h]
  \centering
  \begin{tabular}{lp{8cm}}
    \toprule
    \strong{\code{R} Function}  & \strong{Description} \\
    \hline
        \strong{Pre-process functions}  & \\
        \code{merge\_dbs} & Harmonization of the data sources \\
        \code{select\_pred} & Selection of matching variables  \\   
        \strong{Functions of data fusion} & \\
        \code{OT\_outcome} & Data fusion with OT theory using the \code{OUTCOME} or \code{R-OUTCOME} algorithms.\\
          \code{OT\_joint} & Data fusion with OT theory using the \code{JOINT} or \code{R-JOINT} algorithms.\\
        \strong{Post-process function}  & \\
          \code{verif\_OT} & Quality assessment of the data fusion \\
        \bottomrule
  \end{tabular}
  \caption{A brief description of the main functions of OTrecod}
\label{tab:tab2}
\end{table}

All the intermediate functions integrated in the \texttt{OT\_outcome} and \texttt{OT\_joint} functions (\texttt{proxim\_dist}, \texttt{avg\_dist\_closest}, \texttt{indiv\_grp\_closest}, \texttt{indiv\_grp\_optimal}), and their related documentations, are all included and usable separately in the package. They have been kept available for users to ensure a great flexibility as other interesting functions like \texttt{power\_set} that returns the power set of a set. This function did not exist on \texttt{R} until now and could be of interest for specialists of algebra. These functions are not described here but detailed in the related \href{https://cran.r-project.org//web//packages//OTrecod//OTrecod.pdf}{pdf manual} of the package.

\hypertarget{functionalities-overview}{%
\section{Functionalities overview}\label{functionalities-overview}}

\hypertarget{expected-structure-of-the-input-databases-as-arguments}{%
\subsubsection{Expected structure of the input databases as arguments}\label{expected-structure-of-the-input-databases-as-arguments}}

The functions of recoding \texttt{OT\_outcome} and \texttt{OT\_joint} require a specific structure of data.frame as input arguments. Described in Table \ref{tab:tab3}, it must be the result of two overlayed databases made of at least four variables:

\begin{itemize}
\tightlist
\item
  A first variable, discrete or categorical, corresponding to the database identifier, stored in factor or not, but with only two classes or levels (for example: \emph{A} and \emph{B}, \emph{1} and \emph{2} or otherwise).
\item
  The target variable of the first database (or top database) denoted \emph{Y} for example, whose values related to the second database are missing. This variable can be discrete or categorical stored in factor, ordered factor or not.
\item
  In the same way, the target variable of the second database (or below database) denoted \emph{Z} for example, whose values related to the first database are missing.
\item
  At least one shared variable (defined as a variable with the same label and the same encoding in the two distinct data sources). The type of shared variables can be continuous, categorical stored in factor or not, complete or not. Nevertheless, few constraints must be noticed related to this question. First, in a critical situation where only one shared variable exists, this latter cannot be incomplete. Second, continuous shared variables are actually not allowed in the current version of the function \texttt{OT\_joint}, therefore, these variables must be transformed beforehand.
\end{itemize}

\begin{table}[h]
 \begin{center}
  \begin{tabular}{cccccc}
    \toprule
    DB & Y & Z & $X_1$ & $X_2$ & $X_3$ \\
    \hline
   1 & (600-800] & <NA> & M & Yes &  50 \\
   1 & (600-800] & <NA> & M & No  &  32 \\
   1 & [200-600] & <NA> & W & No  &  31 \\
   2 &  <NA>     &  G1  & M & No  &  47 \\
   2 &  <NA>     &  G3  & W & Yes &  43 \\
   2 &  <NA>     &  G2  & W & No  &  23 \\
   2 &  <NA>     &  G4  & M & Yes &  22 \\
   2 &  <NA>     &  G2  & W & Yes &  47 \\
   \bottomrule
  \end{tabular} \end{center}\caption{Example of expected structure for two databases $1$ and $2$  with three shared variables $X_1$, $X_2$, $X_3$}
\label{tab:tab3}
\end{table}

As additional examples, users can also refer to the databases \texttt{simu\_data} and \texttt{tab\_test} provided in the package with expected structures. Note that class objects are not expected here as input arguments of these functions to allow users to freely work with or without the use of the pre-process functions provided in the package.

\hypertarget{choice-of-solver}{%
\subsubsection{Choice of solver}\label{choice-of-solver}}

The package \pkg{OTrecod} uses the \texttt{ROI} optimization infrastructure (Theußl, Schwendinger, and Hornik 2017) to solve the optimization problems related to the \texttt{OUTCOME} and \texttt{JOINT} algorithms. The solver GLPK (The GNU Linear Programming Kit (Makhorin 2011)) is the default solver actually integrated in the \texttt{OT\_outcome} and \texttt{OT\_joint} functions for handling linear problems with linear constraints. The ROI infrastructure makes easy for users to switch solvers for comparisons. In many situations, some of them can noticeably reduce the running time of the functions.

For example, the solver Clp (Forrest, Nuez, and Lougee-Heimer 2004) for COINT-OR Linear Programming, known to be particularly convenient in linear and quadratic situations, can be easily installed by users via the related plug-in available in \texttt{ROI} (searchable with the instruction \texttt{ROI\_available\_solvers()}) and following the few instructions detailed in (Theußl, Schwendinger, and Hornik 2020) or via the \href{https://roi.r-forge.r-project.org/installation.html}{dedicated website}.

\hypertarget{an-illustrative-example}{%
\subsubsection{An illustrative example}\label{an-illustrative-example}}

In California (United States), from 1999 to 2018, the Public Schools Accountability Act (PSAA) imposed on its California Department of Education (CDE) to provide annually the results of an Academic Performance Index (\href{https://www.ed-data.org/article/Understanding-the-Academic-Performance-Index-(API)}{API}) which established a ranking of the best public schools of the state.

This numeric score, indicator of school's performance levels, could vary from 200 to 1000 and the performance objective to reach for each school was 800. Information related to the 418 schools (identified by \texttt{cds}) of Nevada (County 29) and to the 362 schools of San Benito (County 35), was respectively collected in two databases, \texttt{api29} and \texttt{api35}, available in the package. The distributions of all the variables in the two databases are provided by following the \texttt{R} commands:

\begin{verbatim}
library(OTrecod)
data(api29); data(api35)
summary(api29) #--------------------------------------------------
\end{verbatim}

\begin{verbatim}
#>      cds                 apicl_2000  stype    awards       acs.core    
#>  Length:418         [200-600] : 93   E:300   No  : 98   Min.   :16.00  
#>  Class :character   (600-800] :180   M: 68   Yes :303   1st Qu.:26.00  
#>  Mode  :character   (800-1000]:145   H: 50   NA's: 17   Median :30.00  
#>                                                         Mean   :31.97  
#>                                                         3rd Qu.:39.00  
#>                                                         Max.   :50.00  
#>     api.stu         acs.k3.20   grad.sch         ell          mobility  
#>  Min.   : 108.0   <=20   :190   0   : 86   [0-10]  :153   [0-20]  :362  
#>  1st Qu.: 336.2   >20    :108   1-10:180   (10-30] : 83   (20-100]: 56  
#>  Median : 447.5   unknown:120   >10 :152   (30-50] : 60                 
#>  Mean   : 577.8                            (50-100]: 93                 
#>  3rd Qu.: 641.5                            NA's    : 29                 
#>  Max.   :2352.0                                                         
#>       meals     full   
#>  [0-25]  :200   1: 85  
#>  (25-50] : 56   2:333  
#>  (50-75] :100          
#>  (75-100]: 62          
#>                        
#> 
\end{verbatim}

\begin{verbatim}
summary(api35) #--------------------------------------------------
\end{verbatim}

\begin{verbatim}
#>      cds            apicl_1999 stype    awards       acs.core    
#>  Length:362         G1:91      E:257   No  :111   Min.   :16.00  
#>  Class :character   G2:90      M: 67   Yes :237   1st Qu.:25.00  
#>  Mode  :character   G3:90      H: 38   NA's: 14   Median :30.00  
#>                     G4:91                         Mean   :31.81  
#>                                                   3rd Qu.:39.00  
#>                                                   Max.   :50.00  
#>     api.stu         acs.k3.20   grad.sch         ell      mobility
#>  Min.   : 102.0   <=20   :227   0   : 50   [0-10]  :164   1:213   
#>  1st Qu.: 363.2   >20    : 30   1-10:241   (10-30] : 99   2:149   
#>  Median : 460.0   unknown:105   >10 : 71   (30-50] : 64           
#>  Mean   : 577.2                            (50-100]: 10           
#>  3rd Qu.: 624.0                            NA's    : 25           
#>  Max.   :2460.0                                                   
#>       meals     full   
#>  [0-25]  : 77   1:244  
#>  (25-50] : 92   2:118  
#>  (50-75] :122          
#>  (75-100]: 71          
#>                        
#> 
\end{verbatim}

The two databases seem to share a majority of variables (same labels, same encodings) of different types, therefore inferential statistics could be ideally considered by combining all the information of the two databases to study the effects of social factors on the results of the API score in 2000.

Nevertheless, while this target variable called \texttt{apicl\_2000} is correctly stored in the database \texttt{api29} and encoded as a three levels ordered factors clearly defined: \texttt{{[}200-600{]}}, \texttt{(600-800{]}} and \texttt{(800-1000{]}}, the only information related to the API score in the database \texttt{api35} is the variable \texttt{apicl\_1999} for the API score collected in 1999, encoded in four unknown balanced classes (\texttt{G1}, \texttt{G2}, \texttt{G3} and \texttt{G4}). As no school is common to the two counties, we easily deduce that these two variables have never been jointly observed.

\textbf{By choosing these two variables as outcomes (called also target variables), the objective of the following examples consists in creating a synthetic database where the missing information related to the API score in 2000 is fully completed in \texttt{api35} by illustrating the use of the main functions of the package}.

\hypertarget{harmonization-of-two-datasources-using-merge_dbs}{%
\subsubsection{\texorpdfstring{Harmonization of two datasources using \texttt{merge\_dbs}}{Harmonization of two datasources using merge\_dbs}}\label{harmonization-of-two-datasources-using-merge_dbs}}

The function \texttt{merge\_dbs} is an optional pre-process function dedicated to data fusion projects that merges two
raw databases by detecting possible discrepancies among the respective sets of variables from one database to
another. The current discrepancy situations detected by the function follow specific rules described below:

\begin{itemize}
\tightlist
\item
  any variable (other than a target one) whose label (or name) is not common to the two data sources is automatically excluded. By default, the remaining variables are denoted shared variables.
\item
  among the subset of shared variables, any variable stored in a different format (or type) from one datasource to another will be automatically discarded from the subset previously generated and its label saved in an output object called \texttt{REMOVE1}.
\item
  among the remaining subset of shared variables, a factor variable (ordered or not) stored with different levels or number of levels from one data source to another will be automatically discarded from the subset and its label saved in an output object called \texttt{REMOVE2}.
\end{itemize}

These situations sometimes require reconciliation actions which are not supported by the actual version of the \texttt{merge\_dbs} function. Therefore, when reconciliation actions are required by the user, they will have to be treated a posteriori and outside the function.

Applied to our example and according to the introduced rules, the first step of our data fusion project consists in studying the coherence between the variables of the databases \texttt{api29} and \texttt{api35} via the following \texttt{R} code:

\begin{verbatim}
step1 = merge_dbs(DB1 = api29, DB2 = api35,
            NAME_Y = "apicl_2000", NAME_Z = "apicl_1999",
            row_ID1 = 1, row_ID2 = 1,  
            ordinal_DB1 = c(2:3, 8:12), 
            ordinal_DB2 = c(2:3, 8:12))
\end{verbatim}

As entry, the raw databases must be declared separately in the \texttt{DB1} and \texttt{DB2} arguments and the name of
the related target variables of each database must be specified via the \texttt{NAME\_Y} and \texttt{NAME\_Z} for \texttt{DB1} and \texttt{DB2} respectively. In presence of row identifiers, the respective column indexes of each database must be set in the argument \texttt{row\_ID1} and \texttt{row\_ID2}. The arguments \texttt{ordinal\_DB1} and \texttt{ordinal\_DB2} list the related
column indexes of all the variables defined as ordinal in the two databases (including also the indexes of the
target variables if necessary). Here, \texttt{apicl\_2000} is clearly an ordinal variable, and, by default, we suppose
that the unknown encoding related to \texttt{apicl\_1999} is also ordinal: the corresponding indexes (2 and 3) are so
added in these two arguments.

After running, the function informs users that no row was dropped from the databases during the merging
because each target variable is fully completed in the two databases. Nine potential predictors are kept while
only one variable is discarded because of discrepancies between the databases: its identity is consequently stored in output and informs user about the nature of the problem: the \texttt{mobility} factor has different levels from one database to the other.

\begin{verbatim}
summary(step1)
\end{verbatim}

\begin{verbatim}
#>               Length Class      Mode     
#> DB_READY      12     data.frame list     
#> ID1_drop       0     -none-     character
#> ID2_drop       0     -none-     character
#> Y_LEVELS       3     -none-     character
#> Z_LEVELS       4     -none-     character
#> REMOVE1        0     -none-     NULL     
#> REMOVE2        1     -none-     character
#> REMAINING_VAR  9     -none-     character
#> IMPUTE_TYPE    1     -none-     character
#> DB1_raw       12     data.frame list     
#> DB2_raw       12     data.frame list     
#> SEED           1     -none-     numeric
\end{verbatim}

\begin{verbatim}
step1$REMOVE1   # List of removed variables because of type's problem
\end{verbatim}

\begin{verbatim}
#> NULL
\end{verbatim}

\begin{verbatim}
step1$REMOVE2   # List of removed factors because of levels' problem
\end{verbatim}

\begin{verbatim}
#> [1] "mobility"
\end{verbatim}

\begin{verbatim}
levels(api29$mobility)  # Verification
\end{verbatim}

\begin{verbatim}
#> [1] "[0-20]"   "(20-100]"
\end{verbatim}

\begin{verbatim}
levels(api29$mobility); levels(api35$mobility)
\end{verbatim}

\begin{verbatim}
#> [1] "[0-20]"   "(20-100]"
\end{verbatim}

\begin{verbatim}
#> [1] "1" "2"
\end{verbatim}

\begin{verbatim}
step1$REMAINING_VAR
\end{verbatim}

\begin{verbatim}
#> [1] "acs.core"  "acs.k3.20" "api.stu"   "awards"    "ell"       "full"     
#> [7] "grad.sch"  "meals"     "stype"
\end{verbatim}

The function returns a list object and notably \texttt{DB\_READY}, a data.frame whose structure corresponds to the expected structure introduced in the previous subsection: a unique database, here result of the superimposition of \texttt{api29} on \texttt{api35} where the first column (\texttt{DB}) corresponds to the database identifier (1 for \texttt{api29} and 2 for \texttt{api35}), the second and third columns (\texttt{Y}, \texttt{Z} respectively) corresponds to the target variables of the two databases. Missing values are automatically assigned by the function to the unknown part of each target variable: in \texttt{Y} when the identifier equals to 2, in \texttt{Z} when the identifier equals to 1. The subset of shared variables whose information is homogeneous between the databases are now stored from the fourth column to the last one. Their identities are available in the output object called \texttt{REMAINING\_VAR}.

The \texttt{merge\_dbs} function can handle incomplete shared variables by using the \texttt{impute} argument. This
option allows to keep the missing information unchanged (the choice by default, and also the case in this
example), to work with complete cases only, to do fast multivariate imputations by chained equations approach
(the function integrates the main functionalities of the \texttt{mice} function of the \CRANpkg{mice} package), or to impute data using the dimensionality reduction method introduced in the \CRANpkg{missMDA} package (see the \href{https://cran.r-project.org//web//packages//OTrecod//OTrecod.pdf}{pdf manual} for details).

\hypertarget{selection-of-matching-variables-using-select_pred}{%
\subsubsection{\texorpdfstring{Selection of matching variables using \texttt{select\_pred}}{Selection of matching variables using select\_pred}}\label{selection-of-matching-variables-using-select_pred}}

In data fusion projects, a selection of shared variables (also called matching variables) appears as an essential step for two main reasons. First, the proportion of shared variables \emph{X} between the two databases can be important (much higher than three variables) and keeping a pointless part of variability between variables could strongly reduce the quality of the fusion. Second, this selection greatly influences the quality of the predictions regardless of the matching technique which is chosen a posteriori (Adamek 1994). The specific context of data fusion is subject to the following constraints:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Y}, \emph{Z} and \emph{X} are never jointly observed in database \emph{A} or \emph{B}, so the relationships between \emph{Y} and \emph{X}, and between \emph{Z} and \emph{X} must be investigated separately.
\item
  matching variables need to be good predictors of both target variables (outcomes) \emph{Y} and \emph{Z} (Cohen 1991), in the sense that they explain relevant parts of variability of the targets.
\end{enumerate}

These particularities suppose that the investigations have to be done separately but in the same manner in
both databases. In this way, a predictor which appears at the same time as highly correlated to \emph{Y} and \emph{Z} will be automatically selected for the fusion. On the contrary, predictors whose effects on \emph{Y} and \emph{Z} seem not obvious will be discarded. Additional recommended rules also emerge from literature:

\begin{itemize}
\tightlist
\item
  concerning the subset of variables that would predict only one of the two targets \emph{Y} or \emph{Z}, D'Orazio, Di Zio, and Scanu (2006) suggests a moderate parsimonious selection remaining too many predictors could complicate the fusion procedure.
\item
  Cibella (2010) and Scanu (2010) suggest to select quality predictors with no error and just a small amount of missing data.
\end{itemize}

The function of the package dedicated to this task is \texttt{select\_pred}. From the \texttt{DB\_READY} database
generated in the previous subsection, studying the outputs related to each database and produced by the following \texttt{R} commands assist users in selecting the best predictors:

\begin{verbatim}
# For the dataset api29 --------------
step2a = select_pred(step1$DB_READY,
                     Y = "Y", Z = "Z", ID = 1, OUT = "Y",
                     quanti  = c(4,6), nominal = c(1,5,7), 
                     ordinal = c(2,3,8:12), thresh_cat = 0.50, 
                     thresh_num = 0.70, RF_SEED = 3011)

# For the dataset api35 --------------
step2b = select_pred(step1$DB_READY,
                     Y = "Y", Z = "Z", ID = 1, OUT = "Z",
                     quanti  = c(4,6), nominal = c(1,5,7), 
                     ordinal = c(2,3,8:12), thresh_cat = 0.50, 
                     thresh_num = 0.70, RF_SEED = 3011)
\end{verbatim}

The \texttt{quanti}, \texttt{nominal}, and \texttt{ordinal} arguments requires vectors of column indexes related to the type
of each variable of the input database. The \texttt{ID} argument specifies the column index of the database identifier. \texttt{Y} and \texttt{Z} expected the respective names of the target variables while \texttt{OUT} provides the target variable to predict (\texttt{Y} or \texttt{Z}).

To detect the subset of best predictors, \texttt{select\_pred} studies the standard pairwise associations between
each shared variable and the outcomes \texttt{Y} and \texttt{Z}, taken separately (by only varying the argument \texttt{OUT}), according to its type: for numeric and/or ordered factor variables, \texttt{select\_pred} calculates the associations using rank correlation coefficients (Spearman) while the Cramer's V criterion (Bergsma 2013) is used for categorical variables and not ordered factors. The related ranking tables of top scoring predictors are available in two distinct output objects: \texttt{cor\_OUTC\_num} and \texttt{cor\_OUTC\_cat}. In our example, the corresponding results, for each database, are:

\begin{verbatim}
### ASSOCIATIONS BETWEEN TARGET VARIABLES AND SHARED VARIABLES

## Results for the api29 dataset -----

step2a$cor_OUTC_num   # Y versus numeric or ordinal predictors
\end{verbatim}

\begin{verbatim}
#>   name1    name2 RANK_COR pv_COR_test   N
#> 7     Y    meals  -0.8030      0.0000 418
#> 4     Y      ell  -0.7514      0.0000 389
#> 5     Y     full   0.3919      0.0000 418
#> 6     Y grad.sch   0.3346      0.0000 418
#> 3     Y  api.stu  -0.1520      0.0018 418
#> 8     Y    stype  -0.1520      0.0018 418
#> 2     Y acs.core  -0.0556      0.2566 418
\end{verbatim}

\begin{verbatim}
step2a$vcrm_OUTC_cat  # Y versus nominal or ordinal predictors
\end{verbatim}

\begin{verbatim}
#>   name1     name2 V_Cramer CorrV_Cramer   N
#> 7     Y     meals   0.6871       0.6835 418
#> 4     Y       ell   0.6015       0.5966 389
#> 5     Y      full   0.4508       0.4459 418
#> 6     Y  grad.sch   0.3869       0.3816 418
#> 3     Y    awards   0.2188       0.2073 401
#> 2     Y acs.k3.20   0.1514       0.1349 418
#> 8     Y     stype   0.1370       0.1185 418
\end{verbatim}

\begin{verbatim}
## Results for the api35 dataset -----

step2b$cor_OUTC_num   # Z versus numeric or ordinal predictors
\end{verbatim}

\begin{verbatim}
#>   name1    name2 RANK_COR pv_COR_test   N
#> 4     Z      ell  -0.7511      0.0000 337
#> 7     Z    meals  -0.7291      0.0000 362
#> 6     Z grad.sch   0.6229      0.0000 362
#> 5     Z     full   0.3997      0.0000 362
#> 3     Z  api.stu  -0.0563      0.2851 362
#> 8     Z    stype   0.0359      0.4959 362
#> 2     Z acs.core   0.0119      0.8219 362
\end{verbatim}

\begin{verbatim}
step2b$vcrm_OUTC_cat  # Z versus nominal or ordinal predictors
\end{verbatim}

\begin{verbatim}
#>   name1     name2 V_Cramer CorrV_Cramer   N
#> 7     Z     meals   0.5181       0.5121 362
#> 6     Z  grad.sch   0.5131       0.5063 362
#> 4     Z       ell   0.4775       0.4701 337
#> 5     Z      full   0.4033       0.3934 362
#> 3     Z    awards   0.2040       0.1818 348
#> 8     Z     stype   0.1320       0.0957 362
#> 2     Z acs.k3.20   0.1223       0.0817 362
\end{verbatim}

The two first tables related to \texttt{api29} highlights the strong associations between \texttt{Y} and the variables \texttt{meals}, \texttt{ell}, \texttt{full} and \texttt{grad.sch} in this order of importance, while the summary tables related to \texttt{api35} highlights the strong associations between \texttt{Z} and the variables \texttt{meals}, \texttt{grad.sch}, \texttt{ell} and \texttt{full}.

It is often not unusual to observe that one or more predictors are in fact linear combinations of others. In
supervised learning areas, these risks of collinearity increase with the number of predictors, and must be detected beforehand to keep only the most parsimonious subset of predictors for fusion. To avoid collinearity situations, the result of a \emph{Farrar and Glauber (FG) test} is provided (Farrar and Glauber 1967). This test is based on the determinant of the rank correlation matrix of the shared variables D (Spearman) and the corresponding test statistic is given by:

\[
S_{FG} = - \left(n-1-\frac{1}{6} (2k+5)\times \ln(\det(D))\right)
\]

where \(n\) is the number of rows and \(k\) is the number of covariates. The null hypothesis supposes that
\(S_{FG}\) follows a chi square distribution with \(k(k-1)/2\) degrees of freedom, and its acceptation indicates an absence of collinearity. In presence of a large number of numeric covariates and/or ordered factors, the approximate Farrar-Glauber test, based on the normal approximation of the null distribution (Kotz, Balakrishnan, and Johnson 2000) can be more adapted and the statistic test becomes:

\[
\sqrt{2S_{FG}} - \sqrt{2k-1}
\]
Users will note that these two tests can often appear highly sensitive in the sense that they tend to easily conclude in favor of multicollinearity. Thus, it is suggested to consider these results as indicators of collinearity between predictors rather than an essential condition of acceptability. The results related to this test are stored in the \texttt{FG\_test} object of the \texttt{select\_pred} output.

In presence of collinearity, \texttt{select\_pred} tests the pairwise associations between all the potential predictors according to their types (Spearman or Cramér's V). The \texttt{thres\_num} argument fixed the threshold beyond which two ranked predictors are considered as redundant while \texttt{thres\_cat} fixed the threshold of acceptability for the Cramér's V criterion in the subgroup of factors. In output, the results stored in the \texttt{collinear\_PB} object permit to identify the corresponding variables. In our example, we observe:

\begin{verbatim}
### DETECTION OF REDUNDANT PREDICTORS

## Results for the api29 dataset -----

## Results of the Farrar-Glauber test 
step2a$FG_test       
\end{verbatim}

\begin{verbatim}
#>           DET_X      pv_FG_test pv_FG_test_appr 
#>      0.04913067      0.00000000      0.00000000
\end{verbatim}

\begin{verbatim}
## Identity of the redundant predictors

step2a$collinear_PB  
\end{verbatim}

\begin{verbatim}
#> $VCRAM
#>        name1 name2 V_Cramer CorrV_Cramer   N
#> 71 acs.k3.20 stype   0.6988       0.6971 418
#> 43       ell meals   0.6696       0.6664 389
#> 34      full meals   0.5215       0.5152 418
#> 
#> $SPEARM
#>      name1 name2 RANK_COR pv_COR_test   N
#> 43     ell meals   0.9047           0 389
#> 62 api.stu stype   0.7069           0 418
\end{verbatim}

\begin{verbatim}
#### Results for the api35 dataset -----

step2b$FG_test    # Significant result
\end{verbatim}

\begin{verbatim}
#>           DET_X      pv_FG_test pv_FG_test_appr 
#>       0.1479339       0.0000000       0.0000000
\end{verbatim}

\begin{verbatim}
step2b$collinear_PB
\end{verbatim}

\begin{verbatim}
#> $VCRAM
#>        name1 name2 V_Cramer CorrV_Cramer   N
#> 71 acs.k3.20 stype   0.6977       0.6957 362
#> 
#> $SPEARM
#> [1] name1       name2       RANK_COR    pv_COR_test N          
#> <0 rows> (or 0-length row.names)
\end{verbatim}

The FG test warns the user against the risks of collinearity between predictors, and the function notably detects strong collinearities between the variables \texttt{meals}, \texttt{ell}, \texttt{full} in the \texttt{api29} (less strong trends in \texttt{api35}): an information that have to be taken into account during the selection. The part of predictors finally kept for the data fusion must be small to improve its quality. When the initial number of shared variables is not too important as in this example, choosing the best candidates between groups of redundant predictors can be made manually by selecting highest ranked predictors in the summary tables previously described. In this way, the variable \texttt{meals} could be preferred to \texttt{ell}, and \texttt{full}, while the variable \texttt{stype} could be dropped. Consequently, a possible final list of predictors could be only composed of the variables \texttt{meals} and \texttt{grad.sch}.

When the number of predictors is important, or when users prefer that an automatic process performs the
variable selection, a random forest procedure can also be used via the \texttt{select\_pred} function. Random forest approaches (Leo Breiman 2001) are here particularly convenient (Grajski et al. 1986) for multiple reasons: it works fine when the number of variables exceeds the number of rows, whatever the types of covariates, it allows to deal with non linearity, to consider correlated predictors, ordered or not ordered outcomes and to rank good candidate predictors through an inbuilt criterion: the variable importance measure.

In few words, random forest processes aggregates many CART models (L. Breiman et al. 1984) with
\texttt{RF\_ntree} bootstrap samples from the raw data source and averaging accuracies from each model permits to reduce the related variances and also the errors of prediction. A standard random forest process provides two distinct measures of importance of each variable for the prediction of the outcome, the Gini importance criterion and the permutation importance criterion, which depends on the appearance frequency of the predictor but also on its place taken up in each tree. For more details about random forest theory, user can consult Leo Breiman (2001) and/or the \href{https://cran.r-project.org//web//packages//randomForest//randomForest.pdf}{pdf manual} of the \CRANpkg{randomForest} (Liaw and Wiener 2002) package.

Strobl, Hothorn, and Zeileis (2009) suggests that the permutation importance criterion, which works with permuted samples (subsampling without replacements) instead of bootstrap ones, is particularly convenient with uncorrelated predictors, but must be replaced by a conditional permutation measurement in presence of strong correlations. \texttt{select\_pred} provides these assessments by integrating \href{https://cran.r-project.org/web/packages/party/party.pdf}{the main functionalities} of the \texttt{cforest} and \texttt{varimp} functions of the package \CRANpkg{party}(Hothorn, Hornik, and Zeileis 2006; Zeileis and Hothorn 2008; Hothorn et al. 2005; Strobl et al. 2007, 2008). However, these measurements must be used with caution, by accounting the following constraints:

\begin{itemize}
\tightlist
\item
  the Gini importance criterion can produce bias in favor of continuous variables and variables with many categories. This criterion is thus not available in the function.
\item
  the permutation importance criterion can overestimate the importance of highly correlated predictors and therefore redundant predictors will be discarded beforehand using the first steps of the process integrated in the function.
\end{itemize}

\begin{table}[h]
  \centering
  \begin{tabular}{clll}
    \toprule
        Possible scenarios
        & \multicolumn{1}{p{2.4cm}}{\centering Correlation  \\ between predictors}
        & \multicolumn{1}{p{2.5cm}}{\centering State of the \\ RF\_condi argument}
        & \multicolumn{1}{p{2.5cm}}{\centering Incomplete \\ information}\\
        \hline
        Same type predictors      & \multicolumn{1}{p{2.4cm}}{\centering NO}  & \multicolumn{1}{p{2.5cm}}{\centering FALSE} & \multicolumn{1}{p{2.5cm}}{\centering Allowed} \\
        Same type predictors      & \multicolumn{1}{p{2.4cm}}{\centering YES} & \multicolumn{1}{p{2.5cm}}{\centering TRUE}  & \multicolumn{1}{p{2.5cm}}{\centering Not allowed} \\
        Different type predictors & \multicolumn{1}{p{2.4cm}}{\centering NO}  & \multicolumn{1}{p{2.5cm}}{\centering FALSE} & \multicolumn{1}{p{2.5cm}}{\centering Allowed} \\
        Different type predictors & \multicolumn{1}{p{2.4cm}}{\centering YES} & \multicolumn{1}{p{2.5cm}}{\centering TRUE}  & \multicolumn{1}{p{2.5cm}}{\centering Not Allowed} \\
        \bottomrule
    \end{tabular}
   \caption{Completing the \code{RF\_condi} argument according to predictors}
\label{tab:tab4}
\end{table}

The \texttt{select\_pred} function allows to proceed with different scenarios according to the type of predictors (Table \ref{tab:tab4} can help users to choose). The first one consists in boiling down to a set of categorical variables (ordered or not) by categorizing all the continuous predictors using the dedicated argument (\texttt{convert\_num} and \texttt{convert\_clss}) and to work with the conditional importance assessments that directly provide unbiased estimations (by setting the \texttt{RF\_condi} argument to \texttt{TRUE}). Users can consult (Hothorn, Hornik, and Zeileis 2006) for more details about the approach and consult the \href{https://cran.r-project.org//web//packages//OTrecod//OTrecod.pdf}{pdf manual} of the package for details about the related arguments of the \texttt{select\_pred} function.
This approach does not take into account incomplete information, so that the method will be applied to complete data only (incomplete rows will be temporarily removed from the study). It is nevertheless possible to impute missing data beforehand by using dedicated pre-existing \texttt{R} packages like \CRANpkg{mice} (van Buuren and Groothuis-Oudshoorn 2011) or by using the \texttt{imput\_cov} function provided in the \CRANpkg{OTrecod} package.

The second possible scenario (always usable in presence of mixed type predictors), consists in the execution
of a standard random forest procedure after taking care to rule out collinearity issues by first selecting unique candidates between potential redundant predictors (in this case, the discarded predictors are stored in the \texttt{drop\_var} output object). This is the default approach used by \texttt{select\_pred} as soon as the \texttt{RF\_condi}
argument is set to \texttt{FALSE} while \texttt{RF} is set to \texttt{TRUE}. This scenario can work in presence of incomplete predictors. By constructing, note that results from random forest procedures stay dependent on successive random draws carried out for the constitution of trees, and it is so suggested to check this stability by testing different random seeds (\texttt{RF\_SEED} argument) before concluding.

The \texttt{R} command previously written provides automatically the results related to the second approach as
additional results. The results from the two datasets show here the permutation importance estimates of each
variable ranked in order of importance and expressed as percentage, after resolving collinearity problems:

\begin{verbatim}
step2a$RF_PRED   # For the api29 dataset
\end{verbatim}

\begin{verbatim}
#>    meals    stype grad.sch   awards acs.core 
#>  71.5529  11.6282  11.1181   5.4674   0.2334
\end{verbatim}

\begin{verbatim}
step2b$RF_PRED   # For the api35 dataset
\end{verbatim}

\begin{verbatim}
#>    meals      ell grad.sch     full    stype  api.stu acs.core   awards 
#>  35.9974  28.3051  22.2094   5.2143   4.0192   1.8965   1.5643   0.7937
\end{verbatim}

The results confirm that the variable \texttt{meals} appeared as the best predictor of the target variables in \texttt{api29} and \texttt{api35} respectively. The variable \texttt{ell} is not present in the first list (see \texttt{RF\_PRED} from \texttt{step2a}) because the variables \texttt{meals} and \texttt{ell} has been detected as strongly collinear (according to the initial chosen threshold) and so \texttt{select\_pred} keep the best choice between the two predictors: \texttt{meals} (the reasoning is the same for \texttt{full} which disappeared from the list).

The \texttt{ell} variable has been kept in the second list (not found as collinear enough to be removed here)
and appears moreover as a good predictor of \texttt{apicl\_1999} in \texttt{api35}. Nevertheless its potential collinearity
problem with \texttt{meals} encourages us not to keep it for the rest of the study. According to this discussion, we
finally keep \texttt{meals}, \texttt{stype} and \texttt{grad.sch} which combines the advantages of being good predictors for the
two target variables while not presenting major problems of collinearity between them.

The following synthetic database (called here \texttt{bdd\_ex}) is now ready for the prediction of the missing \emph{API
scores} in \texttt{api29}, \texttt{api35} or both, using the function \texttt{OT\_outcome} or \texttt{OT\_joint}:

\begin{verbatim}
bdd_ex = step1$DB_READY[, c(1:3,10:12)]; head(bdd_ex,3)
\end{verbatim}

\begin{verbatim}
#>      DB         Y    Z grad.sch    meals stype
#> 2850  1 (600-800] <NA>      >10   [0-25]     H
#> 2851  1 (600-800] <NA>      >10   [0-25]     H
#> 2852  1 [200-600] <NA>     1-10 (75-100]     H
\end{verbatim}

\hypertarget{data-fusion-using-ot_outcome}{%
\subsubsection{\texorpdfstring{Data fusion using \texttt{OT\_outcome}}{Data fusion using OT\_outcome}}\label{data-fusion-using-ot_outcome}}

The \texttt{OT\_outcome} function provides individual predictions of \emph{Z} in \emph{A} (and/or \emph{Y} in \emph{B}) by considering the recoding problem involving optimal transportation of outcomes. In a first step, the aim of the function is so to determine \(\gamma\) from \eqref{eq:modeldim} while this estimate is used in a second step to provide the predictions. The main input arguments of the function and the output values are respectively described in Tables \ref{tab:tab5} and \ref{tab:tab6}.

\begin{table}[h]
  \centering
  \begin{tabular}{lccp{6cm}}
    \toprule
    \strong{Argument}  & \code{OT\_outcome} & \code{OT\_joint} & \strong{Description (default value)} \\
    \hline
        \code{datab}            & \checkmark & \checkmark & Data.frame in the expected structure \\
        \code{index\_DB\_Y\_Z}  & \checkmark & \checkmark & Indexes of the ID, Y, and Z columns (1,2,3) \\
        \code{nominal}          & \checkmark & \checkmark & Column indexes of nominal variables (*) \\
        \code{ordinal}          & \checkmark & \checkmark & Col. indexes of ordinal variables (*) \\
        \code{logic}            & \checkmark & \checkmark & Col. indexes of boolean variables (*) \\
        \code{quanti}           & \checkmark &  & Col. indexes of quantitative variables (*) \\
        \code{convert.num}      & \checkmark &  \checkmark & Col. indexes of the quantitative variables to convert (* or $=$\code{quanti} in \code{OT\_joint}) \\
        \code{convert.clss}     & \checkmark &  \checkmark & Corresponding numbers of desired classes for conversion (*) \\
        \code{which.DB}         & \checkmark &  \checkmark & Specify the target variables to complete: Both or only one (BOTH) \\
        \code{solvR}            & \checkmark &  \checkmark & Choice of the solver to solve the optimization problem (glpk) \\
        \code{dist.choice}      & \checkmark &  \checkmark & Distance function (Euclidean). See Table 7 \\
        \code{percent.knn}      & \checkmark &  \checkmark & Ratio of closest neighbors involved int the computations (1) \\
        \code{indiv.method}     & \checkmark &              & Type of individual predictions (sequential) for \code{OUTCOME} and \code{R-OUTCOME} algorithms \\
        \code{maxrelax}         & \checkmark &  \checkmark & Adding of a relaxation parameter (0) \\
        \code{lambda.reg}       &            &  \checkmark & Adding of regularization parameter (0) \\
        \bottomrule
  \end{tabular}
  \caption{Main arguments of the \code{OT\_outcome} and \code{OT\_joint} functions. (*: NULL as default value)}
\label{tab:tab5}
\end{table}

The arguments \texttt{datab}, \texttt{index\_DB\_Y\_Z}, \texttt{quanti}, \texttt{nominal}, \texttt{ordinal}, and \texttt{logic} are not optional and must be carefully completed before each execution. A unique synthetic data.frame made of two overlayed databases (called \emph{A} and \emph{B} for example) (see Table \ref{tab:tab3}) is expected as \texttt{datab} argument. If this data.frame corresponds to the output objects \texttt{DB\_USED} or \texttt{DB\_READY} of the \texttt{select\_pred} or \texttt{merge\_dbs} functions respectively, then the expected object has the required structure. Otherwise users must be sure that their data.frames are made up of two overlayed databases with at least 4 variables as described in the subsection \protect\hyperlink{optt}{Optimal transportation of outcomes applied to data recoding}.
The order of the variables have no importance in the raw database but will have to be specified a posteriori in the \texttt{index\_DB\_Y\_Z} argument if necessary.

\begin{table}[h]
  \centering
  \begin{tabular}{lccp{6cm}}
    \toprule
    \strong{Value}  & \code{OT\_outcome} & \code{OT\_joint} & \strong{Description} \\
    \hline
        \code{time\_exe}         & \checkmark & \checkmark &  Running time of the algorithm \\
    \code{gamma\_A}          & \checkmark & \checkmark &  Estimation of the joint distribution of $(Y,Z)$ for the prediction of $Z$ in $A$ (*) \\
    \code{gamma\_B}          & \checkmark & \checkmark &  Estimation of the joint distribution of $(Y,Z)$ for the prediction of $Y$ in $B$ (*) \\
    \code{profile}           & \checkmark & \checkmark &  The list of detected profiles of covariates \\
        \code{res.prox}      & \checkmark & \checkmark &  A list that provides all the information related to the estimated proximities between profiles and groups of profiles \\
        \code{estimator\_ZA} & \checkmark & \checkmark &  Estimates of the probability distribution of $Z$ conditional to $X$ and $Y$ in database $A$ (*) \\
        \code{estimator\_YB} & \checkmark & \checkmark &  Estimates of the probability distribution of $Y$ conditional to $X$ and $Z$ in database $B$ (*) \\
        \code{DATA1\_OT}     & \checkmark & \checkmark &  The database A fully completed (if required in input by the \code{which.DB} argument) \\
        \code{DATA2\_OT}     & \checkmark & \checkmark &  The database B fully completed (if required in input by the \code{which.DB} argument) \\
        \bottomrule
  \end{tabular}
  \caption{Values of the \code{OT\_outcome} and \code{OT\_joint} functions. (*: NULL if not required)}
\label{tab:tab6}
\end{table}

The subset of remaining predictors used for fusion may requires prior transformations according to the distance function chosen in input by the user. This process is fixed by setting the \texttt{dist.choice} argument. The distances actually implemented in the function are: the standard Euclidean (\texttt{"E"}) and Manhattan (\texttt{"M"}) distances, the Hamming distance (\texttt{"H"}, for binary covariates), and the Gower distance (\texttt{"G"} sometimes preferred with mixed variables). Automatic transformations prior to the use of each distance function are summarized in Table \ref{tab:tab7}.

\begin{table}[h]
  \centering
   \begin{tabular}{lcccc}
    \toprule
    \strong{Distance function}  & Euclidean & Manhattan &   Gower   & Hamming \\
    \hline
    \strong{Variable transformations} & & & &           \\
    Continuous& Standardized&   Standardized &  No  & Not allowed \\
    Boolean &   Binary &    Binary &    No &    Binary \\
    Nominal &   Disjunctive T&  Disjunctive T   &No&    Disjunctive T \\
    Ordinal &   Discrete&   Discrete    & No    & Disjunctive T \\
    Incomplete information &    Allowed* &  Allowed* &  Allowed* &  Allowed* \\
    \hline
    \code{dist.choice} \strong{argument}    & “E” & “M” & “G”   & “H” \\
        \bottomrule
  \end{tabular}
  \caption{Internal variable transformations related to the choice of each distance function in \code{OT\_outcome} and \code{OT\_joint}. (*) If the number of covariates exceeds 1. T for table}
\label{tab:tab7}
\end{table}

The first version of the OT algorithm described in Garès et al. (2020) was tested on numeric coordinates
extracted from a factor analysis of mixed data (FAMD) fitted on mixed raw covariates (Pagès 2002). This transformation is here available by setting the \texttt{FAMD.coord} argument to \texttt{"YES"}. The minimal percentage
of information explained by the FAMD is also fixable using the \texttt{FAMD.perc} argument. The \texttt{OT\_outcome}
functions proposes four types of models for the prediction of \emph{Y} in \emph{B} (and/or) \emph{Z} in \emph{A}, according to the values of the \texttt{method} and \texttt{maxrelax} arguments:

\begin{itemize}
\tightlist
\item
  When \texttt{maxrelax\ =\ 0} and \texttt{indiv.method\ =\ "sequential"} (default options), the fitted model corresponds to the \texttt{OUTCOME} algorithm described by \(\hat{\mathcal{P}}^0_n\) in equation \eqref{eq:outco}. Assuming that \(Y\) and \(Z\) in \(A\) follow the same distribution as \(Y\) and \(Z\) in \(B\) respectively (\protect\hyperlink{optt}{\textbf{assumption 1}}), this related algorithm derives the joint distribution of \(Y\) and \(Z\) in \(A\) (respectively in \(B\)) in a first step, and uses in a second step, a nearest neighbor procedure to predict missing values of \(Z\) in \(A\) (resp. \(Y\) in \(B\)). This algorithm calculates averaged distances between each subject from \(A\) (resp. \(B\)) and subgroups of subjects from \(B\) (resp. \(A\)) having same levels of \(Z\) in \(B\) (resp. \(Y\) in \(A\)). These calculations can be done using all subjects of each subgroups (by default, \texttt{percent.knn\ =\ 1}) or only restricted parts of them (\texttt{percent.knn\ \textless{}\ 1}).
\item
  When \texttt{maxrelax\ \textgreater{}\ 0} and \texttt{indiv.method\ =\ "sequential"}, \protect\hyperlink{optt}{\textbf{assumption 1}} is alleviated by relaxing the constraints on marginal distributions and an \texttt{R-OUTCOME} algorithm (with relaxation) is fitted. In this case, the \texttt{maxrelax} argument corresponds to the \(\alpha_n\) parameter in \eqref{eq:modeloutcomerelaxed}.
\item
  When \texttt{maxrelax\ =\ 0} and \texttt{indiv.method\ =\ "optimal"}, the second step of the original algorithm (nearest neighbor procedure) is replaced by a linear optimization problem: searching for the individual predictions of \(Z\) in \(A\) (resp. \(Y\) in \(B\)) by minimizing the total of the distances between each individual of \(A\) and individuals of each levels of \(Z\) in \(B\).
\item
  When \texttt{maxrelax\ \textgreater{}\ 0} and \texttt{indiv.method\ =\ "optimal"}, the constraints on marginal distributions of the previous model are also relaxed and the function fits an \texttt{R-OUTCOME} algorithm with relaxation. For these three last situations, the corresponding \texttt{R-OUTCOME} algorithm is so described by \(\hat{\mathcal{P}}^{0-R}_n\) \eqref{eq:modeloutcomerelaxed}.
\end{itemize}

When \texttt{maxrelax\ \textgreater{}\ 0}, it is recommended to calibrate the \texttt{maxrelax} parameter by testing different values according to the stability of individual predictions. In output, the \texttt{gamma\_A} and \texttt{gamma\_B\}} matrices correspond to the estimates of the joint distributions \(\gamma\) of \((Y^A,Z^A)\) and \((Y^B,Z^B)\) respectively \eqref{eq:modeldim}. Moreover, a posteriori estimates of conditional distributions probabilities \((Z^A|Y^A, X^A)\) and \((Y^B|Z^B,X^B)\) \eqref{eq:condprob} are also provided in two lists called \texttt{estimatorZA}, and \texttt{estimatorYB} respectively and the completed databases \(A\) and \(B\) are stored in the \texttt{DATA1\_OT} and \texttt{DATA2\_OT} objects. In particular, the individual predictions are stored in the \texttt{OTpred} column of these data.frames. Moreover, the \texttt{profile} object is a data.frame that stores the profile of covariates encountered in the two data sources while the \texttt{res\_prox} object stored all the distance computations that will be used in the validation step (users can consult details of the \texttt{proxim\_dist} function of the \href{https://cran.r-project.org//web//packages//OTrecod//OTrecod.pdf}{pdf manual}.

The computation of conditional probabilities implies to define the part of individuals considered as neighbors of each encountered profile of covariates. The argument \texttt{prox.dist} fixes this threshold for each profile, following the decision rule for \(A\) (and respectively for \(B\)): a subject \(i\) of \(A\) (or a statistical unit corresponding to a row of \(A\)) will be considered as neighbor of a given profile of shared variables \(C\) as soon as:
\[
 \text{dist}(subject_i,C) <  \text{prox.dist} \times \text{max}_{k=1,\dots , n_A}  \text{dist}(subject_k,C)
 \]
When the number of shared variables is small and all of them are categorical (optimal situation), it is suggested to set the \texttt{prox.dist} argument to \(0\). Finally, using the \texttt{which.DB} argument, users can choose to estimate the individual predictions of \(Y\) and \(Z\) in the two databases (\texttt{which.DB\ =\ "BOTH"}) or only one of the both (\texttt{which.DB\ =\ "A"} for the predictions of \(Z\) in \(A\) or \texttt{which.DB\ =\ "B"} for the predictions \(Y\) in \(B\)).

From the data.frame \texttt{bdd\_ex} built previously, we use the \texttt{OT\_outcome} function to illustrate the prediction of the target variable \texttt{apicl\_2000} in the \texttt{api35} dataset via an \texttt{OUTCOME} algorithm and using an Euclidean distance function:

\begin{verbatim}
outc1 = OT_outcome(bdd_ex, quanti = 1, ordinal = 2:6, 
                   dist.choice = "E", indiv.method = "sequential", 
                   which.DB = "B")

#---------------------------------------
# OT PROCEDURE in progress ...
#---------------------------------------
# Type                     = OUTCOME
# Distance                 = Euclidean
# Percent closest knn      = 100%
# Relaxation parameter     = NO
# Relaxation value         = 0
# Individual pred process  = Sequential
# DB imputed               = B
#---------------------------------------
\end{verbatim}

In \texttt{bdd\_ex}, all variables except the first one (\texttt{DB}: the database identifier) are or can be considered as ordinal factors, and the \texttt{quanti} and \texttt{ordinal} arguments are filled in accordingly. For the prediction of \texttt{apicl\_2000} in the \texttt{api35} dataset (the steps would be the same for the prediction of \texttt{apicl\_1999} in \texttt{api29} by setting \texttt{which.DB\ =\ "A"} or \texttt{"BOTH"}), the optimal transportation theory determines a map \(\gamma\) that pushes, in the distribution of \texttt{apicl\_1999} forward to the distribution of \texttt{apicl\_2000} in the database \texttt{api35}. In this case, \(\gamma^B\) is an estimator of the joint distribution of \texttt{apicl\_2000} and \texttt{apicl\_1999} in \texttt{api35}. The estimation of \(\gamma^B\) is available in the \texttt{gamma\_B} output object while all the profiles of predictors met in the two databases are listed in the \texttt{profile} object:

\begin{verbatim}
outc1$gamma_B
\end{verbatim}

\begin{verbatim}
#>                    G1        G2         G3        G4
#> [200-600]  0.22248804 0.0000000 0.00000000 0.0000000
#> (600-800]  0.02889318 0.2486188 0.15311005 0.0000000
#> (800-1000] 0.00000000 0.0000000 0.09550874 0.2513812
\end{verbatim}

\begin{verbatim}
outc1$profile[1,]         # the first profile
\end{verbatim}

\begin{verbatim}
#>       ID grad.sch meals stype
#> 2850 P_1        3     1     3
\end{verbatim}

The output object \texttt{estimatorYB} is a list that corresponds to the estimations of the conditional probabilities of \texttt{apicl\_2000} in the \texttt{api35} dataset for a given profile of predictors. For example, the conditional probabilities related to the first profile of predictors are:

\begin{verbatim}
outc1$estimatorYB[1,,]    # conditional probabilities (1st profile)
\end{verbatim}

\begin{verbatim}
#>         [,1]      [,2]      [,3]
#> G1 0.3333333 0.3333333 0.3333333
#> G2 0.3333333 0.3333333 0.3333333
#> G3 0.0000000 0.0000000 1.0000000
#> G4 0.0000000 0.0000000 1.0000000
\end{verbatim}

According to these results, we can conclude that: for a subject from \texttt{api35} with a related profile of predictors \texttt{P\_1} and whose levels of \texttt{apicl\_1999} is \texttt{\textquotesingle{}G1\textquotesingle{}}, the probability for \texttt{apicl\_2000} to be predicted \texttt{\textquotesingle{}{[}200,600{]}\textquotesingle{}} is about \(0.63\). Finally, the individual predictions of \texttt{apicl\_2000} in \texttt{api35} are available in the \texttt{DATA2\_OT} object corresponding to the \texttt{OTpred} column:

\begin{verbatim}
head(outc1$DATA2_OT,3)  # The 1st 3 rows only
\end{verbatim}

\begin{verbatim}
#>      DB    Y  Z grad.sch meals stype    OTpred
#> 3895  2 <NA> G1        2     4     1 [200-600]
#> 3896  2 <NA> G3        2     3     1 (600-800]
#> 3897  2 <NA> G2        2     3     2 (600-800]
\end{verbatim}

The \texttt{OUTCOME} algorithm uses here a nearest neighbor procedure to assign the individual predictions from the estimation of \(\gamma\) which can be a drawback as described in (Garès et al. 2020). The \texttt{R-OUTCOME} algorithm proposes an enrichment that directly assigns the individual predictions of \texttt{apicl\_2000} from the estimation of \(\gamma\), without using the nearest neighbor approach. In this case, a linear optimization problem is solved to determine the individual predictions that minimize the sum of the individual distances in \texttt{api35} having the modalities of the target \texttt{apicl\_2000} in \texttt{api29}. The corresponding \texttt{R} command is:

\begin{verbatim}
### R-OUTCOME algorithm: optimal assignments + relaxation parameter = 0
R_outc3 = OT_outcome(bdd_ex, quanti = 1, ordinal = 2:6, 
                     dist.choice = "E" , indiv.method = "optimal",
                     which.DB = "B")
\end{verbatim}

Moreover, the \texttt{OUTCOME} algorithm assumes that the distribution of \texttt{apicl\_2000} is identically distributed in \texttt{api29} and \texttt{api35} (\protect\hyperlink{optt}{\textbf{assumption 1}} from the subsection \protect\hyperlink{optt}{Optimal transportation of outcomes applied to data recoding}) which can appear as a strong hypothesis to hold in many situations. To overcome this constraint, the \texttt{R-OUTCOME} algorithm also allows to relax the \protect\hyperlink{optt}{\textbf{assumption 1}} by adding a relaxation parameter \eqref{eq:modeloutcomerelaxed} in the optimization system. This relaxation can be done by varying the \texttt{maxrelax} argument of the function:

\begin{verbatim}
### R-OUTCOME algorithm: optimal assignments + relaxation parameter = 0.4
R_outc4 = OT_outcome(bdd_ex, quanti = 1, ordinal = 2:6, 
                     dist.choice = "E",
                     indiv.method = "optimal", 
                     maxrelax = 0.4, which.DB = "B")
\end{verbatim}

The running times of these two models can take few minutes. The quality of these models will be compared in Tables \ref{tab:tab8}, \ref{tab:tab9} and \ref{tab:tab10} (subsection \protect\hyperlink{vodf}{Validation of the data fusion using \texttt{verif\_OT}}).

\hypertarget{data-fusion-using-ot_joint}{%
\subsubsection{\texorpdfstring{Data fusion using \texttt{OT\_joint}}{Data fusion using OT\_joint}}\label{data-fusion-using-ot_joint}}

The \texttt{OT\_joint} function provides individual predictions of \(Z\) in \(A\) (and/or \(Y\) in \(B\)) by considering the recoding problem as an optimal transportation problem of covariates and outcomes, that pushes the conditional distribution of \((Y^A|X^A)\) forward to the conditional distribution of \((Z^A|X^A)\) (and conversely \((Z^B|X^B)\) forward to the conditional distribution of \((Y|X)\)). The aim is to determine \(\gamma\) from \eqref{eq:jointkanto}. Because joint distributions of outcomes and covariates are now mapped together (it was not the case with the \texttt{OUTCOME} family of algorithms), it is not required for target variables to be equally distributed (\protect\hyperlink{optt}{\textbf{assumption 1}}).

The call of \texttt{OT\_joint} was thought to propose globally the same structure as those of the function \texttt{OT\_outcome} as described in Tables \ref{tab:tab5} and \ref{tab:tab6} with few differences described below. \texttt{JOINT} and \texttt{R-JOINT} algorithms are usable via \texttt{OT\_joint} for solving the recoding problem depending on the values related to the \texttt{maxrelax} and \texttt{lambda\_reg} arguments:

\begin{itemize}
\tightlist
\item
  When \texttt{maxrelax\ =\ 0} and \texttt{lambda.reg\ =\ 0} (default values), the fitted model corresponds to the \texttt{JOINT} algorithm described by \(\hat{\mathcal{P}}_n\) in equation \eqref{eq:jointkanto}.
\item
  When at least one of these two arguments differs from 0, the \texttt{R-JOINT} algorithm is called. Using \texttt{R-JOINT}, it is so possible to relax constraints on marginal distributions (\texttt{maxrelax\ \textgreater{}\ 0}) and/or add an eventual more or less strong \emph{L1} regularisation term among the constraints of the algorithm by filling the \texttt{lambda\_reg} argument. \texttt{maxrelax} parameter correspond to parameter \(\alpha_n\) in \eqref{eq:ctrelaxnorme} and \texttt{lambda.reg} parameter correspond to parameter \(\lambda\) in \eqref{eq:estimatemodelregularization}.
\end{itemize}

When \texttt{maxrelax\ \textgreater{}\ 0} and/or \texttt{lambda.reg\ \textgreater{}\ 0}, it is recommended to calibrate these two parameters by testing many different values and so studying the stability of the related individual predictions. Nevertheless, by default or as a starting point, (Garès et al. 2020) suggests the use of default values determined from simulated databases: \(0.1\) for the regularization parameter (\texttt{lambda.reg}) and \(0.4\) for the relaxation one (\texttt{maxrelax}). Finally, the output objects are similar to those previously described in the \texttt{OT\_outcome} function.

The implementation of the function is very similar to that of \texttt{OT\_outcome}, and applied to our example, the \texttt{R} code related to a \texttt{JOINT} algorithm is:

\begin{verbatim}
outj1 = OT_joint(bdd_ex, nominal = 1, ordinal = c(2:6), 
                 dist.choice = "E"  , which.DB = "B")

#---------------------------------------
# OT JOINT PROCEDURE in progress ...
#---------------------------------------
# Type                  = JOINT
# Distance              = Euclidean
# Percent closest       = 100%
# Relaxation term       = 0
# Regularization term   = 0
# Aggregation tol cov   = 0.3
# DB imputed            = B
#---------------------------------------


### Extract individual predictions from the OTpred column
head(outj1$DATA2_OT,3)  # The 1st 3 rows only
\end{verbatim}

\begin{verbatim}
#>      DB    Y  Z grad.sch meals stype    OTpred
#> 3895  2 <NA> G1        2     4     1 [200-600]
#> 3896  2 <NA> G3        2     3     1 (600-800]
#> 3897  2 <NA> G2        2     3     2 (600-800]
\end{verbatim}

For relaxing the constraints stated on marginal distributions, we use the \texttt{maxrelax} argument that corresponds to varying the \(\alpha\) parameter of a \texttt{R-JOINT} algorithm (see \eqref{eq:estimatemodelregularization}) and a parameter of regularization \(\lambda\) can be simply added to the previous model as follows:

\begin{verbatim}
### R-JOINT algorithm (relaxation parameter = 0.4)
R_outj1 = OT_joint(bdd_ex, nominal = 1, ordinal = c(2:6), 
                   dist.choice = "E",  maxrelax = 0.4,
                   which.DB = "B")

### R-JOINT algorithm (relaxation parameter = 0.4,
###                                   & regularization parameter = 0.1)
R_outj4 = OT_joint(bdd_ex, nominal = 1, ordinal = c(2:6), 
                   dist.choice = "E",  maxrelax = 0.4,
                   lambda.reg = 0.1,
                   which.DB = "B")
\end{verbatim}

These models are compared in the \protect\hyperlink{vodf}{next subsection}.

\hypertarget{vodf}{%
\subsubsection{\texorpdfstring{Validation of the data fusion using \texttt{verif\_OT}}{Validation of the data fusion using verif\_OT}}\label{vodf}}

Assessing the quality of individual predictions obtained through statistical matching techniques can be a complex task notably because it is legitimate to wonder about the true reliability of a joint distribution estimated from two variables which are never jointly observed (Kadane 2001; Rodgers 1984). When the study aims to identify estimators of interests that improve the understanding of the relationships between variables in the different databases (called macro approach in D'Orazio, Di Zio, and Scanu (2006)) without going until the creation of a complete and unique dataset, uncertainty analyses are usually proposed to estimate the sensitivity of the assessments (Rässler 2002). On the contrary, if the objective is to create a synthetic dataset where the information is available for every unit, the use of auxiliary information or proxy variables must be privileged to assess the quality of the results (Paass 1986). In the absence of complementary information, Rubin (1986) suggests to study the preservation of the relationships at best between the distributions of the target variables. In this way, the \texttt{verif\_OT} function proposes tools to assess quality of the individual predictions provided by the algorithms previously introduced.

The first expected input argument of \texttt{verif\_OT} is an \texttt{\textquotesingle{}otres\textquotesingle{}} object from the \texttt{OT\_outcome} or \texttt{OT\_joint} functions. In our example, this function is firstly applied to the \texttt{out\_c1} model by following the \texttt{R} command:

\begin{verbatim}
### Quality criteria for outc1 (OUTCOME model)
verif_outc1   = verif_OT(outc1, group.class = TRUE, ordinal = FALSE, 
                         stab.prob = TRUE, min.neigb = 5)

### First results related to outc1:
verif_outc1$nb.profil
\end{verbatim}

\begin{verbatim}
#> [1] 27
\end{verbatim}

\begin{verbatim}
verif_outc1$res.prox
\end{verbatim}

\begin{verbatim}
#>        N   V_cram rank_cor 
#>  362.000    0.860    0.892
\end{verbatim}

In output, the \texttt{nb.profil} value gives the number of profiles of predictors globally detected in the two databases \((nb = 27)\). In the example, a profile will be characterized by a combination of values related to the three predictors kept: \texttt{grad.sch}, \texttt{meals} and \texttt{stype}.

The first step of the function is dedicated to the study of the proximity between the two target variables. Therefore, standard criteria (Cramer's V and Spearman's rank correlation coefficient) are used to evaluate the pairwise association between \(Y\) and \(Z\), globally or in one of the two databases only, according to the predictions provided by the output of \texttt{OT\_outcome} or \texttt{OT\_joint}. Only the database \texttt{api35} was completed in the example (because \texttt{which.DB\ =\ "B"} as argument of \texttt{OT\_outcome}) and stored in the \texttt{DATA2\_OT} object, therefore, the criteria compare here the proximity distribution between the predicted values of \texttt{apicl\_2000} (\texttt{Y}) and the observed value of \texttt{apicl\_1999} (\texttt{Z}) in the database \texttt{api35} (\texttt{B}). Regarding independence or a small correlation between \(Y^A\) and \(Z^A\) (or \(Y^B\) and \(Z^B\)) must question about the reliability of the predictions especially when \(Y\) and \(Z\) are supposed to summarize a same information. In the example, whatever the criteria used, we notice via the \texttt{res.prox} object, a strong association between the two target variables in \texttt{api35} which reassures about the consistency of the predictions.
The related confusion matrix between the predicted values of \texttt{apicl\_2000} (\texttt{Y}) and the observed value of \texttt{apicl\_1999} (\texttt{Z}) is stored in the \texttt{conf.mat} object.

Second, the function proposes an optional tool (by setting \texttt{group.clss=\ TRUE}) which evaluates the impact of grouping levels of one factor on the association of \(Y\) and \(Z\). When users have initially no information about one of the two encodings of interest, this approach can be particularly useful to detect ordinal from nominal ones and its principle is as follow. Assuming that \(Y \in \mathcal{Y}\), and \(Z \in \mathcal{Z}\) (\(\mathcal{Y}\) and \(\mathcal{Z}\) are the respective levels) and that \(|\mathcal{Y}| \geq |\mathcal{Z}|\). From \(Y\), successive new variables \(Y’ \in \mathcal{Y}'\) are built, as soon as \(\mathcal{Y}'\) is a partition of \(\mathcal{Y}\) such as \(|\mathcal{Y}'| = |\mathcal{Z}|\) (and inversely for \(Z\) if the levels of \(Z\) is the greatest). The related associations between \(Z\) and \(Y’\) (with now equal number of levels) are then studied using: Cramer's V, rank correlation, Kappa coefficient and confusion matrix and the results are stored in a table called \texttt{res.grp}. The corresponding outputs of the example are:

\begin{verbatim}
verif_outc1$conf.mat
\end{verbatim}

\begin{verbatim}
#>             Z
#> predY         G1  G2  G3  G4 Sum
#>   [200-600]   81   1   1   1  84
#>   (600-800]   10  89  55   1 155
#>   (800-1000]   0   0  34  89 123
#>   Sum         91  90  90  91 362
\end{verbatim}

\begin{verbatim}
verif_outc1$res.grp
\end{verbatim}

\begin{verbatim}
#>   grp levels Z to Y error_rate Kappa Vcramer RankCor
#> 4       G1/G2 G3/G4       13.3 0.794    0.83   0.877
#> 6       G1/G2/G3 G4       19.1 0.714    0.78   0.839
#> 2       G1 G3/G2/G4       28.2 0.593    0.68   0.624
#> 1       G1 G2/G3/G4       37.6 0.457    0.64   0.813
#> 3       G1 G4/G2/G3       43.4 0.374    0.59   0.115
#> 5       G1/G2 G4/G3       43.4 0.326    0.64   0.574
\end{verbatim}

It appears from these results that grouping the levels \texttt{G2} and \texttt{G3} of \texttt{apicl\_1999} (\texttt{Z}) strongly improves the association of this variable with \texttt{apicl\_2000} (\texttt{Y}) (the error rate of the confusion matrix varies from \(43.4\) to \(13.3\)). Moreover the structure of \texttt{conf.mat} confirms that the encoding of \texttt{apicl\_1999} seems to be ordinal.

The third step of the quality process integrated in the function corresponds to the study of the Hellinger distance (Liese and Miescke 2008). This distance function is used as a measure of the discrepancies between the observed and predicted distributions of Y (\(\mathcal{L}(Y^A)\) versus \(\mathcal{L}(\widehat{Y}^B)\)) and/or (\(\mathcal{L}(\widehat{Z}^A)\) versus \(\mathcal{L}(Z^B)\)). For \(Y\) and \(Z\), the definition of the distance is respectively:

\[
\text{dist}_{\text{hell}} (Y^A,\widehat{Y}^B)= \sqrt{\frac{1}{2} \sum_{y\in \mathcal{Y}}\left(\sqrt{\mu_{n,y}^{Y^A}}- \sqrt{\mu_{n,y}^{\widehat{Y}^B}}\right)^2 )}
\]

and

\[
\text{dist}_{\text{hell}} (Z^B,\widehat{Z}^A)= \sqrt{\frac{1}{2} \sum_{z\in \mathcal{Z}}\left(\sqrt{\mu_{n,z}^{Z^B}}- \sqrt{\mu_{n,z}^{\widehat{Z}^A}}\right)^2 )},
\]

where \(\mu_{n,y}^{\widehat{Y}^B}\) and \(\mu_{n,z}^{\widehat{Z}^A}\) correspond to the empirical estimators of \(\mu^{\widehat{Y}^B}\) and \(\mu^{\widehat{Z}^A}\) respectively.

The Hellinger distance varies between 0 (identical) and 1 (strong dissimilarities) while 0.05 can be used as an acceptable threshold below which two distributions can be considered as similar. When \texttt{OUTCOME} models are applied on datasets, this criterion shows that predictions respect \protect\hyperlink{optt}{\textbf{assumption 1}}. It can also be used to determine the best relaxation parameter of an \texttt{R-OUTCOME} model. On the contrary, there is no need to interpret this criterion when an \texttt{R-JOINT} model is applied, because in this case, the \protect\hyperlink{optt}{\textbf{assumption 1}} is not required. Results related to this criterion are stored in the \texttt{hell} object:

\begin{verbatim}
verif_outc1$hell
\end{verbatim}

\begin{verbatim}
#>                 YA_YB ZA_ZB
#> Hellinger dist. 0.008    NA
\end{verbatim}

With a p-value equals to \(0.008\), the \protect\hyperlink{optt}{\textbf{assumption 1}} hold here but it stays interesting to test other algorithms to eventually improve the current one by notably adding relaxation and/or regularization parameters.

Finally, the \texttt{verif\_OT} function uses the mean and standard deviance of the conditional probabilities \(\mathbb{P}(Z=\hat{z}_i|Y=y_i,X=x_i)\) estimated by each model, as indicators of the stability of the individual predictions (provided that \texttt{stab.prob\ =\ TRUE}). It is nevertheless possible that conditional probabilities are computed from too few individuals (according to the frequency of each profile of shared variables met), to be considered as a reliable estimate of the reality. To avoid this problem, trimmed means and standard deviances are suggested by removing these specific probabilities from the computation, using the \texttt{min.neigb} parameter. In output, the results related to this last study are stored in the \texttt{res.stab} object:

\begin{verbatim}
verif_outc1$eff.neig
\end{verbatim}

\begin{verbatim}
#>    Nb.neighbor Nb.Prob
#> 1            1      14
#> 2            2      18
#> 3            3      18
#> 4            4      28
#> 5            5      20
#> 6            6       6
#> 7            7      14
#> 8            8      16
#> 9            9      27
#> 10          10      20
\end{verbatim}

\begin{verbatim}
verif_outc1$res.stab
\end{verbatim}

\begin{verbatim}
#>          N min.N  mean    sd
#> 2nd DB 284     5 0.968 0.122
\end{verbatim}

The first result shows that \(14\) individual predictions among \(362\) (the total number of rows in \texttt{api35}) have been assigned to subjects that exhibits a unique combination of predictors and outcome. From these subjects, it would be obviously overkill to draw conclusions about the reliability of the predictions provided by the model. We therefore decide to fix here a threshold of \(5\) below which it is difficult to extract any information related to the prediction. From the remaining ones, we simply perform the average (\(0.968\)) which could be interpreted as follows: when the fitted model is confronted with a same profile of predictors and a same level of \texttt{apicl\_1999}, more than \(96\) times of a hundred, it will return the same individual prediction for \texttt{apicl\_2000}.

We run a total of \(11\) models to determine the optimal one for the prediction of \texttt{apicl\_2000} in \texttt{api35}.
Every arguments from the syntax of the \texttt{OT\_outcome} and \texttt{OT\_joint} functions stay unchanged with the exception of \texttt{indiv.method}, \texttt{maxrelax}, and \texttt{lambda.reg} which vary.
The values linked to each criterion of the \texttt{verif\_OT} function are summarized in Tables \ref{tab:tab8}, \ref{tab:tab9} and \ref{tab:tab10}. The syntax related to \texttt{verif\_OT} stay unchanged for each model (notably same min.neigb arguments). Note that comparisons of stability predictions between \texttt{OUTCOME} and \texttt{JOINT} models impose that the \texttt{prox.dist} argument of the OT\_outcome function is fixed to \(0\).

\begin{table}[h]
  \centering
  \begin{tabular}{llcccccc}
    \toprule
    \strong{Model}  & \strong{Type} & \strong{Method} & \strong{Relax} & \strong{Regul} &
    \strong{N} & \strong{V\_cram} & \strong{rank\_cor} \\
    \hline
    \code{outc1} & \code{OUTCOME}      & \code{SEQUENTIAL}   & $0.0$ &   -   & 362 & $0.86$ & $0.892$ \\
    \code{R\_outc1} & \code{R-OUTCOME} & \code{SEQUENTIAL}   & $0.4$ &   -   & 362 & $0.94$ & $0.923$ \\
    \code{R\_outc2} & \code{R-OUTCOME} & \code{SEQUENTIAL}   & $0.6$ &   -   & 362 & $0.91$ & $0.917$ \\
    \code{R\_outc3} & \code{R-OUTCOME} & \code{OPTIMAL}      & $0.0$ &   -   & 362 & $0.87$ & $0.911$ \\
    \code{R\_outc4} & \code{R-OUTCOME} & \code{OPTIMAL}      & $0.4$ &   -   & 362 & $0.95$ & $0.939$ \\  
    \code{R\_outc5} & \code{R-OUTCOME} & \code{OPTIMAL}      & $0.6$ &   -   & 362 & $0.92$ & $0.932$ \\  
    \hline
    \code{outj1}    & \code{JOINT}     &      -              & $0.0$ & $0.0$ & 362 & $0.74$ & $0.834$ \\
    \code{R\_outj1} & \code{R-JOINT}   &      -              & $0.4$ & $0.0$ & 362 & $0.95$ & $0.935$ \\
    \code{R\_outj2} & \code{R-JOINT}   &      -              & $0.6$ & $0.0$ & 362 & $0.91$ & $0.927$ \\
    \code{R\_outj3} & \code{R-JOINT}   &      -              & $0.8$ & $0.0$ & 362 & $0.91$ & $0.927$ \\
    \code{R\_outj4} & \code{R-JOINT}   &      -              & $0.4$ & $0.1$ & 362 & $0.95$ & $0.931$ \\
    \bottomrule
  \end{tabular}
    \caption{V Cramer criterion (\code{V\_cram}) and rank correlation (\code{rank\_cor}) between  \code{apicl\_1999} (the observed variable) and \code{apicl\_2000} (the predicted variable) in the \code{api35} database $(N= 362)$. Predictions comes from 11 models with various algorithms, relaxation and regularization parameters. They are very stable and reproductible here when the relaxation parameter differs from 0.}
 \label{tab:tab8}
\end{table}

From these results, we can conclude that:

\begin{itemize}
\tightlist
\item
  whatever the algorithm used (\texttt{OUTCOME} or \texttt{JOINT}), adding a relaxation parameter improves here the association between the target variables (see Table \ref{tab:tab8}).
\item
  according to the results of Table \ref{tab:tab9}, the \texttt{R\_outc2} and \texttt{R\_outc5} models seem not optimal because they are those for which the Hellinger criterion reflects the most clear violation of \protect\hyperlink{optt}{\textbf{assumption 1}} (see Table \ref{tab:tab9}). Therefore, it is here suggested to keep a relaxation parameter less than \(0.6\) in the final model.
\item
  Table \ref{tab:tab8} confirms this trend because adding a too high relaxation parameter seems to potentially affect the quality of the association (the V Cramer and rank correlation decrease when the relaxation parameter increases from \(0.4\) to \(0.6\)). Consequently, in this example, fixing \(0.4\) seems to be an acceptable compromise for the relaxation parameter whatever the \texttt{R-OUTCOME} algorithm used (\(0.3\) could also have been tested here).
\item
  among the remaining models (\texttt{R\_outc1}, \texttt{R\_outc4}, \texttt{R\_outj1}, and \texttt{R\_outj4}), \texttt{R\_outj1} and \texttt{R\_outj4} seem to be those with the most stable predictive potential (Table \ref{tab:tab10}). Moreover, \texttt{R\_outc4} appears here as the best model from the tested \texttt{R-OUTCOME} algorithms.
\item
  adding a regularization parameter to the \texttt{R\_outj1} model caused a decrease in the stability of the predictions (see the value of \texttt{R\_outj4} compared to \texttt{R\_outj1} in Table \ref{tab:tab10}) and we thus conclude in favor of \texttt{R\_outj1} as best model among those tested in the \texttt{JOINT} family of algorithms.
\item
  Table \ref{tab:tab11} shows that the three remaining models (\texttt{R\_outc4}, \texttt{R\_outj1} and \texttt{R\_outj4}) counted between \(92\) an \(97\%\) of common predictions and these last result also reassures the user about the quality of the provided predictions.
\end{itemize}

\begin{table}[h]
  \centering
  \begin{tabular}{llccc}
    \toprule
    \strong{Model}  & \strong{Type} & \strong{Method} & \strong{Relax} & \strong{Hell(YA\_YB)}  \\
    \hline
    \code{outc1}    & \code{OUTCOME}   & \code{SEQUENTIAL}   & $0.0$    & $0.008$   \\
    \code{R\_outc1} & \code{R-OUTCOME} & \code{SEQUENTIAL}   & $0.4$    & $0.085$   \\
    \code{R\_outc2} & \code{R-OUTCOME} & \code{SEQUENTIAL}   & $0.6$    & $0.107$   \\
    \code{R\_outc3} & \code{R-OUTCOME} & \code{OPTIMAL}      & $0.0$    & $0.002$   \\
    \code{R\_outc4} & \code{R-OUTCOME} & \code{OPTIMAL}      & $0.4$    & $0.080$   \\
    \code{R\_outc5} & \code{R-OUTCOME} & \code{OPTIMAL}      & $0.6$    & $0.102$   \\
    \bottomrule
  \end{tabular}
  \caption{Hellinger distances related to the 6 models that used \code{OUTCOME} and \code{R-OUTCOME} algorithms. The values are relatively homogeneous from one model to another and do not indicate strong ditributional divergences (all values are very far from 1). Nevertheless, choosing relaxation parameters higher than $0.4$ can increase the risk of distributional dissimilarities (the criterion moves away from $0.05$ when the relaxation parameter increases).}
   \label{tab:tab9}
\end{table}

\begin{table}[h!]
  \centering
  \begin{tabular}{llcccccc}
    \toprule
    \strong{Model}  & \strong{Type} & \strong{Method} & \strong{Relax} & \strong{Regul} &
    \strong{N} & \strong{mean} & \strong{sd} \\
    \hline
    \code{outc1} & \code{OUTCOME}      & \code{SEQUENTIAL}  & $0.0$ &   -   & 284 & $0.968$ & $0.122$ \\
    \code{R\_outc1} & \code{R-OUTCOME} & \code{SEQUENTIAL}  & $0.4$ &   -   & 284 & $0.950$ & $0.151$ \\
    \code{R\_outc2} & \code{R-OUTCOME} & \code{SEQUENTIAL}  & $0.6$ &   -   & 284 & $0.954$ & $0.145$ \\
    \code{R\_outc3} & \code{R-OUTCOME} & \code{OPTIMAL}     & $0.0$ &   -   & 284 & $0.979$ & $0.100$ \\
    \code{R\_outc4} & \code{R-OUTCOME} & \code{OPTIMAL}     & $0.4$ &   -   & 284 & $0.987$ & $0.080$ \\  
    \code{R\_outc5} & \code{R-OUTCOME} & \code{OPTIMAL}     & $0.6$ &   -   & 284 & $0.983$ & $0.091$  \\ 
    \hline
    \code{outj1}    & \code{JOINT}    &      -              & $0.0$ & $0.0$ & 284 & $0.911$ & $0.116$ \\
    \code{R\_outj1} & \code{R-JOINT}  &      -              & $0.4$ & $0.0$ & 284 & $0.942$ & $0.128$ \\
    \code{R\_outj2} & \code{R-JOINT}  &      -              & $0.6$ & $0.0$ & 284 & $0.953$ & $0.171$ \\
    \code{R\_outj3} & \code{R-JOINT}  &      -              & $0.8$ & $0.0$ & 284 & $0.934$ & $0.199$ \\
    \code{R\_outj4} & \code{R-JOINT}  &      -              & $0.4$ & $0.1$ & 284 & $0.926$ & $0.097$ \\
    \bottomrule
  \end{tabular}
 \caption{Stability of the predictions (\code{min.neigb = 5}). When the \code{R\_outj1} model will be confronted with a same profile of predictors and a same level of \code{apicl\_1999}, more than $94$ times of a hundred ($mean = 0.942$), it will be able to return the same individual prediction for \code{apicl\_2000}. Among the \code{OUTCOME} family of algorithms, \code{R\_outc4} provides here the best stability of prediction while \code{R\_outj2} is the optimal one in the \code{JOINT} family of algorithms.}
 \label{tab:tab10}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{l|ccccc}
  \toprule
    \strong{Model}  & \code{outc1} & \code{R\_outc1} & \code{R\_outc4} & \code{outj1} & \code{R\_outj1}  \\
    \hline  
    \code{R\_outc1} & 0.84 & -    & -    & -    & -       \\
    \code{R\_outc4} & 0.83 & 0.95 & -    & -    & -        \\
    \code{outj1}    & 0.72 & 0.81 & 0.83 & -    & -        \\
    \code{R\_outj1} & 0.83 & 0.91 & 0.94 & 0.84 & -        \\
    \code{R\_outj4} & 0.84 & 0.96 & 0.97 & 0.84 & 0.92     \\
  \bottomrule
  \end{tabular}
  \caption{Ratio of common predictions between two models}
  \label{tab:tab11}
\end{table}

\begin{table}[h!]
\centering
  \begin{tabular}{cccccccccccc}
  \toprule
  \multicolumn{1}{c}{(a)} & \multicolumn{4}{c}{\strong{apicl\_1999}}  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{c}{(b)} & \multicolumn{4}{c}{\strong{apicl\_1999}}\\
\strong{apicl\_2000} & $G1$ & $G2$ & $G3$ & $G4$ &  &  &  \strong{apicl\_2000} & $G1$ & $G2$ & $G3$ & $G4$ \\
\cline{1-5} \cline{8-12} \\
 $[200-600]$  & 91 & 15 &  0 &  0 &  &  &  $[200-600]$    & 91 & 14 &  1 &  0 \\
 $(600-800]$  &  0 & 75 & 90 &  0 &  &  &  $(600-800]$    &  0 & 76 & 89 &  0 \\
 $(800-1000]$ &  0 &  0 &  0 & 91 &  &  &  $(800-1000]$   &  0 &  0 &  0 & 91 \\
 \\
 \bottomrule
  \end{tabular}
  \caption{Confusion matrices in the api35 dataset for the models (a) \code{R\_outc4}  and (b) \code{R\_outj1}}
 \label{tab:tab12}
\end{table}

The confusion matrices related to \texttt{R\_outc4} and \texttt{R\_outj1} are described in Table \ref{tab:tab12} and seems to confirm that the encoding of \texttt{apicl\_2000} in three groups, could simply correspond to the grouping of levels \(G2\) and \(G3\) of the \texttt{api\_cl1999} variable.

Finally, note that the running time of each model took less than 15 seconds with \texttt{R} version \(4.0.3\) for Windows (10 Pro-64bits/ Process Intel \(2.66\) GHz).

\hypertarget{conclusion-and-perspectives}{%
\section{Conclusion and perspectives}\label{conclusion-and-perspectives}}

To our knowledge, \CRANpkg{OTrecod} is the first \texttt{R} package that takes advantage of the optimal transportation theory (Monge 1781) in the context of data fusion and the comparative study of methods described in (Garès and Omer 2022) underlines the promising performances of these approaches.

For more details and examples about the functions, users are invited to consult the \href{https://otrecoding.github.io/OTrecod/}{ARTICLES section of the dedicated website}.

\hypertarget{drawbacks}{%
\section{Drawbacks}\label{drawbacks}}

The functions of \CRANpkg{OTrecod} only apply to a specific data fusion context where there is no overlapping part between the two raw data sources. This package does not deal with record linkage which is already the focus of extensive works (Sariyar and Borg 2010). This package is not adapted when the target variables (outcomes) of the two databases are continuous with an infinite number of values (like weights in grams with decimals for example). Moreover, if the data fusion requires only one shared variable to work, the quality of the fusion depends on the presence of a subset of good shared predictors in the raw databases. Finally, if the function \texttt{OT\_outcome} allows all types of predictors, the current version of the function \texttt{OT\_joint} imposes categorical matching variables only (scale variables are allowed): this restriction should be relaxed in a next version.

\hypertarget{perspectives}{%
\section{Perspectives}\label{perspectives}}

A number of more advanced research still need further investigation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the possibility of extending the OT algorithm for recoding variables to multidimensional frameworks.
\item
  the stability of the algorithm when the matching variables are incomplete and the non-response processes
  are missing at random or not.
\item
  the contribution of calibration techniques in the quality process assessment (Deming and Stephan 1940)
\item
  the creation of a tuning function which defines a grid search to find the optimal combination of parameters
  related to the OT algorithms, could be added in the future versions of the package.
\end{enumerate}

\hypertarget{acknowledgments}{%
\section{Acknowledgments}\label{acknowledgments}}

The authors would especially thank Pierre Navaro (CNRS UMR 6625) for their advices during the implementation of the \CRANpkg{OTrecod} package. This research has received the help from \emph{Region Occitanie} Grant RBIO-2015-14054319 and Mastodons-CNRS Grant.

\newpage

\hypertarget{supplementary-r-code}{%
\section{Supplementary R code}\label{supplementary-r-code}}

\begin{verbatim}
### BASIC R CODE FOR SUMMARY TABLES  8, 9, 10, and 11 ---------

## Validation of each model: Repeat the following R command for each model by
## changing outc1:
verif_outc1   = verif_OT(outc1, group.class = TRUE, ordinal = FALSE, 
                         stab.prob = TRUE, min.neigb = 5)

## Association between Y and Z: Summary Table 8
res.prx = rbind(
           outc1   = verif_outc1$res.prox    , R_outc1 = verif_R_outc1$res.prox, 
           R_outc2 = verif_R_outc2$res.prox  , R_outc3 = verif_R_outc3$res.prox,
           R_outc4 = verif_R_outc4$res.prox  , R_outc5 = verif_R_outc5$res.prox,
           outj1   = verif_outj1$res.prox    , R_outj1 = verif_R_outj1$res.prox,
           R_outj2 = verif_R_outj2$res.prox  , R_outj3 = verif_R_outj3$res.prox,
           R_outj4 = verif_R_outj4$res.prox )
                                
res.prx = data.frame(Model = c("outc1","R_outc2","R_outc3","R_outc4",
                               "R_outc5","R_outc6","outj1", "R_outj1",
                               "R_outj2","R_outj3","R_outj4"), 
                     Type  = c("OUTCOME",rep("R-OUTCOME",5),"JOINT",
                               rep("R-JOINT",4)),
                     Relax = c(0,0.4,0.6,0,0.4,0.6,0,0.4,0.6,0.8,0.4), 
                     Regul = c(rep(0,10),0.1), res.prx)

row.names(res.prx) = NULL; head(res.prx,3)

#    Name      Type Relax Regul   N V_cram rank_cor
#   outc1   OUTCOME   0.0   0.0 362   0.86    0.892
# R_outc1 R-OUTCOME   0.0   0.0 362   0.87    0.911
# R_outc2 R_OUTCOME   0.4   0.0 362   0.93    0.933
#-----

## Hellinger distance: Summary Table 9
res.helld = rbind(
             outc1   = verif_outc1$hell  , R_outc1 = verif_R_outc1$hell,
             R_outc2 = verif_R_outc2$hell, R_outc3 = verif_R_outc3$hell, 
             R_outc4 = verif_R_outc4$hell, R_outc5 = verif_R_outc5$hell, 
             outj1   = verif_outj1$hell  , R_outj1 = verif_R_outj1$hell,
             R_outj2 = verif_R_outj2$hell, R_outj3 = verif_R_outj3$hell,
             R_outj4 = verif_R_outj4$hell )

res.helld = data.frame(res.prx[,1:4], res.helld)
row.names(res.helld) = NULL; res.helld
#-----

## Stability of the prediction: Summary Table 10 
# same R code as for Summary Table 8, changing res.prox by res.stab
#----

## Ratio of common predictions: Table 11

stoc = list(outc1$DATA2_OT$OTpred  , R_outc1$DATA2_OT$OTpred,
            R_outc4$DATA2_OT$OTpred, outj1$DATA2_OT$OTpred  ,
            R_outj1$DATA2_OT$OTpred, R_outj4$DATA2_OT$OTpred)

corpred = matrix(ncol = 6, nrow = 6)
for (i in 1:6){
      for (j in 1:6){
        corpred[i,j] = round(sum(diag(table(stoc[[i]],stoc[[j]])))/362,2)
      }
}; corpred
#-----
\end{verbatim}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-adamek1994fusion}{}}%
Adamek, James C. 1994. {``Fusion: Combining Data from Separate Sources.''} \emph{Marketing Research} 6 (3): 48.

\leavevmode\vadjust pre{\hypertarget{ref-bergsma2013bias}{}}%
Bergsma, Wicher. 2013. {``A Bias-Correction for Cramér's v and Tschuprow's t.''} \emph{Journal of the Korean Statistical Society} 42 (3): 323--28. https://doi.org/\url{https://doi.org/10.1016/j.jkss.2012.10.002}.

\leavevmode\vadjust pre{\hypertarget{ref-breiman2001random}{}}%
Breiman, Leo. 2001. {``Random Forests.''} \emph{Machine Learning} 45 (1): 5--32. \url{https://doi.org/10.1023/A:1010933404324}.

\leavevmode\vadjust pre{\hypertarget{ref-breiman1984classification}{}}%
Breiman, L., J. Friedman, C. J. Stone, and R. A. Olshen. 1984. \emph{Classification and Regression Trees}. Taylor \& Francis. \url{https://books.google.fr/books?id=JwQx-WOmSyQC}.

\leavevmode\vadjust pre{\hypertarget{ref-Castanedo2013}{}}%
Castanedo, Federico. 2013. {``A Review of Data Fusion Techniques.''} \emph{TheScientificWorldJournal} 2013 (January): 704504. \url{https://doi.org/10.1155/2013/704504}.

\leavevmode\vadjust pre{\hypertarget{ref-Cibella2010}{}}%
Cibella, N. 2010. {``How to Choose the Matching Variables, Report WP2, ESS-Net.''} \emph{Statistical Methodology Project on Integration of Surveys and Administrative Data, EUROSTAT}.

\leavevmode\vadjust pre{\hypertarget{ref-cohen1991statistical}{}}%
Cohen, ML. 1991. {``Statistical Matching and Microsimulation Models, Improving Information for Social Policy Decisions, the Use of Microsimulation Modeling, Technical Papers, II.''} \emph{Washington, DC: National Academy}.

\leavevmode\vadjust pre{\hypertarget{ref-Cuturi2013}{}}%
Cuturi, Marco. 2013. {``Sinkhorn Distances: Lightspeed Computation of Optimal Transport.''} In \emph{Advances in Neural Information Processing Systems}, edited by C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger. Vol. 26. Curran Associates, Inc. \url{https://proceedings.neurips.cc/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-d2019package}{}}%
D'Orazio, Marcello. 2022. \emph{StatMatch: Statistical Matching or Data Fusion}. \url{https://CRAN.R-project.org/package=StatMatch}.

\leavevmode\vadjust pre{\hypertarget{ref-d2006statistical}{}}%
D'Orazio, Marcello, Marco Di Zio, and Mauro Scanu. 2006. \emph{Statistical Matching: Theory and Practice}. John Wiley \& Sons.

\leavevmode\vadjust pre{\hypertarget{ref-Das2019}{}}%
Das, Subhadeep, and Dr. Sucheta Tripathy. 2022. \emph{OMICsPCA: An r Package for Quantitative Integration and Analysis of Multiple Omics Assays from Heterogeneous Samples}.

\leavevmode\vadjust pre{\hypertarget{ref-deming1940least}{}}%
Deming, W Edwards, and Frederick F Stephan. 1940. {``On a Least Squares Adjustment of a Sampled Frequency Table When the Expected Marginal Totals Are Known.''} \emph{The Annals of Mathematical Statistics} 11 (4): 427--44.

\leavevmode\vadjust pre{\hypertarget{ref-rohart2017mixomics}{}}%
F, Rohart, Gautier B, Singh A, and Le Cao K-A. 2017. {``mixOmics: An r Package for 'Omics Feature Selection and Multiple Data Integration.''} \emph{PLoS Computational Biology} 13 (11): e1005752. \url{http://www.mixOmics.org}.

\leavevmode\vadjust pre{\hypertarget{ref-farrar1967multicollinearity}{}}%
Farrar, Donald E., and Robert R. Glauber. 1967. {``Multicollinearity in Regression Analysis: The Problem Revisited.''} \emph{The Review of Economics and Statistics} 49 (1): 92--107. \url{http://www.jstor.org/stable/1937887}.

\leavevmode\vadjust pre{\hypertarget{ref-flamary2021}{}}%
Flamary, Rémi, Nicolas Courty, Alexandre Gramfort, Mokhtar Zahdi Alaya, Aurélie Boisbunon, Stanislas Chambon, Laetitia Chapel, et al. 2021. {``{POT : Python Optimal Transport}.''} \emph{{Journal of Machine Learning Research}}, April. \url{https://hal.archives-ouvertes.fr/hal-03264013}.

\leavevmode\vadjust pre{\hypertarget{ref-forrest2004clp}{}}%
Forrest, John, David de la Nuez, and Robin Lougee-Heimer. 2004. {``CLP User Guide.''} \emph{IBM Research}.

\leavevmode\vadjust pre{\hypertarget{ref-gares2019use}{}}%
Garès, Valérie, Chloé Dimeglio, Grégory Guernec, Romain Fantin, Benoit Lepage, Michael R Kosorok, and Nicolas Savy. 2020. {``{On the use of optimal transportation theory to recode variables and application to database merging}.''} \emph{{International Journal of Biostatistics}} 16 (1): article number : 20180106. \url{https://doi.org/10.1515/ijb-2018-0106}.

\leavevmode\vadjust pre{\hypertarget{ref-gares2019regularized}{}}%
Garès, Valérie, and Jérémy Omer. 2022. {``Regularized Optimal Transport of Covariates and Outcomes in Data Recoding.''} \emph{Journal of the American Statistical Association} 117 (537): 320--33. \url{https://doi.org/10.1080/01621459.2020.1775615}.

\leavevmode\vadjust pre{\hypertarget{ref-grajski1986classification}{}}%
Grajski, Kamil A., Leo Breiman, Gonzalo Viana Di Prisco, and Walter J. Freeman. 1986. {``Classification of EEG Spatial Patterns with a Tree-Structured Methodology: CART.''} \emph{IEEE Transactions on Biomedical Engineering} BME-33 (12): 1076--86. \url{https://doi.org/10.1109/TBME.1986.325684}.

\leavevmode\vadjust pre{\hypertarget{ref-hall1997introduction}{}}%
Hall, David, and James Llinas. 1997. {``An Introduction to Multisensor Data Fusion.''} \emph{Proceedings of the IEEE} 85 (February): 6--23. \url{https://doi.org/10.1109/5.554205}.

\leavevmode\vadjust pre{\hypertarget{ref-hastie15a}{}}%
Hastie, Trevor, and Rahul Mazumder. 2021. \emph{softImpute: Matrix Completion via Iterative Soft-Thresholded SVD}. \url{https://CRAN.R-project.org/package=softImpute}.

\leavevmode\vadjust pre{\hypertarget{ref-Hernandez-Ferrer2017}{}}%
Hernandez-Ferrer, Carles, Carlos Ruiz-Arenas, Alba Beltran-Gomila, and Juan R. González. 2017. {``MultiDataSet: An r Package for Encapsulating Multiple Data Sets with Application to Omic Data Integration.''} \emph{BMC Bioinformatics} 18 (1): 36. \url{https://doi.org/10.1186/s12859-016-1455-1}.

\leavevmode\vadjust pre{\hypertarget{ref-hitch}{}}%
Hitchcock, F. L. 1941. {``The Distribution of a Product from Several Sources to Numerous Localities.''} \emph{Journal of Mathematics and Physics / Massachusetts Institute of Technology.} 20: 224--30. \url{https://doi.org/10.1002/sapm1941201224}.

\leavevmode\vadjust pre{\hypertarget{ref-RF3}{}}%
Hothorn, Torsten, Peter Bühlmann, Sandrine Dudoit, Annette Molinaro, and Mark J. Van Der Laan. 2005. {``{Survival ensembles}.''} \emph{Biostatistics} 7 (3): 355--73. \url{https://doi.org/10.1093/biostatistics/kxj011}.

\leavevmode\vadjust pre{\hypertarget{ref-RF1}{}}%
Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. {``Unbiased Recursive Partitioning: A Conditional Inference Framework.''} \emph{Journal of Computational and Graphical Statistics} 15 (3): 651--74. \url{https://doi.org/10.1198/106186006X133933}.

\leavevmode\vadjust pre{\hypertarget{ref-JSSv070i01}{}}%
Josse, Julie, and François Husson. 2016. {``{missMDA}: A Package for Handling Missing Values in Multivariate Data Analysis.''} \emph{Journal of Statistical Software} 70 (1): 1--31. \url{https://doi.org/10.18637/jss.v070.i01}.

\leavevmode\vadjust pre{\hypertarget{ref-kadane1978some}{}}%
Kadane, Joseph B. 2001. {``Some Statistical Problems in Merging Data Files.''} \emph{Journal of Official Statistics} 17: 423--33.

\leavevmode\vadjust pre{\hypertarget{ref-Kan1942}{}}%
Kantorovich, L. 1942. {``{On the transfer of masses}.''} \emph{Doklady Akademii Nauk SSSR} 37: 7--8.

\leavevmode\vadjust pre{\hypertarget{ref-klein2004sensor}{}}%
Klein, Lawrence A. 2004. \emph{Sensor and Data Fusion: A Tool for Information Assessment and Decision Making}. Vol. 138. SPIE press.

\leavevmode\vadjust pre{\hypertarget{ref-kotz2004continuous}{}}%
Kotz, Samuel, Narayanaswamy Balakrishnan, and Norman Johnson. 2000. \emph{Continuous Multivariate Distributions: Models and Applications, Volume 1, Second Edition}. Vol. 1. Wiley Series in Probability and Mathematical Statistics. John Wiley \& Sons. \url{https://doi.org/10.1002/0471722065}.

\leavevmode\vadjust pre{\hypertarget{ref-Liaw2002}{}}%
Liaw, Andy, and Matthew Wiener. 2002. {``Classification and Regression by randomForest.''} \emph{R News} 2 (3): 18--22. \url{https://CRAN.R-project.org/doc/Rnews/}.

\leavevmode\vadjust pre{\hypertarget{ref-liese2007statistical}{}}%
Liese, Friedrich, and Klaus -J. Miescke. 2008. {``Statistical Decision Theory.''} In \emph{Statistical Decision Theory: Estimation, Testing, and Selection}, 1--52. New York, NY: Springer New York. \url{https://doi.org/10.1007/978-0-387-73194-0_3}.

\leavevmode\vadjust pre{\hypertarget{ref-little2002statistical}{}}%
Little, R. J. A., and D. B. Rubin. 2019. \emph{Statistical Analysis with Missing Data, Third Edition}. Wiley Series in Probability and Mathematical Statistics. Probability and Mathematical Statistics. Wiley.

\leavevmode\vadjust pre{\hypertarget{ref-makhorin2011gnu}{}}%
Makhorin, Andrew. 2011. {``GNU Linear Programming Kit, Reference Manual.''} \emph{Free Software Foundation} 4.

\leavevmode\vadjust pre{\hypertarget{ref-Mayer2019}{}}%
Mayer, Imke, Aude Sportisse, Julie Josse, Nicholas Tierney, and Nathalie Vialaneix. 2019. {``R-Miss-Tastic: A Unified Platform for Missing Values Methods and Workflows.''} arXiv. \url{https://doi.org/10.48550/ARXIV.1908.04822}.

\leavevmode\vadjust pre{\hypertarget{ref-monge1781memoire}{}}%
Monge, G. 1781. {``{Mémoire sur la Théorie des Déblais et des Remblais}.''} \emph{Histoire de l'Académie Royale Des Sciences de Paris}, 666--704.

\leavevmode\vadjust pre{\hypertarget{ref-Muzellec2020MissingDI}{}}%
Muzellec, Boris, Julie Josse, Claire Boyer, and Marco Cuturi. 2020. {``Missing Data Imputation Using Optimal Transport.''} In \emph{ICML}.

\leavevmode\vadjust pre{\hypertarget{ref-paass1986statistical}{}}%
Paass, Gerhard. 1986. {``Statistical Match: Evaluation of Existing Procedures and Improvements by Using Additional Information.''} \emph{Microanalytic Simulation Models to Support Social and Financial Policy}, 401--20.

\leavevmode\vadjust pre{\hypertarget{ref-pages2004analyse}{}}%
Pagès, J. 2002. {``Analyse Factorielle Multiple Appliquée Aux Variables Qualitatives Et Aux Données Mixtes.''} \emph{Revue de Statistique Appliquée} 50 (4): 5--37. \url{http://eudml.org/doc/106525}.

\leavevmode\vadjust pre{\hypertarget{ref-rassler2002statistical}{}}%
Rässler, Susanne. 2002. {``Frequentist Theory of Statistical Matching.''} In \emph{Statistical Matching: A Frequentist Theory, Practical Applications, and Alternative Bayesian Approaches}, 15--43. New York, NY: Springer New York. \url{https://doi.org/10.1007/978-1-4613-0053-3_2}.

\leavevmode\vadjust pre{\hypertarget{ref-rodgers1984evaluation}{}}%
Rodgers, Willard L. 1984. {``An Evaluation of Statistical Matching.''} \emph{Journal of Business \& Economic Statistics} 2 (1): 91--102. \url{http://www.jstor.org/stable/1391358}.

\leavevmode\vadjust pre{\hypertarget{ref-rubin1986statistical}{}}%
Rubin, Donald B. 1986. {``Statistical Matching Using File Concatenation with Adjusted Weights and Multiple Imputations.''} \emph{Journal of Business \& Economic Statistics} 4 (1): 87--94. \url{http://www.jstor.org/stable/1391390}.

\leavevmode\vadjust pre{\hypertarget{ref-sariyar2010recordlinkage}{}}%
Sariyar, Murat, and Andreas Borg. 2010. {``The RecordLinkage Package: Detecting Errors in Data.''} \emph{The R Journal} 2 (2): 61--67.

\leavevmode\vadjust pre{\hypertarget{ref-scanu2010recommendations}{}}%
Scanu, M. 2010. {``Recommendations on Statistical Matching, Report WP2, ESS-Net.''} \emph{Statistical Methodology Project on Integration of Surveys and Administrative Data}.

\leavevmode\vadjust pre{\hypertarget{ref-Schuhmacher2020}{}}%
Schuhmacher, Dominic, Björn Bähre, Carsten Gottschlich, Valentin Hartmann, Florian Heinemann, and Bernhard Schmitzer. 2022. \emph{{transport}: Computation of Optimal Transport Plans and Wasserstein Distances}. \url{https://cran.r-project.org/package=transport}.

\leavevmode\vadjust pre{\hypertarget{ref-missF2022}{}}%
Stekhoven, Daniel J. 2022. \emph{missForest: Nonparametric Missing Value Imputation Using Random Forest}.

\leavevmode\vadjust pre{\hypertarget{ref-Stekhoven2012}{}}%
Stekhoven, Daniel J., and Peter Bühlmann. 2012. {``MissForest--Non-Parametric Missing Value Imputation for Mixed-Type Data.''} \emph{Bioinformatics (Oxford, England)} 28 (January): 112--18.

\leavevmode\vadjust pre{\hypertarget{ref-RF5}{}}%
Strobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. 2008. {``Conditional Variable Importance for Random Forests.''} \emph{BMC Bioinformatics} 9 (1): 307. \url{https://doi.org/10.1186/1471-2105-9-307}.

\leavevmode\vadjust pre{\hypertarget{ref-RF4}{}}%
Strobl, Carolin, Anne-Laure Boulesteix, Achim Zeileis, and Torsten Hothorn. 2007. {``Bias in Random Forest Variable Importance Measures: Illustrations, Sources and a Solution.''} \emph{BMC Bioinformatics} 8 (1): 25. \url{https://doi.org/10.1186/1471-2105-8-25}.

\leavevmode\vadjust pre{\hypertarget{ref-RJ-2009-013}{}}%
Strobl, Carolin, Torsten Hothorn, and Achim Zeileis. 2009. {``{Party on!}''} \emph{{The R Journal}} 1 (2): 14--17. \url{https://doi.org/10.32614/RJ-2009-013}.

\leavevmode\vadjust pre{\hypertarget{ref-epubwu5858}{}}%
Theußl, Stefan, Florian Schwendinger, and Kurt Hornik. 2017. {``ROI: The {R} Optimization Infrastructure Package.''} Research Report Series / Department of Statistics and Mathematics 133. Vienna: WU Vienna University of Economics; Business. \url{http://epub.wu.ac.at/5858/}.

\leavevmode\vadjust pre{\hypertarget{ref-theussl2020roi}{}}%
---------. 2020. {``{ROI}: An Extensible {R} Optimization Infrastructure.''} \emph{Journal of Statistical Software} 94 (15): 1--64. \url{https://doi.org/10.18637/jss.v094.i15}.

\leavevmode\vadjust pre{\hypertarget{ref-Buuren2011}{}}%
van Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. {``{mice}: Multivariate Imputation by Chained Equations in {R}.''} \emph{Journal of Statistical Software} 45 (3): 1--67. \url{https://doi.org/10.18637/jss.v045.i03}.

\leavevmode\vadjust pre{\hypertarget{ref-van1995python}{}}%
Van Rossum, Guido, and Fred L Drake Jr. 1995. \emph{Python Reference Manual}. Centrum voor Wiskunde en Informatica Amsterdam.

\leavevmode\vadjust pre{\hypertarget{ref-Vantaggi08}{}}%
Vantaggi, Barbara. 2008. {``Statistical Matching of Multiple Sources: A Look Through Coherence.''} \emph{Int. J. Approx. Reasoning} 49 (November): 701--11. \url{https://doi.org/10.1016/j.ijar.2008.07.005}.

\leavevmode\vadjust pre{\hypertarget{ref-dev2022}{}}%
Wickham, Hadley, Jim Hester, Winston Chang, and Jennifer Bryan. 2022. \emph{Devtools: Tools to Make Developing {R} Packages Easier}. \url{https://CRAN.R-project.org/package=devtools}.

\leavevmode\vadjust pre{\hypertarget{ref-RF2}{}}%
Zeileis, Achim, and Torsten Hothorn. 2008. {``Model-Based Recursive Partitioning.''} \emph{Journal of Computational and Graphical Statistics - J COMPUT GRAPH STAT} 17 (June): 492--514. \url{https://doi.org/10.1198/106186008X319331}.

\leavevmode\vadjust pre{\hypertarget{ref-Zhu2019}{}}%
Zhu, Ziwei, Tengyao Wang, and Richard J. Samworth. 2019. {``High-Dimensional Principal Component Analysis with Heterogeneous Missingness.''} arXiv. \url{https://doi.org/10.48550/ARXIV.1906.12125}.

\end{CSLReferences}

\bibliography{OTrecod-article.bib}

\address{%
Gregory Guernec\\
Université de Toulouse, INSERM, UPS\\%
INSERM, CERPOP, UMR 1295\\ Toulouse, France\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-0002-0668-8606}{0000-0002-0668-8606}}\\%
\href{mailto:gregory.guernec@inserm.fr}{\nolinkurl{gregory.guernec@inserm.fr}}%
}

\address{%
Valerie Gares\\
INSA, Université de Rennes\\%
CNRS, IRMAR, UMR 6625\\ Rennes, France\\
%
\url{http://vgares.perso.math.cnrs.fr/contact.html}\\%
%
\href{mailto:valerie.gares@insa-rennes.fr}{\nolinkurl{valerie.gares@insa-rennes.fr}}%
}

\address{%
Jeremy Omer\\
INSA, Université de Rennes\\%
CNRS, IRMAR, UMR 6625\\ Rennes, France\\
%
\url{https://jeremyomer.wixsite.com/recherche}\\%
%
\href{mailto:jeremy.omer@insa-rennes.fr}{\nolinkurl{jeremy.omer@insa-rennes.fr}}%
}

\address{%
Philippe Saint-Pierre\\
IMT, Université Paul Sabatier, Toulouse\\%
CNRS UMR 5219\\ Toulouse, France\\
%
\url{https://perso.math.univ-toulouse.fr/psaintpi/}\\%
%
\href{mailto:philippe.saint-pierre@univ-tlse3.fr}{\nolinkurl{philippe.saint-pierre@univ-tlse3.fr}}%
}

\address{%
Nicolas Savy\\
IMT, Université Paul Sabatier, Toulouse\\%
CNRS UMR 5219\\ Toulouse, France\\
%
\url{https://perso.math.univ-toulouse.fr/savy/}\\%
%
\href{mailto:nicolas.savy@univ-tlse3.fr}{\nolinkurl{nicolas.savy@univ-tlse3.fr}}%
}
