# Generated by `rjournal_article()` using `knitr::purl()`: do not edit by hand
# Please edit hopkins.Rmd to modify this file

## ----setup,echo=FALSE,include=FALSE-------------------------------------------
library(pacman)
pacman::p_load(hopkins, knitr, spatstat.data)


## ----simulation, echo=FALSE, message=FALSE, fig.cap="Comparison of the theoretical distribution (black) and empirical distributions of Hopkins statistic calculated with exponents $d=1$ (red) and $d=3$ (blue).", fig.width=6, fig.height=4, fig.align="center"----
library(hopkins)
D <- 3 # dimension of data, columns(X)
N <- 100 # number of events, rows(X)
M <- 10 # number of events sampled
B <- 1000 # number of simulations

set.seed(12345)
hop1 <- hop2 <- NULL
for(ii in 1:B){
  X <- matrix(runif(N*D), ncol=D)
  hop1[ii] <- hopkins::hopkins(X, m=M, d=1)
  hop2[ii] <- hopkins::hopkins(X, m=M, d=3)
}
# Now the plots
plot(density(hop1), col="red", xlim=c(0,1), main="", xlab="")
lines(density(hop2), col="blue")
xv <- seq(0,1,length=100)
lines(xv, dbeta( xv, M, M) , col="black", lwd=2)
legend("topleft",
       c("Hopkins stat (d=1)", "Hopkins stat (d=3)", "Beta(10,10)"),
       text.col=c("red","blue","black")
       )


## ---- message=FALSE, echo=FALSE-----------------------------------------------
show_hopkins <- function (X, m, d=ncol(X), U=NULL, k=1, method="simple") {
  if(ncol(X) > 2) stop("show_hopkins() only accepts 2D data")
  
  if (!(is.matrix(X)) & !(is.data.frame(X))) 
    stop("X must be data.frame or matrix")

  if (m >= nrow(X)) 
    stop("m must be no larger than num of samples")

  if(missing(U)) {
    # U is a matrix of column-wise uniform values sampled from the space of X
    colmin <- apply(X, 2, min)
    colmax <- apply(X, 2, max)    
    U <- matrix(0, ncol = ncol(X), nrow = m)
    for (i in 1:ncol(X)) {
      U[, i] <- runif(m, min = colmin[i], max = colmax[i])
    }
  } else {
    # The user has provided the uniform values.
  }
  
  # Random sample of rows in X
  j <- sample(1:nrow(X), m)
  W <- X[j, , drop=FALSE]   # Need 'drop' in case X is single-column

  if(method=="simple") {
    # distance between each row of W and each row of X
    dwx <- as.matrix(pdist::pdist(W,X))
    # Caution: W[i,] is the same point as X[k[i],] and the distance between them is 0,
    # but we do not want to consider that when calculating the minimum distance
    # between W[i,] and X, so change the distance from 0 to Inf
    for(i in 1:m) dwx[i,j[i]] <- Inf
    
    plot(X, main="", xlab="", ylab="", col="gray20", cex=1.5)
    # Show nearest events in X
    points(W, col="blue", pch="W", cex=.5)
    # From W, nearest points in X
    # Run the line setting some distances to Inf. Then:
    x_near_w <- apply(dwx, 1, which.min)
    x0 <- W[,1]; y0 <- W[,2]
    x1 <- X[x_near_w, 1] ; y1 <- X[x_near_w, 2]
    arrows(x0, y0, x1, y1, col="skyblue", length=0.1)
    w_distances <- sqrt((x1-x0)^2 + (y1-y0)^2)
    w_mids <- cbind((x0+x1)/2, (y0+y1)/2)
    text(w_mids+.02, labels=round(w_distances,3), cex=0.8, col="blue")
    
  
    # distance from each row of W to the NEAREST row of X
    dwx <- apply(dwx, 1, min)
    
    # distance between each row of U and each row of X
    dux <- as.matrix(pdist::pdist(U,X)) # rows of dux refer to U, cols refer to X
    
    # From U, nearest points in X
    points(U, col="red", pch="U", cex=.5)
    x_near_u <- apply(dux, 1, which.min)
    x0 <- U[,1]; y0 <- U[,2]
    x1 <- X[x_near_u, 1] ; y1 <- X[x_near_u, 2]
    arrows(x0, y0, x1, y1, col="pink", length=0.1)
    u_distances <- sqrt((x1-x0)^2 + (y1-y0)^2)
    u_mids <- cbind((x0+x1)/2, (y0+y1)/2)
    text(u_mids+.02, labels=round(u_distances,3), cex=0.8, col="red")
    
    # distance from each row of U to the NEAREST row of X
    dux <- apply(dux, 1, min)
  } else { # method="torus"
    #browser()
    rng <- t(apply(X,2,range))

    # Note: Since W is a sample from X, the 1st nearest point in X will
    # always be the same point with distance 0, so add 1 to k.
    nearw <- donut::nnt(X, W, k=k+1, torus=1:ncol(W), ranges=rng )
    dwx <- nearw$nn.dists[,k+1]

    # For U, find the 1st nearest point in X, k=1.
    nearu <- donut::nnt(X, U, k=k, torus=1:ncol(W), ranges=rng )
    dux <- nearu$nn.dists[,k]
  }


  title( main=paste0(substitute(X),
                     " data: H = ",
                     round( sum(dux^d) / sum( dux^d + dwx^d ) , 2) ))

  return(  sum(dux^d) / sum( dux^d + dwx^d ) )
}


## ----cells, echo=FALSE, message=FALSE, fig.width=5, fig.height=5, fig.align="center", fig.cap="Plot of the events (black circles), randomly-selected events (blue W) with nearest neighbors (blue arrows), and random points (red U) with nearest neighbors (red arrows)."----
library(hopkins)
library(spatstat.data)
data(cells) # Systematic data
cells = data.frame(x=cells$x, y=cells$y)
set.seed(17) #17
h1 <- show_hopkins(cells, m=3)


## -----------------------------------------------------------------------------
(.046^2 + .081^2 + .021^2) / 
  ( (.046^2 + .081^2 + .021^2) + (.152^2 + .14^2 + .139^2) ) # 0.13


## -----------------------------------------------------------------------------
set.seed(17)
hopkins(cells, m=3)


## ---- echo=FALSE, message=FALSE-----------------------------------------------
# cells systematic
set.seed(42)
hopstat <- NULL
for(ii in 1:100) hopstat[ii] <- hopkins::hopkins(cells, 5)
#round( c(mean(hopstat), sd(hopstat) ), 2) # .21 .06 Gastner .21 .05


## ----japanesepines, echo=FALSE, fig.width=5, fig.height=5, fig.align="center", fig.cap="Plot of the japanesepines data (black circles), randomly-selected events (blue W), nearest neighbors (blue arrows) and random points (red U) and nearest neighbors (red arrows)."----
data(japanesepines) # Random data
japanesepines <- data.frame(x=japanesepines$x, y=japanesepines$y)
set.seed(28)
h2 <- show_hopkins(japanesepines, m=3)


## -----------------------------------------------------------------------------
(.023^2+.076^2+.07^2) /
  ((.023^2+.076^2+.07^2) + (.104^2+.1^2+.058^2)) # .32


## ---- echo=FALSE, message=FALSE-----------------------------------------------
# japanesepines random
set.seed(42)
hopstat <- NULL
for(ii in 1:100) hopstat[ii] <- hopkins::hopkins(japanesepines, 7)
#round( c(mean(hopstat), sd(hopstat) ), 2) # .48 .11 Gastner .50 .10


## ----redwood, fig.cap="Plot of the redwood  data (black circles), randomly-selected events (blue W), nearest neighbors (blue arrows) and random points (red U) and nearest neighbors (red arrows).", fig.width=5, fig.height=5, fig.align="center", echo=FALSE----
data(redwood) # Clustered data
redwood = data.frame(x=redwood$x, y=redwood$y)
set.seed(44)
h3 <- show_hopkins(redwood, m=3)


## -----------------------------------------------------------------------------
(.085^2+.078^2+.158^2) /
  ((.085^2+.078^2+.158^2) + (.028^2+.028^2+.12^2)) # .71


## ---- echo=FALSE, message=0---------------------------------------------------
# redwood clustered data
set.seed(42)
hopstat <- NULL
for(ii in 1:100) hopstat[ii] <- hopkins::hopkins(redwood, 6)
#round( c(mean(hopstat), sd(hopstat) ), 2) # .79 .13. Gastner .84 .05


## ----adolfsson, echo=FALSE----------------------------------------------------
prop_clusterable <- function(X, B=100){
  M <- floor(nrow(X) * .1)
  d1 <- dd <- d1s <- dds <- NULL
  for(ii in 1:B){
    d1[ii] <- hopkins::hopkins(X, m=M, d=1, method="simple")
    dd[ii] <- hopkins::hopkins(X, m=M, method="simple")
    # Appears that Adolfsson did NOT scale the data
    #d1s[ii] <- hopkins::hopkins(scale(X), m=m, d=1)
    #dds[ii] <- hopkins::hopkins(scale(X), m=m)
  }
  crit <- qbeta(.95, M, M)
  return(c( round( sum(d1 > crit) / length(d1), 2),
           #round( sum(d1s > crit) / length(dd), 2),
           round( sum(dd > crit) / length(dd), 2)
           #round( sum(dds > crit) / length(dd), 2)
           ) )
}
set.seed(1)
tab <- rbind(
prop_clusterable(faithful),
prop_clusterable(iris[,-5]),
prop_clusterable(as.data.frame(rivers))  ,
prop_clusterable(swiss),
prop_clusterable(attitude),
prop_clusterable(cars),
prop_clusterable(trees),
prop_clusterable(USJudgeRatings),
prop_clusterable(USArrests) # 12 min for B=1, method="torus"
)
tab <- data.frame(dataset=c("faithful","iris","rivers","swiss","attitude",
                         "cars","trees","USJudgeRatings","USArrests"),
                  n=c(272,150,141,47,30,50,31,43,50),
                  D=c(2,5,1,6,7,2,3,12,4),
                  Adolfsson=c(1.00, 1.00, .92,.41, .00, .19, .18, .69,.01),
                  Hopkins1=tab[,1],
                  HopkinsD=tab[,2])


## ----tabinteractive, echo=FALSE, eval = knitr::is_html_output()---------------
#> knitr::kable(tab, format = "html", caption = "R dataset name, number of columns and rows, and proportion of times that Hopkins statistic is significant.")


## ----tabstatic, echo=FALSE, eval = knitr::is_latex_output()-------------------
library(magrittr)
knitr::kable(tab, format = "latex", caption = "R dataset name, size, and proportion of times the Hopkins statistic is significant.") %>% 
  kableExtra::kable_styling(font_size = 7)


## ----swiss, echo=FALSE, fig.cap="Pairwise scatterplots of the swiss data", fig.width=5, fig.height=5, fig.align="center"----
pairs(swiss)


## ----trees, echo=FALSE, fig.cap="Pairwise scatterplots of the trees data", fig.width=3, fig.height=3, fig.align="center"----
pairs(trees)


## ----toruscalc, eval=FALSE, cache=TRUE, echo=FALSE, fig.width=6, fig.height=5, fig.align="center"----
#> library(hopkins)
#> library(doParallel)
#> ncores <- detectCores()
#> cl <- makeCluster(ncores-1)
#> registerDoParallel(cl)
#> 
#> # 45 sec D=5
#> N <- 100 # number of events, rows(X)
#> D <- 5 # dimension of data, columns(X)
#> M <- 10 # number of events sampled
#> B <- 1000 # number of simulations
#> K <- 1 # number of nearest neighbors
#> 
#> # parallel computation
#> set.seed(1)
#> #system.time(
#>   out <- foreach(ii=1:B, .combine=rbind) %dopar% {
#>     X <- matrix(runif(N*D), ncol=D)
#>     c(hopkins::hopkins(X, m=M, d=D, method="simple"),
#>       hopkins::hopkins(X, m=M, d=D, k=K, method="torus"))
#>   }
#> #)
#> stopCluster(cl)
#> save(out, file="hopkins_simple_vs_torus.Rdata")


## ----torus, echo=FALSE, fig.align="center", fig.height=5, fig.width=6, fig.cap="Comparison of the theoretical Beta(10,10) distribution (black) and empirical distributions of 1000 Hopkins statistics calculated with simple geometry (blue) and torus geometry (green)."----
load(file="hopkins_simple_vs_torus.Rdata")
hop1 <- out[,1] ; hop2 <- out[,2]
# Now the plots
#windows(width=4, height=6)
#png(file="hopkins_simple_vs_torus.png", width=600, height=350)
plot(density(hop1), col="blue", xlim=c(0,1), main="", xlab="", ylim=c(0,5), lwd=2)
lines(density(hop2), col="forestgreen", lwd=2)
xv <- seq(0,1,length=100)
lines(xv, dbeta( xv, M, M) , col="black", lwd=1)
legend("topleft",
       c(paste0("Hopkins simple (D=",D,")"),
         paste0("Hopkins torus (D=",D,")"),
         paste0("Beta(",M,",",M,")")) ,
         text.col=c("blue","forestgreen","black")
       )


## ----frame, echo=FALSE, message=FALSE, fig.width=6, fig.height=4, fig.align="center", fig.cap="Random events in a unit square (left) and unit-diameter circle (right)."----
library(purrr)
set.seed(1)
X1 <- matrix(runif(500*2), ncol=2)
dist <- sqrt((X1[,1]-0.5)^2 + (X1[,2]-0.5)^2)  # distance from (0.5, 0.5)
X2 <- X1[dist < 0.5,]
set.seed(1)
probs1 <- map_dbl(1:100, .f= ~ hopkins::hopkins(X1, m=10)) %>% quantile(probs=c(1:3/4))
probs2 <- map_dbl(1:100, .f= ~ hopkins::hopkins(X2, m=10)) %>% quantile(probs=c(1:3/4))
op <- par(mfrow=c(1,2), pty="s")
plot(X1, xlim=c(0,1), ylim=c(0,1), xlab="", ylab="") # Square
title( paste0("H: ", round(probs1[2],2)))
plot(X2, xlim=c(0,1), ylim=c(0,1), xlab="", ylab="") # Circular
title( paste0("H: ", round(probs2[2],2)))
par(op)

