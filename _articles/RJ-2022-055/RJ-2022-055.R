# Generated by `rjournal_pdf_article()` using `knitr::purl()`: do not edit by hand
# Please edit RJ-2022-055.Rmd to modify this file

## ----setup,echo=FALSE,include=FALSE-------------------------------------------
library(pacman)
pacman::p_load(hopkins, knitr, spatstat.data)
knitr::opts_chunk$set(
  fig.retina = 5
)


## ----simulation, echo=FALSE, message=FALSE, fig.cap="Results of a simulation study of the distribution of Hopkins statistic. The red and blue lines are the empirical density curves of 1000 Hopkins statistics calculated with exponents $d=1$ (red) and $d=3$ (blue). The black line is the theoretical distribution of the Hopkins statistic. The red line is very far away from the black line and shows that calculating Hopkins statistic with exponent $d=1$ is incorrect.", fig.width=6, fig.height=4, fig.align="center", out.width = "90%"----
library(hopkins)
D <- 3 # dimension of data, columns(X)
N <- 100 # number of events, rows(X)
M <- 10 # number of events sampled
B <- 1000 # number of simulations

set.seed(12345)
hop1 <- hop2 <- NULL
for(ii in 1:B){
  X <- matrix(runif(N*D), ncol=D)
  hop1[ii] <- hopkins::hopkins(X, m=M, d=1)
  hop2[ii] <- hopkins::hopkins(X, m=M, d=3)
}
# Now the plots
plot(density(hop1), col="red", xlim=c(0,1), main="", xlab="")
lines(density(hop2), col="blue")
xv <- seq(0,1,length=100)
lines(xv, dbeta( xv, M, M) , col="black", lwd=2)
legend("topleft",
       c("Hopkins stat (d=1)", "Hopkins stat (d=3)", "Beta(10,10)"),
       text.col=c("red","blue","black")
       )


## ---- message=FALSE, echo=FALSE-----------------------------------------------
show_hopkins <- function (X, m, d=ncol(X), U=NULL, k=1, method="simple") {
  if(ncol(X) > 2) stop("show_hopkins() only accepts 2D data")
  
  if (!(is.matrix(X)) & !(is.data.frame(X))) 
    stop("X must be data.frame or matrix")

  if (m >= nrow(X)) 
    stop("m must be no larger than num of samples")

  if(missing(U)) {
    # U is a matrix of column-wise uniform values sampled from the space of X
    colmin <- apply(X, 2, min)
    colmax <- apply(X, 2, max)    
    U <- matrix(0, ncol = ncol(X), nrow = m)
    for (i in 1:ncol(X)) {
      U[, i] <- runif(m, min = colmin[i], max = colmax[i])
    }
  } else {
    # The user has provided the uniform values.
  }
  
  # Random sample of rows in X
  j <- sample(1:nrow(X), m)
  W <- X[j, , drop=FALSE]   # Need 'drop' in case X is single-column

  if(method=="simple") {
    # distance between each row of W and each row of X
    dwx <- as.matrix(pdist::pdist(W,X))
    # Caution: W[i,] is the same point as X[k[i],] and the distance between them is 0,
    # but we do not want to consider that when calculating the minimum distance
    # between W[i,] and X, so change the distance from 0 to Inf
    for(i in 1:m) dwx[i,j[i]] <- Inf
    
    plot(X, main="", xlab="", ylab="", col="gray20", cex=1.5)
    # Show nearest events in X
    points(W, col="blue", pch="W", cex=.5)
    # From W, nearest points in X
    # Run the line setting some distances to Inf. Then:
    x_near_w <- apply(dwx, 1, which.min)
    x0 <- W[,1]; y0 <- W[,2]
    x1 <- X[x_near_w, 1] ; y1 <- X[x_near_w, 2]
    arrows(x0, y0, x1, y1, col="skyblue", length=0.1)
    w_distances <- sqrt((x1-x0)^2 + (y1-y0)^2)
    w_mids <- cbind((x0+x1)/2, (y0+y1)/2)
    text(w_mids+.02, labels=round(w_distances,3), adj=0, cex=0.8, col="blue")
    
  
    # distance from each row of W to the NEAREST row of X
    dwx <- apply(dwx, 1, min)
    
    # distance between each row of U and each row of X
    dux <- as.matrix(pdist::pdist(U,X)) # rows of dux refer to U, cols refer to X
    
    # From U, nearest points in X
    points(U, col="red", pch="U", cex=.5)
    x_near_u <- apply(dux, 1, which.min)
    x0 <- U[,1]; y0 <- U[,2]
    x1 <- X[x_near_u, 1] ; y1 <- X[x_near_u, 2]
    arrows(x0, y0, x1, y1, col="pink", length=0.1)
    u_distances <- sqrt((x1-x0)^2 + (y1-y0)^2)
    u_mids <- cbind((x0+x1)/2, (y0+y1)/2)
    text(u_mids+.02, labels=round(u_distances,3), cex=0.8, col="red")
    
    # distance from each row of U to the NEAREST row of X
    dux <- apply(dux, 1, min)
  } else { # method="torus"
    #browser()
    rng <- t(apply(X,2,range))

    # Note: Since W is a sample from X, the 1st nearest point in X will
    # always be the same point with distance 0, so add 1 to k.
    nearw <- donut::nnt(X, W, k=k+1, torus=1:ncol(W), ranges=rng )
    dwx <- nearw$nn.dists[,k+1]

    # For U, find the 1st nearest point in X, k=1.
    nearu <- donut::nnt(X, U, k=k, torus=1:ncol(W), ranges=rng )
    dux <- nearu$nn.dists[,k]
  }


  title( main=paste0(substitute(X),
                     " data: H = ",
                     round( sum(dux^d) / sum( dux^d + dwx^d ) , 2) ))

  return(  sum(dux^d) / sum( dux^d + dwx^d ) )
}


## ----cells, echo=FALSE, message=FALSE, fig.width=5, fig.height=5, fig.align="center", fig.cap="An example of how Hopkins statistic is calculated with systematically-spaced data. The black circles are the events of the `cells` data. Each blue `W` represents a randomly-chosen event. Each blue arrow points from a `W` to the nearest-neighboring event. Each red `U` is a new, randomly-generated point. Each red arrow points from a `U` to the nearest-neighboring event. The numbers are the length of the arrows. In systematically-spaced data, red arrows tend to be shorter than blue arrows.", out.width = "60%"----
library(hopkins)
library(spatstat.data)
data(cells) # Systematic data
cells = data.frame(x=cells$x, y=cells$y)
set.seed(17) #17
h1 <- show_hopkins(cells, m=3)


## -----------------------------------------------------------------------------
(.046^2 + .081^2 + .021^2) / 
  ( (.046^2 + .081^2 + .021^2) + (.152^2 + .14^2 + .139^2) )


## -----------------------------------------------------------------------------
set.seed(17)
hopkins(cells, m=3)


## ---- echo=FALSE, message=FALSE-----------------------------------------------
# cells systematic
set.seed(42)
hopstat <- NULL
for(ii in 1:100) hopstat[ii] <- hopkins::hopkins(cells, 5)
#round( c(mean(hopstat), sd(hopstat) ), 2) # .21 .06 Gastner .21 .05


## ----japanesepines, echo=FALSE, fig.width=5, fig.height=5, fig.align="center", fig.cap="An example of how Hopkins statistic is calculated with randomly-spaced data. The black circles are the events of the `japanesepines` data. Each blue `W` represents a randomly-chosen event. Each blue arrow points from a `W` to the nearest-neighboring event. Each red `U` is a new, randomly-generated point. Each red arrow points from a `U` to the nearest-neighboring event. The numbers are the length of the arrows. In randomly-spaced data, red arrows tend to be similar in length to blue arrows.", out.width = "60%"----
data(japanesepines) # Random data
japanesepines <- data.frame(x=japanesepines$x, y=japanesepines$y)
set.seed(28)
h2 <- show_hopkins(japanesepines, m=3)


## -----------------------------------------------------------------------------
(.023^2+.076^2+.07^2) /
  ((.023^2+.076^2+.07^2) + (.104^2+.1^2+.058^2))


## ---- echo=FALSE, message=FALSE-----------------------------------------------
# japanesepines random
set.seed(42)
hopstat <- NULL
for(ii in 1:100) hopstat[ii] <- hopkins::hopkins(japanesepines, 7)
#round( c(mean(hopstat), sd(hopstat) ), 2) # .48 .11 Gastner .50 .10


## ----redwood, fig.cap="An example of how Hopkins statistic is calculated with clustered data. The black circles are the events of the `redwood` data. Each blue `W` represents a randomly-chosen event. Each blue arrow points from a `W` to the nearest-neighboring event. Each red `U` is a new, randomly-generated point. Each red arrow points from a `U` to the nearest-neighboring event. The numbers are the length of the arrows. In clustered data, red arrows tend to be longer in length than blue arrows.", fig.width=5, fig.height=5, fig.align="center", echo=FALSE, out.width = "60%"----
data(redwood) # Clustered data
redwood = data.frame(x=redwood$x, y=redwood$y)
set.seed(44)
h3 <- show_hopkins(redwood, m=3)


## -----------------------------------------------------------------------------
(.085^2+.078^2+.158^2) /
  ((.085^2+.078^2+.158^2) + (.028^2+.028^2+.12^2))


## ---- echo=FALSE, message=0---------------------------------------------------
# redwood clustered data
set.seed(42)
hopstat <- NULL
for(ii in 1:100) hopstat[ii] <- hopkins::hopkins(redwood, 6)
#round( c(mean(hopstat), sd(hopstat) ), 2) # .79 .13. Gastner .84 .05


## ----adolfsson, echo=FALSE----------------------------------------------------
prop_clusterable <- function(X, B=100){
  M <- floor(nrow(X) * .1)
  d1 <- dd <- d1s <- dds <- NULL
  for(ii in 1:B){
    d1[ii] <- hopkins::hopkins(X, m=M, d=1, method="simple")
    dd[ii] <- hopkins::hopkins(X, m=M, method="simple")
    # Appears that Adolfsson did NOT scale the data
    #d1s[ii] <- hopkins::hopkins(scale(X), m=m, d=1)
    #dds[ii] <- hopkins::hopkins(scale(X), m=m)
  }
  crit <- qbeta(.95, M, M)
  return(c( round( sum(d1 > crit) / length(d1), 2),
           #round( sum(d1s > crit) / length(dd), 2),
           round( sum(dd > crit) / length(dd), 2)
           #round( sum(dds > crit) / length(dd), 2)
           ) )
}
set.seed(1)
tab <- rbind(
prop_clusterable(faithful),
prop_clusterable(iris[,-5]),
prop_clusterable(as.data.frame(rivers))  ,
prop_clusterable(swiss),
prop_clusterable(attitude),
prop_clusterable(cars),
prop_clusterable(trees),
prop_clusterable(USJudgeRatings),
prop_clusterable(USArrests) # 12 min for B=1, method="torus"
)
tab <- data.frame(dataset=c("faithful","iris","rivers","swiss","attitude",
                         "cars","trees","USJudgeRatings","USArrests"),
                  n=c(272,150,141,47,30,50,31,43,50),
                  D=c(2,5,1,6,7,2,3,12,4),
                  Adolfsson=c(1.00, 1.00, .92,.41, .00, .19, .18, .69,.01),
                  Hopkins1=tab[,1],
                  HopkinsD=tab[,2])


## ----tabinteractive, echo=FALSE, eval = knitr::is_html_output()---------------
#> knitr::kable(tab, format = "html", caption = "In this table, `dataset` is the R dataset name, `n` is the number of rows in the data, `D` is the number of columns in the data, `Adolfsson` is the the proportion of 100 times that Hopkins statistic was significant as appearing in the paper by Adolfsson, Ackerman, and Brownsteain (2017), `Hopkins1` is the proportion of 100 times that Hopkins statistic was significant when calculated with the exponent $d=1$ (similar to the `clustertend` package), and `HopkinsD` is the proportion of 100 times that Hopkins statistic was significant when calculated with the exponent $d=D$. Since the `Adolfsson` and `Hopkins1` columns are similar (within samling variation), it appears that Adolfsson, Ackerman, and Brownstein (2017) used the `clustertend` package to calculate Hopkins statistic.")


## ----tabstatic, echo=FALSE, eval = knitr::is_latex_output()-------------------
library(magrittr)
knitr::kable(tab, format = "latex", caption = "In this table, `dataset` is the R dataset name, `n` is the number of rows in the data, `D` is the number of columns in the data, `Adolfsson` is the the proportion of 100 times that Hopkins statistic was significant as appearing in the paper by Adolfsson et al. (2017), `Hopkins1` is the proportion of 100 times that Hopkins statistic was significant when calculated with the exponent $d=1$ (similar to the `clustertend` package), and `HopkinsD` is the proportion of 100 times that Hopkins statistic was significant when calculated with the exponent $d=D$. Since the `Adolfsson` and `Hopkins1` columns are similar (within samling variation), it appears that Adolfsson et al. (2017) used the `clustertend` package to calculate Hopkins statistic.") %>% 
  kableExtra::kable_styling(font_size = 7)


## ----swiss, echo=FALSE, fig.cap="Pairwise scatterplots of the R dataset `swiss`. The meaning of the variables is not important here. Because some panels show a lack of spatial randomness of the data, we would expect Hopkins statistic to be significant.", fig.width=8, fig.height=8, fig.align="center", layout = "l-body", out.width = "90%"----
pairs(swiss)


## ----trees, echo=FALSE, fig.cap="Pairwise scatterplots of the R dataset `trees`. The data are `Girth`, `Height`, and `Volume` of 31 black cherry trees. Because all panels show a lack of spatial randomness of the data, we would expect Hopkins statistic to be significant.", fig.width=6, fig.height=6, fig.align="center", layout = "l-body", out.width = "70%"----
pairs(trees)


## ----toruscalc, eval=FALSE, cache=TRUE, echo=FALSE, fig.width=6, fig.height=5, fig.align="center"----
#> library(hopkins)
#> library(doParallel)
#> ncores <- detectCores()
#> cl <- makeCluster(ncores-1)
#> registerDoParallel(cl)
#> 
#> # 45 sec D=5
#> N <- 100 # number of events, rows(X)
#> D <- 5 # dimension of data, columns(X)
#> M <- 10 # number of events sampled
#> B <- 1000 # number of simulations
#> K <- 1 # number of nearest neighbors
#> 
#> # parallel computation
#> set.seed(1)
#> #system.time(
#>   out <- foreach(ii=1:B, .combine=rbind) %dopar% {
#>     X <- matrix(runif(N*D), ncol=D)
#>     c(hopkins::hopkins(X, m=M, d=D, method="simple"),
#>       hopkins::hopkins(X, m=M, d=D, k=K, method="torus"))
#>   }
#> #)
#> stopCluster(cl)
#> save(out, file="hopkins_simple_vs_torus.Rdata")


## ----torus, echo=FALSE, fig.align="center", fig.height=5, fig.width=6, fig.cap="Results of a simulation study considering how the spatial geometry affects Hopkins statistic. The thin black line is the theoretical distribution of Hopkins statistic. The blue and green lines are the empirical density curves of 1000 Hopkins statistics calculated with simple geometry (blue) and torus geometry (green). Calculating Hopkins statistic with a torus geometry aligns closely to the theoretical distribution.", out.width = "90%"----
load(file="hopkins_simple_vs_torus.Rdata")
hop1 <- out[,1] ; hop2 <- out[,2]
# Now the plots
#windows(width=4, height=6)
#png(file="hopkins_simple_vs_torus.png", width=600, height=350)
D=5 # Hardcoded here because the previous code chunk is not evaluated
plot(density(hop1), col="blue", xlim=c(0,1), main="", xlab="", ylim=c(0,5), lwd=2)
lines(density(hop2), col="forestgreen", lwd=2)
xv <- seq(0,1,length=100)
lines(xv, dbeta( xv, M, M) , col="black", lwd=1)
legend("topleft",
       c(paste0("Hopkins simple (D=",D,")"),
         paste0("Hopkins torus (D=",D,")"),
         paste0("Beta(",M,",",M,")")) ,
         text.col=c("blue","forestgreen","black")
       )


## ----frame, echo=FALSE, message=FALSE, fig.width=6, fig.height=4, fig.align="center", fig.cap="The left figure shows 250 points simulated randomly in a unit square. As expected, the value of Hopkins statistic is close to 0.5. The right figure shows the same points, but only those inside a unit-diameter circle. The value of Hopkins statistic H is much larger than 0.5. Although both figures depict spatially-uniform points, the square shape of the sampling frame affects the value of Hopkins statistic.", layout = "l-body", out.width = "90%"----
library(purrr)
set.seed(1)
X1 <- matrix(runif(500*2), ncol=2)
dist <- sqrt((X1[,1]-0.5)^2 + (X1[,2]-0.5)^2)  # distance from (0.5, 0.5)
X2 <- X1[dist < 0.5,]
set.seed(1)
probs1 <- map_dbl(1:100, .f= ~ hopkins::hopkins(X1, m=10)) %>% quantile(probs=c(1:3/4))
probs2 <- map_dbl(1:100, .f= ~ hopkins::hopkins(X2, m=10)) %>% quantile(probs=c(1:3/4))
op <- par(mfrow=c(1,2), pty="s")
plot(X1, xlim=c(0,1), ylim=c(0,1), xlab="", ylab="") # Square
title( paste0("H: ", round(probs1[2],2)))
plot(X2, xlim=c(0,1), ylim=c(0,1), xlab="", ylab="") # Circular
title( paste0("H: ", round(probs2[2],2)))
par(op)

