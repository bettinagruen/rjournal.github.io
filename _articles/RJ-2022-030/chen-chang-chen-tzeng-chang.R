# Generated by `rjournal_article()` using `knitr::purl()`: do not edit by hand
# Please edit chen-chang-chen-tzeng-chang.Rmd to modify this file

## ----setup, include=FALSE, tidy=FALSE-----------------------------------------
knitr::opts_chunk$set(cache=TRUE) 


## ----INSTALL, echo=FALSE, eval=FALSE, tidy=F, message=F-----------------------
#> install.packages("dslabs")
#> install.packages("TensorTest2D")
#> #install.packages("devtools")
#> #devtools::install_github("yuting1214/TensorTest2D")


## ----lib, echo = FALSE--------------------------------------------------------
library(TensorTest2D)


## ----omics, eval=TRUE, echo=TRUE, warning=FALSE-------------------------------
library(TensorTest2D)
data(omics)
# The size of the data P, Q, n
print(dim(omics$omics))


## ----omics_model, eval=TRUE, echo=TRUE, warning=FALSE-------------------------
set.seed(100) # Set seed for reproducibility
# Try from rank-1 to rank-3 models
omicsAIC <- numeric(3)
for (k in 1:3) {
  # Temporary storage for the rank-k model for withdrawing its AIC value
  omicsTmp <- tensorReg2D(y = omics$Y, X = omics$omics, 
                          W = matrix(1, length(omics$Y), 1),
                          n_R = k, family = "gaussian", 
                          opt = 1, max_ite = 1000, tol = 10^(-7) )
  omicsAIC[k] <- omicsTmp$IC[1] # AIC
}
sprintf('Rank-%d model is the best with smallest AIC = %4.4f', which.min(omicsAIC), min(omicsAIC))

# Train the tensor regression model of rank 1
omicsMdl <- tensorReg2D(y = omics$Y, X = omics$omics, 
                        W = matrix(1, length(omics$Y), 1),
                        n_R = which.min(omicsAIC), family = "gaussian", 
                        opt = 1, max_ite = 1000, tol = 10^(-7) )

# Return the results of significance tests for all coefficients 
summary(omicsMdl)

# Estimated coefficients
print(round(omicsMdl$B_EST, 3))

# The standard deviation of the coefficients
print(round(omicsMdl$B_SD, 3))

# The p-values of the coefficients by the Wald test
print(round(omicsMdl$B_PV, 3))


## ----omics_plot, eval=TRUE, echo=TRUE, warning=FALSE, fig.cap="The image plot of the values for t-statistics of matrix covariate in the omics data.  The effective pixels identified by the tensor regression model are marked out by the $\\boxtimes$ symbol.", fig.height=6, out.width="45%", fig.align="center"----
plot(x = omicsMdl, method = "none", alpha = 0.05, type = "tval",
     showlabels = TRUE, plot.legend = TRUE) 


## ---- echo=FALSE--------------------------------------------------------------
par(mfrow = c(1, 1), mar = c(5,4,4,2)+.1)


## ----mnistpreproc, echo=FALSE, out.width="95%", fig.align="center", fig.cap="Data pre-processing for the MNIST dataset."----
knitr::include_graphics("chen-chang-chen-tzeng-chang_files/figure-latex/mnist_preproc.png")


## ----mnistmean, echo=FALSE, out.width="95%", fig.align="center", fig.cap="The mean plots of pre-processed images in the training dataset."----
knitr::include_graphics("chen-chang-chen-tzeng-chang_files/figure-latex/mnist_preproc_out.png")


## ----read_mnist_r, eval=TRUE, echo=TRUE---------------------------------------
library(TensorTest2D)
data(mnist_mp2c2)
mnist_train <- mnist_mp2c2$train
mnist_test <- mnist_mp2c2$test


## ----mnist_train_data, echo=TRUE, warning=FALSE-------------------------------
library(abind)
# Draw image data of labels 2 and 5
x0_all <- mnist_train$image[,,which(mnist_train$label == 2)]
x1_all <- mnist_train$image[,,which(mnist_train$label == 5)]
# Random sampling from MNIST training set for each label
nSampleEach <- 1000
n0 <- dim(x0_all)[3]; n1 <- dim(x1_all)[3]
set.seed(2021)
s0 <- sample(1:n0, nSampleEach, replace = FALSE)  
s1 <- sample(1:n1, nSampleEach, replace = FALSE)  
# Normalizing image values into [-0.5, 0.5]
x0 <- x0_all[,,s0]/255 - 0.5
x1 <- x1_all[,,s1]/255 - 0.5
# Combine training data
train_X <- abind(x0, x1, along = 3)
# Add negligible noise for the images 
# (so no constant zero values in one pixel over all covariate matrices)
set.seed(2021) # Set seed for reproducibility
train_n <- array((rnorm(prod(dim(train_X)), 0, 0.1)), dim(train_X))
train_Xn <- train_X + train_n # Contaminated images
# Define Y = 0 for label 2, and Y = 1 for label 5
train_y <- c(rep(0, dim(x0)[3]), rep(1, dim(x1)[3]))


## ----mnist_tune, eval=F, echo=F, warning=FALSE,message=F----------------------
#> # Try from rank-1 to rank-6 models
#> logitAIC <- numeric(6)
#> for (k in 1:6) {
#>   lgMdlTmp <- tensorReg2D(y = train_y, X = train_Xn,
#>                           W = matrix(1, length(train_y), 1),
#>                           n_R = k, family = "binomial",
#>                           opt = 1, max_ite = 100, tol = 10^(-7) )
#>   logitAIC[k] <- lgMdlTmp$IC[1] # AIC
#> }
#> sprintf('Rank-%d model is the best with smallest AIC = %4.4f', which.min(logitAIC), min(logitAIC))
#> # "Rank-4 model is the best with smallest AIC = 325.9683"


## ----mnist_train, eval=TRUE, echo=TRUE, warning=FALSE-------------------------
# Train the logistic tensor regression model
lgMdl <- tensorReg2D(y = train_y, X = train_Xn, 
                      W = matrix(1, length(train_y), 1),
                      n_R = 4, family = "binomial", 
                      opt = 1, max_ite = 100, tol = 10^(-7) )
# Print model summary (not run)
#summary(lgMdl)
# Print the p-values of the estimates
cat("FDR-adjusted p-values of B_pq:\n")
round(matrix(p.adjust(as.vector(lgMdl$B_PV), method = "fdr"), 10, 10), 3)


## ----mnisttval, eval=TRUE, echo=TRUE, warning=FALSE, fig.cap="The image plot of the values for t-statistics of matrix covariate in the handwritten label data.  The effective pixels identified by the logistic tensor regression model are marked out by the $\\boxtimes$ marks.", fig.height=6, out.width="45%", fig.align="center"----
plot(x = lgMdl, method = "fdr", alpha = 0.05, type = "tval", 
     showlabels = TRUE, plot.legend = TRUE) 


## ----mnistplot, eval=TRUE, echo=TRUE, warning=FALSE, fig.cap="Effective pixels identified by the logistic tensor rgression model.", fig.height=3, out.width="95%", fig.align="center"----
xm0 <- xm1 <- matrix(0, dim(train_X)[1], dim(train_X)[2])
# Background image: mean image of label 2
for (k in 1:dim(x0)[3]) {
  xm0 <- xm0 + (1/nSampleEach)*x0[,,k]
}
# Background image: mean image of label 5
for (k in 1:dim(x1)[3]) {
  xm1 <- xm1 + (1/nSampleEach)*x1[,,k]
}
# Draw for visualizing effective pixels for both background images 
par(mfrow = c(1, 2), mar = c(1, 1, 1, 1))
plot(x = lgMdl, method = "fdr", alpha = 0.05, background = xm0, 
     showlabels = FALSE, plot.legend = FALSE, col = gray(seq(0, 1, 0.05)))
plot(x = lgMdl, method = "fdr", alpha = 0.05, background = xm1, 
     showlabels = FALSE, plot.legend = FALSE, col = gray(seq(0, 1, 0.05)))


## ----mnist_test, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE-----------
# Normalize image values of the testing data into [-0.5, 0.5]
tx0 <- mnist_test$image[,,which(mnist_test$label == 2)]/255 - 0.5
tx1 <- mnist_test$image[,,which(mnist_test$label == 5)]/255 - 0.5
# Combine testing data and assign the vector of the true responses
test_X <- abind(tx0, tx1, along = 3)
test_y <- c(rep(0, dim(tx0)[3]), rep(1, dim(tx1)[3]))
# Print some predictions with different settings of type
pred_link <- predict(lgMdl, test_X, type = "link")
pred_prob <- predict(lgMdl, test_X, type = "response")
head(round(pred_link, digits = 2))
head(round(pred_prob, digits = 4))
# Comput the prediction accuracy for the testing data
pred_test_y <- (pred_prob > .5)
cat(
  sprintf("Accuracy = %2.2f%%", 
          100*sum(pred_test_y == test_y)/length(test_y)))


## ----mnist_lasso, eval=TRUE, echo=TRUE, warning=FALSE, fig.cap="Effective pixels identified by the LASSO model.", fig.height=3, out.width="95%", fig.align="center", warning=FALSE, message=FALSE----
library(glmnet)
# Vectorize the hand-written images
xv <- t(sapply(1:dim(train_X)[3], function(k) as.vector(train_X[,,k])))
# Train the LASSO model using cross-validation
set.seed(2021) # Set seed for reproducibility
l1Mdl <- cv.glmnet(xv, train_y, family = "binomial", alpha = 1, standardize = FALSE)
# Draw the LASSO coefficients from the best model
l1B <- matrix(l1Mdl$glmnet.fit$beta[,which.min(l1Mdl$cvm)], 10, 10)
# The LASSO estimates
print(round(l1B, digits = 3))
# Draw for visualizing effective pixels identified by LASSO for both background images 
par(mfrow = c(1, 2), mar = c(1, 1, 1, 1))
draw.coef(img = xm0, marks = l1B, markstyle = "bi-dir", showlabels = FALSE, 
          plot.legend = FALSE, grids = FALSE, col = gray(seq(0, 1, 0.05)))
draw.coef(img = xm1, marks = l1B, markstyle = "bi-dir", showlabels = FALSE, 
          plot.legend = FALSE, grids = FALSE, col = gray(seq(0, 1, 0.05)))


## ---- echo=FALSE--------------------------------------------------------------
par(mfrow = c(1, 1), mar = c(1,5,5,2)+.1)

