% !TeX root = RJwrapper.tex
\title{eat: An R Package for fitting Efficiency Analysis Trees}
\author{by Miriam Esteve, Victor Espa√±a, Juan Aparicio, and Xavier Barber}

\maketitle

\abstract{%
\CRANpkg{eat} is a new package for R that includes functions to estimate
production frontiers and technical efficiency measures through
non-parametric techniques based upon regression trees. The package
specifically implements the main algorithms associated with a recently
introduced methodology for estimating the efficiency of a set of
decision-making units in Economics and Engineering through Machine
Learning techniques, called Efficiency Analysis Trees
\citep{esteve2020}. The package includes code for estimating input- and
output-oriented radial measures, input- and output-oriented Russell
measures, the directional distance function and the weighted additive
model, plotting graphical representations of the production frontier by
tree structures, and determining rankings of importance of input
variables in the analysis. Additionally, it includes the code to perform
an adaptation of Random Forest in estimating technical efficiency. This
paper describes the methodology and implementation of the functions, and
reports numerical results using a real data base application.
}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Efficiency analysis refers to the discipline of estimating production
frontiers while measuring the efficiency of a set of observations, named
Decision Making Units (DMUs), which use several inputs to produce
several outputs. In the literature of Economics, Engineering and
Operations Research, the estimation of production frontiers is a current
topic of interest \citep[see, for
example,][]{arnaboldi2014, aparicio2017, odonell2018}. In this line,
many models for estimating production frontiers have been developed,
resorting to parametric and non-parametric approaches. In the
non-parametric approach, a functional form does not need to be specified
(e.g.~a Cobb-Douglas production function) through the specification of a
set of parameters to be estimated, since they are usually data-driven.
Additionally, non-parametric models innately cope with multiple-output
scenarios. In contrast, the parametric approach aggregates the outputs
into a single production index or attempts to model the technology using
a dual cost function \citep{orea2019}. These are some of the advantages
that makes the non-parametric approaches for measuring technical
efficiency more appealing than their parametric counterparts.

Contextualizing the non-parametric measurement of efficiency analysis
requires outlining the following works. \citet{farrel1957} was a
renowned opponent of estimating efficiency by determining average
performance and, indeed, he was the first author in the literature to
introduce a method for constructing production frontiers as the maximum
producible output from an input bundle. Inspired by \citet{koopmans1951}
and \citet{debreu1951}, Farrell introduced a piece-wise linear upper
enveloping surface of the data cloud as the specification of the
production frontier, satisfying some microeconomics postulates: free
disposability, convexity and minimal extrapolation. A DMU is considered
technically inefficient if it is located below the frontier.
Furthermore, Farrell's measure of efficiency, inspired by
\citet{shephard1953}, is based on radial movements (equiproportional
changes) from technically inefficient observations to their benchmarks
located at the estimated production frontier. In the same context as
Farrell, \citet{afriat1972} determined a production frontier under
non-decreasing and concavity mathematical assumptions and, at the same
time, as close as possible to the sample of observations. Finally, in
the same line of research, \citet{charnes1978} and \citet{banker1984}
proposed Data Envelopment Analysis (DEA), rooted in mathematical
programming to provide a relative efficiency assessment of a set of DMUs
by the construction of a piece-wise linear frontier. Along with DEA,
Free Disposal Hull (FDH) is another of the most recognized
non-parametric models for estimating production frontiers. FDH is a
deterministic model introduced by \citet{deprins1984}, which is only
based on the free disposability and minimal extrapolation principles, as
opposed to DEA, which also assumes convexity. In fact, FDH can be
considered the skeleton of DEA, since the convex hull of the former
coincides with DEA's frontier \citep[see][]{daraio2005}. In addition,
other recent alternative non-parametric techniques for estimating
production frontiers are: \citet{banker1992} and \citet{banker1993}, who
showed that DEA can be interpreted as a Maximum Likelihood estimator;
Simar and Wilson
\citetext{\citeyear{simar1998}; \citeyear{simar2000a}; \citeyear{simar2000b}},
who introduced how to determine confidence intervals for the efficiency
score of each DMU through adapting the bootstrapping methodology by
\citet{effron1979} to the context of FDH and DEA; or Kuosmanen and
Johnson \citetext{\citeyear{kuosmanen2010}; \citeyear{kuosmanen2017}},
who have recently shown that DEA may be interpreted as non-parametric
least-squares regression, subject to shape constraints on the production
frontier and sign constraints on residuals; to name a few.

However, few of the above methodologies are based upon Machine Learning
techniques, despite being a rising research field
\citetext{\citealp[see, for example, the recent papers
by][]{khezrimotlagh2019}; \citealp[and][]{zhu2019}; \citealp[or the book
by][]{charles2020}}. Recently, a bridge has been built between these
literatures, Machine Learning and production theory, through a new
technique proposed in \citet{esteve2020}, called Efficiency Analysis
Trees. This new method shares some similarities with the standard FDH
technique. In contrast to FDH, Efficiency Analysis Trees overcomes the
well-known problem of overfitting linked to FDH and DEA, by using
cross-validation to prune back the deep tree obtained in a first stage.
\citet{esteve2020} also showed that the performance of Efficiency
Analysis Trees, checked through Monte Carlo simulations, clearly
outperforms the FDH technique with respect to bias and mean squared
error.

Many of the standard models for estimating technical efficiency are
nowadays available as R packages such as: \CRANpkg{Benchmarking}
\citep{benchmarking}, for estimating technologies and measuring
efficiencies using Data Envelopment Analysis (DEA) and Stochastic
Frontier Analysis (SFA); \CRANpkg{nonparaeff} \citep{nonparaeff}, for
measuring efficiency and productivity using DEA and its variations;
\CRANpkg{npbr} \citep{npbr}, which covers data envelopment techniques
based on piece-wise polynomials, splines, local linear fitting, extreme
values and kernel smoothing; \CRANpkg{snfa} \citep{snfa}, which fits
both a smooth analogue of DEA and a non-parametric analogue of SFA; or
\CRANpkg{semsfa} \citep{semsfa}, which, in a first stage, estimates
Stochastic Frontier Models by semiparametric or non-parametric
regression techniques to relax parametric restrictions and, in a second
stage, applies a technique based on pseudolikelihood or the method of
moments for estimating variance parameters. Additionally, there are
other packages on efficiency measurement developed for alternative
platforms. In MATLAB \citep{Matlab}, we can find the
\pkg{Data Envelopment Analysis Toolbox} \citep{alvarez2020}, which
implements the main DEA models and solves measures like the directional
distance function (with desirable and undesirable outputs), the weighted
additive model, and the Malmquist-Luenberger index; or the
\pkg{Total Factor Productivity Toolbox} \citep{balk2018}, which includes
functions to calculate the main Total Factor Productivity indices and
their decomposition by DEA models. In Stata \citep{stata}, it is
possible to find a similar package in \citet{ji2010}.

In this paper, we introduce a new package in R, called \pkg{eat}, for
fitting regression trees to estimate production frontiers in
microeconomics and engineering, by implementing the main features of
Efficiency Analysis Trees \citep{esteve2020}. In particular, \pkg{eat}
includes a complete set of baseline functions, covering a wide range of
efficiency models fitted by Efficiency Analysis Trees \citep{esteve2020}
and Random Forest \citep{esteve2021}, and reporting numerical and
graphical results. \pkg{eat} is available as free software, under the
GNU General Public License version 3, and can be downloaded from the
Comprehensive R Archive Network (CRAN) at
\url{https://CRAN.R-project.org/package=eat}, including supplementary
material as datasets or vignettes to replicate all the results presented
in this paper. In addition, \pkg{eat} is hosted on an open source
repository on GitHub at \url{https://github.com/MiriamEsteve/EAT}. The
main objective of this package is the estimation of a production
frontier through regression trees satisfying the microeconomic
principles of free disposability, convexity and deterministic data. Free
disposability states that if a certain input and output bundle is
producible, then any input and output bundle that presents a greater
value for inputs and a lower value for outputs is also producible. In
some sense, it means that doing it worse is always feasible. Convexity
means that if two input-output bundles are assumed producible, then any
convex combination of them are also feasible. Finally, the deterministic
quality means that the observations that belong to the data sample have
been observed without noise. In other words, the technology always
contains all these observations and, graphically, the production
frontier envelops all the data cloud from above.

The efficiency measurement field has witnessed the introduction of many
different technical efficiency measures throughout the last decades.
Regarding the technical efficiency measures implemented in the new
\pkg{eat} package, it is worth mentioning that numerical scores and
barplots are provided for the output-oriented and input-oriented BCC
radial models \citep{banker1984}, the directional distance function
\citep{chambers1998}, the weighted additive model
\citetext{\citealp{lovell1995}; \citealp[and][]{cooper1999}} and the
output-oriented and input-oriented Russell measures \citep{fare1978}.
Additionally, the adaptation of Random Forest \citep{breiman2001} for
dealing with ensembles of Efficiency Analysis Trees, recently introduced
in \citet{esteve2021} and denoted as RF+EAT, has been also incorporated
into the new \pkg{eat} package. The frontier estimator based on Random
Forest, which is associated with more robust results, also allows to
determine out-of-sample efficiency evaluation for the assessed DMUs.
Another remarkable aspect of Efficiency Analysis Trees is the inherited
ability to calculate feature importance as performed by other tree-based
models of Machine Learning. Specifically, this fact allows the
researchers to know which are the most relevant variables for obtaining
efficiency and thus getting an explanation of the level of technical
efficiency identified for each assessed unit. This ranking of importance
variable has been implemented in the \pkg{eat} package. Finally, and
from a data visualization point of view, the obtained frontier from
Efficiency Analysis Trees can be represented by means of a tree
structure, ideal for high-dimensional scenarios where the patterns
between the inputs and the efficient levels of outputs are very complex.
In addition, FDH and DEA standard models have been included in the new
package in order to facilitate comparison with the efficiency scores
determined by the Efficiency Analysis Trees technique. Also, the
convexification of the estimation of the technology provided by
Efficiency Analysis Trees, named Convexified Efficiency Analysis Trees
(CEAT) by \citet{aparicio2021}, is implemented in the \pkg{eat} package,
with the objective of determining estimations under the axiom of
convexity.

The functions included in the \pkg{eat} package are summarized in Table
\hyperref[Tab:tab1]{1}. This table comprises two columns divided into
four subsections for Efficiency Analysis Trees, Random Forest for
Efficiency Analysis Trees, Convexified Efficiency Analysis Trees and
functions for data simulation. The first column is the name of the main
functions and the second one is the description of the functions and the
reference of the paper in which we can find the most detailed
theoretical explanation of the corresponding function.

The paper is organized as follows. The following section summarises the
two methodologies implemented in the \pkg{eat} package in R: Efficiency
Analysis Trees and Random Forest for Efficiency Analysis Trees. Section
\protect\hyperlink{section3}{Data Structure} describes the data
structures that characterize the production possibility sets, the
structure of the functions, the results, etc., and briefly explains
which data are used to illustrate the package. Section
\protect\hyperlink{section4}{Basic functions of the library} presents
the basic methods. The next Section \protect\hyperlink{section5}{Basic
EAT and RFEAT models} deals with the measurement of economic efficiency
of FDH, DEA, Efficiency Analysis Trees, Random Forest for Efficiency
Analysis Trees and Convexified Efficiency Analysis Trees models.
Advanced options, including displaying and exporting results can be
found in Section \protect\hyperlink{section6}{Advanced options and
displaying and exporting results}. Section
\protect\hyperlink{section7}{Conclusions} concludes.

\begin{center}
\begin{longtable}{|m{2.8cm}|p{10.5cm}|}
\caption{\pkg{eat} package functions. \label{Tab:tab1}} \\

\hline
\multicolumn{1}{|c|}{\textbf{Function}} &
\multicolumn{1}{c|}{\textbf{Description}}
\\ \hline
\endfirsthead

\multicolumn{2}{c} %
{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
\hline
\multicolumn{1}{|c|}{\textbf{Function}} &
\multicolumn{1}{c|}{\textbf{Description}}
\\ \hline
\endhead

\hline
\multicolumn{2}{|r|}{{Continued on next page}}
\\ \hline
\endfoot

\hline
\endlastfoot

\multicolumn{2}{|l|}{Subsection 1: Efficiency Analysis Trees}
\\ \hline

\rule{0pt}{3ex}
\code{EAT} &
\rule{0pt}{3ex}
It generates a pruned Efficiency Analysis Trees model and returns an \code{EAT} object.

\\
\code{bestEAT} &
It computes the root mean squared error (RMSE) for a set of Efficiency Analysis Trees models made up of a set of user-entered hyperparameters. These models are fitted with a training sample and evaluated with a test sample.

\\
\code{efficiencyEAT} &
It computes the efficiency scores of a set of DMUs through an Efficiency Analysis Trees model and returns a \code{data.frame}. The FDH scores can also be computed. Alternative mathematical programming models for calculating the efficiency scores are:
\begin{itemize}
  \item \code{"BCC.OUT"}: The output-oriented BCC radial model.
  \item \code{"BCC.INP"}: The input-oriented BCC radial model.
  \item \code{"DDF"}: The directional distance function.
  \item \code{"RSL.OUT"}: The output-oriented Russell model.
  \item \code{"RSL.INP"}: The input-oriented Russell model.
  \item \code{"WAM.MIP"}: The weighted additive model with Measure of Inefficiency Proportion.
  \item \code{"WAM.RAM"}: The weighted additive model with Range Adjusted Measure of Inefficiency.
\end{itemize}

\\
\code{efficiencyJitter} &
It returns a jitter plot (from \code{ggplot2}) that represents the dispersion of the efficiency scores of the set of DMUs in the leaf nodes of an Efficiency Analysis Trees model. Mean and standard deviation of scores are shown.

\\
\code{efficiencyDensity} &
It returns a density plot (from \code{ggplot2}) to compare the distribution of efficiency scores between two given models (\code{"EAT"}, \code{"FDH"}, \code{"CEAT"}, \code{"DEA"} and \code{"RFEAT"} are available).

\\
\rule{0pt}{3ex}
\code{plotEAT} &
\rule{0pt}{3ex}
It returns a plot of the tree-structure (from \code{ggparty} and \code{partykit}) of an Efficiency Analysis Trees model.

\\
\code{frontier} &
It returns a plot (from \code{ggplot2}) of the estimated production function obtained by an Efficiency Analysis Trees model in a two-dimensional scenario (1 input and 1 output). Optionally, the FDH frontier can be plotted.

\\
\code{predict} &
Generic function to predict the expected output by an \code{EAT} object. The result is a \code{data.frame} with the predicted values.

\\
\code{rankingEAT} &
It returns a \code{data.frame} with the scores of variable importance obtained by an Efficiency Analysis Trees model and optionally a barplot representing the variable importance.

\\ \hline
\multicolumn{2}{|l|}{Subsection 2: Random Forest for Efficiency Analysis Trees}
\\ \hline

\rule{0pt}{3ex}
\code{RFEAT} &
\rule{0pt}{3ex}
It generates a Random Forest for Efficiency Analysis Trees model and returns an \code{RFEAT} object.

\\
\code{bestRFEAT} &
It computes the root mean squared error (RMSE) for a set of Random Forest for Efficiency Analysis Trees models made up of a set of user-entered hyperparameters. These models are fitted with a training sample and evaluated with a test sample.

\\
\code{efficiencyRFEAT} &
It computes the efficiency scores of a set of DMUs through a Random Forest for Efficiency Analysis Trees model and returns a \code{data.frame}. The FDH scores can also be computed. Only the output-oriented BCC radial model is available.

\\
\code{plotRFEAT} &
It returns a line plot (from \code{ggplot2}) with the Out-of-Bag (OOB) error for a random forest consisting of k trees.

\\
\code{predict} &
Generic function to predict the expected output by an \code{RFEAT} object. The result is a \code{data.frame} with the predicted values.

\\
\code{rankingRFEAT} &
It returns a \code{data.frame} with the scores of variable importance obtained by a Random Forest for Efficiency Analysis Trees model and optionally a barplot representing the variable importance.

\\ \hline
\multicolumn{2}{|l|}{Subsection 3: Convexified Efficiency Analysis Trees}
\\ \hline

\rule{0pt}{3ex}
\code{efficiencyCEAT} &
\rule{0pt}{3ex}
It computes the efficiency scores of a set of DMUs through a Convexified Efficiency Analysis Trees model and returns a \code{data.frame}. The DEA scores can also be computed. Alternative mathematical programming models for calculating the efficiency scores are:
\begin{itemize}
  \item \code{"BCC.OUT"}: The output-oriented BCC radial model.
  \item \code{"BCC.INP"}: The input-oriented BCC radial model.
  \item \code{"DDF"}: The directional distance function.
  \item \code{"RSL.OUT"}: The output-oriented Russell model.
  \item \code{"RSL.INP"}: The input-oriented Russell model.
  \item \code{"WAM.MIP"}: The weighted additive model with Measure of Inefficiency Proportion.
  \item \code{"WAM.RAM"}: The weighted additive model with Range Adjusted Measure of Inefficiency.
\end{itemize}

\\ \hline
\multicolumn{2}{|l|}{Subsection 4: Functions for data simulation}
\\ \hline

\rule{0pt}{3ex}
\code{Y1.sim} &
\rule{0pt}{3ex}
It returns a \code{data.frame} with simulated data in a single output scenario (1, 3, 6, 9, 12 and 15 inputs can be generated).

\\
\code{X2Y2.sim} &
It returns a \code{data.frame} with simulated data in a scenario with 2 inputs and 2 outputs.

\end{longtable}
\end{center}

\hypertarget{section2}{%
\section{Background}\label{section2}}

\hypertarget{section2.1}{%
\subsection{Efficiency Analysis Trees}\label{section2.1}}

In this section, we briefly introduce the main fundaments of Efficiency
Analysis Trees. Nevertheless, we first need to introduce some notation
related to the standard Free Disposal Hull (FDH) and Classification and
Regression Trees (CART) techniques.

We consider the observation of \(n\) Decision Making Units (DMUs), which
consumes \({\it \textbf{x}}_{i} =(x_{1i} ,...,x_{mi})\)
\(\in R_{+}^{m}\) quantity of inputs for the production of
\({\it \textbf{y}}_{i} =(y_{1i} ,...,y_{si} )\in R_{+}^{s}\) quantity of
outputs\footnote{We use bold for denoting vectors, and non-bold for scalars.}.
The dataset is denoted in a compact way as
\(\aleph =\left\{\left({\it \textbf{x},\textbf{y}}\right)\right\}_{i=1,...,n}^{}\).
The so-called production possibility set or technology, which is the set
of technically feasible combinations of
\(({\it \textbf{x},\textbf{y}})\), is defined, in general terms, as:

\begin{equation} \label{(1)} 
\psi =\left\{({\it \textbf{x},\textbf{y}})\in R_{+}^{m+s} :{\it \textbf{x}\; }{\it can\; produce\; \textbf{y}}\right\}.
\end{equation}

On this set, certain assumptions are usually made
\citep[see,][]{fare1995}, such as: monotonicity (free disposability) of
inputs and outputs, which means that if
\(({\it \textbf{x},\textbf{y}})\in \psi\), then
\(({\it \textbf{x'},\textbf{y'}})\in \psi\), as long as
\({\it \textbf{x'}\; }\ge {\it \textbf{x}}\) and
\({\it \textbf{y'}\; }\le {\it \textbf{y}}\); and convexity, i.e., if
\(\left({\it \textbf{x}},{\it \textbf{y}}\right)\in \psi\) and
\(\left({\it \textbf{x'}},{\it \textbf{y'}}\right)\in \psi\), then
\(\lambda \left({\it \textbf{x}},{\it \textbf{y}}\right)+\left(1-\lambda \right)\left({\it \textbf{x'}},{\it \textbf{y'}}\right)\in \psi\),
\(\forall \lambda \in \left[0,1\right]\). In the case of the FDH
estimator, only free disposability is assumed. Additionally, FDH is
assumed to be deterministic. In other words, the production possibility
set determined by FDH always contains all the observations that belong
to the data sample and, graphically, the production frontier envelops
the data cloud from above. Also, FDH satisfies the minimal extrapolation
postulate, which is associated with the typical problem-solving
principle of Occam's razor. That is, additional requirements are needed
to select the right estimator because there are a lot of possible
estimators that can meet free disposability and the deterministic
quality. In this sense, according to Occam's razor, the most
conservative estimate of the production frontier would be that related
to a surface that would envelop the data from above, satisfy free
disposability and, at the same time, be as close as possible to the data
cloud. In contrast, the DEA estimator requires stronger assumptions,
such as convexity of the set \(\psi\).

With regard to the measurement of technical efficiency, a certain part
of the set \(\psi\) is actually of interest. It is the efficient
frontier or production frontier of \(\psi\), which is defined as
\(\partial ({\it \boldsymbol{\psi}}):=\left\{({\it \textbf{x}},{\it \textbf{y}})\in {\it \boldsymbol{\psi}}:\hat{{\it \textbf{x}}}<{\it \textbf{x}},\hat{{\it \textbf{y}}}>{\it \textbf{y}}\Rightarrow (\hat{{\it \textbf{x}}},\hat{{\it \textbf{y}}})\notin {\it \boldsymbol{\psi}}\right\}\).
Technical efficiency is understood as the distance from a point
belonging to \(\psi\) to the production frontier
\(\partial ({\it \boldsymbol{\psi}})\). In particular,
\citet{deprins1984} proposed the FDH estimator of the set \(\psi\) from
the dataset \(\aleph\) as:

\begin{equation} \label{(2)} 
\hat{{\it \boldsymbol{\psi}}}_{FDH} =\left\{\left({\it \textbf{x}},{\it \textbf{y}}\right)\in R_{+}^{m+s} :\exists i=1,...,n\, \, {\rm such\; that}\, {\it \textbf{y}}\le {\it \textbf{y}}_{i} ,{\it \textbf{x}}\ge {\it \textbf{x}}_{i} \right\}. 
\end{equation}

The FDH technique is very attractive because it is based on very few
suppositions, but it suffers from overfitting due to its construction.
This problem is shared by other well-known data-based approaches. For
example, Classification and Regression Trees (CART), a technique that
belongs to the field of machine learning, suffer problems of overfitting
when a deep tree is developed. However, this problem can be fixed using
a cross-validation process to prune the deep tree. The principle behind
CART is relatively simple: a certain criterion is chosen to recursively
generate binary partitions of the data until a meaningful division is no
longer possible or a stopping rule is maintained. The graphic result of
this approach is a tree that starts at the root node, develops through
the intermediate nodes and ends at the terminal nodes, also known as
leaves. The binary nature of CART is represented by each parent node,
except for the leaves, giving rise to two child nodes.

Next, we briefly introduce the recent technique named Efficiency
Analysis Trees by \citet{esteve2020}. This technique allows the
estimation of production frontiers, fulfilling the common axioms of
microeconomics, through a data-based approach that is not founded on any
particular distribution on the data noise and, in addition, creates a
step function as a estimator. It shares these characteristics with the
FDH technique, but the overfitting problem related to FDH can be solved
through cross-validation based on pruning.

We now introduce the main steps of the algorithm linked to the
Efficiency Analysis Trees technique. Let us assume that we have a node
\(t\) in the tree structure to be split. This node contents a subset of
the original sample \(\aleph\). The algorithm has to select an input
variable \(j\), \(j=1,...,m\), and a threshold \(s_{j} \in S_{j}\),
where \(S_{j}\) is the set of possible thresholds for variable \(j\),
such that the sum of the mean squared error (MSE) calculated for the
data that belong to the left child node \(t_{L}\) and the MSE
corresponding to the data belonging to the right child node \(t_{R}\) is
minimized. The data of the left child node \(t_{L}\) satisfies the
condition \(x_{j} <s_{j}\), while the data of the right child node
\(t_{R}\) satisfies the condition \(x_{j} \ge s_{j}\). Additionally, in
the algorithm, the set \(S_{j}\) is defined from the observed values of
the input \(j\) in the data sample \(\aleph\). Formally, the split
consists in selecting the combination \(\left(x_{j} ,s_{j} \right)\)
which minimizes
\(R\left(t_{L} \right)+R\left(t_{R} \right)=\frac{1}{n} \sum _{\left({\it x}_{i} ,{\it y}_{i} \right)\in t_{L} }\sum _{r=1}^{s}\left(y_{ri} -y_{r} \left(t_{L} \right)\right)^{2} +\frac{1}{n} \sum _{\left({\it x}_{i} ,{\it y}_{i} \right)\in t_{R} }\sum _{r=1}^{s}\left(y_{ri} -y_{r} \left(t_{R} \right)\right)^{2}\),
where \(y_{r} \left(t\right)\) denotes the estimation of the \(r\)-th
output of the node \(t\). One of the most important aspects in the
production context is how to define \(y_{r} \left(t\right)\) in each
node for fulfilling the free disposability property during the growing
process of the tree. In this sense, the notion of Pareto-dominance
between nodes introduced in \citet{esteve2020} is really relevant.

As described above, each node \(t\) is defined by a series of conditions
in the input space as \(\left\{x_{j} <s_{j} \right\}\) or
\(\left\{x_{j} \ge s_{j} \right\}\). In this sense, after executing the
split, a region in the input space is created. This region in the input
space is called the ``support'' of node \(t\) and is defined as
\({\rm supp}\left(t\right)=\left\{{\it \textbf{x}}\in R_{+}^{m} :a_{j}^{t} \le x_{j} <b_{j}^{t} ,j=1,...,m\right\}\).
The parameters \(a_{j}^{t}\) and \(b_{j}^{t}\) are originated from the
several thresholds selected during the splitting process. Giving the
notion of support of a node, it is possible to establish the concept of
Pareto-dominance. Let \(k=1,...,K\) be the total number of splits
executed. Let \(T_{k} \left(\aleph \right)\) be the tree built after the
\textit{k-}th split. Let \(\tilde{T}_{k} \left(\aleph \right)\) be the
set of leaves in the tree \(T_{k} \left(\aleph \right)\). More notation:
let \(t^{*} \in \tilde{T}_{k} \left(\aleph \right)\) be the node to be
split in a certain step of the algorithm, then
\(T\left(k|t^{*} \to t_{L} ,t_{R} \right)\) denotes the tree associated
with this specific split. Let \(k=1,...,K\) and
\(t\in \tilde{T}_{k} \left(\aleph \right)\), then the set of
Pareto-dominant nodes of node \(t\) is defined as
\(P_{T_{k} \left(\aleph \right)} \left(t\right)=\{t'\in \tilde{T}_{k} \left(\aleph \right)-t:\exists {\it \textbf{x}}\in {\rm supp}\left(t\right),\exists {\it \textbf{x'}}\in {\rm supp}\left(t'\right)\, \, {\rm such\; that}\,\)
\({\it \textbf{x'}} \le {\it \textbf{x}}\}\).
\(P_{T_{k} \left(\aleph \right)} \left(t\right)\) contains all the nodes
such that at least one input vector in its corresponding support,
non-necessarily observed, dominates at least one input vector belonging
to the support of node \(t\) (in the Pareto sense). To do so, in
practice, it is only necessary to compare the components of
\({\it \textbf{a}}^{t'}\) and \({\it \textbf{b}}^{t}\). Specifically,
\({\it \textbf{a}}^{t'} <{\it \textbf{b}}^{t}\) if and only if
\(t'\in P_{T_{k} \left(\aleph \right)} \left(t\right)\).

Now, we return to how to estimate the outputs in each child node with
the aim of guaranteeing the satisfaction of free disposability. For any
node \(t^{*} \in \tilde{T}_{k} \left(\aleph \right)\), the way to
estimate the value of the outputs for the right child node is through
the estimation of the outputs of its parent node, i.e.,
\(y_{r} \left(t_{R} \right)=y_{r} (t^{*} ),\, \, r=1,...,s,\) while the
estimation of outputs for the left child node is:

\begin{equation} \label{(3)} 
y_{r} \left(t_{L} \right)=\max \left\{\max \left\{y_{ri} :\left({\it \textbf{x}}_{i} ,{\it \textbf{y}}_{i} \right)\in t_{L} \right\},y_{r} \left(I_{T\left(k|t^{*} \to t_{L} ,t_{R} \right)} \left(t_{L} \right)\right)\right\},\, \, r=1,...,s, 
\end{equation}

where
\(y_{r} \left(I_{T\left(k|t^{*} \to t_{L} ,t_{R} \right)} \left(t_{L} \right)\right)= \max \left\{y_{r} \left(t'\right):t'\in I_{T\left(k|t^{*} \to t_{L} ,t_{R} \right)} \left(t_{L} \right)\right\}\)
and \(y_{r} \left(t'\right)\) is the estimation of the output \(y_{r}\)
at node \(t'\in \tilde{T}\left(k|t^{*} \to t_{L} ,t_{R} \right)\),
\(r=1,...,s\). This way of estimating the output values guarantees free
disposability.

Accordingly, the algorithm selects the best pair
\(\left(x_{j^{*} } ,s_{j^{*} } \right)\) such that the sum of the MSE of
the left and right child nodes is minimized. Once the split of node
\(t^{*}\) is executed, the tree
\(T\left(k|t^{*} \to t_{L}^{*} ,t_{R}^{*} \right)\) is obtained. This
process continues until bipartition is not possible because all the data
in a node have the same input values or a certain stopping rule is
satisfied. The usual stopping rule is
\(n\left(t\right)\le n_{\min } = 5\), where \(n(t)\) is the sample size
of node \(t\). The final tree built is denoted as
\(T_{\max } \left(\aleph \right)\), which usually is a deep tree.

\(T_{\max} \left(\aleph \right)\) suffers from the same problem as FDH,
i.e., overfitting. \citet{esteve2020} proposed to prune the tree
exploiting the same technique as \citet{breiman1984}. This pruning
process resorts to the notion of the error-complexity measure
\(R_{\alpha} \left(T\left(\aleph \right)\right)\), which is a
combination between a measure of the accuracy of the tree, defined as
the sum of the MSE determined at each leaf node, and a measure of the
number of leaf nodes. Also,
\(R_{\alpha } \left(T\left(\aleph \right)\right)\) depends on a
parameter \(\alpha\), which compensates the values of the two components
of the error:
\(R_{\alpha } \left(T\left(\aleph \right)\right)=R\left(T\left(\aleph \right)\right)+\alpha \left|\tilde{T}\left(\aleph \right)\right|\).
The idea behind the pruning of \(T_{\max } \left(\aleph \right)\) is to
minimize \(R_{\alpha } \left(T\left(\aleph \right)\right)\). The pruning
process is also based on cross-validation (see \citet{breiman1984} for
more details). The tree resulting from the pruned process is
\(T^{*} \left(\aleph \right)\). This tree doesn't suffer from the
overfitting problem. For this reason, the use of
\(T^{*} \left(\aleph \right)\) is recommended rather than
\(T_{\max } \left(\aleph \right)\), unless a descriptive analysis of the
sample is required.

Finally,
\({\it \textbf{d}}_{T^{*} \left(\aleph \right)} \left({\it \textbf{x}}\right)\)
will denote hereinafter the multi-dimensional estimator defined from
\(T^{*} \left(\aleph \right)\) and the sample \(\aleph\), i.e.,
\(d_{rT^{*} \left(\aleph \right)} \left({\it \textbf{x}}\right)=\sum _{t\in T^{*} \left(\aleph \right)}y_{r} \left(t\right)I\left({\it \textbf{x}}\in t\right)\),
for all \(r=1,...,s\), with \(I\left(\cdot \right)\) being the
indication function. From this estimator, it is possible to define a
production possibility set or technology estimated from the Efficiency
Analysis Trees technique as:

\begin{equation} \label{(4)} 
\hat{\Psi}_{T^{*} (\aleph)} =\left\{\left({\it \textbf{x}},{\it \textbf{y}}\right)\in R_{+}^{m+s} :{\it \textbf{y}}\le {\it \textbf{d}}_{T^{*} \left(\aleph \right)} \left({\it \textbf{x}}\right)\right\}. 
\end{equation}

\(\hat{\Psi }_{T^{*} \left(\aleph \right)}\) satisfies free
disposability and the deterministic quality.

By analogy with the existing relationship between FDH and DEA, it is
possible to derive an estimation of \(\Psi\) by the convexification of
the set \(\hat{\Psi }_{T^{*}}\). In this sense, the convexification of
the production possibility set derived from EAT would be as follows:

\begin{equation} \label{(5)} 
conv\left(\hat{\Psi}_{T^{*} } \right)=\left\{\left({\it \textbf{x}},{\it \textbf{y}}\right)\in R_{+}^{m+s} :{\it \textbf{x}}\ge \sum _{t\in \tilde{T}^{*} }^{}\lambda _{t} {\it \textbf{a}}^{t}  ,{\it \textbf{y}}\le \sum _{t\in \tilde{T}^{*} }^{}\lambda _{t} {\it \textbf{d}}_{T^{*} } \left({\it \textbf{a}}^{t} \right) ,\sum _{t\in \tilde{T}^{*} }^{}\lambda_{t}  {\rm =1}{\it ,\boldsymbol{\lambda}}\ge 0_{\left|\tilde{T}^{*} \right|} \right\}. 
\end{equation}

Under the convexity assumption, the EAT methodology is known as the
Convexified Efficiency Analysis Trees technique (hereinafter referred to
as CEAT) \citep[see][]{aparicio2021}.

\hypertarget{section2.2}{%
\subsection{Random Forest for Efficiency Analysis
Trees}\label{section2.2}}

In this section, we briefly describe the extension of the approach by
\citet{esteve2020} to the context of using ensembles of trees to provide
estimates of production frontiers \citep[see][]{esteve2021}.
Specifically, we briefly revise the way to adapt the standard Random
Forest \citep{breiman2001} for estimating production frontiers
satisfying fundamental postulates of microeconomics, such as free
disposability. The adaptation of Random Forest to the estimation of
production frontiers by \citet{esteve2021} is the first one that focuses
on the introduction of a methodology for measuring technical efficiency
that is robust to the resampling of data and, at the same time, to the
specification of input variables.

Data robustness and resampling methods for input modeling are both
topics of interest in the literature on technical efficiency
measurement. Regarding robustness to data, Simar and Wilson
\citetext{\citeyear{simar1998}; \citeyear{simar2000a}; \citeyear{simar2000b}}
were the first ones to adapt the bootstrapping methodology
\citep{effron1979} to the context of DEA and FDH. As regards the
importance of the robustness of input and output variables in
non-parametric efficiency analysis, since the beginning of DEA and FDH,
researchers have always been aware that the selection of input and
output variables to be considered in efficiency analysis is one of the
crucial issues in the specification of the model. In practice, the
researchers' previous experience may lead to the selection of some
inputs and outputs considered essential to represent the underlying
technology. However, there may be other variables whose inclusion in the
model the analyst is not always sure of \citep{pastor2002}. Some
approaches focus on balancing the experience of researchers with the
information provided by observations \citep[see, for
example,][]{banker1993, banker1996, pastor2002}. Another recent approach
is based, in contrast, on determining efficiency scores that are robust
to variable selection by considering all the possible combinations of
inputs and outputs and their aggregation \citep{landete2017}.

On the whole, Random Forest \citep{breiman2001} is an ensemble learning
method that works by constructing a multitude of decision trees by CART
\citep{breiman1984} at training time and aggregating the information of
the individual trees in a final prediction value. In particular, when
Random Forest is applied to regression problems, the final estimator
corresponds to the mean of each individual prediction
\citep{breiman2001}. Random Forest modifies the growing process of an
individual tree as follows, by: (i) applying bootstrapping on the data
training for each individual tree and (ii) selecting a random subset of
the predictors in each iteration. In this way, given a learning sample
\(\aleph\) of size \(n\), Random Forest repeatedly selects random
samples of size \(n\) with replacement of the set \(\aleph\). Then, the
method fits the trees to these samples but, to do this, it uses a
modified tree learning algorithm that chooses, in each candidate
division of the learning process, a random subset of predictors. The
reason for doing this is due to the instability of the model. It is
known that individual decision trees, such as CART, are very unstable
\citep{berk2016}. This means that completely different tree structures
are given when the training data is modified slightly. In this way, the
result of applying Random Forest is an estimator that overcomes
overfitting and instability problems in general, resulting in a
substantial reduction in variance.

The algorithm associated with the adaptation of the Random Forest
technique to the world of technical efficiency assessment, called
RF+EAT, is introduced in \citet{esteve2021}. The steps that must be
carried out in Random Forest for Efficiency Analysis Trees are shown in
Algorithm \hyperref[alg:algo1]{1}. This algorithm is based on the
typical algorithm of Random Forest that can be found in
\citet{kuhn2013}. In Algorithm \hyperref[alg:algo1]{1}, the first step
consists of selecting the number of trees that will make up the forest,
that is, the hyperparameter \(p\). Then, \(p\) (bootstrap) random
samples from the original data sample with replacement are generated.
Next, the Efficiency Analysis Trees algorithm by \citet{esteve2020} is
applied to each subsample applying the stopping rule
\(n\left(t\right)\le n_{\min}\), but without pruning. Also, in this
algorithm, \(n_{\min}\) is treated as an additional hyperparameter that
could be tuned. During the execution of the Efficiency Analysis Trees
algorithm, a subset of input variables (\(mtry\)) from the original set
is randomly selected each time the splitting subroutine is applied. To
do that, one of the following five thumb rules is used following the
literature:

\begin{itemize}
\item
  Breiman's Rule: \(mtry=\frac{m}{3}\),
\item
  Rule DEA1: \(mtry=\frac{n\left(t\right)}{2} -s\)
  \citep{golany1989, homburg2001},
\item
  Rule DEA2: \(mtry=\frac{n\left(t\right)}{3} -s\)
  \citep{nunamaker1985, banker1989, friedman1998, raab2002},
\item
  Rule DEA3: \(mtry=\frac{n\left(t\right)}{2s}\) \citep{dyson2001},
\item
  Rule DEA4:
  \(mtry=\min \left\{\frac{n\left(t\right)}{s}, \frac{n\left(t\right)}{3} -s\right\}\)
  \citep{cooper2007}.
\end{itemize}

\begin{algorithm}
\label{alg:algo1}
\SetAlgoLined
\textbf{Input}: $p$, number of trees \\
                $\aleph$, original data \\
\textbf{Output}: $\{T(\aleph_q): q = 1,..., p\}$ \\
 \For{$q=1$ \textbf{to} $p$}{
  $\aleph_q :=$ Bootstrap sample of $\aleph$ \\
  $T(\aleph_q):=$ Efficiency Analysis Tree trained on $\aleph_q$ \\
  \ForEach{split}{
   Randomly in $T(\aleph_q)$ selects \textit{mtry}$(\leq m)$ of the original inputs using a specific rule\;
   Select the best input in $T(\aleph_q)$ among the \textit{mtry} inputs and split the data
   }{
   $T(\aleph_q)$ is completed when $n(t) \leq n_{min}, \forall t$ leaf node of $T(\aleph_q)$ \\
   ($T(\aleph_q)$ is not pruned)
  }
 }
 \caption{Random Forest for Efficiency Analysis Trees algorithm for estimating production frontiers}
\end{algorithm}

Once Algorithm \hyperref[alg:algo1]{1} has been applied, \(p\) fitted
trees are determined with the aim of obtaining an output estimation
giving an input vector \({\it \textbf{x}}\in R_{+}^{m}\). In this
regard, we have
\(T\left(\aleph _{1} \right),...,T\left(\aleph _{p} \right)\) tree
structures derived from the application of the Efficiency Analysis Trees
algorithm on the \(p\) bootstrap subsamples
\(\aleph _{1} ,...,\aleph _{p}\). Given an input vector
\({\it \textbf{x}}\in R_{+}^{m}\), an output estimator is determined by
averaging the individual estimator corresponding to each tree:

\begin{equation} \label{(6)} 
{\it \textbf{y}}^{RF+EAT\left(\aleph \right)} \left({\it \textbf{x}}\right):=\frac{1}{p} \sum_{q=1}^{p}{\it \textbf{d}}_{T\left(\aleph _{q} \right)} \left({\it \textbf{x}}\right).  
\end{equation}

where
\({\it \textbf{d}}_{T\left(\aleph _{q} \right)} \left({\it \textbf{x}}\right)\)
denotes the output estimator associated with each tree structure
\(T\left(\aleph _{q} \right)\), given an input vector
\({\it \textbf{x}}\in R_{+}^{m}\).

In addition, this estimator allows the technology or production
possibility set to be defined as:

\begin{equation} \label{(7)} 
\hat{\Psi }_{RF+EAT} =\left\{\left({\it \textbf{x}},{\it \textbf{y}}\right)\in R_{+}^{m+s} :{\it \textbf{y}}\le {\it \textbf{y}}^{RF+EAT\left(\aleph \right)} \left({\it \textbf{x}}\right)\right\}. 
\end{equation}

As happens with the standard Random Forest, Random Forest for Efficiency
Analysis Trees also exploits the Out-Of-Bag (OOB) concept. The OOB
estimate at observation
\(\left({\it \textbf{x}}_{i} ,{\it \textbf{y}}_{i} \right)\) consists in
evaluating the prediction of the ensemble just using the individual
models \(T\left(\aleph _{q} \right)\) whose corresponding bootstrap
samples \(\aleph _{q}\) are such that
\(\left({\it \textbf{x}}_{i},{\it \textbf{y}}_{i} \right)\notin \aleph _{q}\).
From this definition, the generalization error is defined as the average
of the OOB estimates calculated over all the observations in the
learning sample \(\aleph\):

\begin{equation} \label{(8)} 
err^{RF+EAT\left(\aleph \right)} =\frac{1}{n} \sum_{\left(x_{i} ,y_{i} \right)\in \aleph }\sum_{r=1}^{s}\left(y_{ri} -y_{r}^{RF+EAT\left(\aleph \right)} \left({\it \textbf{x}}_{i} \right)\right)^{2}.    
\end{equation}

The generalization error is useful for determining a measure of variable
importance, which can be used for creating a sorted list of inputs
\(x_{1} ,...,x_{m}\). The way to calculate the input importance of
variable \(x_{j}\) is: firstly, generate a new database,
\(\aleph ^{j}\), identical to the original one \(\aleph\), where
specifically the values of variable \(x_{j}\) were randomly permuted;
secondly, apply Algorithm \hyperref[alg:algo1]{1} on the new `virtual'
learning sample \(\aleph ^{j}\); thirdly, determine the value of the
generalization error, i.e., \(err^{RF+EAT\left(\aleph ^{j} \right)}\);
and, finally, calculate the percentage increase of the generalization
error when variable \(x_{j}\) is shuffled as:

\begin{equation} \label{(9)} 
\% Inc^{RF+EAT} \left(x_{j} \right)=100\cdot \left(\frac{err^{RF+EAT\left(\aleph ^{j} \right)} -err^{RF+EAT\left(\aleph \right)} }{err^{RF+EAT\left(\aleph \right)}} \right). 
\end{equation}

\hypertarget{section3}{%
\section{Data structure}\label{section3}}

Data are managed as a regular R \texttt{data.frame} in the \pkg{eat}
functions (\texttt{matrix} is often accepted but will be converted to a
\texttt{data.frame} in the functions pre-processing). The main functions
of the \pkg{eat} package are \texttt{EAT()} and \texttt{RFEAT()}, which
return structured objects named \texttt{EAT} and \texttt{RFEAT},
respectively. These objects contain fields with relevant information
such as the estimation results or the arguments introduced by the user
in the function call.

The fields of the \texttt{EAT} object are the following:

\begin{itemize}
\tightlist
\item
  \texttt{data}: Contains the input and output variables.

  \begin{itemize}
  \tightlist
  \item
    \texttt{df}: Data introduced by the user in a \texttt{data.frame}
    structure after being preprocessed.
  \item
    \texttt{x}: Input indexes in \texttt{df}.
  \item
    \texttt{y}: Output indexes in \texttt{df}.
  \item
    \texttt{input\_names}: Name of the input variables in \texttt{df}.
  \item
    \texttt{output\_names}: Name of the output variables in \texttt{df}.
  \item
    \texttt{row\_names}: Name of the observations in \texttt{df}.
  \end{itemize}
\item
  \texttt{control}: Contains the hyperparameters selected by the user.

  \begin{itemize}
  \tightlist
  \item
    \texttt{fold}: Number of folds in which is divided \texttt{df} to
    apply cross-validation.
  \item
    \texttt{numStop}: Minimum number of observations in a node.
  \item
    \texttt{max.leaves}: Maximum number of leaf nodes.
  \item
    \texttt{max.depth}: Maximum number of nodes between the root node
    (not included) and the furthest leaf node.
  \item
    \texttt{na.rm}: A logical variable that indicates if \texttt{NA}
    rows should be ignored.
  \end{itemize}
\item
  \texttt{tree}: \texttt{list} containing the nodes of the fitted
  Efficiency Analysis Trees model. Each node is made up of the following
  elements:

  \begin{itemize}
  \tightlist
  \item
    \texttt{id}: Node index
  \item
    \texttt{F}: Father node index.
  \item
    \texttt{SL}: Left child node index.
  \item
    \texttt{SR}: Right child node index.
  \item
    \texttt{index}: Set of indexes corresponding to the observations in
    a node.
  \item
    \texttt{R}: Error at the node.
  \item
    \texttt{xi}: Index of the variable that produces the split in a
    node.
  \item
    \texttt{s}: Threshold of the variable \texttt{xi}.
  \item
    \texttt{a}: The components of the vector \({\it \textbf{a}}^{t}\).
  \item
    \texttt{b}: The components of the vector \({\it \textbf{b}}^{t}\).
  \end{itemize}
\item
  \texttt{nodes\_df}: Contains the following information related to the
  nodes of the fitted Efficiency Analysis Trees model in a
  \texttt{data.frame} structure:

  \begin{itemize}
  \tightlist
  \item
    \texttt{id}: Node index
  \item
    \texttt{N}: Number of observations in a node.
  \item
    \texttt{Proportion}: Proportion of observations in a node.
  \item
    \texttt{y}: Fitted values.
  \item
    \texttt{R}: Error at the node.
  \item
    \texttt{index}: Indexes of the observations in a node.
  \end{itemize}
\item
  \texttt{model}: Contains the following information related to the
  fitted Efficiency Analysis Trees model:

  \begin{itemize}
  \tightlist
  \item
    \texttt{nodes}: Number of nodes in the tree.
  \item
    \texttt{leaf\_nodes}: Number of leaf nodes in the tree.
  \item
    \texttt{a}: The components of the vector \({\it \textbf{a}}^{t}\).
  \item
    \texttt{y}: Output estimation for each leaf node.
  \end{itemize}
\end{itemize}

Regarding the \texttt{RFEAT} object, it contains the following fields:

\begin{itemize}
\item
  \texttt{data}: same fields as the \texttt{EAT} object.
\item
  \texttt{control}: Contains the hyperparameters selected by the user.

  \begin{itemize}
  \tightlist
  \item
    \texttt{numStop}: Minimum number of observations in a node.
  \item
    \texttt{m}: Number of trees that make up the random forest.
  \item
    \texttt{s\_mtry}: Number of inputs that can be randomly selected in
    each split.
  \item
    \texttt{na.rm}: A logical variable that indicates if \texttt{NA}
    rows should be ignored.
  \end{itemize}
\item
  \texttt{forest}: A \texttt{list} containing the individual Efficiency
  Analysis Trees that make up the random forest.
\item
  \texttt{Error}: The Out-of-Bag error at the random forest.
\item
  \texttt{OOB}: A \texttt{list} containing the observations used for
  training each Efficiency Analysis Tree that makes up the random
  forest.
\end{itemize}

\hypertarget{section3.1}{%
\subsection{Dataset and statistical sources}\label{section3.1}}

\begin{Schunk}
\begin{Sinput}
# We load the library
library("eat")

# We load the data
data("PISAindex")
\end{Sinput}
\end{Schunk}

We illustrate all the models presented in this paper resorting to a
single dataset (\texttt{PISAindex}) available in the \pkg{eat} package.
Our dataset consists of 72 countries with 3 outputs and 13 inputs. The
output data have been collected by the PISA (Programme for International
Student Assessment) 2018 survey \citep{oecd2018} and refers to the
average score in mathematics, reading and science domains for schools in
these countries. Regarding the input data, the variables have been
collected from the \citet{SocialProgressIndex} and are related to the
socioeconomic environment of these countries. These inputs can be
classified into four blocks as follows:

\begin{itemize}
\tightlist
\item
  Basic Human Needs:

  \begin{itemize}
  \tightlist
  \item
    Nutrition and Basic Medical Care (\texttt{NBMC})
  \item
    Water and Sanitation (\texttt{WS})
  \item
    Shelter (\texttt{S})
  \item
    Personal Safety (\texttt{PS}).
  \end{itemize}
\item
  Foundations of Wellbeing:

  \begin{itemize}
  \tightlist
  \item
    Access to Basic Knowledge (\texttt{ABK})
  \item
    Access to Information and Communications (\texttt{AIC})
  \item
    Health and Wellness (\texttt{HW})
  \item
    Environmental Quality (\texttt{EQ}).
  \end{itemize}
\item
  Opportunity:

  \begin{itemize}
  \tightlist
  \item
    Personal Rights (\texttt{PR})
  \item
    Personal Freedom and Choice (\texttt{PFC})
  \item
    Inclusiveness (\texttt{I})
  \item
    Access to Advanced Education (\texttt{AAE}).
  \end{itemize}
\item
  Economy:

  \begin{itemize}
  \tightlist
  \item
    Gross Domestic Product based on Purchasing Power Parity
    (\texttt{GDP\_PPP}).
  \end{itemize}
\end{itemize}

Finally, in order to simplify the examples and reduce computation time,
a subset of variables is selected as follows:

\begin{Schunk}
\begin{Sinput}
# Inputs (5): PR, PFC, I, AAE, GDP_PPP
# Outputs (3): S_PISA, R_PISA, M_PISA
PISAindex <- PISAindex[, c(3, 4, 5, 14, 15, 16, 17, 18)]

head(PISAindex)
\end{Sinput}
\begin{Soutput}
#>     S_PISA R_PISA M_PISA    PR   PFC     I   AAE GDP_PPP
#> SGP    551    549    569 71.70 87.90 48.26 74.31  97.745
#> JPN    529    504    527 94.07 82.40 62.32 81.29  41.074
#> KOR    519    514    526 92.71 79.06 63.54 86.32  41.894
#> EST    530    523    523 95.67 84.10 55.58 73.16  35.308
#> NLD    503    485    519 96.34 89.04 75.82 82.99  56.455
#> POL    511    512    516 86.41 78.25 57.58 76.21  31.766
\end{Soutput}
\end{Schunk}

Table 2 reports the descriptive statistics for these variables (outputs
and inputs).

\begin{Schunk}
\begin{table}

\caption{\label{tab:descr-lat}Descriptive statistics (averages, standard deviations, minimum, median and maximum) of input‚Äìoutput.}
\centering
\fontsize{7}{9}\selectfont
\begin{tabular}[t]{l|l|r|r|r|r|r}
\hline
variable & type & mean & sd & min & median & max\\
\hline
S\_PISA & output & 455.06 & 48.32 & 336.00 & 466.00 & 551.00\\
\hline
R\_PISA & output & 450.89 & 50.52 & 340.00 & 466.00 & 549.00\\
\hline
M\_PISA & output & 454.81 & 52.17 & 325.00 & 463.50 & 569.00\\
\hline
PR & input & 81.62 & 17.98 & 21.14 & 88.40 & 98.07\\
\hline
PFC & input & 75.42 & 11.03 & 47.25 & 78.19 & 91.65\\
\hline
I & input & 54.17 & 17.07 & 12.37 & 55.51 & 81.91\\
\hline
AAE & input & 69.87 & 10.75 & 48.37 & 71.65 & 90.43\\
\hline
GDP\_PPP & input & 36.04 & 21.91 & 7.44 & 31.42 & 114.11\\
\hline
\end{tabular}
\end{table}

\end{Schunk}

\hypertarget{section4}{%
\section{Basic functions of the library}\label{section4}}

In this section, we introduce the main functions of the library related
to Efficiency Analysis Trees and Random Forest for Efficiency Analysis
Trees. To execute the following examples, the package \pkg{eat} must be
loaded and the seed 100 must be set for reproducibility.

\begin{Schunk}
\begin{Sinput}
# We set the seed
set.seed(100)
\end{Sinput}
\end{Schunk}

\hypertarget{the-eat-basic-model}{%
\subsection{The EAT basic model}\label{the-eat-basic-model}}

The basic model of Efficiency Analysis Trees that we explained in
subsection \protect\hyperlink{section2.1}{Efficiency Analysis Trees} can
be implemented in R using the function \texttt{EAT()}:

\begin{Schunk}
\begin{Sinput}
EAT(
  data, x, y,
  numStop = 5,
  fold = 5,
  max.depth = NULL,
  max.leaves = NULL,
  na.rm = TRUE
)
\end{Sinput}
\end{Schunk}

The \texttt{EAT()} function is the cornerstone of the \pkg{eat} library.
The minimum arguments of this function are the data (\texttt{data})
containing the study variables, the indexes of the predictor variables
or inputs (\texttt{x}) and the indexes of the predicted variables or
outputs (\texttt{y}). Additionally, the \texttt{numStop}, \texttt{fold},
\texttt{max.depth} and \texttt{max.leaves} arguments are included for
more experienced users in the fields of machine learning and tree-based
models. Modifying these four hyperparameters allows obtaining different
frontier estimates and therefore selecting the one that best suits the
needs of the analysis. The description of these parameters is as
follows:

\begin{itemize}
\item
  \texttt{numStop} refers to the minimum number of observations in a
  node to be split and is directly related to the size of the tree. The
  higher the value of \texttt{numStop}, the smaller the size of the
  tree.
\item
  \texttt{fold} refers to the number of parts in which the \texttt{data}
  is divided to apply the cross-validation technique. Variations in the
  \texttt{fold} argument are not directly related to the size of the
  tree.
\item
  \texttt{max.depth} limits the number of nodes between the root node
  (not included) and the furthest leaf node. When this argument is
  introduced, the typical process of growth-pruning is not carried out.
  In this case, the tree is allowed to grow to the required depth.
\item
  \texttt{max.leaves} determines the maximum number of leaf nodes. As in
  \texttt{max.depth}, the process of growth-pruning is not performed. In
  this respect, the tree grows until the required number of leaf nodes
  is reached, and then, the tree is returned.
\end{itemize}

Notice that including the arguments \texttt{max.depth} or
\texttt{max.leaves} reduces the computation time by eliminating the
pruning procedure. However, the pruning process is preferred if the
objective of the study is inferential instead of simply descriptive. If
both are included at the same time, a \texttt{warning} message is
displayed and only \texttt{max.depth} is used.

As an example, using data from subsection
\protect\hyperlink{section3.1}{Dataset and statistical sources}, we next
create a multi response tree using the suitable code as follows. Results
are returned as an \texttt{EAT} object, as explained in Section
\protect\hyperlink{section3}{Data structure}.

\begin{Schunk}
\begin{Sinput}
modelEAT <- EAT(data = PISAindex, x = 4:8, y = 1:3)
\end{Sinput}
\end{Schunk}

\hypertarget{the-rfeat-basic-model}{%
\subsection{The RFEAT basic model}\label{the-rfeat-basic-model}}

The basic model of Random Forest for Efficiency Analysis Trees that we
explained in subsection \protect\hyperlink{section2.2}{Random Forest for
Efficiency Analysis Trees} can be implemented in R using the function
\texttt{RFEAT()}:

\begin{Schunk}
\begin{Sinput}
RFEAT(
  data, x, y,
  numStop = 5,
  m = 50,
  s_mtry = "BRM",
  na.rm = TRUE
)
\end{Sinput}
\end{Schunk}

The \texttt{RFEAT()} function has also been developed with the aim of
providing greater statistical robustness to the results obtained by the
\texttt{EAT()} function. The \texttt{RFEAT()} function requires the
\texttt{data} containing the variables for the analysis, \texttt{x} and
\texttt{y} corresponding to the inputs and outputs indexes respectively,
the minimum number of observations in a node for a split to be attempted
(\texttt{numStop}) and \texttt{na.rm} to ignore observations with
\texttt{NA} cells. All these arguments are used for the construction of
the \(p\) (this is denoted with \texttt{m} in the \texttt{RFEAT()}
function) individual Efficiency Analysis Trees that make up the random
forest. Finally, the argument \texttt{s\_mtry} indicates the number of
inputs that can be randomly selected in each split. It can be set as any
integer although there are also certain predefined values. Let \(m\) be
the number of inputs, let \(s\) be the number of outputs and let
\(n(t)\) be the number of observations in a node. Then, the predefined
values for \texttt{s\_mtry} are:

\begin{itemize}
\item
  \texttt{BRM} = \(\frac{m}{3}\),
\item
  \texttt{DEA1} = \(\frac{n\left(t\right)}{2}-s\),
\item
  \texttt{DEA2} = \(\frac{n\left(t\right)}{3}-s\),
\item
  \texttt{DEA3} = \(\frac{n\left(t\right)}{2s}\),
\item
  \texttt{DEA4} =
  \(\min \left\{\frac{n\left(t\right)}{s}, \frac{n\left(t\right)}{3}-s \right\}\).
\end{itemize}

As an example, using data from subsection
\protect\hyperlink{section3.1}{Dataset and statistical sources}, we next
create a forest with 30 trees. Results are returned as an \texttt{RFEAT}
object, as explained in Section \protect\hyperlink{section3}{Data
structure}.

\begin{Schunk}
\begin{Sinput}
modelRFEAT <- RFEAT(data = PISAindex, x = 4:8, y = 1:3, m = 30)
\end{Sinput}
\end{Schunk}

\hypertarget{predictions}{%
\subsection{Predictions}\label{predictions}}

The estimators of the Efficiency Analysis Trees and Random Forest for
Efficiency Analysis Trees can be computed in R using the function
\texttt{predict()}:

\begin{Schunk}
\begin{Sinput}
predict(
  object,
  newdata,
  x, ...
)
\end{Sinput}
\end{Schunk}

Regarding the arguments of \texttt{predict()}, \texttt{object} can be an
\texttt{EAT} or an \texttt{RFEAT} object, \texttt{newdata} refers to a
\texttt{data.frame} and \texttt{x} to the set of inputs to be used. This
function returns a \texttt{data.frame} with the expected output for a
set of observations. For predictions using an \texttt{EAT} object, only
one tree is used. However, for the RFEAT model, the output is predicted
by each of the \(p\) (\texttt{m} in the \texttt{RFEAT()} function)
individual trees trained and subsequently the mean value of all
predictions is obtained.

As an example, we next evaluate the last 3 DMUs from the data of
subsection \protect\hyperlink{section3.1}{Dataset and statistical
sources} and the corresponding EAT and RFEAT models. Results are
returned in a \texttt{data.frame} structure with the output predictions:

\begin{Schunk}
\begin{Sinput}
predict(object = modelEAT, newdata = tail(PISAindex, 3), x = 4:8)
\end{Sinput}
\begin{Soutput}
#>   S_PISA_pred R_PISA_pred M_PISA_pred
#> 1         428         424         437
#> 2         377         359         368
#> 3         377         359         368
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
predict(object = modelRFEAT, newdata = tail(PISAindex, 3), x = 4:8)
\end{Sinput}
\begin{Soutput}
#>   S_PISA_pred R_PISA_pred M_PISA_pred
#> 1    439.9667    435.1333    441.2000
#> 2    402.0667    389.0667    403.9000
#> 3    399.0333    389.3333    399.6333
\end{Soutput}
\end{Schunk}

In the same way, the user can also create a new \texttt{data.frame} and
calculate predictions for it as follows:

\begin{Schunk}
\begin{Sinput}
new <- data.frame(AAE = c(61, 72), PR = c(76, 81), I = c(41, 55), GDP_PPP = c(19, 31),
                  PFC = c(67, 78))
                  
predict(object = modelEAT, newdata = new, x = 1:5)
\end{Sinput}
\begin{Soutput}
#>   S_PISA_pred R_PISA_pred M_PISA_pred
#> 1         428         424         421
#> 2         481         479         488
\end{Soutput}
\end{Schunk}

\hypertarget{importance-of-predictor-variables}{%
\subsection{Importance of predictor
variables}\label{importance-of-predictor-variables}}

The way to compute in R the predictor variables importance in the
Efficiency Analysis Trees methodology is using the functions
\texttt{rankingEAT()} or \texttt{rankingRFEAT()}:

\begin{Schunk}
\begin{Sinput}
# Through Efficiency Analysis Trees
rankingEAT(
  object,
  barplot = TRUE,
  threshold = 70,
  digits = 2
)

# Through Random Forest for Efficiency Analysis Trees
rankingRFEAT(
  object,
  barplot = TRUE,
  digits = 2
)
\end{Sinput}
\end{Schunk}

These functions allow a selection of variables by calculating a score of
importance through Efficiency Analysis Trees or Random Forest for
Efficiency Analysis Trees, respectively. These importance scores
represent how influential each variable is in the model. Regarding the
Efficiency Analysis Trees {[}\texttt{RankingEAT()}{]}, the notion of
surrogate splits by \citet{breiman1984} was implemented. In this regard,
the measure of importance of a variable \(x_j\) is defined as the sum
over all nodes of the decrease in mean squared error produced by the
best surrogate split on \(x_j\) at each node (see Definition 5.9 in
\citet{breiman1984}). Since only the relative magnitudes of these
measures are interesting for researchers, the actual measures of
importance that we report are normalized. In this way, the most
important variable has always a value of 100, and the others are in the
range 0 to 100. As for the Random Forest for Efficiency Analysis Trees
{[}\texttt{RankingRFEAT()}{]}, \eqref{(9)} was implemented for each
input variable. Regarding the available arguments of the functions, the
user can specify the number of decimal units (\texttt{digits}) and
include a barplot (from \texttt{ggplot2}) with the scores of importance
(\texttt{barplot}). Additionally, the \texttt{rankingEAT()} function
allows to display a horizontal line in the graph to facilitate the
cut-off point between important and irrelevant variables
(\texttt{threshold}).

As an example, we next use the objects \texttt{modelEAT} (an
\texttt{EAT} object from the \texttt{EAT()} function) and
\texttt{modelRFEAT} (an \texttt{RFEAT} object from the \texttt{RFEAT()}
function) created in the previous section to assess the predictors used.
These functions return the name of the predictor variables, the scores
of importance (in the range 0-100 for the \texttt{rankingEAT()}
function) and a barplot (without horizontal line for the
\texttt{rankingRFEAT()} function).

\begin{Schunk}
\begin{Sinput}
rankingEAT(object = modelEAT)
\end{Sinput}
\begin{Soutput}
#> $scores
#>         Importance
#> AAE         100.00
#> GDP_PPP      82.13
#> I            72.58
#> PR           72.45
#> PFC          29.07
#> 
#> $barplot
\end{Soutput}
\begin{figure}

{\centering \includegraphics{EAT_files/figure-latex/ex6-1} 

}

\caption[Barplot generated by applying `rankingEAT()` to the PISAindex database to determine the ranking of variable importance]{Barplot generated by applying `rankingEAT()` to the PISAindex database to determine the ranking of variable importance.}\label{fig:ex6}
\end{figure}

\end{Schunk}

\begin{Schunk}
\begin{Sinput}
rankingRFEAT(object = modelRFEAT)
\end{Sinput}
\begin{Soutput}
#> $scores
#>         Importance
#> PR            1.75
#> PFC          -1.64
#> GDP_PPP      -2.16
#> I            -2.87
#> AAE          -3.67
#> 
#> $barplot
\end{Soutput}
\begin{figure}

{\centering \includegraphics{EAT_files/figure-latex/ex7-1} 

}

\caption[Barplot generated by applying `rankingRFEAT()` to the PISAindex database to determine the ranking of variable importance]{Barplot generated by applying `rankingRFEAT()` to the PISAindex database to determine the ranking of variable importance.}\label{fig:ex7}
\end{figure}

\end{Schunk}

Note that negative scores may appear when calculating the importance of
variables using the \texttt{rankingRFEAT()} function. The appearance of
this type of (negative) score can be understood as, if that variable
were removed from the model, \emph{ceteris paribus}, then an improvement
in the predictive capacity of the model would be produced.

\hypertarget{section5}{%
\section{Basic EAT and RFEAT models}\label{section5}}

Efficiency scores are numerical values that indicate the degree of
efficiency of a set of Decision Making Units (DMU). In the \pkg{eat}
package, these scores can be calculated through an Efficiency Analysis
Trees model, a Random Forest for Efficiency Analysis Trees model or a
Convexified Efficiency Analysis Trees model. The code is as follows:

\begin{Schunk}
\begin{Sinput}
# For Efficiency Analysis Trees
efficiencyEAT(
  data, x, y, object, scores_model, digits = 3,
  FDH = TRUE, print.table = FALSE, na.rm = TRUE
)

# For Random Forest for Efficiency Analysis Trees
efficiencyRFEAT(
  data, x, y, object, digits = 3, 
  FDH = TRUE, print.table = FALSE, na.rm = TRUE
)

# For Convexified Efficiency Analysis Trees
efficiencyCEAT(
  data, x, y, object, scores_model, digits = 3,
  DEA = TRUE, print.table = FALSE, na.rm = TRUE
)
\end{Sinput}
\end{Schunk}

A dataset (\texttt{data}) and the corresponding indexes of input(s)
(\texttt{x}) and output(s) (\texttt{y}) must be entered. It is
recommended that the \texttt{data} with the DMUs whose efficiency is to
be calculated coincide with those used to estimate the frontier.
However, it is also possible to calculate the efficiency scores for new
\texttt{data}. The efficiency scores are calculated using the
mathematical programming model included in the argument
\texttt{scores\_model}. The following models are available:

\begin{itemize}
\item
  \texttt{BCC.OUT}: The output-oriented radial model \citep{banker1984}.
\item
  \texttt{BCC.INP}: The input-oriented radial model \citep{banker1984}.
\item
  \texttt{RSL.OUT}: The output-oriented Russell model \citep{fare1978}.
\item
  \texttt{RSL.INP}: The input-oriented Russell model \citep{fare1978}.
\item
  \texttt{DDF}: The Directional Distance Function \citep{chambers1998}.
\item
  \texttt{WAM.MIP}: The Measure of Inefficiency Proportions as a type of
  Weighted Additive Model \citep{lovell1995}.
\item
  \texttt{WAM.RAM}: The Range-Adjusted Measure of Inefficiency as a type
  of Weighted Additive Model \citep{lovell1995, cooper1999}.
\end{itemize}

FDH or DEA scores can optionally be computed by setting
\texttt{FDH\ =\ TRUE} or \texttt{DEA\ =\ TRUE}, respectively. Finally, a
summary descriptive table of the efficiency scores can be displayed with
the argument \texttt{print.table\ =\ TRUE}.

\hypertarget{the-output-oriented-radial-model}{%
\subsection{The output-oriented radial
model}\label{the-output-oriented-radial-model}}

The output-oriented radial model determines the efficiency score for
\(\left({\it \textbf{x}}_{k} ,{\it \textbf{y}}_{k} \right)\in R_{+}^{m+s}\)
by equiproportionally increasing all its outputs while maintaining
inputs constant:
\(\phi \left({\it \textbf{x}}_{k}, {\it \textbf{y}}_{k} \right)=\max \{\phi _{k} \in R:\)
\(\left({\it \textbf{x}}_{k} ,\phi_{k} {\it \textbf{y}}_{k} \right)\in \Psi \}\).

The efficiency score
\(\phi \left({\it \textbf{x}}_{k}, {\it \textbf{y}}_{k} \right)\) can be
estimated through FDH by plugging \(\hat{\Psi }_{FDH}\) from \eqref{(2)}
into
\(\max \left\{\phi _{k} \in R:\left({\it \textbf{x}}_{k} ,\phi _{k} {\it \textbf{y}}_{k} \right)\in \Psi \right\}\)
in place of \(\Psi\). In that case, the optimization problem can be
rewritten as a mixed-integer linear optimization program, as follows:

\begin{equation} \label{(10)} 
\begin{array}{lllll} 
{\phi ^{FDH} \left({\it \textbf{x}}_{k} ,{\it \textbf{y}}_{k} \right)=} & {\max } & {\phi,} & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} x_{ji} \le x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} y_{ri} \ge \phi y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} = 1,} & {} & {} \\ 
{} & {} & {\lambda _{i} \in \left\{0,1\right\},} & {i=1,...,n} & {} 
\end{array} 
\end{equation}

The Linear Programming model that should be solved under Data
Envelopment Analysis would be:

\begin{equation} \label{(11)} 
\begin{array}{lllll} 
{\phi ^{DEA} \left({\it \textbf{x}}_{k} ,{\it \textbf{y}}_{k} \right)=} & {\max } & {\phi_{}, } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} x_{ji}  \le x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} y_{ri}  \ge \phi_{} y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i}  =1,} & {} & {} \\ 
{} & {} & {\lambda _{i} \ge 0,} & {i=1,...,n} & {} 
\end{array} 
\end{equation}

The following Mixed-Integer Linear Program should be solved for
Efficiency Analysis Trees:

\begin{equation} \label{(12)} 
\begin{array}{lllll} 
{\phi ^{EAT} \left({\it \textbf{x}}_{k} ,{\it \textbf{y}}_{k} \right)=} & {\max } & {\phi, } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}} \lambda_{t} {\it a}_{j}^{t}  \le x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}} \lambda_{t} {\it d}_{rT^{*}} \left({\it \textbf{a}}^{t} \right) \ge \phi y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}}^{}\lambda_{t}  =1,} & {} & {} \\ 
{} & {} & {\lambda _{t} \in \left\{0,1\right\},} & {t\in \tilde{T}^{*} } & {} 
\end{array} 
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "BCC.OUT"} in \texttt{efficiencyEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, 
                        scores_model = "BCC.OUT", digits = 2, 
                        print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev. Min Q1 Median   Q3  Max
#>    EAT 1.03      0.04   1  1   1.01 1.01 1.16
#>    FDH 1.01      0.02   1  1   1.00 1.00 1.12
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     EAT_BCC_OUT FDH_BCC_OUT
#> CHL        1.09        1.05
#> SAU        1.05        1.00
#> CAN        1.01        1.01
\end{Soutput}
\end{Schunk}

Finally, the optimization model that should be solved for Convexified
Efficiency Analysis Trees is:

\begin{equation} \label{(13)} 
\begin{array}{lllll} 
{\phi ^{CEAT} \left({\it \textbf{x}}_{k} ,{\it \textbf{y}}_{k} \right)=} & {\max } & {\phi, } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}}^{}\lambda_{t} {\it a}_{j}^{t}  \le x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}}^{}\lambda_{t} {\it d}_{rT^{*}} \left({\it \textbf{a}}^{t} \right) \ge \phi y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}}^{}\lambda_{t}=1,} & {} & {} \\ 
{} & {} & {\lambda _{t} \ge 0,} & {t\in \tilde{T}^{*}} & {} 
\end{array}
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "BCC.OUT"} in \texttt{efficiencyCEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, 
                         scores_model = "BCC.OUT", digits = 2,
                         print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev. Min   Q1 Median   Q3  Max
#>   CEAT 1.11      0.07   1 1.05   1.09 1.09 1.31
#>    DEA 1.05      0.04   1 1.01   1.05 1.05 1.18
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     CEAT_BCC_OUT DEA_BCC_OUT
#> BLR         1.07        1.00
#> SVK         1.07        1.04
#> RUS         1.04        1.00
\end{Soutput}
\end{Schunk}

In the case of the output-oriented radial model, \citet{esteve2021}
showed how this measure can be computed through Random Forest where
\(\phi \left({\it \textbf{x}}_{k} ,{\it \textbf{y}}_{k} \right)=\max \left\{\phi _{k} \in R:\left({\it \textbf{x}}_{k} ,\phi_{k} {\it \textbf{y}}_{k} \right)\in \Psi \right\}\)
can be estimated by substituting the theoretical production possibility
set \(\Psi\) by its estimation \(\hat{\Psi}_{RF+EAT}\), i.e.,
\(\phi ^{RF+EAT} \left({\it \textbf{x}}_{k} ,{\it \textbf{y}}_{k} \right)= \max \left\{\phi _{k} \in R:\left({\it \textbf{x}}_{k} ,\phi _{k} {\it \textbf{y}}_{k} \right)\in \hat{\Psi }_{RF+EAT} \right\}\).
In particular,
\(\phi^{RF+EAT} \left({\it \textbf{x}}_{k} ,{\it \textbf{y}}_{k} \right)\)
may be calculated as:

\begin{equation} \label{(14)} 
\phi^{RF+EAT} \left({\it \textbf{x}}_{k} ,{\it \textbf{y}}_{k} \right)={\mathop{\min }\limits_{r=1,...,s}} \left\{\frac{y_{r}^{RF+EAT\left(\aleph \right)} \left({\it \textbf{x}}_{k} \right)}{y_{rk} } \right\},  
\end{equation}

where
\(y_{r}^{RF+EAT\left(\aleph \right)} \left({\it \textbf{x}}_{k} \right)\)
is the estimation of the r-th output given the input bundle
\({\it \textbf{x}}_{k}\).

In R, this model can be computed using \texttt{efficiencyREAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyRFEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelRFEAT, 
                          digits = 2, print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev.  Min Q1 Median   Q3  Max
#>  RFEAT 1.03      0.04 0.94  1   1.02 1.02 1.15
#>    FDH 1.01      0.02 1.00  1   1.00 1.00 1.12
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     RFEAT_BCC_OUT FDH_BCC_OUT
#> SGP          0.94           1
#> HUN          0.99           1
#> MEX          1.00           1
\end{Soutput}
\end{Schunk}

\hypertarget{the-input-oriented-radial-model}{%
\subsection{The input-oriented radial
model}\label{the-input-oriented-radial-model}}

By analogy with the previous section, where the output-oriented radial
model was shown, it is possible to calculate the input-oriented radial
technical efficiency of the input-output bundle
\((\textbf{x}_k, \textbf{y}_k)\) by solving the following Mixed-Integer
Linear Program, counterpart to \eqref{(10)}:

\begin{equation} \label{(15)} 
\begin{array}{lllll} {} & {\min } & {\theta, } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} x_{ji}  \le \theta_{} x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} y_{ri}  \ge y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i}  =1,} & {} & {} \\ 
{} & {} & {\lambda_{i} \in \left\{0,1\right\},} & {i=1,...,n} & {} 
\end{array} 
\end{equation}

The same type of technical measure can be estimated through DEA by
convexification of the production frontier generated by FDH. Next, we
show the Linear Programming model that should be solved in that case:

\begin{equation} \label{(16)} 
\begin{array}{lllll}  
{} & {\min } & {\theta, } & {} & {}  \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} x_{ji}  \le \theta_{} x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} y_{ri}  \ge y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i}  =1,} & {} & {} \\ 
{} & {} & {\lambda _{i} \ge 0,} & {i=1,...,n} & {} 
\end{array} 
\end{equation}

The input-oriented radial model in the case of the Efficiency Analysis
Trees technique can be determined through the following Mixed-Integer
Linear Program:

\begin{equation} \label{(17)} 
\begin{array}{lllll} 
{} & {\min } & {\theta,} & {} & {}  \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it a}_{j}^{t}  \le \theta x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it d}_{rT^{*}} \left({\it \textbf{a}}^{t} \right) \ge y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda _{t}  =1,} & {} & {} \\ 
{} & {} & {\lambda _{t} \in \left\{0,1\right\},} & {t\in \tilde{T}^{*} } & {} 
\end{array} 
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "BCC.INP"} in \texttt{efficiencyEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, 
                        scores_model = "BCC.INP", digits = 2,
                        print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev.  Min   Q1 Median   Q3 Max
#>    EAT 0.94      0.06 0.69 0.90   0.96 0.96   1
#>    FDH 0.98      0.03 0.90 0.97   1.00 1.00   1
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     EAT_BCC_INP FDH_BCC_INP
#> DEU        0.88        0.92
#> KAZ        1.00        1.00
#> SRB        0.99        1.00
\end{Soutput}
\end{Schunk}

Additionally, under the Convexified Efficiency Analysis Trees technique,
the optimization model corresponding to the convexification of the
production possibility set derived from \(conv(\hat{\Psi }_{T^{*}})\)
from \eqref{(5)} should be solved in order to determine an estimation of
the input-oriented radial measure as follows:

\begin{equation} \label{(18)} 
\begin{array}{lllll} 
{} & {\min } & {\theta,} & {} & {}  \\
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it a}_{j}^{t}  \le \theta_{} x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it d}_{rT^{*}} \left({\it \textbf{a}}^{t} \right) \ge y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t}  =1,} & {} & {} \\ 
{} & {} & {\lambda _{t} \ge 0,} & {t\in \tilde{T}^{*} } & {} 
\end{array} 
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "BCC.INP"} in \texttt{efficiencyCEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, 
                         scores_model = "BCC.INP", digits = 2, 
                         print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev.  Min   Q1 Median   Q3 Max
#>   CEAT 0.82      0.08 0.69 0.76   0.81 0.81   1
#>    DEA 0.92      0.07 0.72 0.87   0.91 0.91   1
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     CEAT_BCC_INP DEA_BCC_INP
#> QAT         0.93        1.00
#> CYP         0.69        0.78
#> CHE         0.73        0.85
\end{Soutput}
\end{Schunk}

\hypertarget{the-output-oriented-russell-measure}{%
\subsection{The output-oriented Russell
measure}\label{the-output-oriented-russell-measure}}

The output-oriented Russell measure under FDH must be calculated through
the following optimization model:

\begin{equation} \label{(19)} 
\begin{array}{lllll} 
{} & {\max } & {\frac{1}{s} \sum_{r=1}^{s}\phi_{r},} & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} x_{ji}  \le x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} y_{ri}  \ge \phi_{r} y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t}=1,} & {} & {} \\ 
{} & {} & {\lambda_{i} \in \left\{0,1\right\},} & {i=1,...,n} & {} \\ 
{} & {} & {{\it \boldsymbol{\phi}}\ge {\bf 1}_{s}.} 
\end{array}
\end{equation}

Under DEA, the corresponding model would be:

\begin{equation} \label{(20)} 
\begin{array}{lllll} 
{} & {\max } & {\frac{1}{s} \sum_{r=1}^{s}\phi_{r},} & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} x_{ji} \le x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} y_{ri} \ge \phi_{r} y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t}=1,} & {} & {} \\ 
{} & {} & {\lambda_{i} \ge 0,} & {i=1,...,n} & {} \\ 
{} & {} & {{\it \boldsymbol{\phi}}\ge {\bf 1}_{s}.} 
\end{array}
\end{equation}

If we resort to the Efficiency Analysis Trees technique, then the model
to be solved should be the following:

\begin{equation} \label{(21)} 
\begin{array}{lllll} 
{} & {\max } & {\frac{1}{s} \sum_{r=1}^{s}\phi_{r},} & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}}\lambda_{t} {\it a}_{j}^{t} \le x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}}\lambda_{t} {\it d}_{rT^{*}} ({\it \textbf{a}}^{t}) \ge \phi_{r} y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t}=1,} & {} & {} \\ 
{} & {} & {\lambda_{t} \in \left\{0,1\right\},} & {i=1,...,n} & {} \\ 
{} & {} & {{\it \boldsymbol{\phi}}\ge {\bf 1}_{s}.} 
\end{array}
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "RSL.OUT"} in \texttt{efficiencyEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, 
                        scores_model = "RSL.OUT", digits = 2,
                        print.table = TRUE)

scores %>% sample_n(3)
\end{Sinput}
\end{Schunk}

Finally, under the Convexified Efficiency Analysis Trees technique, the
model would be:

\begin{equation} \label{(22)} 
\begin{array}{lllll} 
{} & {\max } & {\frac{1}{s} \sum_{r=1}^{s}\phi_{r},  } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it a}_{j}^{t}  \le x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it d}_{rT^{*}} \left({\it \textbf{a}}^{t} \right) \ge \phi_{r} y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t}  =1,} & {} & {} \\ 
{} & {} & {\lambda_{t} \ge 0,} & {i=1,...,n} & {} \\ 
{} & {} & {{\it \boldsymbol{\phi}}\ge {\bf 1}_{s}.} 
\end{array}
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "RSL.OUT"} in \texttt{efficiencyCEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, 
                         scores_model = "RSL.OUT", digits = 2, 
                         print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev. Min   Q1 Median   Q3  Max
#>   CEAT 1.13      0.08   1 1.07   1.10 1.10 1.34
#>    DEA 1.06      0.05   1 1.02   1.06 1.06 1.22
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     CEAT_RSL_OUT DEA_RSL_OUT
#> LVA         1.07        1.05
#> CHL         1.18        1.13
#> MAR         1.14        1.00
\end{Soutput}
\end{Schunk}

\hypertarget{the-input-oriented-russell-measure}{%
\subsection{The input-oriented Russell
measure}\label{the-input-oriented-russell-measure}}

By analogy with the output-oriented Russell measure, the input-oriented
Russell measure should be calculated through the following optimization
models, depending on the selected approach:

\begin{equation} \label{(23)} 
\begin{array}{lllll} 
{} & {\min } & {\frac{1}{m} \sum_{j=1}^{m}\theta_{j},  } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} x_{ji} \le \theta_{j} x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} y_{ri} \ge y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t}  =1,} & {} & {} \\ 
{} & {} & {\lambda_{i} \in \left\{0,1\right\},} & {i=1,...n} & {} \\ 
{} & {} & {{\it \boldsymbol{\theta}}\le {\bf 1}_{m}.} 
\end{array}
\end{equation}

Under DEA, the corresponding model would be:

\begin{equation} \label{(24)} 
\begin{array}{lllll} 
{} & {\min } & {\frac{1}{m} \sum_{j=1}^{m}\theta_{j},} & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} x_{ji}  \le \theta_{j} x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} y_{ri}  \ge y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t}=1,} & {} & {} \\ 
{} & {} & {\lambda _{i} \ge 0,} & {i=1,...n} & {} \\ 
{} & {} & {{\it \boldsymbol{\theta}}\le {\bf 1}_{m}.}  
\end{array}
\end{equation}

If we resort to the Efficiency Analysis Trees technique, then the model
to be solved should be the following:

\begin{equation} \label{(25)} 
\begin{array}{lllll} 
{} & {\min } & {\frac{1}{m} \sum_{j=1}^{m}\theta_{j},  } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it a}_{j}^{t}  \le \theta_{j} x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it d}_{rT^{*}} \left({\it \textbf{a}}^{t} \right) \ge y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t}=1,} & {} & {} \\ 
{} & {} & {\lambda _{t} \in \left\{0,1\right\},} & {i=1,...,n} & {} \\ 
{} & {} & {{\it \boldsymbol{\theta}}\le {\bf 1}_{m}.}
\end{array} 
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "RSL.INP"} in \texttt{efficiencyEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, 
                        scores_model = "RSL.INP", digits = 2, 
                        print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev.  Min   Q1 Median   Q3  Max
#>    EAT 0.58      0.09 0.43 0.52   0.56 0.56 0.81
#>    FDH 0.87      0.10 0.59 0.81   0.86 0.86 1.00
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     EAT_RSL_INP FDH_RSL_INP
#> LBN        0.58        0.73
#> MAR        0.69        0.97
#> MKD        0.58        0.87
\end{Soutput}
\end{Schunk}

Finally, under the Convexified Efficiency Analysis Trees technique, the
model would be:

\begin{equation} \label{(26)} 
\begin{array}{lllll} 
{} & {\min } & {\frac{1}{m} \sum_{j=1}^{m}\theta_{j},  } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it a}_{j}^{t}  \le \theta_{j} x_{jk} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it d}_{rT^{*}} \left({\it \textbf{a}}^{t} \right) \ge y_{rk} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t}  =1,} & {} & {} \\ 
{} & {} & {\lambda _{t} \ge 0,} & {i=1,...,n} & {} \\ 
{} & {} & {{\it \boldsymbol{\theta}}\le {\bf 1}_{m}.}
\end{array} 
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "RSL.INP"} in \texttt{efficiencyCEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, 
                         scores_model = "RSL.INP", digits = 2,
                         print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev.  Min   Q1 Median   Q3  Max
#>   CEAT 0.54      0.08 0.43 0.49   0.53 0.53 0.79
#>    DEA 0.80      0.11 0.59 0.74   0.78 0.78 1.00
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     CEAT_RSL_INP DEA_RSL_INP
#> ARG         0.44        0.59
#> LUX         0.44        0.65
#> CHE         0.49        0.74
\end{Soutput}
\end{Schunk}

\hypertarget{the-directional-distance-function}{%
\subsection{The directional distance
function}\label{the-directional-distance-function}}

\citet{chambers1998} introduced the directional distance function (DDF)
as a technical efficiency measure that projects
\((\textit{x}_k, \textit{y}_k)\) through a pre-assigned direction
\(\textbf{g}=(-\textbf{g}_{j}^{-},+\textbf{g}_{r}^{+}) \ne 0_{m+s}, \textbf{g}_{j}^{-} \in R^{m}, \textbf{g}_{r}^{+} \in R^{s}\)
to the efficiency frontier of the corresponding technology. Under FDH,
the DDF is calculated as follows:

\begin{equation} \label{(27)} 
\begin{array}{lllll} 
{} & {\max } & {\beta_{k}, } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} x_{ji}  \le x_{jk} -\beta_{k} g_{j}^{-} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} y_{ri}  \ge y_{rk} +\beta_{k} g_{r}^{+} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda _{i}  =1,} & {} & {} \\ 
{} & {} & {\lambda _{i} \in \left\{0,1\right\},} & {i=1,...,n} & {} 
\end{array} 
\end{equation}

The corresponding linear program in DEA is as follows:

\begin{equation} \label{(28)} 
\begin{array}{lllll} 
{} & {\max } & {\beta_{k}, } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} x_{ji} \le x_{jk} -\beta_{k} g_{j}^{-} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} y_{ri}  \ge y_{rk} +\beta_{k} g_{r}^{+} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i}=1,} & {} & {} \\ 
{} & {} & {\lambda_{i} \ge 0,} & {i=1,...,n} & {} 
\end{array} 
\end{equation}

In the context of Efficiency Analysis Trees, the DDF is calculated
through the following Mixed-Integer Linear Program:

\begin{equation} \label{(29)} 
\begin{array}{lllll} 
{} & {\max } & {\beta_{k}, } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}}^{}\lambda_{t}{\it a}_{j}^{t}  \le x_{jk} -\beta_{k} g_{j}^{-} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}}^{}\lambda_{t}{\it d}_{rT^{*}} \left({\it \textbf{a}}^{t} \right) \ge y_{rk} +\beta_{k} g_{r}^{+} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}}^{}\lambda_{t}=1,} & {} & {} \\ 
{} & {} & {\lambda _{t} \in \left\{0,1\right\},} & {t\in \tilde{T}^{*}. } & {} 
\end{array} 
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "DDF"} in \texttt{efficiencyEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, 
                        scores_model = "DDF", digits = 2, 
                        print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev. Min Q1 Median   Q3  Max
#>    EAT 0.02      0.02   0  0   0.01 0.01 0.13
#>    FDH 0.01      0.01   0  0   0.00 0.00 0.05
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     EAT_DDF FDH_DDF
#> MDA    0.00    0.00
#> LVA    0.00    0.00
#> BRA    0.03    0.01
\end{Soutput}
\end{Schunk}

In the case of Convexified Efficiency Analysis Trees, the optimization
model is as follows.

\begin{equation} \label{(30)} 
\begin{array}{lllll} 
{} & {\max } & {\beta _{k}, } & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it a}_{j}^{t} \le x_{jk} -\beta_{k} g_{j}^{-} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t} {\it d}_{rT^{*}} \left({\it \textbf{a}}^{t} \right) \ge y_{rk} +\beta_{k} g_{r}^{+} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*} }^{}\lambda_{t}=1,} & {} & {} \\ 
{} & {} & {\lambda _{t} \ge 0,} & {t\in \tilde{T}^{*}. } & {} 
\end{array} 
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "DDF"} in \texttt{efficiencyCEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, 
                         scores_model = "DDF", digits = 2, 
                         print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev. Min   Q1 Median   Q3  Max
#>   CEAT 0.07      0.04   0 0.05   0.06 0.06 0.18
#>    DEA 0.03      0.03   0 0.00   0.03 0.03 0.12
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     CEAT_DDF DEA_DDF
#> IRL     0.05    0.03
#> LBN     0.15    0.10
#> FRA     0.07    0.06
\end{Soutput}
\end{Schunk}

\hypertarget{the-weighted-additive-model}{%
\subsection{The weighted additive
model}\label{the-weighted-additive-model}}

The additive model measures technical efficiency based on input excesses
and output shortfalls. It characterizes efficiency in terms of the input
and output slacks: \(\textbf{s}^{-} \in R^{m}\) and
\(\textbf{s}^{+} \in R^{s}\), respectively. The \pkg{eat} package
implements the weighted additive model formulation of
\citet{lovell1995}, where
\(({\it \textbf{w}}^{-} ,{\it \textbf{w}}^{+})\in R_{+}^{m} \times R_{+}^{s}\)
are the input and output weights whose elements can vary across DMUs.

In the case of the FDH, the optimization program to be solved would be:

\begin{equation} \label{(31)} 
\begin{array}{lllll} 
{} & {\max } & {\sum_{j=1}^{m}w_{j}^{-} {\it s}_{jk}^{-} + \sum_{r=1}^{s}w_{r}^{+} {\it s}_{rk}^{+},} & {} & {} \\
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} x_{ji} \le x_{jk} - {\it s}_{jk}^{-} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} y_{ri} \ge y_{rk} + {\it s}_{rk}^{+} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} = 1,} & {} & {} \\ 
{} & {} & {\lambda _{i} \in \left\{0,1\right\},} & {i=1,...,n} & {} \\ 
{} & {} & {{\it \textbf{s}}_{k}^{-} \ge \textbf{0}_{m}, {\it \textbf{s}}_{k}^{+} \ge \textbf{0}_{s}.} 
\end{array}
\end{equation}

Under DEA, the model would be as follows:

\begin{equation} \label{(32)} 
\begin{array}{lllll} 
{} & {\max } & {\sum_{j=1}^{m}w_{j}^{-} {\it s}_{jk}^{-} + \sum_{r=1}^{s}w_{r}^{+} {\it s}_{rk}^{+},} & {} & {} \\
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} x_{ji} \le x_{jk} - {\it s}_{jk}^{-} ,} & {j=1,...,m} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{t} y_{ri} \ge y_{rk} + {\it s}_{rk}^{+} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} = 1,} & {} & {} \\ 
{} & {} & {\lambda _{i} \ge 0,} & {i=1,...,n} & {} \\ 
{} & {} & {{\it \textbf{s}}_{k}^{-} \ge \textbf{0}_{m}, {\it \textbf{s}}_{k}^{+} \ge \textbf{0}_{s}.} 
\end{array}
\end{equation}

Within the framework of Efficiency Analysis Trees, the weighted additive
model would be calculated as follows:

\begin{equation} \label{(33)} 
\begin{array}{lllll} 
{} & {\max } & {\sum_{j=1}^{m}w_{j}^{-} {\it s}_{jk}^{-} +\sum_{r=1}^{s}w_{r}^{+} {\it s}_{rk}^{+},} & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}}^{}\lambda_{t} {\it a}_{j}^{t}  \le x_{jk} - {\it s}_{jk}^{-} ,} & {j=1,...,m} & {} \\
{} & {} & {\sum_{t\in \tilde{T}^{*}}^{}\lambda_{t} {\it d}_{rT^{*}} \left({\it \textbf{a}}^{t} \right) \ge y_{rk} +{\it s}_{rk}^{+} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} = 1,} & {} & {} \\ 
{} & {} & {\lambda _{i} \in \left\{0,1\right\},} & {i=1,...,n} & {} \\ 
{} & {} & {{\it \textbf{s}}_{k}^{-} \ge \textbf{0}_{m}, {\it \textbf{s}}_{k}^{+} \ge \textbf{0}_{s}.} 
\end{array}
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "WAM.MIP"} for the Measure of Inefficiency
Proportions or \texttt{"WAM.RAM"} for the Range-Adjusted Measure of
Inefficiency \citep{cooper1999} in \texttt{efficiencyEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, 
                        scores_model = "WAM.MIP", digits = 2,
                        print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev.  Min   Q1 Median   Q3  Max
#>    EAT 2.07      0.56 0.61 1.58   2.18 2.18 3.14
#>    FDH 0.40      0.58 0.00 0.00   0.00 0.00 2.37
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     EAT_WAM_MIP FDH_WAM_MIP
#> MLT        2.54           0
#> SVN        1.86           0
#> HRV        2.30           0
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT,
                        scores_model = "WAM.RAM", digits = 2,
                        print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model    Mean Std. Dev.   Min      Q1  Median      Q3     Max
#>    EAT 2704.65   1677.11 455.3 1467.78 2425.09 2425.09 8293.59
#>    FDH  959.09   1388.56   0.0    0.00    0.00    0.00 5413.27
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     EAT_WAM_RAM FDH_WAM_RAM
#> ITA     2192.98        0.00
#> LVA     1890.38        0.00
#> CAN     1724.26     1182.66
\end{Soutput}
\end{Schunk}

And, finally, the Convexified Efficiency Analysis Trees weighted
additive model would be:

\begin{equation} \label{(34)} 
\begin{array}{lllll} 
{} & {\max } & {\sum_{j=1}^{m}w_{j}^{-} {\it s}_{jk}^{-} +\sum_{r=1}^{s}w_{r}^{+} {\it s}_{rk}^{+},} & {} & {} \\ 
{} & {s.t.} & {} & {} & {} \\ 
{} & {} & {\sum_{t\in \tilde{T}^{*}}^{}\lambda_{t} {\it a}_{j}^{t}  \le x_{jk} - {\it s}_{jk}^{-} ,} & {j=1,...,m} & {} \\
{} & {} & {\sum_{t\in \tilde{T}^{*}}^{}\lambda_{t} {\it d}_{rT^{*}} \left({\it \textbf{a}}^{t} \right) \ge y_{rk} +{\it s}_{rk}^{+} ,} & {r=1,...,s} & {} \\ 
{} & {} & {\sum_{i=1}^{n}\lambda_{i} = 1,} & {} & {} \\ 
{} & {} & {\lambda _{i} \ge 0,} & {i=1,...,n} & {} \\ 
{} & {} & {{\it \textbf{s}}_{k}^{-} \ge \textbf{0}_{m}, {\it \textbf{s}}_{k}^{+} \ge \textbf{0}_{s}.} 
\end{array}
\end{equation}

In R, this model can be computed by setting
\texttt{scores\_model\ =\ "WAM.MIP"} for the Measure of Inefficiency
Proportions or \texttt{"WAM.RAM"} for the Range-Adjusted Measure of
Inefficiency in \texttt{efficiencyCEAT()}:

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT,
                         scores_model = "WAM.MIP", digits = 2,
                         print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model Mean Std. Dev.  Min   Q1 Median   Q3  Max
#>   CEAT 2.39      0.45 0.96 2.24   2.50 2.50 3.14
#>    DEA 0.90      0.62 0.00 0.23   1.07 1.07 2.37
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     CEAT_WAM_MIP DEA_WAM_MIP
#> ISR         2.74        1.13
#> ITA         2.73        1.31
#> SGP         1.91        0.00
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT,
                         scores_model = "WAM.RAM", digits = 2,
                         print.table = TRUE)
\end{Sinput}
\begin{Soutput}
#>  Model    Mean Std. Dev.    Min      Q1  Median      Q3      Max
#>   CEAT 5285.99   2573.13 612.54 3362.98 4836.79 4836.79 12088.64
#>    DEA 2413.66   1821.33   0.00  746.22 2495.20 2495.20  7167.29
\end{Soutput}
\begin{Sinput}
scores %>% sample_n(3)
\end{Sinput}
\begin{Soutput}
#>     CEAT_WAM_RAM DEA_WAM_RAM
#> KOR      1538.61        0.00
#> ROU      7239.16     4727.17
#> EST       612.54        0.00
\end{Soutput}
\end{Schunk}

\hypertarget{section6}{%
\section{Advanced options and displaying and exporting
results}\label{section6}}

\hypertarget{advanced-optimization-options}{%
\subsection{Advanced optimization
options}\label{advanced-optimization-options}}

The \texttt{bestEAT()} and \texttt{bestRFEAT()} functions are aimed at
finding the value of the hyperparameters that minimize the root mean
squared error (RMSE) calculated from a test sample through an Efficiency
Analysis Trees or a Random Forest for Efficiency Analysis Trees model
fitted using a training sample. The code of these functions is as
follows:

\begin{Schunk}
\begin{Sinput}
# Hyperparameter tuning for Efficiency Analysis Trees
bestEAT(
  training, test, x, y,
  numStop = 5, fold = 5,
  max.depth = NULL,
  max.leaves = NULL,
  na.rm = TRUE
  )

# Hyperparameter tuning for Random Forest for Efficiency Analysis Trees
bestRFEAT(
  training, test, x, y,
  numStop = 5, m = 50,
  s_mtry = c("5", "BRM"), 
  na.rm = TRUE
  )
\end{Sinput}
\end{Schunk}

Here is an example of using the \texttt{bestEAT()} function. First, the
PISAindex database explained in Section
\protect\hyperlink{section3}{Data structure} is divided into a training
subset with 70\% of the DMUs and a test subset with the remaining 30\%
(these values can be modified).

\begin{Schunk}
\begin{Sinput}
n <- nrow(PISAindex)              # Observations in the dataset
selected <- sample(1:n, n * 0.7)  # Training indexes
training <- PISAindex[selected, ] # Training set
test <- PISAindex[- selected, ]   # Test set
\end{Sinput}
\end{Schunk}

Then, we can apply the \texttt{bestEAT()} function. This function, and
its equivalent \texttt{bestRFEAT()}, requires a training set
(\texttt{training}) on which to fit an Efficiency Analysis Trees model
(with cross-validation), a test set (\texttt{test}) on which to
calculate the root mean squared error and the input and output indexes
(\texttt{x} and \texttt{y}, respectively). The rest of the arguments
(\texttt{numStop}, \texttt{fold}, \texttt{max.depth} and
\texttt{max.leaves} in case of using the \texttt{bestEAT()} function)
are used to create a grid of combinations that determines the number of
models to fit. Notice that it is not possible to enter \texttt{NULL} and
a certain value in \texttt{max.depth} or \texttt{max.leaves} arguments
at the same time (i.e.~\texttt{max.depth\ =\ c(NULL,\ 5,\ 3)}).

In the following example, the arguments \texttt{numStop\ =\ (3,\ 5,\ 7)}
and \texttt{fold\ =\ (5,\ 7)} are entered and, consequently, six
different models are constructed and fitted with
\{\texttt{numStop\ =\ 3}, \texttt{fold\ =\ 5}\},
\{\texttt{numStop\ =\ 3}, \texttt{fold\ =\ 7}\},
\{\texttt{numStop\ =\ 5}, \texttt{fold\ =\ 5}\},
\{\texttt{numStop\ =\ 5}, \texttt{fold\ =\ 7}\},
\{\texttt{numStop\ =\ 7}, \texttt{fold\ =\ 5}\} and
\{\texttt{numStop\ =\ 7}, \texttt{fold\ =\ 7}\}. Let us show a numerical
example:

\begin{Schunk}
\begin{Sinput}
bestEAT(training = training, test = test, x = 4:8, y = 1:3, 
        numStop = c(3, 5, 7), fold = c(5, 7))
\end{Sinput}
\begin{Soutput}
#>   numStop fold   RMSE leaves
#> 1       5    5  74.74     15
#> 2       7    7  83.61     13
#> 3       3    5  84.17     13
#> 4       3    7  84.17     13
#> 5       7    5  86.39      8
#> 6       5    7 104.93      8
\end{Soutput}
\end{Schunk}

The best model is given by the hyperparameters \{\texttt{numStop\ =\ 5},
\texttt{fold\ =\ 5}\} with RMSE = 74.74 and 15 leaf nodes. Note that
sometimes it might be interesting to select a model with a higher RMSE
but with a lower number of leaf nodes. With this result, we fit the
final Efficiency Analysis Trees model using all the original data.

\begin{Schunk}
\begin{Sinput}
bestEAT_model <- EAT(data = PISAindex, x = 4:8, y = 1:3, numStop = 5, fold = 5)
\end{Sinput}
\end{Schunk}

\hypertarget{displaying-results}{%
\subsection{Displaying results}\label{displaying-results}}

\hypertarget{general-functions-for-the-eat-object}{%
\subsubsection{General functions for the EAT
object}\label{general-functions-for-the-eat-object}}

The simplest functions to use in order to explore the results of an
\texttt{EAT} object are \texttt{print()} and \texttt{summary()}. The
function \texttt{print()} returns the tree-structure of an Efficiency
Analysis Trees model; while the function \texttt{summary()} returns
general information about the fitted model. We show the results with an
example:

\begin{Schunk}
\begin{Sinput}
modelEAT2 <- EAT(data = PISAindex, x = 7, y = 3)
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
print(modelEAT2) # [node] y: [prediction] || R: error n(t): n¬∫ of DMUs
\end{Sinput}
\begin{Soutput}
#>  [1] y: [ 569 ] || R: 15724.19 n(t): 72 
#>  
#>  |  [2] AAE < 70.12 --> y: [ 486 ] || R: 3094.43 n(t): 34 
#>  
#>  |   |  [4] AAE < 60.75 --> y: [ 472 ] <*> || R: 1553.93 n(t): 17 
#>  
#>  |   |  [5] AAE >= 60.75 --> y: [ 486 ] <*> || R: 1006.94 n(t): 17 
#>  
#>  |  [3] AAE >= 70.12 --> y: [ 569 ] <*> || R: 3753.38 n(t): 38 
#>  
#> <*> is a leaf node
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
# Primary & surrogate splits: Node i --> {SL, SR} || var --> {R: error, s: threshold}
summary(modelEAT2)
\end{Sinput}
\begin{Soutput}
#> 
#>   Formula:  M_PISA ~ AAE 
#> 
#>  # ========================== # 
#>  #   Summary for leaf nodes   # 
#>  # ========================== # 
#>  
#>  id n(t)  % M_PISA    R(t)
#>   3   38 53    569 3753.38
#>   4   17 24    472 1553.93
#>   5   17 24    486 1006.94
#> 
#>  # ========================== # 
#>  #            Tree            # 
#>  # ========================== # 
#>  
#>  Interior nodes: 2 
#>      Leaf nodes: 3 
#>     Total nodes: 5 
#>  
#>            R(T): 6314.25 
#>         numStop: 5 
#>            fold: 5 
#>       max.depth: 
#>      max.leaves:
#>  
#>  # ========================== # 
#>  # Primary & surrogate splits # 
#>  # ========================== # 
#>  
#>  Node 1 --> {2,3} || AAE --> {R: 6847.81, s: 70.12}
#> 
#>  Node 2 --> {4,5} || AAE --> {R: 2560.88, s: 60.75}
\end{Soutput}
\end{Schunk}

\hypertarget{representing-the-efficiency-scores}{%
\subsubsection{Representing the efficiency
scores}\label{representing-the-efficiency-scores}}

\texttt{efficiencyJitter()} returns a jitter plot from \texttt{ggplot2}.
This graphic shows how DMUs are grouped into leaf nodes in a model built
using the \texttt{EAT()} function where each leaf node groups DMUs with
the same level of resources. A black dot and a black line represent,
respectively, the mean value and the standard deviation of the scores
(\texttt{df\_scores} from the \texttt{efficiencyEAT()} or the
\texttt{efficiencyCEAT()} functions) of a given node. Additionally,
efficient DMU labels are always displayed based on the model entered in
the \texttt{scores\_model} argument. Finally, the user can specify an
upper bound (\texttt{upb}) and a lower bound (\texttt{lwb}) in order to
show, in addition, the labels whose efficiency score lies between them.
The code is as follows:

\begin{Schunk}
\begin{Sinput}
efficiencyJitter(
  object,
  df_scores,
  scores_model,
  upb = NULL,
  lwb = NULL
)
\end{Sinput}
\end{Schunk}

As an example, using data from Section \protect\hyperlink{section3}{Data
structure}, we create a new Efficiency Analysis Trees model containing
only the \texttt{AAE} and the \texttt{M\_PISA} variables. Next, we
evaluate the Efficiency Analysis Trees efficiency scores corresponding
to the output-oriented radial model and plot them through
\texttt{efficiencyJitter()}.

\begin{Schunk}
\begin{Sinput}
scores <- efficiencyEAT(data = PISAindex, x = 7, y = 3, object = modelEAT2, 
                        scores_model = "BCC.OUT", digits = 2, 
                        print.table = FALSE)
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{figure}

{\centering \includegraphics{EAT_files/figure-latex/ex27-1} 

}

\caption[Jitter plot generated by 'efficiencyJitter()' to show how the countries are grouped inside three particular leaf nodes]{Jitter plot generated by 'efficiencyJitter()' to show how the countries are grouped inside three particular leaf nodes.}\label{fig:ex27}
\end{figure}
\end{Schunk}

\texttt{efficiencyDensity()} returns a density plot from
\texttt{ggplot2}. This graphic allows to verify the similarity between
the scores obtained by the different available methodologies
(\texttt{EAT}, \texttt{FDH}, \texttt{CEAT}, \texttt{DEA} and
\texttt{RFEAT}) in the \pkg{eat} package.

\begin{Schunk}
\begin{Sinput}
efficiencyDensity(
  df_scores,
  model = c("EAT", "FDH")
)
\end{Sinput}
\end{Schunk}

In this case, a comparison between the scores of the EAT and FDH models
is shown, where it can be clearly seen how FDH is less restrictive when
determining a unit as efficient:

\begin{Schunk}
\begin{figure}

{\centering \includegraphics{EAT_files/figure-latex/ex28-1} 

}

\caption[Density plot generated by 'efficiencyDensity()' to show the difference between the score obtained by EAT and FDH]{Density plot generated by 'efficiencyDensity()' to show the difference between the score obtained by EAT and FDH.}\label{fig:ex28}
\end{figure}
\end{Schunk}

\hypertarget{other-graphics}{%
\subsubsection{Other graphics}\label{other-graphics}}

In the limited case of using only one input for producing only one
output, we can display the frontier (from \texttt{ggplot2}) estimated by
the \texttt{EAT()} function through the \texttt{frontier()} function:

\begin{Schunk}
\begin{Sinput}
frontier(
  object, FDH = TRUE,
  observed.data = TRUE,
  observed.color = "black",
  pch = 19,vsize = 1,
  rwn = FALSE,
  max.overlaps = 10
)
\end{Sinput}
\end{Schunk}

Optionally, the frontier estimated by FDH can also be plotted if
\texttt{FDH\ =\ TRUE}. Observed DMUs can be showed by a scatterplot if
\texttt{observed.data\ =\ TRUE} and its color, shape and size can be
modified with \texttt{observed.color}, \texttt{pch} and \texttt{size}
respectively. Finally, row names can be included with
\texttt{rwn\ =\ TRUE}.

As an example, we use data simulated from the \pkg{eat} package to
generate a \texttt{data.frame} with 50 rows (\texttt{N} = DMUs) and 1
input (\texttt{nX}):

\begin{Schunk}
\begin{Sinput}
simulated <- Y1.sim(N = 50, nX = 1)
modelEAT3 <- EAT(data = simulated, x = 1, y = 2)
\end{Sinput}
\end{Schunk}

Then, we apply the \texttt{frontier()} function, where it can be
observed how the Efficiency Analysis Trees model generalizes the results
obtained by the FDH model:

\begin{Schunk}
\begin{figure}

{\centering \includegraphics{EAT_files/figure-latex/ex29-1} 

}

\caption[Plot of productions functions corresponding to the EAT and the FDH estimator when 'frontier()' is applied]{Plot of productions functions corresponding to the EAT and the FDH estimator when 'frontier()' is applied.}\label{fig:ex29}
\end{figure}
\end{Schunk}

The function \texttt{frontier()} shown above only works for the simple
case of a low-dimensional scenario with one input and one output. For
multiple input and/or output scenarios, the typical tree-structure
showing the relationships between outputs and inputs is given by the
function \texttt{plotEAT()}.

\begin{Schunk}
\begin{Sinput}
plotEAT(
  object
)
\end{Sinput}
\end{Schunk}

The nodes of the tree are colored according to the variable by which the
split is performed or they are black, in the case of being a leaf node.
For each node, we can obtain the following information:

\begin{itemize}
\item
  \texttt{id}: node index.
\item
  \texttt{R}: error at the node.
\item
  \texttt{n(t)}: number of DMUs at the node.
\item
  input variable associated with the split.
\item
  \texttt{y}: vector of output predictions.
\end{itemize}

Next, we limit the growth of an Efficiency Analysis Trees model to a
maximum size of 5 (\texttt{max.depth\ =\ 4}) and display the
tree-structure using the \texttt{plotEAT()} function:

\begin{Schunk}
\begin{figure}

{\centering \includegraphics{EAT_files/figure-latex/ex30-1} 

}

\caption[Plot of the tree structure obtained through an EAT model with the parameter max.depth defined as 4]{Plot of the tree structure obtained through an EAT model with the parameter max.depth defined as 4.}\label{fig:ex30}
\end{figure}
\end{Schunk}

Finally, the function \texttt{plotRFEAT()} returns the Out-Of-Bag error
for a random forest consisting of k trees. The code of the function and
an example with the object \texttt{modelRFEAT} are shown above:

\begin{Schunk}
\begin{Sinput}
plotRFEAT(
  object
)
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{figure}

{\centering \includegraphics{EAT_files/figure-latex/ex31-1} 

}

\caption[Plot of the OOB error corresponding to 30 different RFEAT where k represents the number of trees belonging to each RF]{Plot of the OOB error corresponding to 30 different RFEAT where k represents the number of trees belonging to each RF.}\label{fig:ex31}
\end{figure}
\end{Schunk}

In view of the results, it can be seen how the OOB error presents a
great variability for a small number of trees, however, it usually
levels off. In our case, it seems that the OOB error levels off from 20
trees onwards around an OOB error of 56, so it could be interesting not
to include a greater number of trees in the random forest in order to
reduce the computational cost.

\hypertarget{section7}{%
\section{Conclusions}\label{section7}}

The \pkg{eat} package allows the estimation of production frontiers in
microeconomics and engineering through suitable adaptations of
Regression Trees and Random Forest. In the first case, the package
implements in R the so-called Efficiency Analysis Trees (EAT) by
\citet{esteve2020}, which is a non-parametric technique that competes
against the more standard Free Disposal Hull (FDH) technique. In this
regard, the EAT technique overcomes the overfitting problem suffered by
the FDH technique. FDH is based on three microeconomic postulates.
First, the technology determined by FDH satisfies free disposability in
inputs and outputs. Second, it is assumed to be deterministic, that is,
the production possibility set built by this technique always contains
all the observations that belong to the data sample. Third, FDH meets
the minimal extrapolation principle. This last postulate implies that
FDH generates the smallest set that satisfies the first two postulates.
Consequently, the derived efficient frontier is as close to the data as
possible, generating overfitting problems. In contrast, the Efficiency
Analysis Trees (EAT) technique meets the first two postulates but does
not satisfy the minimal extrapolation principle. This fact avoids
possible overfitting problems. The difficulty for non-overfitted models
lies in where to locate the production possibility set in such a way
that it is close to the (unknown) technology associated with the
underlying Data Generating Process. In the case of EAT, it is achieved
through cross-validation and pruning. A subsequent convexification of
the EAT estimation of the technology, known as CEAT by its acronym,
yields an alternative estimate of the production possibility set in
contrast to the traditional Data Envelopment Analysis (DEA) technique.
In the second case, an ensemble of tree models is fitted and aggregated
with the objective of achieving robustness in the estimation of the
production frontier \citep{esteve2021}.

Several functions have been implemented in the \pkg{eat} package for
determining the best model, through a pruning process based on
cross-validation, graphing the results, calculating a ranking of
importance of inputs and comparing the efficiency scores estimated by
EAT with respect to the standard approaches, i.e., FDH and DEA, through
a list of standard technical efficiency measures. We refer to the input
and output-oriented radial models, the input and output-oriented Russell
measures, the Directional Distance Function and the Weighted Additive
model.

Throughout the paper, we have also shown how to organize the data, use
the available functions, and interpret the results. In particular, to
illustrate the different functions implemented in the package, we
applied all of them on a common empirical example so that results can
easily be compared. In this way, we believe that the \pkg{eat} package
is a valid self-contained R package for the measurement of technical
efficiency from the popular machine learning technique: Decision Trees.
Finally, since the code is freely available in an open source
repository, users will benefit from the collaboration and review of the
community. Users may check and modify the code to adapt it to their own
needs and extend it with new definitions.

\hypertarget{acknowledgments}{%
\section{Acknowledgments}\label{acknowledgments}}

M. Esteve, V. Espa√±a and J. Aparicio thank the grant
PID2019-105952GB-I00 funded by Ministerio de Ciencia e Innovaci√≥n/
Agencia Estatal de Investigaci√≥n /10.13039/501100011033. Additionally,
M. Esteve gratefully acknowledges the financial support from the Spanish
Ministry of Science, Innovation and Universities under Grant
FPU17/05365. X. Barber gratefully acknowledges the financial support
from the Spanish Ministry of Science and the State Research Agency under
grant PID2019-106341GB-I00. X Barber and J. Aparicio gratefully
acknowledge the financial support from the University Miguel Hernandez
and the Vice-Rectorate for Research under grant AW1020IP-2020/NAC/00073.
This work was also supported by the Generalitat Valenciana under Grant
ACIF/2021 (V. Espa√±a).

\bibliography{EAT.bib}

\address{%
Miriam Esteve\\
Miguel Hernandez University\\%
Center of Operations Research\\ 03202 Elche, Spain\\
%
\url{https://cio.umh.es/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-5908-0581}{0000-0002-5908-0581}}\\%
\href{mailto:miriam.estevec@umh.es}{\nolinkurl{miriam.estevec@umh.es}}%
}

\address{%
Victor Espa√±a\\
Miguel Hernandez University\\%
Center of Operations Research\\ 03202 Elche, Spain\\
%
\url{https://cio.umh.es/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-1807-6180}{0000-0002-1807-6180}}\\%
\href{mailto:vespana@umh.es}{\nolinkurl{vespana@umh.es}}%
}

\address{%
Juan Aparicio\\
Miguel Hernandez University\\%
Center of Operations Research\\ 03202 Elche, Spain\\
%
\url{https://cio.umh.es/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-0867-0004}{0000-0002-0867-0004}}\\%
\href{mailto:j.aparicio@umh.es}{\nolinkurl{j.aparicio@umh.es}}%
}

\address{%
Xavier Barber\\
Miguel Hernandez University\\%
Center of Operations Research\\ 03202 Elche, Spain\\
%
\url{https://cio.umh.es/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0003-3079-5855}{0000-0003-3079-5855}}\\%
\href{mailto:xbarber@umh.es}{\nolinkurl{xbarber@umh.es}}%
}
