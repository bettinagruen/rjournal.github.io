<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>The R Journal</title>
    <link>https://rjournal-distill.netlify.app/</link>
    <atom:link href="https://rjournal-distill.netlify.app/articles.xml" rel="self" type="application/rss+xml"/>
    <description>Articles published in the R Journal</description>
    <image>
      <title>The R Journal</title>
      <url>https://rjournal-distill.netlify.app/resources/favicon.ico</url>
      <link>https://rjournal-distill.netlify.app/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Mon, 01 Mar 2021 00:00:00 +0000</lastBuildDate>
    <item>
      <title>stratamatch: Prognostic Score Stratification Using a Pilot Design</title>
      <dc:creator>Rachael C. Aikens</dc:creator>
      <dc:creator>Joseph Rigdon</dc:creator>
      <dc:creator>Justin Lee</dc:creator>
      <dc:creator>Michael Baiocchi</dc:creator>
      <dc:creator>Andrew B. Goldstone</dc:creator>
      <dc:creator>Peter Chiu</dc:creator>
      <dc:creator>Y. Joseph Woo</dc:creator>
      <dc:creator>Jonathan H. Chen</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-063</link>
      <description>stratamatch: Prognostic Score Stratification Using a Pilot Design</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-063</guid>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>BayesSPsurv: An R Package to Estimate Bayesian (Spatial) Split-Population Survival Models</title>
      <dc:creator>Brandon Bolte</dc:creator>
      <dc:creator>Nicolás Schmidt</dc:creator>
      <dc:creator>Sergio Béjar</dc:creator>
      <dc:creator>Nguyen Huynh</dc:creator>
      <dc:creator>Bumba Mukherjee</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-068</link>
      <description>BayesSPsurv: An R Package to Estimate Bayesian (Spatial) Split-Population Survival Models</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-068</guid>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Reproducible Summary Tables with the gtsummary Package</title>
      <dc:creator>Daniel D. Sjoberg</dc:creator>
      <dc:creator>Karissa Whiting</dc:creator>
      <dc:creator>Michael Curry</dc:creator>
      <dc:creator>Jessica A. Lavery</dc:creator>
      <dc:creator>Joseph Larmarange</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-053</link>
      <description>Reproducible Summary Tables with the gtsummary Package</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-053</guid>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Regularized Transformation Models: The tramnet Package</title>
      <dc:creator>Lucas Kook</dc:creator>
      <dc:creator>Torsten Hothorn</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-054</link>
      <description>Regularized Transformation Models: The tramnet Package</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-054</guid>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Automating Reproducible, Collaborative Clinical Trial Document Generation with the listdown Package</title>
      <dc:creator>Michael Kane</dc:creator>
      <dc:creator>Xun Jiang</dc:creator>
      <dc:creator>Simon Urbanek</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-051</link>
      <description>Automating Reproducible, Collaborative Clinical Trial Document Generation with the listdown Package</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-051</guid>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Towards a Grammar for Processing Clinical Trial Data</title>
      <dc:creator>Michael J. Kane</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-052</link>
      <description>Towards a Grammar for Processing Clinical Trial Data</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-052</guid>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff's Alpha Coefficient</title>
      <dc:creator>John Hughes</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-046</link>
      <description>krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff's Alpha Coefficient</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-046</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Working with CRSP/COMPUSTAT in R: Reproducible Empirical Asset Pricing</title>
      <dc:creator>Majeed Simaan</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-047</link>
      <description>Working with CRSP/COMPUSTAT in R: Reproducible Empirical Asset Pricing</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-047</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Analyzing Dependence between Point Processes in Time Using IndTestPP</title>
      <dc:creator>Ana C. Cebrián</dc:creator>
      <dc:creator>Jesús Asín</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-049</link>
      <description>Analyzing Dependence between Point Processes in Time Using IndTestPP</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-049</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Conversations in Time: Interactive Visualization to Explore Structured Temporal Data</title>
      <dc:creator>Earo Wang</dc:creator>
      <dc:creator>Dianne Cook</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-050</link>
      <description>Conversations in Time: Interactive Visualization to Explore Structured Temporal Data</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-050</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>distr6: R6 Object-Oriented Probability Distributions Interface in R</title>
      <dc:creator>Raphael Sonabend</dc:creator>
      <dc:creator>Franz J. Király</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-055</link>
      <description>distr6: R6 Object-Oriented Probability Distributions Interface in R</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-055</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>gofCopula: Goodness-of-Fit Tests for Copulae</title>
      <dc:creator>Ostap Okhrin</dc:creator>
      <dc:creator>Simon Trimborn</dc:creator>
      <dc:creator>Martin Waltz</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-060</link>
      <description>gofCopula: Goodness-of-Fit Tests for Copulae</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-060</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>ROCnReg: An R Package for Receiver Operating Characteristic Curve Inference With and Without Covariates</title>
      <dc:creator>María Xosé Rodríguez-Álvarez</dc:creator>
      <dc:creator>Vanda Inácio</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-066</link>
      <description>ROCnReg: An R Package for Receiver Operating Characteristic Curve Inference With and Without Covariates</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-066</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>OneStep : Le Cam's One-step Estimation Procedure</title>
      <dc:creator>Alexandre Brouste</dc:creator>
      <dc:creator>Christophe Dutang</dc:creator>
      <dc:creator>Darel Noutsa Mieniedou</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-044</link>
      <description>OneStep : Le Cam's One-step Estimation Procedure</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-044</guid>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The HBV.IANIGLA Hydrological Model</title>
      <dc:creator>Ezequiel Toum</dc:creator>
      <dc:creator>Mariano H. Masiokas</dc:creator>
      <dc:creator>Ricardo Villalba</dc:creator>
      <dc:creator>Pierre Pitte</dc:creator>
      <dc:creator>Lucas Ruiz</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-059</link>
      <description>The HBV.IANIGLA Hydrological Model</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-059</guid>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The R Package smicd: Statistical Methods for Interval-Censored Data</title>
      <dc:creator>Paul Walter</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-045</link>
      <description>The R Package smicd: Statistical Methods for Interval-Censored Data</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-045</guid>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>RLumCarlo: Simulating Cold Light using Monte Carlo Methods</title>
      <dc:creator>Sebastian Kreutzer</dc:creator>
      <dc:creator>Johannes Friedrich</dc:creator>
      <dc:creator>Vasilis Pagonis</dc:creator>
      <dc:creator>Christian Laag</dc:creator>
      <dc:creator>Ena Rajovic</dc:creator>
      <dc:creator>Christoph Schmidt</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-043</link>
      <description>RLumCarlo: Simulating Cold Light using Monte Carlo Methods</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-043</guid>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>pdynmc: A Package for Estimating Linear Dynamic Panel Data Models Based on Nonlinear Moment Conditions</title>
      <dc:creator>Markus Fritsch</dc:creator>
      <dc:creator>Andrew Adrian Yu Pua</dc:creator>
      <dc:creator>Joachim Schnurbus</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-035</link>
      <description>pdynmc: A Package for Estimating Linear Dynamic Panel Data Models Based on Nonlinear Moment Conditions</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-035</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>DChaos: An R Package for Chaotic Time Series Analysis</title>
      <dc:creator>Julio E. Sandubete</dc:creator>
      <dc:creator>Lorenzo Escot</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-036</link>
      <description>DChaos: An R Package for Chaotic Time Series Analysis</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-036</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>IndexNumber: An R Package for Measuring the Evolution of Magnitudes</title>
      <dc:creator>Alejandro Saavedra-Nieves</dc:creator>
      <dc:creator>Paula Saavedra-Nieves</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-038</link>
      <description>IndexNumber: An R Package for Measuring the Evolution of Magnitudes</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-038</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>ROBustness In Network (robin): an R Package for Comparison and Validation of Communities </title>
      <dc:creator>Valeria Policastro</dc:creator>
      <dc:creator>Dario Righelli</dc:creator>
      <dc:creator>Annamaria Carissimo</dc:creator>
      <dc:creator>Luisa Cutillo</dc:creator>
      <dc:creator>Italia De Feis</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-040</link>
      <description>ROBustness In Network (robin): an R Package for Comparison and Validation of Communities </description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-040</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Package wsbackfit for Smooth Backfitting Estimation of Generalized Structured Models</title>
      <dc:creator>Javier Roca-Pardiñas</dc:creator>
      <dc:creator>María Xosé Rodríguez-Álvarez</dc:creator>
      <dc:creator>Stefan Sperlich</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-042</link>
      <description>Package wsbackfit for Smooth Backfitting Estimation of Generalized Structured Models</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-042</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>garchx: Flexible and Robust GARCH-X Modeling</title>
      <dc:creator>Genaro Sucarrat</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-057</link>
      <description>garchx: Flexible and Robust GARCH-X Modeling</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-057</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Finding Optimal Normalizing Transformations via \pkg{bestNormalize}</title>
      <dc:creator>Ryan A. Peterson</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-041</link>
      <description>


&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The  package contains a suite of transformation-estimating functions that can be used to normalize data. The function of the same name attempts to find and execute the best of all of these potential normalizing transformations. In this package, we define “normalize” as in “to render data Gaussian,” rather than transform them to a specific scale.&lt;/p&gt;
&lt;p&gt;There are many instances where researchers may want to normalize a variable. First, there is the (often problematic) assumption of normality of the outcome (conditional on the covariates) in the classical linear regression problem. Over the years, many methods have been used to relax this assumption: generalized linear models, quantile regression, survival models, etc. One technique that is still somewhat popular in this context is to “beat the data” to look normal via some kind of normalizing transformation. This could be something as simple as a log transformation or something as complex as a Yeo-Johnson transformation &lt;span class="citation"&gt;(Yeo and Johnson 2000)&lt;/span&gt;. In fact, many complex normalization methods were designed expressly to find a transformation that could render regression residuals Gaussian. While perhaps not the most elegant solution to the problem, often, this technique works well as a quick solution. Another increasingly popular application of normalization occurs in applied regression settings with highly skewed distributions of the covariates &lt;span class="citation"&gt;(Kuhn and Johnson 2013)&lt;/span&gt;. In these settings, there exists the tendency to have high leverage points (and highly influential points), even when one centers and scales the covariates. When examining interactions, these influential points can become especially problematic since the leverage of that point gets amplified for every interaction in which it is involved. Normalization of such covariates can mitigate their leverage and influence, thereby allowing for easier model selection and more robust downstream predictor manipulations (such as principal components analysis), which can otherwise be sensitive to skew or outliers. As a result, popular model selection packages such as  &lt;span class="citation"&gt;(Kuhn 2017)&lt;/span&gt; and  &lt;span class="citation"&gt;(Kuhn and Wickham 2018)&lt;/span&gt; have built-in mechanisms to normalize the predictor variables (they call this “preprocessing”). This concept is unique in that it forgoes the assumption of linearity between the outcome (Y) and the covariate, opting instead for a linear relationship between Y and the transformed value of the covariate (which in many cases may be more plausible).&lt;/p&gt;
&lt;p&gt;This package is designed to make normalization effortless and consistent. We have also introduced Ordered Quantile (ORQ) normalization via the  function, which uses a rank mapping of the observed data to the normal distribution in order to guarantee normally distributed transformed data (if ties are not present). We have shown how ORQ normalization performs very consistently across different distributions, successfully normalizing left- or right-skewed data, multi-modal data, and even data generated from a Cauchy distribution &lt;span class="citation"&gt;(Peterson and Cavanaugh 2019)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this paper, we describe our R package , which is available via the Comprehensive R Archive Network (CRAN). First, we describe normalization methods that have been developed and that we implement in the package. Second, we describe the novel cross-validation-based estimation procedure, which we utilize to judge the normalization efficacy of our suite of normalization transformations. Third, we go through some basic examples of  functionality and a simple implementation of our methods within the  package. We illustrate a more in-depth use-case in a car pricing application, performing a transform-both-sides regression as well as comparing the performance of several predictive models fit via . Finally, we conclude by discussing the pros and cons of normalization in general and future directions for the package.&lt;/p&gt;
&lt;h1 id="normalization-methods"&gt;Normalization methods&lt;/h1&gt;
&lt;p&gt;Many normalization transformation functions exist, and though some can be implemented well in existing R packages,  puts them all under the same umbrella syntax. This section describes each transformation contained in the  suite.&lt;/p&gt;
&lt;h2 id="the-box-cox-transformation"&gt;The Box-Cox transformation&lt;/h2&gt;
&lt;p&gt;The Box-Cox transformation was famously proposed in &lt;span class="citation"&gt;Box and Cox (1964)&lt;/span&gt; and can be implemented with differing syntax and methods in many existing packages in R (e.g., ,  &lt;span class="citation"&gt;(Venables and Ripley 2002)&lt;/span&gt;, and more). It is a straightforward transformation that typically only involves one parameter, &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
g(x; \lambda) = \boldsymbol 1 _{(\lambda \neq 0)} \frac{x^\lambda-1}{\lambda} 
+ \boldsymbol 1_{(\lambda = 0)} \log x\text{ ,}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; refers to the datum in its original unit (pre-transformation). Given multiple observations, the &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; parameter can be estimated via maximum likelihood, and &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; must be greater than zero.&lt;/p&gt;
&lt;h2 id="the-yeo-johnson-transformation"&gt;The Yeo-Johnson transformation&lt;/h2&gt;
&lt;p&gt;The Yeo-Johnson transformation &lt;span class="citation"&gt;(Yeo and Johnson 2000)&lt;/span&gt; attempts to find the value of &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; in the following equation that minimizes the Kullback-Leibler distance between the normal distribution and the transformed distribution.&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{aligned}
g(x;\lambda) &amp;amp;= 
\boldsymbol 1 _{(\lambda \neq 0, x \geq 0)} \frac{(x+1)^\lambda-1}{\lambda} \\
&amp;amp;+ \boldsymbol 1_{(\lambda = 0, x \geq 0)} \log (x+1) \\
&amp;amp;+ \boldsymbol 1_{(\lambda \neq 2, x &amp;lt; 0)} \frac{(1-x)^{2-\lambda}-1}{\lambda - 2} \\
&amp;amp;+ \boldsymbol 1_{(\lambda = 2, x &amp;lt; 0)} -\log (1-x) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This method has the advantage of working without having to worry about the domain of &lt;span class="math inline"&gt;\(x\)&lt;/span&gt;. As with the Box-Cox &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt;, this &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; parameter can be estimated via maximum likelihood.&lt;/p&gt;
&lt;h2 id="the-lambert-w-x-f-transformation"&gt;The Lambert W x F transformation&lt;/h2&gt;
&lt;p&gt;The Lambert W x F transformation, proposed in &lt;span class="citation"&gt;Goerg (2011)&lt;/span&gt; and implemented in the  package, is essentially a mechanism that de-skews a random variable &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; using moments. The method is motivated by a system theory and is alleged to be able to transform any random variable into any other kind of random variable, thus being applicable to a large number of contexts. One of the package’s main functions is , which is similar in spirit to the purpose of this package. However, this method may not perform as well on certain shapes of distributions as other candidate transformations; see &lt;span class="citation"&gt;Peterson and Cavanaugh (2019)&lt;/span&gt; for some examples.&lt;/p&gt;
&lt;p&gt;The  transformation can handle three types of transformations: skewed, heavy-tailed, and skewed heavy-tailed. For more details on this transformation, consult the  documentation. While the transformations contained and implemented by  are reversible (i.e., 1-1), in rare circumstances, we have observed that the  function can yield non-reversible transformations.&lt;/p&gt;
&lt;h2 id="the-ordered-quantile-technique"&gt;The Ordered Quantile technique&lt;/h2&gt;
&lt;p&gt;The ORQ normalization technique () is based on the following transformation (originally discussed, as far as we can find, in &lt;span class="citation"&gt;Bartlett (1947)&lt;/span&gt; and further developed in &lt;span class="citation"&gt;Van der Waerden (1952)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;Let &lt;span class="math inline"&gt;\(\underline x\)&lt;/span&gt; refer to the original data. Then the transformation is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
g(\underline x) = \Phi ^{-1} \left(\frac{\text{rank} (\underline x) - 1/2}{\text{length}(\underline x) }\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This nonparametric transformation as defined works well on the observed data, but it is not trivial to implement in modern settings where the transformation needs to be applied on new data; we discussed this issue and our solution to it in &lt;span class="citation"&gt;Peterson and Cavanaugh (2019)&lt;/span&gt;. Basically, on new data &lt;em&gt;within&lt;/em&gt; the range of the original data, ORQ normalization will linearly interpolate between two of the original data points. On new data &lt;em&gt;outside&lt;/em&gt; the range of the original data, the transformation extrapolates using a shifted logit approximation of the ranks to the original data. This is visualized below via the  data set on the  variable.&lt;/p&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://rjournal-distill.netlify.app//articles/RJ-2021-041/figs/orq_vis-1.png" alt="ORQ normalization visualization on Fisher's iris data." width="100%" /&gt;
&lt;p class="caption"&gt;
(#fig:orq_vis)ORQ normalization visualization on Fisher’s iris data.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The shifted logit extrapolation ensures that the function is 1-1 and can handle data outside the original (observed) domain. The effects of the approximation will usually be relatively minimal since we should not expect to see many observations outside the observed range if the training set sample size is large relative to the test set. The ORQ technique will not guarantee a normal distribution in the presence of ties, but it still could yield the best normalizing transformation when compared to the other possible approaches. More information on ORQ normalization can be found in &lt;span class="citation"&gt;Peterson and Cavanaugh (2019)&lt;/span&gt; or in the  documentation.&lt;/p&gt;
&lt;h2 id="other-included-transformations"&gt;Other included transformations&lt;/h2&gt;
&lt;p&gt;In addition to the techniques above, the  package performs and evaluates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math inline"&gt;\(\log_b(x + a)\)&lt;/span&gt; where &lt;span class="math inline"&gt;\(a = \max(0, -\min(x) + \epsilon)\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(b = 10\)&lt;/span&gt; by default&lt;/li&gt;
&lt;li&gt;&lt;span class="math inline"&gt;\(\sqrt{x + a}\)&lt;/span&gt; where &lt;span class="math inline"&gt;\(a = \max(0, -\min(x))\)&lt;/span&gt; by default&lt;/li&gt;
&lt;li&gt;&lt;span class="math inline"&gt;\(\exp(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math inline"&gt;\(\text {arcsinh}(x) = log(x + \sqrt{x^2 + 1})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="other-not-included-transformations"&gt;Other not-included transformations&lt;/h2&gt;
&lt;p&gt;A range of other normalization techniques has been proposed that are not included in this package (at the time of writing). These include (but are not limited to): Modified Box-Cox &lt;span class="citation"&gt;(Box and Cox 1964)&lt;/span&gt;, Manly’s Exponential &lt;span class="citation"&gt;(Manly 1976)&lt;/span&gt;, John/Draper’s Modulus &lt;span class="citation"&gt;(John and Draper 1980)&lt;/span&gt;, and Bickel/Doksum’s Modified Box-Cox &lt;span class="citation"&gt;(Bickel and Doksum 1981)&lt;/span&gt;. However, it is straightforward to add new transformations into the same framework as other included transformations; each one is treated as its own S3 class, so in order to add other transformations, all one must do is define a new S3 class and provide the requisite S3 methods. To this end, we encourage readers to submit a pull request to &lt;a href="https://github.com/petersonR/bestNormalize"&gt;the package’s GitHub page&lt;/a&gt; with new transformation techniques that could be then added as a default in . Otherwise, in a later section, we show how users can implement custom transformations alongside the default ones described above.&lt;/p&gt;
&lt;h1 id="which-transformation-best-normalizes-the-data"&gt;Which transformation “best normalizes” the data?&lt;/h1&gt;
&lt;p&gt;The  function selects the best transformation according to an extra-sample estimate of the Pearson P statistic divided by its degrees of freedom (&lt;span class="math inline"&gt;\(DF\)&lt;/span&gt;). This P statistic is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
P = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}\text{ ,}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class="math inline"&gt;\(O_i\)&lt;/span&gt; is the number observed, and &lt;span class="math inline"&gt;\(E_i\)&lt;/span&gt; is the number of expected (under the hypothesis of normality) to fall into “bin” &lt;span class="math inline"&gt;\(i\)&lt;/span&gt;. The bins (or “classes”) are built such that observations will fall into each one with equal probability under the hypothesis of normality. A variety of alternative normality tests exist, but this particular one is relatively interpretable as a goodness of fit test, and the ratio &lt;span class="math inline"&gt;\(P/DF\)&lt;/span&gt; can be compared between transformations as an absolute measure of departure from normality. Specifically, if the data in question follow a normal distribution, this ratio will be close to 1 or lower. The transformation which produces data with the lowest normality statistic is thus the most effective at normalizing the data, and gets selected by . The  package utilizes  &lt;span class="citation"&gt;(Gross and Ligges 2015)&lt;/span&gt; to compute this statistic; more information on its computation and degrees of freedom can be found in &lt;span class="citation"&gt;D’Agostino (1986)&lt;/span&gt; and &lt;span class="citation"&gt;Thode (2002)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Normality statistics for all candidate transformations can be estimated and compared with one simple call to , whose output makes it easy to see which transformations are viable and which are not. We have found that while complicated transformations are often &lt;em&gt;most&lt;/em&gt; effective and therefore selected automatically, sometimes a simple transformation (e.g., the log or identity transforms) may be almost as effective, and ultimately the latter type will yield more interpretable results.&lt;/p&gt;
&lt;p&gt;It is worth noting that when the normality statistic is estimated on in-sample data, the ORQ technique is predestined to be most effective since it is forcing its transformed data to follow a normal distribution exactly &lt;span class="citation"&gt;(Peterson and Cavanaugh 2019)&lt;/span&gt;. For this reason, by default, the  function calculates an &lt;em&gt;out-of-sample&lt;/em&gt; estimate for the &lt;span class="math inline"&gt;\(P/DF\)&lt;/span&gt; statistic. Since this method necessitates cross-validation, it can be computationally frustrating for three reasons: (1) the results and the chosen transformation can depend on the seed, (2) it takes considerably longer to estimate than the in-sample statistic, and (3) it is unclear how to choose the number of folds and repeats.&lt;/p&gt;
&lt;p&gt;In order to mediate these issues, we have built several features into . Issue (1) is only important for small sample sizes, and when it is a concern, the best transformations should look similar to one another. We address two solutions to (2) in the next section. In short, we have methods to parallelize or simplify the estimation of the statistic. For (3), we recommend 10-fold cross-validation with 5 repeats as the default, but if the sample is small, we suggest using 5 (or fewer) folds instead with more repeats; accurate estimation of &lt;span class="math inline"&gt;\(P/DF\)&lt;/span&gt; requires a relatively large fold size (as a rule of thumb, 20 observations per fold seems to be enough for most cases, but this unfortunately depends on the distribution of the observed data).&lt;/p&gt;
&lt;h1 id="simple-examples"&gt;Simple examples&lt;/h1&gt;
&lt;p&gt;In this section, we illustrate a simple use-case of the functions provided in .&lt;/p&gt;
&lt;h2 id="basic-implementation"&gt;Basic implementation&lt;/h2&gt;
&lt;p&gt;First, we will generate and plot some skewed data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x &amp;lt;- rgamma(250, 1, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://rjournal-distill.netlify.app//articles/RJ-2021-041/figs/simple_hist-1.png" alt="Simulated skewed data for simple example." width="2.5in" height="1.3in" /&gt;
&lt;p class="caption"&gt;
(#fig:simple_hist)Simulated skewed data for simple example.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To perform a suite of potential transformations and see how effectively they normalized this vector, simply call :&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;(BNobject &amp;lt;- bestNormalize(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best Normalizing transformation with 250 Observations
 Estimated Normality Statistics (Pearson P / df, lower =&amp;gt; more normal):
 - arcsinh(x): 1.7917
 - Box-Cox: 1.0442
 - Center+scale: 3.0102
 - Exp(x): 9.5306
 - Log_b(x+a): 1.7072
 - orderNorm (ORQ): 1.1773
 - sqrt(x + a): 1.144
 - Yeo-Johnson: 1.1875
Estimation method: Out-of-sample via CV with 10 folds and 5 repeats
 
Based off these, bestNormalize chose:
Standardized Box Cox Transformation with 250 nonmissing obs.:
 Estimated statistics:
 - lambda = 0.3254863 
 - mean (before standardization) = -0.3659267 
 - sd (before standardization) = 0.9807881 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, the Box-Cox transformation performed the best, though many other transformations performed similarly. We can visualize the suite of transformations using the built-in  method:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(BNobject, leg_loc = &amp;quot;topleft&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://rjournal-distill.netlify.app//articles/RJ-2021-041/figs/bn_plot-1.png" alt="The suite of transformations estimated by default in \pkg{bestNormalize} (trained on simulated right-skewed data)." width="100%" /&gt;
&lt;p class="caption"&gt;
(#fig:bn_plot)The suite of transformations estimated by default in  (trained on simulated right-skewed data).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Finally, we can execute the best performing normalization on new data with  or reverse the transformation with . Note that normalized values can either be obtained using  or by extracting  from the object. The best transformation, in this case, is plotted in Figure 4.&lt;/p&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://rjournal-distill.netlify.app//articles/RJ-2021-041/figs/hist_best-1.png" alt="Summary of transformations performed on simulated right-skewed data." width="100%" /&gt;
&lt;p class="caption"&gt;
(#fig:hist_best)Summary of transformations performed on simulated right-skewed data.
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="performing-transformations-individually"&gt;Performing transformations individually&lt;/h2&gt;
&lt;p&gt;Each method can be performed (and stored) individually:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;(arcsinh_obj &amp;lt;- arcsinh_x(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Standardized asinh(x) Transformation with 250 nonmissing obs.:
 Relevant statistics:
 - mean (before standardization) = 0.7383146 
 - sd (before standardization) = 0.5458515 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;(boxcox_obj &amp;lt;- boxcox(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Standardized Box Cox Transformation with 250 nonmissing obs.:
 Estimated statistics:
 - lambda = 0.3254863 
 - mean (before standardization) = -0.3659267 
 - sd (before standardization) = 0.9807881 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;(yeojohnson_obj &amp;lt;- yeojohnson(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Standardized Yeo-Johnson Transformation with 250 nonmissing obs.:
 Estimated statistics:
 - lambda = -0.7080476 
 - mean (before standardization) = 0.4405464 
 - sd (before standardization) = 0.2592004 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;(lambert_obj &amp;lt;- lambert(x, type = &amp;quot;s&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Standardized Lambert WxF Transformation of type s with 250 nonmissing obs.:
 Estimated statistics:
 - gamma = 0.3729
 - mean (before standardization) = 0.6781864 
 - sd (before standardization) = 0.7123011 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;(orderNorm_obj &amp;lt;- orderNorm(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;orderNorm Transformation with 250 nonmissing obs and no ties 
 - Original quantiles:
   0%   25%   50%   75%  100% 
0.001 0.268 0.721 1.299 4.161 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All normalization techniques in  have their own class with convenient S3 methods and documentation. For instance, we can use the  method to perform the transformation on new values using the objects we have just created, visualizing them in a plot:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;xx &amp;lt;- seq(min(x), max(x), length = 100)
plot(xx, predict(arcsinh_obj, newdata = xx), type = &amp;quot;l&amp;quot;, col = 1)
lines(xx, predict(boxcox_obj, newdata = xx), col = 2)
lines(xx, predict(yeojohnson_obj, newdata = xx), col = 3)
lines(xx, predict(orderNorm_obj, newdata = xx), col = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://rjournal-distill.netlify.app//articles/RJ-2021-041/figs/vis_data2-1.png" alt="Manually plotting transformations trained on simulated right-skewed data." width="4.375in" height="2.5in" /&gt;
&lt;p class="caption"&gt;
(#fig:vis_data2)Manually plotting transformations trained on simulated right-skewed data.
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="in-sample-normalization-efficacy"&gt;In-sample normalization efficacy&lt;/h2&gt;
&lt;p&gt;To examine how each of the normalization methods performed (in-sample), we can visualize the transformed values in histograms (Figure 6), which plot the transformed data, , stored in the transformation objects we created previously.&lt;/p&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://rjournal-distill.netlify.app//articles/RJ-2021-041/figs/hist_trans-1.png" alt="Normalized values for trained transformations on simulated right-skewed data." width="4in" height="4in" /&gt;
&lt;p class="caption"&gt;
(#fig:hist_trans)Normalized values for trained transformations on simulated right-skewed data.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Evidently, ORQ normalization appears to have worked perfectly to normalize the data (as expected), and the Box-Cox method seemed to do quite well too.&lt;/p&gt;
&lt;h2 id="out-of-sample-normalization-efficacy"&gt;Out-of-sample normalization efficacy&lt;/h2&gt;
&lt;p&gt;The  function performs repeated (r=5) 10-fold cross-validation (CV) by default and stores the estimated normality statistic for each left-out fold/repeat into . Users can access and visualize these results via a boxplot (see below), which may give some insight into whether the transformation is truly preferred by the normality statistic or if another (possibly simpler) transformation can be applied that would achieve the approximately the same results. In this example, Box-Cox, square-root, Yeo-Johnson, and ORQ seem to do similarly well, whereas the identity transform, hyperbolic arc-sine, logging, and exponentiation are performing worse.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;boxplot(BNobject$oos_preds, log = &amp;#39;y&amp;#39;)
abline(h = 1, col = &amp;quot;green3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://rjournal-distill.netlify.app//articles/RJ-2021-041/figs/boxplot-1.png" alt="Cross-validation results for each normalization method, where our estimated normality statistic is plotted on the y-axis." width="100%" /&gt;
&lt;p class="caption"&gt;
(#fig:boxplot)Cross-validation results for each normalization method, where our estimated normality statistic is plotted on the y-axis.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Leave-one-out CV can be optionally performed in  via the  argument, which, if set to , will compute the leave-one-out CV transformations for each observation and method. Specifically,  will be run &lt;span class="math inline"&gt;\(n\)&lt;/span&gt; separate times where each observation is individually left out of the fitting process and subsequently plugged back in to get a “leave-one-out transformed value.” Instead of taking the mean across repeats and folds, in this case, we estimate normalization efficacy using the full distribution of leave-one-out transformed values. This option is computationally intensive. Note that as with the “in-sample” normality statistics, the leave-one-out CV approach tends to select the ORQ transformation since ORQ’s performance improves as the number of points in the training set relative to the testing set increases.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;bestNormalize(x, loo = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best Normalizing transformation with 250 Observations
 Estimated Normality Statistics (Pearson P / df, lower =&amp;gt; more normal):
 - arcsinh(x): 4.42
 - Box-Cox: 0.7055
 - Center+scale: 8.258
 - Exp(x): 62.085
 - Log_b(x+a): 3.546
 - orderNorm (ORQ): 0.012
 - sqrt(x + a): 0.9145
 - Yeo-Johnson: 1.608
Estimation method: Out-of-sample via leave-one-out CV
 
Based off these, bestNormalize chose:
orderNorm Transformation with 250 nonmissing obs and no ties 
 - Original quantiles:
   0%   25%   50%   75%  100% 
0.001 0.268 0.721 1.299 4.161 &lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="important-features"&gt;Important features&lt;/h1&gt;
&lt;h2 id="improving-speed-of-estimation"&gt;Improving speed of estimation&lt;/h2&gt;
&lt;p&gt;Because  uses repeated CV by default to estimate the out-of-sample normalization efficacy, it can be quite slow for larger objects. There are several means of speeding up the process. Each comes with some pros and cons. The first option is to specify . This will highly speed up the process. However, for reasons previously discussed, ORQ normalization will always be chosen unless . Therefore, a user might as well use the  function directly as opposed to only setting  since the end result will be the same (and  will run much faster). Note below that the in-sample normality results may differ slightly from the leave-one-out even when this may be unexpected (i.e., for the log transformation); this is due to slight differences in the standardization statistics.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;bestNormalize(x, allow_orderNorm = FALSE, out_of_sample = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best Normalizing transformation with 250 Observations
 Estimated Normality Statistics (Pearson P / df, lower =&amp;gt; more normal):
 - arcsinh(x): 4.401
 - Box-Cox: 0.7435
 - Center+scale: 8.087
 - Exp(x): 64.6975
 - Log_b(x+a): 3.47
 - sqrt(x + a): 0.9145
 - Yeo-Johnson: 1.7125
Estimation method: In-sample
 
Based off these, bestNormalize chose:
Standardized Box Cox Transformation with 250 nonmissing obs.:
 Estimated statistics:
 - lambda = 0.3254863 
 - mean (before standardization) = -0.3659267 
 - sd (before standardization) = 0.9807881 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option to improve estimation efficiency is to use the built-in parallelization functionality. The repeated CV process can be parallelized via the  argument and the  and  &lt;span class="citation"&gt;(Gaujoux 2020)&lt;/span&gt; packages. A cluster can be set up with  and passed to  via the  argument.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cl &amp;lt;- parallel::makeCluster(5)
b &amp;lt;- bestNormalize(x, cluster = cl, r = 10, quiet = TRUE)
parallel::stopCluster(cl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The amount by which this parallelization will speed up the estimation of out-of-sample estimates depends (for the most part) on the number of repeats, the number of cores, and the sample size of the vector to be normalized. The plot below shows the estimation time for a run of  with 15 repeats of 10-fold CV on a gamma-distributed random variable with various sample sizes and numbers of cores.&lt;/p&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://rjournal-distill.netlify.app//articles/RJ-2021-041/figs/parallel_timings.pdf" alt="Potential speedup using parallelization functionality." width="100%" /&gt;
&lt;p class="caption"&gt;
(#fig:parallel_timings)Potential speedup using parallelization functionality.
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="implementation-with-caret-recipes"&gt;Implementation with caret, recipes&lt;/h2&gt;
&lt;p&gt;The  and the  functions can be utilized in conjunction with the  package to preprocess data in machine learning workflows with  &lt;span class="citation"&gt;(Kuhn and Wickham 2020)&lt;/span&gt; or in combination with . The basic usage within  is shown below; for implementation with , refer to this paper’s application.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rec &amp;lt;- recipe( ~ ., data = iris)  %&amp;gt;%                       # Initialize recipe
  step_best_normalize(all_predictors(), -all_nominal()) %&amp;gt;% # Transform predictors
  prep(iris) %&amp;gt;%                                            # Prep (train) recipe
  bake(iris)                                                # Bake (apply) recipe&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Options can be supplied to  to speed up or alter performance via the  argument, which passes a list of options to .&lt;/p&gt;
&lt;h2 id="additional-customization"&gt;Additional customization&lt;/h2&gt;
&lt;p&gt;Two important means of customization are available: 1) users may add custom transformation functions to be assessed alongside the default suite of normalization methods, and 2) users may change the statistic used “under the hood” by  to estimate the departure from normality of the transformed data. This section contains examples and guidance for both extensions.&lt;/p&gt;
&lt;h3 id="adding-user-defined-functions"&gt;1) Adding user-defined functions&lt;/h3&gt;
&lt;p&gt;Via the  argument, users can use ’s machinery to compare custom, user-defined transformation functions to those included in the package. Below, I consider an example where a user may wish to compare the cube-root function with those provided in the package.  requires two functions to implement this: the transformation function and an associated  method. The custom cube-root transformation shown below is simple, but its skeleton can readily be made arbitrarily more complex.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;## Define custom function
cuberoot_x &amp;lt;- function(x, ...) {
  x.t &amp;lt;- (x)^(1/3)
  
  # Get in-sample normality statistic results
  ptest &amp;lt;- nortest::pearson.test(x.t)
  
  val &amp;lt;- list(
    x.t = x.t,
    x = x,
    n = length(x.t) - sum(is.na(x)), 
    norm_stat = unname(ptest$statistic / ptest$df)
  )
  
  # Assign class, return
  class(val) &amp;lt;- c(&amp;#39;cuberoot_x&amp;#39;)
  val
}

# S3 method that is used to apply the transformation to newly observed data
predict.cuberoot_x &amp;lt;- function(object, newdata = NULL, inverse = FALSE, ...) {
  
  # If no data supplied and not inverse
  if (is.null(newdata) &amp;amp; !inverse)
    newdata &amp;lt;- object$x
  
  # If no data supplied and inverse transformation is requested
  if (is.null(newdata) &amp;amp; inverse)
    newdata &amp;lt;- object$x.t
  
  # Perform inverse transformation
  if (inverse) {
    # Reverse-cube-root (cube)
    val &amp;lt;-  newdata^3
    
    # Otherwise, perform transformation as estimated
  } else if (!inverse) {
    val &amp;lt;- (newdata)^(1/3)
  }
  
  # Return transformed data
  unname(val)
}

## Optional: print S3 method
print.cuberoot_x &amp;lt;- function(x, ...) {
  cat(&amp;#39;cuberoot(x) Transformation with&amp;#39;, x$n, &amp;#39;nonmissing obs.\n&amp;#39;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These functions can then be passed as a named list to :&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;custom_transform &amp;lt;- list(
  cuberoot_x = cuberoot_x,
  predict.cuberoot_x = predict.cuberoot_x,
  print.cuberoot_x = print.cuberoot_x
)

set.seed(123129)
x &amp;lt;- rgamma(100, 1, 1)
(b &amp;lt;- bestNormalize(x = x, new_transforms = custom_transform))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best Normalizing transformation with 100 Observations
 Estimated Normality Statistics (Pearson P / df, lower =&amp;gt; more normal):
 - arcsinh(x): 1.2347
 - Box-Cox: 1.0267
 - Center+scale: 2.0027
 - cuberoot_x: 0.9787
 - Exp(x): 4.7947
 - Log_b(x+a): 1.3547
 - orderNorm (ORQ): 1.1627
 - sqrt(x + a): 1.0907
 - Yeo-Johnson: 1.0987
Estimation method: Out-of-sample via CV with 10 folds and 5 repeats
 
Based off these, bestNormalize chose:
cuberoot(x) Transformation with 100 nonmissing obs.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, the cube-root was the best normalizing transformation for this gamma-distributed random variable, performing comparably to the Box-Cox transformation.&lt;/p&gt;
&lt;h3 id="re-defining-normality"&gt;2) Re-defining normality&lt;/h3&gt;
&lt;p&gt;The question “what is normal?” outside of a statistical discussion is quite loaded and subjective. Even in statistical discussions, many authors have contributed to the question of how to best detect departures from normality; these solutions are diverse, and several have been implemented well in  already. In order to accommodate those with varying opinions on the best definition of normality, we have included a feature that allows users to specify a custom definition of a normality statistic. This customization can be accomplished via the  argument, which takes a function that will then be applied in lieu of the Pearson test statistic divided by its degree of freedom to assess normality.&lt;/p&gt;
&lt;p&gt;The user-defined function must take an argument , which indicates the data on which a user wants to evaluate the statistic.&lt;/p&gt;
&lt;p&gt;Here is an example using the Lilliefors (Kolmogorov-Smirnov) normality test statistic:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;bestNormalize(x, norm_stat_fn = function(x) nortest::lillie.test(x)$stat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best Normalizing transformation with 100 Observations
 Estimated Normality Statistics (using custom normalization statistic)
 - arcsinh(x): 0.1958
 - Box-Cox: 0.1785
 - Center+scale: 0.2219
 - Exp(x): 0.3299
 - Log_b(x+a): 0.1959
 - orderNorm (ORQ): 0.186
 - sqrt(x + a): 0.1829
 - Yeo-Johnson: 0.1872
Estimation method: Out-of-sample via CV with 10 folds and 5 repeats
 
Based off these, bestNormalize chose:
Standardized Box Cox Transformation with 100 nonmissing obs.:
 Estimated statistics:
 - lambda = 0.3281193 
 - mean (before standardization) = -0.1263882 
 - sd (before standardization) = 0.9913552 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is an example using the Lillifors (Kolmogorov-Smirnov) normality test’s -value:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;(dont_do_this &amp;lt;- bestNormalize(x, norm_stat_fn = function(x) nortest::lillie.test(x)$p))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best Normalizing transformation with 100 Observations
 Estimated Normality Statistics (using custom normalization statistic)
 - arcsinh(x): 0.4327
 - Box-Cox: 0.4831
 - Center+scale: 0.2958
 - Exp(x): 0.0675
 - Log_b(x+a): 0.3589
 - orderNorm (ORQ): 0.4492
 - sqrt(x + a): 0.4899
 - Yeo-Johnson: 0.4531
Estimation method: Out-of-sample via CV with 10 folds and 5 repeats
 
Based off these, bestNormalize chose:
Standardized exp(x) Transformation with 100 nonmissing obs.:
 Relevant statistics:
 - mean (before standardization) = 6.885396 
 - sd (before standardization) = 13.66084 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note:  will attempt to minimize this statistic by default, which is definitely not what you want to do when calculating the -value. This is seen in the example above, where the &lt;strong&gt;worst&lt;/strong&gt; normalization transformation, exponentiation, is chosen. In this case, a user is advised to either manually select the best one or reverse their defined normalization statistic (in this case by subtracting it from 1):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;best_transform &amp;lt;- names(which.max(dont_do_this$norm_stats))
do_this &amp;lt;- dont_do_this$other_transforms[[best_transform]]
or_this &amp;lt;- bestNormalize(x, norm_stat_fn = function(x) 1-nortest::lillie.test(x)$p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A -value for normality should not be routinely used as the sole selector of a normalizing transformation. A normality test’s -value, as a measure of the departure from normality, is confounded by the sample size (a high sample size may yield strong evidence of a practically insignificant departure from normality). Therefore, we suggest the statistic used should estimate the departure from normality rather the strength of evidence against normality &lt;span class="citation"&gt;(e.g., Royston 1991)&lt;/span&gt;.&lt;/p&gt;
&lt;h1 id="application-to-autotrader-data"&gt;Application to Autotrader data&lt;/h1&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;The  data set was scraped from the &lt;a href="https://www.autotrader.com/"&gt;autotrader website&lt;/a&gt; as part of this package (and because at the time of data collection in 2017, the package author needed to purchase a car). We apply the  functionality to de-skew mileage, age, and price in a pricing model. See  for more information on this data set.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data(&amp;quot;autotrader&amp;quot;)
autotrader$yearsold &amp;lt;- 2017 - autotrader$Year&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;
(#tab:unnamed-chunk-10)Sample characteristics of &lt;code&gt;autotrader&lt;/code&gt; data.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
Overall (N=6,283)
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Make
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Acura
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
185 (2.9%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Buick
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
252 (4.0%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Chevrolet
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
1,257 (20.0%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;GMC
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
492 (7.8%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Honda
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
1,029 (16.4%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Hyundai
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
381 (6.1%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Mazda
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
272 (4.3%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Nissan
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
735 (11.7%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Pontiac
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
63 (1.0%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Toyota
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
1,202 (19.1%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Volkswagen
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
415 (6.6%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Price ($)
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Mean (SD)
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
17,145 (8,346)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Range
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
722 - 64,998
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Mileage
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Mean (SD)
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
63,638 (49,125)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Range
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
2 - 325,556
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Year
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Mean (SD)
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
2011.9 (3.5)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Range
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
2000.0 - 2016.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Age (years old)
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Mean (SD)
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
5.1 (3.5)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
&lt;ul&gt;
&lt;li&gt;Range
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
1.0 - 17.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="transform-both-sides-regression"&gt;Transform-both-sides regression&lt;/h2&gt;
&lt;p&gt;Transform-both-sides (TBS) regression has several benefits that have been explored thoroughly elsewhere (see &lt;span class="citation"&gt;Harrell (2015)&lt;/span&gt; for an overview). Importantly, TBS regression can often (though not always) yield models that better satisfy assumptions of linear regression and mitigate the influence of outliers/skew. This approach has been shown to be useful in shrinking the size of prediction intervals while maintaining closer to nominal coverage in this data set &lt;span class="citation"&gt;(Peterson and Cavanaugh 2019)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;First, we will normalize the outcome (price).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;(priceBN &amp;lt;- bestNormalize(autotrader$price))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best Normalizing transformation with 6283 Observations
 Estimated Normality Statistics (Pearson P / df, lower =&amp;gt; more normal):
 - arcsinh(x): 3.8573
 - Box-Cox: 2.2291
 - Center+scale: 3.5532
 - Log_b(x+a): 3.8573
 - orderNorm (ORQ): 1.1384
 - sqrt(x + a): 2.1977
 - Yeo-Johnson: 2.2291
Estimation method: Out-of-sample via CV with 10 folds and 5 repeats
 
Based off these, bestNormalize chose:
orderNorm Transformation with 6283 nonmissing obs and ties
 - 2465 unique values 
 - Original quantiles:
   0%   25%   50%   75%  100% 
  722 11499 15998 21497 64998 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the estimated normality statistic for the ORQ transformation is close to 1, so we know it is performing quite well despite the ties in the data. It is also performing considerably better than all of the other transformations.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;(mileageBN &amp;lt;- bestNormalize(autotrader$mileage))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best Normalizing transformation with 6283 Observations
 Estimated Normality Statistics (Pearson P / df, lower =&amp;gt; more normal):
 - arcsinh(x): 3.4332
 - Box-Cox: 3.0903
 - Center+scale: 14.7488
 - Log_b(x+a): 3.4354
 - orderNorm (ORQ): 1.1514
 - sqrt(x + a): 5.1041
 - Yeo-Johnson: 3.0891
Estimation method: Out-of-sample via CV with 10 folds and 5 repeats
 
Based off these, bestNormalize chose:
orderNorm Transformation with 6283 nonmissing obs and ties
 - 6077 unique values 
 - Original quantiles:
    0%    25%    50%    75%   100% 
     2  29099  44800  88950 325556 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, the ORQ normalization performed best for mileage.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;(yearsoldBN &amp;lt;- bestNormalize(autotrader$yearsold))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best Normalizing transformation with 6283 Observations
 Estimated Normality Statistics (Pearson P / df, lower =&amp;gt; more normal):
 - arcsinh(x): 83.2706
 - Box-Cox: 83.2909
 - Center+scale: 83.4324
 - Exp(x): 574.3318
 - Log_b(x+a): 83.0756
 - orderNorm (ORQ): 81.3615
 - sqrt(x + a): 83.4373
 - Yeo-Johnson: 84.0028
Estimation method: Out-of-sample via CV with 10 folds and 5 repeats
 
Based off these, bestNormalize chose:
orderNorm Transformation with 6283 nonmissing obs and ties
 - 17 unique values 
 - Original quantiles:
  0%  25%  50%  75% 100% 
   1    3    4    7   17 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For age, we see something peculiar; none of the normalizing transformations performed well according to the normality statistics. By plotting the data, it becomes evident that the frequency of ties in age makes it very difficult to find a normalizing transformation (see figure below). Even so,  is chosen as it has the lowest estimated &lt;span class="math inline"&gt;\(P/DF\)&lt;/span&gt; statistic.&lt;/p&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://rjournal-distill.netlify.app//articles/RJ-2021-041/figs/hist_app-1.png" alt="Distributions of car variables before and after normalization." width="4.2in" height="2.1in" /&gt;
&lt;p class="caption"&gt;
(#fig:hist_app)Distributions of car variables before and after normalization.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Next, we will fit a linear model on the transformed values of each variable for our TBS regression. The reverse-transformation functions will allow us to visualize how these variables affect model predictions in terms of their original units.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;p.t &amp;lt;- priceBN$x.t; m.t &amp;lt;- mileageBN$x.t; yo.t &amp;lt;- yearsoldBN$x.t
fit &amp;lt;- lm(p.t ~ m.t + yo.t)&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;
(#tab:unnamed-chunk-11)TBS regression results for autotrader data.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
Variable
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Estimate
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Std. Error
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
t value
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Pr(&amp;gt;|t|)
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
Intercept
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.005
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.010
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.553
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.58
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
g(Mileage)
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
-0.234
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.016
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
-14.966
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
&amp;lt; 0.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
g(Age)
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
-0.441
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.016
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
-27.134
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
&amp;lt; 0.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unsurprisingly, we find that there are very significant relationships between transformed car price, mileage, and age. However, to interpret these values, we must resort to visualizations since there is no inherent meaning of a “one-unit increase” in the ORQ normalized measurements. We utilize the  package &lt;span class="citation"&gt;(Breheny and Burchett 2017)&lt;/span&gt; to perform our visualizations, using  in conjunction with ’s  and  options to view the relationship in terms of the original unit for the response and covariate respectively (formatting omitted). For the sake of illustration, we have also plotted the estimated effect of a generalized additive (spline) model fit with  &lt;span class="citation"&gt;(Wood 2011)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;visreg(fit, &amp;quot;m.t&amp;quot;)
visreg(fit, &amp;quot;m.t&amp;quot;, 
       partial = TRUE,
       trans = function(price.t) 
         predict(priceBN, newdata = price.t, inverse = TRUE)/1000, 
       xtrans = function(mileage.t) 
         predict(mileageBN, newdata = mileage.t, inverse = TRUE)
       )&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://rjournal-distill.netlify.app//articles/RJ-2021-041/figs/linear_visreg-1.png" alt="TBS regression visualized on transformed units (left) and original units (right)." width="80%" /&gt;
&lt;p class="caption"&gt;
(#fig:linear_visreg)TBS regression visualized on transformed units (left) and original units (right).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Below, we visualize the age effect, demonstrating how one might visualize the effect outside of  (plot formatting is omitted).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Set up data for plotting line
new_yo &amp;lt;- seq(min(autotrader$yearsold), max(autotrader$yearsold), len = 100)
newX &amp;lt;- data.frame(yearsold = new_yo, mileage = median(autotrader$mileage))
newXt &amp;lt;- data.frame(yo.t = predict(yearsoldBN, newX$yearsold), 
                    m.t = predict(mileageBN, newX$mileage))

line_vals_t &amp;lt;- predict(fit, newdata = newXt) # Calculate line (transformed)
line_vals &amp;lt;- predict(priceBN, newdata = line_vals_t, inverse = TRUE)
plot(autotrader$yearsold, autotrader$price)
lines(new_yo, line_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://rjournal-distill.netlify.app//articles/RJ-2021-041/figs/gam_tbs_model-1.png" alt="Age effect on car price (re-transformed to original unit)." width="80%" /&gt;
&lt;p class="caption"&gt;
(#fig:gam_tbs_model)Age effect on car price (re-transformed to original unit).
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="implementation-with-recipes"&gt;Implementation with recipes&lt;/h2&gt;
&lt;p&gt;To build a predictive model for the price variable that uses each vehicle’s model and make in addition to its mileage and age, we can utilize the  and  functionality to do so. This section outlines how to use  in conjunction with these other popular ML packages. Price is logged instead of ORQ transformed in order to facilitate the interpretation of measures for prediction accuracy.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tidymodels)
library(caret)
library(recipes)

set.seed(321)
df_split &amp;lt;- initial_split(autotrader, prop = .9)
df_train &amp;lt;- training(df_split)
df_test &amp;lt;- testing(df_split)

rec &amp;lt;- recipe(price ~ Make + model +  mileage + status + Year, df_train) %&amp;gt;% 
  step_mutate(years_old = 2017 - Year) %&amp;gt;% 
  step_rm(Year) %&amp;gt;% 
  step_log(price) %&amp;gt;% 
  step_best_normalize(all_predictors(), -all_nominal()) %&amp;gt;% 
  step_other(all_nominal(), threshold = 10) %&amp;gt;% 
  step_dummy(all_nominal()) %&amp;gt;% 
  prep()

fit1 &amp;lt;- train(price ~ ., bake(rec, NULL), method = &amp;#39;glmnet&amp;#39;)
fit2 &amp;lt;- train(price ~ ., bake(rec, NULL), method = &amp;#39;earth&amp;#39;)
fit3 &amp;lt;- train(price ~ ., bake(rec, NULL), method = &amp;#39;rf&amp;#39;)

r &amp;lt;- resamples(fits &amp;lt;- list(glmnet = fit1, earth = fit2, rf = fit3))
summary(r) # Extra-sample CV results&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;
(#tab:unnamed-chunk-14)CV prediction accuracy of various ML methods.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Min.
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
1st Qu.
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Median
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Mean
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
3rd Qu.
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Max.
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
NA’s
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength="3"&gt;
&lt;td colspan="8" style="border-bottom: 1px solid;"&gt;
&lt;strong&gt;MAE&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;padding-left: 2em;" indentlevel="1"&gt;
glmnet
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.181
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.184
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.186
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.189
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.194
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.198
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;padding-left: 2em;" indentlevel="1"&gt;
earth
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.147
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.151
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.154
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.155
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.158
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.163
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;padding-left: 2em;" indentlevel="1"&gt;
rf
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.136
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.141
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.143
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.144
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.147
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.157
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength="3"&gt;
&lt;td colspan="8" style="border-bottom: 1px solid;"&gt;
&lt;strong&gt;RMSE&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;padding-left: 2em;" indentlevel="1"&gt;
glmnet
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.242
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.247
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.252
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.256
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.264
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.276
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;padding-left: 2em;" indentlevel="1"&gt;
earth
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.203
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.209
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.214
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.217
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.226
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.235
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;padding-left: 2em;" indentlevel="1"&gt;
rf
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.193
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.208
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.213
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.210
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.215
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.217
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength="3"&gt;
&lt;td colspan="8" style="border-bottom: 1px solid;"&gt;
&lt;strong&gt;RSQ&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;padding-left: 2em;" indentlevel="1"&gt;
glmnet
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.767
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.772
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.785
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.782
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.789
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.801
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;padding-left: 2em;" indentlevel="1"&gt;
earth
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.807
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.833
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.845
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.842
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.855
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.864
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;padding-left: 2em;" indentlevel="1"&gt;
rf
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.835
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.845
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.855
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.854
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.860
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.873
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Evidently, the random forest generally performed better in cross-validated prediction metrics, achieving a higher R-squared (RSQ), lower root-mean-squared error (RMSE), and lower mean absolute error (MAE). Since price was logged, RMSE and MAE are on the log scale. For the test set, we calculate these quantities in price’s original unit (2017 US dollars) using the  package &lt;span class="citation"&gt;(Kuhn and Vaughan 2020)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Out of sample prediction accuracy
results &amp;lt;- lapply(fits, function(x) {
  p &amp;lt;- c(predict(x, newdata = bake(rec, df_test)))
  yardstick::metrics(data.frame(est = exp(p), truth = df_test$price), 
                     truth = truth, estimate = est)
})
results&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
&lt;caption&gt;
(#tab:unnamed-chunk-16)Test data prediction accuracy of various ML methods. RMSE and MAE can be interpreted in terms of 2017 US dollars.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
Method
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
RMSE
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
RSQ
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
MAE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
glmnet
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
4076
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.772
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
2847
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
earth
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
3619
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.814
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
2500
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
rf
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
3257
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.853
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
2294
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;After normalization of mileage and age, a random forest had the optimal predictive performance on car price given a car’s make, model, age, and mileage compared to other ML models, achieving out-of-sample R-squared 0.853 on a left-out test data set. We conjecture that the random forest performs best because it can better capture differential depreciation by make and model than the other methods.&lt;/p&gt;
&lt;h1 id="discussion"&gt;Discussion&lt;/h1&gt;
&lt;p&gt;We have shown how the  package can effectively and efficiently find the best normalizing transformation for a vector or set of vectors. However, normalization is by no means something that should be applied universally and without motivation. In situations where units have meaning, normalizing prior to analysis can contaminate the relationships suspected in the data and/or reduce predictive accuracy. Further, depending on the type of transformations used, interpreting regression coefficients post-transformation can be difficult or impossible without using a figure since the transformation function itself will look completely different for different distributions. So, while normalization transformations may well be able to increase the robustness of results and mitigate violations to the classical linear regression assumption of Gaussian residuals, it is by no means a universal solution.&lt;/p&gt;
&lt;p&gt;On the other hand, when hypotheses are exploratory or when data is of poor quality with high amounts of skew/outliers, normalization can be an effective means of mitigating downstream issues this can cause in the analyses. For example, in machine learning contexts, some predictor manipulations rely on second-order statistics (e.g., principal components analysis or partial least squares), for which the variance calculation can be sensitive to skew and outliers. Normalizing transformations can improve the quality and stability of these calculations. Similarly, predictor normalization reduces the tendency for high-leverage points to have their leverage propagated into engineered features such as interactions or polynomials. Ultimately, these benefits can often produce predictive models that are more robust and stable.&lt;/p&gt;
&lt;p&gt;We focused on making this package useful in a variety of machine learning workflows. We are enthusiastic in our support of , and will continue to maintain the package while it is found to be useful by R users. We hope to continue to build up the repertoire of candidate transformations using the same infrastructure so that additional ones can be considered by default in the future.&lt;/p&gt;
&lt;h1 id="references"&gt;References&lt;/h1&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-bartlett1947" class="csl-entry"&gt;
Bartlett, M. S. 1947. &lt;span&gt;“The Use of Transformations.”&lt;/span&gt; &lt;em&gt;Biometrics&lt;/em&gt; 3 (1): 39–52. &lt;a href="https://doi.org/10.2307/3001536"&gt;https://doi.org/10.2307/3001536&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-BickelDoksum" class="csl-entry"&gt;
Bickel, Peter J., and Kjell A. Doksum. 1981. &lt;span&gt;“An Analysis of Transformations Revisited.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 76 (374): 296–311. &lt;a href="https://doi.org/10.1080/01621459.1981.10477649"&gt;https://doi.org/10.1080/01621459.1981.10477649&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-BoxCox1964" class="csl-entry"&gt;
Box, G. E. P., and D. R. Cox. 1964. &lt;span&gt;“An Analysis of Transformations.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society. Series B (Methodological)&lt;/em&gt; 26 (2): 211–52. &lt;a href="https://doi.org/10.2307/2984418"&gt;https://doi.org/10.2307/2984418&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-visreg" class="csl-entry"&gt;
Breheny, Patrick, and Woodrow Burchett. 2017. &lt;span&gt;“Visualization of Regression Models Using Visreg.”&lt;/span&gt; &lt;em&gt;The R Journal&lt;/em&gt; 9 (2): 56–71.
&lt;/div&gt;
&lt;div id="ref-d1986goodness" class="csl-entry"&gt;
D’Agostino, Ralph B. 1986. &lt;em&gt;Goodness-of-Fit-Techniques&lt;/em&gt;. Vol. 68. CRC press.
&lt;/div&gt;
&lt;div id="ref-doRNG" class="csl-entry"&gt;
Gaujoux, Renaud. 2020. &lt;em&gt;doRNG: Generic Reproducible Parallel Backend for ’Foreach’ Loops&lt;/em&gt;. &lt;a href="https://CRAN.R-project.org/package=doRNG"&gt;https://CRAN.R-project.org/package=doRNG&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-goerg2011" class="csl-entry"&gt;
Goerg, Georg M. 2011. &lt;span&gt;“Lambert w Random Variables-a New Family of Generalized Skewed Distributions with Applications to Risk Estimation.”&lt;/span&gt; &lt;em&gt;The Annals of Applied Statistics&lt;/em&gt; 5 (3): 2197–2230. &lt;a href="https://doi.org/10.1214/11-AOAS457"&gt;https://doi.org/10.1214/11-AOAS457&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-nortest" class="csl-entry"&gt;
Gross, Juergen, and Uwe Ligges. 2015. &lt;em&gt;Nortest: Tests for Normality&lt;/em&gt;. &lt;a href="https://CRAN.R-project.org/package=nortest"&gt;https://CRAN.R-project.org/package=nortest&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-harrell" class="csl-entry"&gt;
Harrell, Frank E. 2015. &lt;em&gt;Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis&lt;/em&gt;. Springer.
&lt;/div&gt;
&lt;div id="ref-JohnDraper" class="csl-entry"&gt;
John, J. A., and N. R. Draper. 1980. &lt;span&gt;“An Alternative Family of Transformations.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society. Series C (Applied Statistics)&lt;/em&gt; 29 (2): 190–97. &lt;a href="https://doi.org/10.2307/2986305"&gt;https://doi.org/10.2307/2986305&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-caret" class="csl-entry"&gt;
Kuhn, Max. 2017. &lt;em&gt;Caret: Classification and Regression Training&lt;/em&gt;. &lt;a href="https://CRAN.R-project.org/package=caret"&gt;https://CRAN.R-project.org/package=caret&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-kuhn2013APM" class="csl-entry"&gt;
Kuhn, Max, and Kjell Johnson. 2013. &lt;em&gt;Applied Predictive Modeling&lt;/em&gt;. Springer. &lt;a href="https://doi.org/10.1007/978-1-4614-6849-3"&gt;https://doi.org/10.1007/978-1-4614-6849-3&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-yardstick" class="csl-entry"&gt;
Kuhn, Max, and Davis Vaughan. 2020. &lt;em&gt;Yardstick: Tidy Characterizations of Model Performance&lt;/em&gt;. &lt;a href="https://CRAN.R-project.org/package=yardstick"&gt;https://CRAN.R-project.org/package=yardstick&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-recipes" class="csl-entry"&gt;
Kuhn, Max, and Hadley Wickham. 2018. &lt;em&gt;Recipes: Preprocessing Tools to Create Design Matrices&lt;/em&gt;. &lt;a href="https://CRAN.R-project.org/package=recipes"&gt;https://CRAN.R-project.org/package=recipes&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-tidymodels" class="csl-entry"&gt;
———. 2020. &lt;em&gt;Tidymodels: Easily Install and Load the ’Tidymodels’ Packages&lt;/em&gt;. &lt;a href="https://CRAN.R-project.org/package=tidymodels"&gt;https://CRAN.R-project.org/package=tidymodels&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-Manly" class="csl-entry"&gt;
Manly, B. F. J. 1976. &lt;span&gt;“Exponential Data Transformations.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society. Series D (The Statistician)&lt;/em&gt; 25 (1): 37–42. &lt;a href="https://doi.org/10.2307/2988129"&gt;https://doi.org/10.2307/2988129&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-orq_paper" class="csl-entry"&gt;
Peterson, Ryan A., and Joseph E. Cavanaugh. 2019. &lt;span&gt;“Ordered Quantile Normalization: A Semiparametric Transformation Built for the Cross-Validation Era.”&lt;/span&gt; &lt;em&gt;Journal of Applied Statistics&lt;/em&gt;, 1–16. &lt;a href="https://doi.org/10.1080/02664763.2019.1630372"&gt;https://doi.org/10.1080/02664763.2019.1630372&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-normality" class="csl-entry"&gt;
Royston, Patrick. 1991. &lt;span&gt;“Estimating Departure from Normality.”&lt;/span&gt; &lt;em&gt;Statistics in Medicine&lt;/em&gt; 10 (8): 1283–93. https://doi.org/&lt;a href="https://doi.org/10.1002/sim.4780100811"&gt;https://doi.org/10.1002/sim.4780100811&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-thode2002testing" class="csl-entry"&gt;
Thode, Henry C. 2002. &lt;em&gt;Testing for Normality&lt;/em&gt;. Vol. 164. CRC press.
&lt;/div&gt;
&lt;div id="ref-van1952" class="csl-entry"&gt;
Van der Waerden, BL. 1952. &lt;span&gt;“Order Tests for the Two-Sample Problem and Their Power.”&lt;/span&gt; In &lt;em&gt;Indagationes Mathematicae (Proceedings)&lt;/em&gt;, 55:453–58. Elsevier.
&lt;/div&gt;
&lt;div id="ref-MASS" class="csl-entry"&gt;
Venables, W. N., and B. D. Ripley. 2002. &lt;em&gt;Modern Applied Statistics with s&lt;/em&gt;. Fourth. New York: Springer. &lt;a href="http://www.stats.ox.ac.uk/pub/MASS4"&gt;http://www.stats.ox.ac.uk/pub/MASS4&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-mgcv" class="csl-entry"&gt;
Wood, S. N. 2011. &lt;span&gt;“Fast Stable Restricted Maximum Likelihood and Marginal Likelihood Estimation of Semiparametric Generalized Linear Models.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society (B)&lt;/em&gt; 73 (1): 3–36.
&lt;/div&gt;
&lt;div id="ref-YeoJohnson" class="csl-entry"&gt;
Yeo, In‐Kwon, and Richard A. Johnson. 2000. &lt;span&gt;“A New Family of Power Transformations to Improve Normality or Symmetry.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 87 (4): 954–59. &lt;a href="https://doi.org/10.1093/biomet/87.4.954"&gt;https://doi.org/10.1093/biomet/87.4.954&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5>622dfa5358b71d3c3c60ee26b2aa7a31</distill:md5>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-041</guid>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-041/figs/orq_vis-1.png" medium="image" type="image/png" width="1344" height="960"/>
    </item>
    <item>
      <title>Statistical Quality Control with the qcr Package</title>
      <dc:creator>Miguel Flores</dc:creator>
      <dc:creator>Rubén Fernández-Casal</dc:creator>
      <dc:creator>Salvador Naya</dc:creator>
      <dc:creator>Javier Tarrío-Saavedra</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-034</link>
      <description>Statistical Quality Control with the qcr Package</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-034</guid>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Linear Regression with Stationary Errors: the R Package slm</title>
      <dc:creator>Emmanuel Caron</dc:creator>
      <dc:creator>Jérôme Dedecker</dc:creator>
      <dc:creator>Bertrand Michel</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-030</link>
      <description>Linear Regression with Stationary Errors: the R Package slm</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-030</guid>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>clustcurv: An R Package for Determining Groups in Multiple Curves </title>
      <dc:creator>Nora M. Villanueva</dc:creator>
      <dc:creator>Marta Sestelo</dc:creator>
      <dc:creator>Luis Meira-Machado</dc:creator>
      <dc:creator>Javier Roca-Pardiñas</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-032</link>
      <description>clustcurv: An R Package for Determining Groups in Multiple Curves </description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-032</guid>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Benchmarking R packages for Calculation of Persistent Homology</title>
      <dc:creator>Eashwar V. Somasundaram</dc:creator>
      <dc:creator>Shael E. Brown</dc:creator>
      <dc:creator>Adam Litzler</dc:creator>
      <dc:creator>Jacob G. Scott</dc:creator>
      <dc:creator>Raoul R. Wadhwa</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-033</link>
      <description>Benchmarking R packages for Calculation of Persistent Homology</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-033</guid>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Wide-to-tall Data Reshaping Using Regular Expressions and the nc Package</title>
      <dc:creator>Toby Dylan Hocking</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-029</link>
      <description>Wide-to-tall Data Reshaping Using Regular Expressions and the nc Package</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-029</guid>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Unidimensional and Multidimensional Methods for Recurrence Quantification Analysis with crqa</title>
      <dc:creator>Moreno I. Coco</dc:creator>
      <dc:creator>Dan Mønster</dc:creator>
      <dc:creator>Giuseppe Leonardi</dc:creator>
      <dc:creator>Rick Dale</dc:creator>
      <dc:creator>Sebastian Wallot</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-062</link>
      <description>Unidimensional and Multidimensional Methods for Recurrence Quantification Analysis with crqa</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-062</guid>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The bdpar Package: Big Data Pipelining Architecture for R</title>
      <dc:creator>Miguel Ferreiro-Díaz</dc:creator>
      <dc:creator>Tomás R. Cotos-Yáñez</dc:creator>
      <dc:creator>José R. Méndez</dc:creator>
      <dc:creator>David Ruano-Ordás</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-065</link>
      <description>The bdpar Package: Big Data Pipelining Architecture for R</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-065</guid>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>JMcmprsk: An R Package for Joint Modelling of Longitudinal and Survival Data with Competing Risks</title>
      <dc:creator>Hong Wang</dc:creator>
      <dc:creator>Ning Li</dc:creator>
      <dc:creator>Shanpeng Li</dc:creator>
      <dc:creator>Gang Li</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-028</link>
      <description>JMcmprsk: An R Package for Joint Modelling of Longitudinal and Survival Data with Competing Risks</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-028</guid>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>exPrior: An R Package for the Formulation of Ex-Situ Priors</title>
      <dc:creator>Falk Heße</dc:creator>
      <dc:creator>Karina Cucchi</dc:creator>
      <dc:creator>Nura Kawa</dc:creator>
      <dc:creator>Yoram Rubin</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-031</link>
      <description>exPrior: An R Package for the Formulation of Ex-Situ Priors</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-031</guid>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>penPHcure: Variable Selection in Proportional Hazards Cure Model with Time-Varying Covariates</title>
      <dc:creator>Alessandro Beretta</dc:creator>
      <dc:creator>Cédric Heuchenne</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-061</link>
      <description>penPHcure: Variable Selection in Proportional Hazards Cure Model with Time-Varying Covariates</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-061</guid>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A Method for Deriving Information from Running R Code</title>
      <dc:creator>Mark P.J. van der Loo</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-056</link>
      <description>A Method for Deriving Information from Running R Code</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-056</guid>
      <pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>npcure: An R Package for Nonparametric Inference in Mixture Cure Models</title>
      <dc:creator>Ana López-Cheda</dc:creator>
      <dc:creator>M. Amalia Jácome</dc:creator>
      <dc:creator>Ignacio López-de-Ullibarri</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-027</link>
      <description>npcure: An R Package for Nonparametric Inference in Mixture Cure Models</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-027</guid>
      <pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>SEEDCCA: An Integrated R-Package for Canonical Correlation Analysis and Partial Least Squares</title>
      <dc:creator>Bo-Young Kim</dc:creator>
      <dc:creator>Yunju Im</dc:creator>
      <dc:creator>Jae Keun Yoo</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-026</link>
      <description>SEEDCCA: An Integrated R-Package for Canonical Correlation Analysis and Partial Least Squares</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-026</guid>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
