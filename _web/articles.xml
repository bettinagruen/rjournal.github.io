<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>The R Journal</title>
    <link>https://rjournal-distill.netlify.app/</link>
    <atom:link href="https://rjournal-distill.netlify.app/articles.xml" rel="self" type="application/rss+xml"/>
    <description>Articles published in the R Journal</description>
    <image>
      <title>The R Journal</title>
      <url>https://rjournal-distill.netlify.app/resources/favicon.ico</url>
      <link>https://rjournal-distill.netlify.app/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Tue, 08 Jun 2021 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Conversations in Time: Interactive Visualization to Explore Structured Temporal Data</title>
      <dc:creator>Earo Wang</dc:creator>
      <dc:creator>Dianne Cook</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-050</link>
      <description>Temporal data often has a hierarchical structure, defined by categorical variables describing different levels, such as political regions or sales products. The nesting of categorical variables produces a hierarchical structure. The \CRANpkg{tsibbletalk} package is developed to allow a user to interactively explore temporal data, relative to the nested or crossed structures. It can help to discover differences between category levels, and uncover interesting periodic or aperiodic slices. The package implements a shared `tsibble` object that allows for linked brushing between coordinated views, and a shiny module that aids in wrapping timelines for seasonal patterns. The tools are demonstrated using two data examples: domestic tourism in Australia and pedestrian traffic in Melbourne.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-050</guid>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-050/figure/highlight-retail-1.png" medium="image" type="image/png" width="1248" height="691"/>
    </item>
    <item>
      <title>stratamatch: Prognostic Score Stratification Using a Pilot Design</title>
      <dc:creator>Rachael C. Aikens</dc:creator>
      <dc:creator>Joseph Rigdon</dc:creator>
      <dc:creator>Justin Lee</dc:creator>
      <dc:creator>Michael Baiocchi</dc:creator>
      <dc:creator>Andrew B. Goldstone</dc:creator>
      <dc:creator>Peter Chiu</dc:creator>
      <dc:creator>Y. Joseph Woo</dc:creator>
      <dc:creator>Jonathan H. Chen</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-063</link>
      <description>Optimal propensity score matching has emerged as one of the most ubiquitous approaches
 for causal inference studies on observational data. However, outstanding critiques of the statistical
 properties of propensity score matching have cast doubt on the statistical efficiency of this technique,
 and the poor scalability of optimal matching to large data sets makes this approach inconvenient
 if not infeasible for sample sizes that are increasingly commonplace in modern observational data.
 The stratamatch package provides implementation support and diagnostics for ‘stratified matching
 designs,’ an approach that addresses both of these issues with optimal propensity score matching for
 large-sample observational studies. First, stratifying the data enables more computationally efficient
 matching of large data sets. Second, stratamatch implements a ‘pilot design’ approach in order to
 stratify by a prognostic score, which may increase the precision of the effect estimate and increase
 power in sensitivity analyses of unmeasured confounding.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-063</guid>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>BayesSPsurv: An R Package to Estimate Bayesian (Spatial) Split-Population Survival Models</title>
      <dc:creator>Brandon Bolte</dc:creator>
      <dc:creator>Nicolás Schmidt</dc:creator>
      <dc:creator>Sergio Béjar</dc:creator>
      <dc:creator>Nguyen Huynh</dc:creator>
      <dc:creator>Bumba Mukherjee</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-068</link>
      <description>Survival data often include a fraction of units that are susceptible to an event of interest
 as well as a fraction of “immune” units. In many applications, spatial clustering in unobserved risk
 factors across nearby units can also affect their survival rates and odds of becoming immune. To
 address these methodological challenges, this article introduces our BayesSPsurv R-package, which
 fits parametric Bayesian Spatial split-population survival (cure) models that can account for spatial
 autocorrelation in both subpopulations of the user’s time-to-event data. Spatial autocorrelation is
 modeled with spatially weighted frailties, which are estimated using a conditionally autoregressive
 prior. The user can also fit parametric cure models with or without nonspatial i.i.d. frailties, and
 each model can incorporate time-varying covariates. BayesSPsurv also includes various functions to
 conduct pre-estimation spatial autocorrelation tests, visualize results, and assess model performance,
 all of which are illustrated using data on post-civil war peace survival.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-068</guid>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Reproducible Summary Tables with the gtsummary Package</title>
      <dc:creator>Daniel D. Sjoberg</dc:creator>
      <dc:creator>Karissa Whiting</dc:creator>
      <dc:creator>Michael Curry</dc:creator>
      <dc:creator>Jessica A. Lavery</dc:creator>
      <dc:creator>Joseph Larmarange</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-053</link>
      <description>The gtsummary package provides an elegant and flexible way to create publication-ready
 summary tables in R. A critical part of the work of statisticians, data scientists, and analysts is
 summarizing data sets and regression models in R and publishing or sharing polished summary tables.
 The gtsummary package was created to streamline these everyday analysis tasks by allowing users
 to easily create reproducible summaries of data sets, regression models, survey data, and survival
 data with a simple interface and very little code. The package follows a tidy framework, making it
 easy to integrate with standard data workflows, and offers many table customization features through
 function arguments, helper functions, and custom themes.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-053</guid>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Regularized Transformation Models: The tramnet Package</title>
      <dc:creator>Lucas Kook</dc:creator>
      <dc:creator>Torsten Hothorn</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-054</link>
      <description>The tramnet package implements regularized linear transformation models by combining the
 flexible class of transformation models from tram with constrained convex optimization implemented
 in CVXR. Regularized transformation models unify many existing and novel regularized regression
 models under one theoretical and computational framework. Regularization strategies implemented
 for transformation models in tramnet include the Lasso, ridge regression, and the elastic net and
 follow the parameterization in glmnet. Several functionalities for optimizing the hyperparameters,
 including model-based optimization based on the mlrMBO package, are implemented. A multitude
 of S3 methods is deployed for visualization, handling, and simulation purposes. This work aims at
 illustrating all facets of tramnet in realistic settings and comparing regularized transformation models
 with existing implementations of similar models.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-054</guid>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-054/feature.png" medium="image" type="image/png" width="1950" height="1200"/>
    </item>
    <item>
      <title>Automating Reproducible, Collaborative Clinical Trial Document Generation with the \pkg{listdown} Package</title>
      <dc:creator>Michael Kane</dc:creator>
      <dc:creator>Xun Jiang</dc:creator>
      <dc:creator>Simon Urbanek</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-051</link>
      <description>The conveyance of clinical trial explorations and analysis results from a statistician to a clinical investigator is a critical component of the drug development and clinical research cycle. Automating the process of generating documents for data descriptions, summaries, exploration, and analysis allows the statistician to provide a more comprehensive view of the information captured by a clinical trial, and efficient generation of these documents allows the statistican to focus more on the conceptual development of a trial or trial analysis and less on the implementation of the summaries and results on which decisions are made. This paper explores the use of the \pkg{listdown} package for automating reproducible documents in clinical trials that facilitate the collaboration between statisticians and clinicians as well as defining an analysis pipeline for document generation.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-051</guid>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-051/waterfall.png" medium="image" type="image/png" width="1260" height="800"/>
    </item>
    <item>
      <title>Towards a Grammar for Processing Clinical Trial Data</title>
      <dc:creator>Michael J. Kane</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-052</link>
      <description>The goal of this paper is to help define a path toward a grammar for processing clinical trials by a) defining a format in which we would like to represent data from standardized clinical trial data b) describing a standard set of operations to transform clinical trial data into this format, and c) to identify a set of verbs and other functionality to facilitate data processing and encourage reproducibility in the processing of these data. It provides a background on standard clinical trial data and goes through a simple preprocessing example illustrating the value of the proposed approach through the use of the \pkg{forceps} package, which is currently being used for data of this kind.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-052</guid>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff's Alpha Coefficient</title>
      <dc:creator>John Hughes</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-046</link>
      <description>R package krippendorffsalpha provides tools for measuring agreement using Krippendorff’s
 α coefficient, a well-known nonparametric measure of agreement (also called inter-rater reliability
 and various other names). This article first develops Krippendorff’s α in a natural way and situates
 α among statistical procedures. Then, the usage of package krippendorffsalpha is illustrated via
 analyses of two datasets, the latter of which was collected during an imaging study of hip cartilage.
 The package permits users to apply the α methodology using built-in distance functions for the
 nominal, ordinal, interval, or ratio levels of measurement. User-defined distance functions are also
 supported. The fitting function can accommodate any number of units, any number of coders, and
 missingness. Bootstrap inference is supported, and the bootstrap computation can be carried out in
 parallel.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-046</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-046/feature.png" medium="image" type="image/png" width="4263" height="3324"/>
    </item>
    <item>
      <title>Working with CRSP/COMPUSTAT in R: Reproducible Empirical Asset Pricing</title>
      <dc:creator>Majeed Simaan</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-047</link>
      <description>It is common to come across SAS or Stata manuals while working on academic empirical
 finance research. Nonetheless, given the popularity of open-source programming languages such as R,
 there are fewer resources in R covering popular databases such as CRSP and COMPUSTAT. The aim
 of this article is to bridge the gap and illustrate how to leverage R in working with both datasets. As
 an application, we illustrate how to form size-value portfolios with respect to Fama and French (1993)
 and study the sensitivity of the results with respect to different inputs. Ultimately, the purpose of the
 article is to advocate reproducible finance research and contribute to the recent idea of “Open Source
 Cross-Sectional Asset Pricing”, proposed by Chen and Zimmermann (2020).</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-047</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-047/feature.png" medium="image" type="image/png" width="1005" height="951"/>
    </item>
    <item>
      <title>Analyzing Dependence between Point Processes in Time Using IndTestPP</title>
      <dc:creator>Ana C. Cebrián</dc:creator>
      <dc:creator>Jesús Asín</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-049</link>
      <description>The need to analyze the dependence between two or more point processes in time appears in
 many modeling problems related to the occurrence of events, such as the occurrence of climate events
 at different spatial locations or synchrony detection in spike train analysis. The package IndTestPP
 provides a general framework for all the steps in this type of analysis, and one of its main features is the
 implementation of three families of tests to study independence given the intensities of the processes,
 which are not only useful to assess independence but also to identify factors causing dependence.
 The package also includes functions for generating different types of dependent point processes,
 and implements computational statistical inference tools using them. An application to characterize
 the dependence between the occurrence of extreme heat events in three Spanish locations using the
 package is shown.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-049</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-049/feature.png" medium="image" type="image/png" width="1813" height="1808"/>
    </item>
    <item>
      <title>distr6: R6 Object-Oriented Probability Distributions Interface in R</title>
      <dc:creator>Raphael Sonabend</dc:creator>
      <dc:creator>Franz J. Király</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-055</link>
      <description>distr6 is an object-oriented (OO) probability distributions interface leveraging the extensibil
ity and scalability of R6 and the speed and efficiency of Rcpp. Over 50 probability distributions are
 currently implemented in the package with ‘core’ methods, including density, distribution, and gener
ating functions, and more ‘exotic’ ones, including hazards and distribution function anti-derivatives.
 In addition to simple distributions, distr6 supports compositions such as truncation, mixtures, and
 product distributions. This paper presents the core functionality of the package and demonstrates
 examples for key use-cases. In addition, this paper provides a critical review of the object-oriented
 programming paradigms in R and describes some novel implementations for design patterns and core
 object-oriented features introduced by the package for supporting distr6 components.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-055</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>gofCopula: Goodness-of-Fit Tests for Copulae</title>
      <dc:creator>Ostap Okhrin</dc:creator>
      <dc:creator>Simon Trimborn</dc:creator>
      <dc:creator>Martin Waltz</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-060</link>
      <description>The last decades show an increased interest in modeling various types of data through
 copulae. Different copula models have been developed, which lead to the challenge of finding the
 best fitting model for a particular dataset. From the other side, a strand of literature developed a list
 of different Goodness-of-Fit (GoF) tests with different powers under different conditions. The usual
 practice is the selection of the best copula via the p-value of the GoF test. Although this method is not
 purely correct due to the fact that non-rejection does not imply acception, this strategy is favored by
 practitioners. Unfortunately, different GoF tests often provide contradicting outputs. The proposed
 R-package brings under one umbrella 13 most used copulae plus their rotated variants together
 with 16 GoF tests and a hybrid one. The package offers flexible margin modeling, automatized
 parallelization, parameter estimation, as well as a user-friendly interface, and pleasant visualizations
 of the results. To illustrate the functionality of the package, two exemplary applications are provided.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-060</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>ROCnReg: An R Package for Receiver Operating Characteristic Curve Inference With and Without Covariates</title>
      <dc:creator>María Xosé Rodríguez-Álvarez</dc:creator>
      <dc:creator>Vanda Inácio</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-066</link>
      <description>This paper introduces the package ROCnReg that allows estimating the pooled ROC
 curve, the covariate-specific ROC curve, and the covariate-adjusted ROC curve by different methods,
 both from (semi) parametric and nonparametric perspectives and within Bayesian and frequentist
 paradigms. From the estimated ROC curve (pooled, covariate-specific, or covariate-adjusted), several
 summary measures of discriminatory accuracy, such as the (partial) area under the ROC curve and the
 Youden index, can be obtained. The package also provides functions to obtain ROC-based optimal
 threshold values using several criteria, namely, the Youden index criterion and the criterion that
 sets a target value for the false positive fraction. For the Bayesian methods, we provide tools for
 assessing model fit via posterior predictive checks, while the model choice can be carried out via
 several information criteria. Numerical and graphical outputs are provided for all methods. This is
 the only package implementing Bayesian procedures for ROC curves.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-066</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>OneStep : Le Cam's One-step Estimation Procedure</title>
      <dc:creator>Alexandre Brouste</dc:creator>
      <dc:creator>Christophe Dutang</dc:creator>
      <dc:creator>Darel Noutsa Mieniedou</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-044</link>
      <description>The OneStep package proposes principally an eponymic function that numerically computes
 Le Cam’s one-step estimator, which is asymptotically efficient and can be computed faster than the
 maximum likelihood estimator for large datasets. Monte Carlo simulations are carried out for several
 examples (discrete and continuous probability distributions) in order to exhibit the performance of Le
 Cam’s one-step estimation procedure in terms of efficiency and computational cost on observation
 samples of finite size.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-044</guid>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-044/feature.png" medium="image" type="image/png" width="3000" height="2100"/>
    </item>
    <item>
      <title>The HBV.IANIGLA Hydrological Model</title>
      <dc:creator>Ezequiel Toum</dc:creator>
      <dc:creator>Mariano H. Masiokas</dc:creator>
      <dc:creator>Ricardo Villalba</dc:creator>
      <dc:creator>Pierre Pitte</dc:creator>
      <dc:creator>Lucas Ruiz</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-059</link>
      <description>Over the past 40 years, the HBV (Hydrologiska Byråns Vattenbalansavdelning) hydrological
 model has been one of the most used worldwide due to its robustness, simplicity, and reliable results.
 Despite these advantages, the available versions impose some limitations for research studies in
 mountain watersheds dominated by ice-snow melt runoff (i.e., no glacier module, a limited number of
 elevation bands, among other constraints). Here we present HBV.IANIGLA, a tool for hydroclimatic
 studies in regions with steep topography and/or cryospheric processes which provides a modular
 and extended implementation of the HBV model as an R package. To our knowledge, this is the first
 modular version of the original HBV model. This feature can be very useful for teaching hydrological
 modeling, as it offers the possibility to build a customized, open-source model that can be adjusted to
 different requirements of students and users.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-059</guid>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The R Package smicd: Statistical Methods for Interval-Censored Data</title>
      <dc:creator>Paul Walter</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-045</link>
      <description>The package allows the use of two new statistical methods for the analysis of interval
censored data: 1) direct estimation/prediction of statistical indicators and 2) linear (mixed) regression
 analysis. Direct estimation of statistical indicators, for instance, poverty and inequality indicators,
 is facilitated by a non parametric kernel density algorithm. The algorithm is able to account for
 weights in the estimation of statistical indicators. The standard errors of the statistical indicators are
 estimated with a non parametric bootstrap. Furthermore, the package offers statistical methods for
 the estimation of linear and linear mixed regression models with an interval-censored dependent
 variable, particularly random slope and random intercept models. Parameter estimates are obtained
 through a stochastic expectation-maximization algorithm. Standard errors are estimated using a
 non parametric bootstrap in the linear regression model and by a parametric bootstrap in the linear
 mixed regression model. To handle departures from the model assumptions, fixed (logarithmic) and
 data-driven (Box-Cox) transformations are incorporated into the algorithm.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-045</guid>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-045/feature.png" medium="image" type="image/png" width="3000" height="1800"/>
    </item>
    <item>
      <title>RLumCarlo: Simulating Cold Light using Monte Carlo Methods</title>
      <dc:creator>Sebastian Kreutzer</dc:creator>
      <dc:creator>Johannes Friedrich</dc:creator>
      <dc:creator>Vasilis Pagonis</dc:creator>
      <dc:creator>Christian Laag</dc:creator>
      <dc:creator>Ena Rajovic</dc:creator>
      <dc:creator>Christoph Schmidt</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-043</link>
      <description>The luminescence phenomena of insulators and semiconductors (e.g., natural minerals such
 as quartz) have various application domains. For instance, Earth Sciences and archaeology exploit
 luminescence as a dating method. Herein, we present the R package RLumCarlo implementing sets of
 luminescence models to be simulated with Monte Carlo (MC) methods. MC methods make a powerful
 ally to all kinds of simulation attempts involving stochastic processes. Luminescence production
 is such a stochastic process in the form of charge (electron-hole pairs) interaction within insulators
 and semiconductors. To simulate luminescence-signal curves, we distribute single and independent
 MC processes to virtual MC clusters. RLumCarlo comes with a modularized design and consistent
 user interface: (1) C++ functions represent the modeling core and implement models for specific
 stimulations modes. (2) R functions give access to combinations of models and stimulation modes,
 start the simulation and render terminal and graphical feedback. The combination of MC clusters
 supports the simulation of complex luminescence phenomena.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-043</guid>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-043/feature.png" medium="image" type="image/png" width="1302" height="1117"/>
    </item>
    <item>
      <title>pdynmc: A Package for Estimating Linear Dynamic Panel Data Models Based on Nonlinear Moment Conditions</title>
      <dc:creator>Markus Fritsch</dc:creator>
      <dc:creator>Andrew Adrian Yu Pua</dc:creator>
      <dc:creator>Joachim Schnurbus</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-035</link>
      <description>This paper introduces pdynmc, an R package that provides users sufficient flexibility
 and precise control over the estimation and inference in linear dynamic panel data models. The
 package primarily allows for the inclusion of nonlinear moment conditions and the use of iterated
 GMM; additionally, visualizations for data structure and estimation results are provided. The current
 implementation reflects recent developments in literature, uses sensible argument defaults, and
 aligns commercial and noncommercial estimation commands. Since the understanding of the model
 assumptions is vital for setting up plausible estimation routines, we provide a broad introduction
 of linear dynamic panel data models directed towards practitioners before concisely describing the
 functionality available in pdynmc regarding instrument type, covariate type, estimation methodology,
 and general configuration. We then demonstrate the functionality by revisiting the popular firm-level
 dataset of Arellano and Bond (1991).</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-035</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-035/feature.png" medium="image" type="image/png" width="2100" height="2100"/>
    </item>
    <item>
      <title>DChaos: An R Package for Chaotic Time Series Analysis</title>
      <dc:creator>Julio E. Sandubete</dc:creator>
      <dc:creator>Lorenzo Escot</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-036</link>
      <description>Chaos theory has been hailed as a revolution of thoughts and attracting ever-increasing
 attention of many scientists from diverse disciplines. Chaotic systems are non-linear deterministic
 dynamic systems which can behave like an erratic and apparently random motion. A relevant field
 inside chaos theory is the detection of chaotic behavior from empirical time-series data. One of the
 main features of chaos is the well-known initial-value sensitivity property. Methods and techniques
 related to testing the hypothesis of chaos try to quantify the initial-value sensitive property estimating
 the so-called Lyapunov exponents. This paper describes the main estimation methods of the Lyapunov
 exponent from time series data. At the same time, we present the DChaos library. R users may
 compute the delayed-coordinate embedding vector from time series data, estimates the best-fitted
 neural net model from the delayed-coordinate embedding vectors, calculates analytically the partial
 derivatives from the chosen neural nets model. They can also obtain the neural net estimator of the
 Lyapunov exponent from the partial derivatives computed previously by two different procedures
 and four ways of subsampling by blocks. To sum up, the DChaos package allows the R users to test
 robustly the hypothesis of chaos in order to know if the data-generating process behind time series
 behaves chaotically or not. The package’s functionality is illustrated by examples.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-036</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-036/feature.png" medium="image" type="image/png" width="828" height="513"/>
    </item>
    <item>
      <title>IndexNumber: An R Package for Measuring the Evolution of Magnitudes</title>
      <dc:creator>Alejandro Saavedra-Nieves</dc:creator>
      <dc:creator>Paula Saavedra-Nieves</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-038</link>
      <description>Index numbers are descriptive statistical measures useful in economic settings for comparing
 simple and complex magnitudes registered, usually in two time periods. Although this theory has
 a large history, it still plays an important role in modern today’s societies where big amounts of
 economic data are available and need to be analyzed. After a detailed revision on classical index
 numbers in literature, this paper is focused on the description of the R package IndexNumber with
 strong capabilities for calculating them. Two of the four real data sets contained in this library
 are used for illustrating the determination of the index numbers in this work. Graphical tools are
 also implemented in order to show the time evolution of considered magnitudes simplifying the
 interpretation of the results.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-038</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>ROBustness In Network (robin): an R Package for Comparison and Validation of Communities </title>
      <dc:creator>Valeria Policastro</dc:creator>
      <dc:creator>Dario Righelli</dc:creator>
      <dc:creator>Annamaria Carissimo</dc:creator>
      <dc:creator>Luisa Cutillo</dc:creator>
      <dc:creator>Italia De Feis</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-040</link>
      <description>In network analysis, many community detection algorithms have been developed. However,
 their implementation leaves unaddressed the question of the statistical validation of the results. Here,
 we present robin (ROBustness In Network), an R package to assess the robustness of the community
 structure of a network found by one or more methods to give indications about their reliability. The
 procedure initially detects if the community structure found by a set of algorithms is statistically
 significant and then compares two selected detection algorithms on the same graph to choose the
 one that better fits the network of interest. We demonstrate the use of our package on the American
 College Football benchmark dataset.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-040</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-040/feature.png" medium="image" type="image/png" width="1280" height="720"/>
    </item>
    <item>
      <title>Finding Optimal Normalizing Transformations via \pkg{bestNormalize}</title>
      <dc:creator>Ryan A. Peterson</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-041</link>
      <description>The \pkg{bestNormalize} R package was designed to help users find a transformation that can effectively normalize a vector regardless of its actual distribution. Each of the many normalization techniques that have been developed has its own strengths and weaknesses, and deciding which to use until data are fully observed is difficult or impossible. This package facilitates choosing between a range of possible transformations and will automatically return  the best one, i.e., the one that makes data look the *most* normal. To evaluate and compare  the normalization efficacy across a suite of possible transformations, we developed a  statistic based on a goodness of fit test divided by its degrees of freedom.  Transformations can be seamlessly trained and applied to newly observed data  and can be implemented in conjunction with \pkg{caret} and \pkg{recipes} for  data preprocessing in machine learning workflows. Custom transformations and  normalization statistics are supported.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-041</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-041/feature.png" medium="image" type="image/png" width="2033" height="1442"/>
    </item>
    <item>
      <title>Package wsbackfit for Smooth Backfitting Estimation of Generalized Structured Models</title>
      <dc:creator>Javier Roca-Pardiñas</dc:creator>
      <dc:creator>María Xosé Rodríguez-Álvarez</dc:creator>
      <dc:creator>Stefan Sperlich</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-042</link>
      <description>A package is introduced that provides the weighted smooth backfitting estimator for
a large family of popular semiparametric regression models. This family is known as generalized
structured models, comprising, for example, generalized varying coefficient model, generalized
additive models, mixtures, potentially including parametric parts. The kernel-based weighted
smooth backfitting belongs to the statistically most efficient procedures for this model class. Its
asymptotic properties are well-understood thanks to the large body of literature about this estimator.
The introduced weights allow for the inclusion of sampling weights, trimming, and efficient estimation
under heteroscedasticity. Further options facilitate easy handling of aggregated data, prediction,
and the presentation of estimation results. Cross-validation methods are provided which can be used
for model and bandwidth selection.1</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-042</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-042/feature.png" medium="image" type="image/png" width="2400" height="1800"/>
    </item>
    <item>
      <title>garchx: Flexible and Robust GARCH-X Modeling</title>
      <dc:creator>Genaro Sucarrat</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-057</link>
      <description>The garchx package provides a user-friendly, fast, flexible, and robust framework for the
 estimation and inference of GARCH(p, q, r)-X models, where p is the ARCH order, q is the GARCH
 order, r is the asymmetry or leverage order, and ’X’ indicates that covariates can be included. Quasi
 Maximum Likelihood (QML) methods ensure estimates are consistent and standard errors valid,
 even when the standardized innovations are non-normal or dependent, or both. Zero-coefficient
 restrictions by omission enable parsimonious specifications, and functions to facilitate the non-standard
 inference associated with zero-restrictions in the null-hypothesis are provided. Finally, in the formal
 comparisons of precision and speed, the garchx package performs well relative to other prominent
 GARCH-packages on CRAN.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-057</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Statistical Quality Control with the qcr Package</title>
      <dc:creator>Miguel Flores</dc:creator>
      <dc:creator>Rubén Fernández-Casal</dc:creator>
      <dc:creator>Salvador Naya</dc:creator>
      <dc:creator>Javier Tarrío-Saavedra</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-034</link>
      <description>The R package qcr for Statistical Quality Control (SQC) is introduced and described. It
     includes a comprehensive set of univariate and multivariate SQC tools that completes and increases
     the SQC techniques available in R. Apart from integrating different R packages devoted to SQC (qcc,
     MSQC), qcr provides nonparametric tools that are highly useful when Gaussian assumption is not
     met. This package computes standard univariate control charts for individual measurements, x̄, S, R,
     p, np, c, u, EWMA, and CUSUM. In addition, it includes functions to perform multivariate control
     charts such as Hotelling T2 , MEWMA and MCUSUM. As representative features, multivariate
     nonparametric alternatives based on data depth are implemented in this package: r, Q and S control
     charts. The qcr library also estimates the most complete set of capability indices from first to
     the fourth generation, covering the nonparametric alternatives, and performing the corresponding
     capability analysis graphical outputs, including the process capability plots. Moreover, Phase I and
     II control charts for functional data are included.

    Prácticas de CEC con R</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-034</guid>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-034/feature.png" medium="image" type="image/png" width="2479" height="3508"/>
    </item>
    <item>
      <title>Linear Regression with Stationary Errors: the R Package slm</title>
      <dc:creator>Emmanuel Caron</dc:creator>
      <dc:creator>Jérôme Dedecker</dc:creator>
      <dc:creator>Bertrand Michel</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-030</link>
      <description>This paper introduces the R package slm, which stands for Stationary Linear Models.
The package contains a set of statistical procedures for linear regression in the general context where
the error process is strictly stationary with a short memory. We work in the setting of Hannan (1973),
who proved the asymptotic normality of the (normalized) least squares estimators (LSE) under
very mild conditions on the error process. We propose different ways to estimate the asymptotic
covariance matrix of the LSE and then to correct the type I error rates of the usual tests on the
parameters (as well as confidence intervals). The procedures are evaluated through different sets of
simulations.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-030</guid>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-030/feature.png" medium="image" type="image/png" width="581" height="657"/>
    </item>
    <item>
      <title>clustcurv: An R Package for Determining Groups in Multiple Curves </title>
      <dc:creator>Nora M. Villanueva</dc:creator>
      <dc:creator>Marta Sestelo</dc:creator>
      <dc:creator>Luis Meira-Machado</dc:creator>
      <dc:creator>Javier Roca-Pardiñas</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-032</link>
      <description>In many situations, it could be interesting to ascertain whether groups of curves can be
 performed, especially when confronted with a considerable number of curves. This paper introduces
 an R package, known as clustcurv, for determining clusters of curves with an automatic selection of
 their number. The package can be used for determining groups in multiple survival curves as well as
 for multiple regression curves. Moreover, it can be used with large numbers of curves. An illustration
 of the use of clustcurv is provided, using both real data examples and artificial data.
 Keywords: multiple curves, number of groups, nonparametric, survival analysis, regression models,
 cluster</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-032</guid>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-032/feature.png" medium="image" type="image/png" width="2100" height="1200"/>
    </item>
    <item>
      <title>Benchmarking R packages for Calculation of Persistent Homology</title>
      <dc:creator>Eashwar V. Somasundaram</dc:creator>
      <dc:creator>Shael E. Brown</dc:creator>
      <dc:creator>Adam Litzler</dc:creator>
      <dc:creator>Jacob G. Scott</dc:creator>
      <dc:creator>Raoul R. Wadhwa</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-033</link>
      <description>Several persistent homology software libraries have been implemented in R. Specifically,
 the Dionysus, GUDHI, and Ripser libraries have been wrapped by the TDA and TDAstats CRAN
 packages. These software represent powerful analysis tools that are computationally expensive and, to
 our knowledge, have not been formally benchmarked. Here, we analyze runtime and memory growth
 for the 2 R packages and the 3 underlying libraries. We find that datasets with less than 3 dimensions
 can be evaluated with persistent homology fastest by the GUDHI library in the TDA package. For
 higher-dimensional datasets, the Ripser library in the TDAstats package is the fastest. Ripser and
 TDAstats are also the most memory-efficient tools to calculate persistent homology.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-033</guid>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-033/feature.png" medium="image" type="image/png" width="2700" height="1800"/>
    </item>
    <item>
      <title>Wide-to-tall Data Reshaping Using Regular Expressions and the nc Package</title>
      <dc:creator>Toby Dylan Hocking</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-029</link>
      <description>Regular expressions are powerful tools for extracting tables from non-tabular text data.
 Capturing regular expressions that describe the information to extract from column names can be
 especially useful when reshaping a data table from wide (few rows with many regularly named
 columns) to tall (fewer columns with more rows). We present the R package nc (short for named
 capture), which provides functions for wide-to-tall data reshaping using regular expressions. We
 describe the main new ideas of nc, and provide detailed comparisons with related R packages (stats,
 utils, data.table, tidyr, tidyfast, tidyfst, reshape2, cdata).</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-029</guid>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-029/feature.png" medium="image" type="image/png" width="700" height="300"/>
    </item>
    <item>
      <title>Unidimensional and Multidimensional Methods for Recurrence Quantification Analysis with crqa</title>
      <dc:creator>Moreno I. Coco</dc:creator>
      <dc:creator>Dan Mønster</dc:creator>
      <dc:creator>Giuseppe Leonardi</dc:creator>
      <dc:creator>Rick Dale</dc:creator>
      <dc:creator>Sebastian Wallot</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-062</link>
      <description>Recurrence quantification analysis is a widely used method for characterizing patterns in
 time series. This article presents a comprehensive survey for conducting a wide range of recurrence
based analyses to quantify the dynamical structure of single and multivariate time series and capture
 coupling properties underlying leader-follower relationships. The basics of recurrence quantification
 analysis (RQA) and all its variants are formally introduced step-by-step from the simplest auto
recurrence to the most advanced multivariate case. Importantly, we show how such RQA methods can
 be deployed under a single computational framework in R using a substantially renewed version of
 our crqa 2.0 package. This package includes implementations of several recent advances in recurrence
based analysis, among them applications to multivariate data and improved entropy calculations
 for categorical data. We show concrete applications of our package to example data, together with a
 detailed description of its functions and some guidelines on their usage.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-062</guid>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The bdpar Package: Big Data Pipelining Architecture for R</title>
      <dc:creator>Miguel Ferreiro-Díaz</dc:creator>
      <dc:creator>Tomás R. Cotos-Yáñez</dc:creator>
      <dc:creator>José R. Méndez</dc:creator>
      <dc:creator>David Ruano-Ordás</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-065</link>
      <description>In the last years, big data has become a useful paradigm for taking advantage of multiple
 sources to find relevant knowledge in real domains (such as the design of personalized marketing
 campaigns or helping to palliate the effects of several fatal diseases). Big data programming tools and
 methods have evolved over time from a MapReduce to a pipeline-based archetype. Concretely the use
 of pipelining schemes has become the most reliable way of processing and analyzing large amounts of
 data. To this end, this work introduces bdpar, a new highly customizable pipeline-based framework
 (using the OOP paradigm provided by R6 package) able to execute multiple preprocessing tasks over
 heterogeneous data sources. Moreover, to increase the flexibility and performance, bdpar provides
 helpful features such as (i) the definition of a novel object-based pipe operator (%&gt;|%), (ii) the ability to
 easily design and deploy new (and customized) input data parsers, tasks, and pipelines, (iii) only-once
 execution which avoids the execution of previously processed information (instances), guaranteeing
 that only new both input data and pipelines are executed, (iv) the capability to perform serial or
 parallel operations according to the user needs, (v) the inclusion of a debugging mechanism which
 allows users to check the status of each instance (and find possible errors) throughout the process.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-065</guid>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>JMcmprsk: An R Package for Joint Modelling of Longitudinal and Survival Data with Competing Risks</title>
      <dc:creator>Hong Wang</dc:creator>
      <dc:creator>Ning Li</dc:creator>
      <dc:creator>Shanpeng Li</dc:creator>
      <dc:creator>Gang Li</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-028</link>
      <description>In this paper, we describe an R package named JMcmprsk, for joint modelling of longitudinal
 and survival data with competing risks. The package in its current version implements two joint
 models of longitudinal and survival data proposed to handle competing risks survival data together
 with continuous and ordinal longitudinal outcomes respectively (Elashoff et al., 2008; Li et al., 2010).
 The corresponding R implementations are further illustrated with real examples. The package also
 provides simulation functions to simulate datasets for joint modelling with continuous or ordinal
 outcomes under the competing risks scenario, which provide useful tools to validate and evaluate
 new joint modelling methods.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-028</guid>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-028/feature.png" medium="image" type="image/png" width="555" height="394"/>
    </item>
    <item>
      <title>exPrior: An R Package for the Formulation of Ex-Situ Priors</title>
      <dc:creator>Falk Heße</dc:creator>
      <dc:creator>Karina Cucchi</dc:creator>
      <dc:creator>Nura Kawa</dc:creator>
      <dc:creator>Yoram Rubin</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-031</link>
      <description>The exPrior package implements a procedure for formulating informative priors of geo
statistical properties for a target field site, called ex-situ priors and introduced in Cucchi et al. (2019).
 The procedure uses a Bayesian hierarchical model to assimilate multiple types of data coming from
 multiple sites considered as similar to the target site. This prior summarizes the information contained
 in the data in the form of a probability density function that can be used to better inform further
 geostatistical investigations at the site. The formulation of the prior uses ex-situ data, where the data
 set can either be gathered by the user or come in the form of a structured database. The package is
 designed to be flexible in that regard. For illustration purposes and for easiness of use, the package is
 ready to be used with the worldwide hydrogeological parameter database (WWHYPDA) Comunian
 and Renard (2009).</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-031</guid>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-031/feature.png" medium="image" type="image/png" width="3508" height="2480"/>
    </item>
    <item>
      <title>penPHcure: Variable Selection in Proportional Hazards Cure Model with Time-Varying Covariates</title>
      <dc:creator>Alessandro Beretta</dc:creator>
      <dc:creator>Cédric Heuchenne</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-061</link>
      <description>We describe the penPHcure R package, which implements the semiparametric proportional
hazards (PH) cure model of Sy and Taylor (2000) extended to time-varying covariates and the variable
 selection technique based on its SCAD-penalized likelihood proposed by Beretta and Heuchenne
 (2019a). In survival analysis, cure models are a useful tool when a fraction of the population is likely to
 be immune from the event of interest. They can separate the effects of certain factors on the probability
 of being susceptible and on the time until the occurrence of the event. Moreover, the penPHcure
 package allows the user to simulate data from a PH cure model, where the event-times are generated
 on a continuous scale from a piecewise exponential distribution conditional on time-varying covariates,
 with a method similar to Hendry (2014). We present the results of a simulation study to assess the
 finite sample performance of the methodology and illustrate the functionalities of the penPHcure
 package using criminal recidivism data.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-061</guid>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A Method for Deriving Information from Running R Code</title>
      <dc:creator>Mark P.J. van der Loo</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-056</link>
      <description>It is often useful to tap information from a running R script. Obvious use cases include
 monitoring the consumption of resources (time, memory) and logging. Perhaps less obvious cases
 include tracking changes in R objects or collecting the output of unit tests. In this paper, we demonstrate
 an approach that abstracts the collection and processing of such secondary information from the
 running R script. Our approach is based on a combination of three elements. The first element is
 to build a customized way to evaluate code. The second is labeled local masking and it involves
 temporarily masking a user-facing function so an alternative version of it is called. The third element
 we label local side effect. This refers to the fact that the masking function exports information to the
 secondary information flow without altering a global state. The result is a method for building systems
 in pure R that lets users create and control secondary flows of information with minimal impact on
 their workflow and no global side effects.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-056</guid>
      <pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>npcure: An R Package for Nonparametric Inference in Mixture Cure Models</title>
      <dc:creator>Ana López-Cheda</dc:creator>
      <dc:creator>M. Amalia Jácome</dc:creator>
      <dc:creator>Ignacio López-de-Ullibarri</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-027</link>
      <description>Mixture cure models have been widely used to analyze survival data with a cure fraction.
 They assume that a subgroup of the individuals under study will never experience the event (cured
 subjects). So, the goal is twofold: to study both the cure probability and the failure time of the
 uncured individuals through a proper survival function (latency). The R package npcure implements a
 completely nonparametric approach for estimating these functions in mixture cure models, considering
 right-censored survival times. Nonparametric estimators for the cure probability and the latency as
 functions of a covariate are provided. Bootstrap bandwidth selectors for the estimators are included.
 The package also implements a nonparametric covariate significance test for the cure probability,
 which can be applied with a continuous, discrete, or qualitative covariate.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-027</guid>
      <pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-027/feature.png" medium="image" type="image/png" width="1800" height="1800"/>
    </item>
    <item>
      <title>SEEDCCA: An Integrated R-Package for Canonical Correlation Analysis and Partial Least Squares</title>
      <dc:creator>Bo-Young Kim</dc:creator>
      <dc:creator>Yunju Im</dc:creator>
      <dc:creator>Jae Keun Yoo</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-026</link>
      <description>Canonical correlation analysis (CCA) has a long history as an explanatory statistical method
 in high-dimensional data analysis and has been successfully applied in many scientific fields such as
 chemometrics, pattern recognition, genomic sequence analysis, and so on. The so-called seedCCA is a
 newly developed R package that implements not only the standard and seeded CCA but also partial
 least squares. The package enables us to fit CCA to large-p and small-n data. The paper provides a
 complete guide. Also, the seeded CCA application results are compared with the regularized CCA in
 the existing R package. It is believed that the package, along with the paper, will contribute to high
dimensional data analysis in various science field practitioners and that the statistical methodologies
 in multivariate analysis become more fruitful.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-026</guid>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-026/feature.png" medium="image" type="image/png" width="1408" height="1404"/>
    </item>
  </channel>
</rss>
