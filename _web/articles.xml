<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>The R Journal</title>
    <link>https://rjournal-distill.netlify.app/</link>
    <atom:link href="https://rjournal-distill.netlify.app/articles.xml" rel="self" type="application/rss+xml"/>
    <description>Articles published in the R Journal</description>
    <image>
      <title>The R Journal</title>
      <url>https://rjournal-distill.netlify.app/resources/favicon.ico</url>
      <link>https://rjournal-distill.netlify.app/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Sun, 12 Dec 2021 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Rejoinder: Software Engineering and R Programming</title>
      <dc:creator>Melina Vidoni</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-112</link>
      <description>It is a pleasure to take part in such fruitful discussion about the relationship between Software Engineering and R programming, and what could be gain by allowing each to look more closely at the other. Several discussants make valuable arguments that ought to be further discussed.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-112</guid>
      <pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The R Quest: from Users to Developers</title>
      <dc:creator>Simon Urbanek</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-111</link>
      <description>R is not a programming language, and this produces the inherent dichotomy between analytics and software engineering. With the emergence of data science, the opportunity exists to bridge this gap, especially through teaching practices.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-111</guid>
      <pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A Unifying Framework for Parallel and Distributed Processing in R using Futures</title>
      <dc:creator>Henrik Bengtsson</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-048</link>
      <description>A future is a programming construct designed for concurrent and asynchronous evaluation of code, making it particularly useful for parallel processing. The future package implements the Future API for programming with futures in R. This minimal API provides sufficient constructs for implementing parallel versions of well-established, high-level map-reduce APIs. The future ecosystem supports exception handling, output and condition relaying, parallel random number generation, and automatic identification of globals lowering the threshold to parallelize code. The Future API bridges parallel frontends with parallel backends, following the philosophy that end-users are the ones who choose the parallel backend while the developer focuses on what to parallelize. A variety of backends exist, and third-party contributions meeting the specifications, which ensure that the same code works on all backends, are automatically supported. The future framework solves several problems not addressed by other parallel frameworks in R.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-048</guid>
      <pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-048/preview.png" medium="image" type="image/png" width="438" height="372"/>
    </item>
    <item>
      <title>We Need Trustworthy R Packages</title>
      <dc:creator>William Michael Landau</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-109</link>
      <description>There is a need for rigorous software engineering in R packages, and there is a need for new research to bridge scientific computing with more traditional computing. Automated tools, interdisciplinary graduate courses, code reviews, and a welcoming developer community will continue to democratize best practices. Democratized software engineering will improve the quality, correctness, and integrity of scientific software, and by extension, the disciplines that rely on it.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-109</guid>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The bdpar Package: Big Data Pipelining Architecture for R</title>
      <dc:creator>Miguel Ferreiro-Díaz</dc:creator>
      <dc:creator>Tomás R. Cotos-Yáñez</dc:creator>
      <dc:creator>José R. Méndez</dc:creator>
      <dc:creator>David Ruano-Ordás</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-065</link>
      <description>In the last years, big data has become a useful paradigm for taking advantage of multiple
 sources to find relevant knowledge in real domains (such as the design of personalized marketing
 campaigns or helping to palliate the effects of several fatal diseases). Big data programming tools and
 methods have evolved over time from a MapReduce to a pipeline-based archetype. Concretely the use
 of pipelining schemes has become the most reliable way of processing and analyzing large amounts of
 data. To this end, this work introduces bdpar, a new highly customizable pipeline-based framework
 (using the OOP paradigm provided by R6 package) able to execute multiple preprocessing tasks over
 heterogeneous data sources. Moreover, to increase the flexibility and performance, bdpar provides
 helpful features such as (i) the definition of a novel object-based pipe operator (%&gt;|%), (ii) the ability to
 easily design and deploy new (and customized) input data parsers, tasks, and pipelines, (iii) only-once
 execution which avoids the execution of previously processed information (instances), guaranteeing
 that only new both input data and pipelines are executed, (iv) the capability to perform serial or
 parallel operations according to the user needs, (v) the inclusion of a debugging mechanism which
 allows users to check the status of each instance (and find possible errors) throughout the process.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-065</guid>
      <pubDate>Fri, 16 Jul 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>g2f as a Novel Tool to Find and Fill Gaps in Metabolic Networks</title>
      <dc:creator>Daniel Osorio</dc:creator>
      <dc:creator>Kelly Botero</dc:creator>
      <dc:creator>Andrés Pinzón Velasco</dc:creator>
      <dc:creator>Nicolás Mendoza-Mejía</dc:creator>
      <dc:creator>Felipe Rojas-            Rodríguez</dc:creator>
      <dc:creator>George Barreto</dc:creator>
      <dc:creator>Janneth González</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-064</link>
      <description>During the building of a genome-scale metabolic model, there are several dead-end metabo lites and substrates which cannot be imported, produced, nor used by any reaction incorporated in the network. The presence of these dead-end metabolites can block out the net flux of the objective function when it is evaluated through Flux Balance Analysis (FBA), and when it is not blocked, bias in the biological conclusions increase. In this aspect, the refinement to restore the connectivity of the network can be carried out manually or using computational algorithms. The g2f package was designed as a tool to find the gaps from dead-end metabolites and fill them from the stoichiometric reactions of a reference, filtering candidate reactions using a weighting function. Additionally, this algorithm allows downloading all the sets of gene-associated stoichiometric reactions for a specific organism from the KEGG database. Our package is compatible with both 4.0.0 and 3.6.0 R versions.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-064</guid>
      <pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>ROCnReg: An R Package for Receiver Operating Characteristic Curve Inference With and Without Covariates</title>
      <dc:creator>María Xosé Rodríguez-Álvarez</dc:creator>
      <dc:creator>Vanda Inácio</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-066</link>
      <description>This paper introduces the package ROCnReg that allows estimating the pooled ROC
 curve, the covariate-specific ROC curve, and the covariate-adjusted ROC curve by different methods,
 both from (semi) parametric and nonparametric perspectives and within Bayesian and frequentist
 paradigms. From the estimated ROC curve (pooled, covariate-specific, or covariate-adjusted), several
 summary measures of discriminatory accuracy, such as the (partial) area under the ROC curve and the
 Youden index, can be obtained. The package also provides functions to obtain ROC-based optimal
 threshold values using several criteria, namely, the Youden index criterion and the criterion that
 sets a target value for the false positive fraction. For the Bayesian methods, we provide tools for
 assessing model fit via posterior predictive checks, while the model choice can be carried out via
 several information criteria. Numerical and graphical outputs are provided for all methods. This is
 the only package implementing Bayesian procedures for ROC curves.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-066</guid>
      <pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A New Versatile Discrete Distribution</title>
      <dc:creator>Rolf Turner</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-067</link>
      <description>This paper introduces a new flexible distribution for discrete data. Approximate moment estimators of the parameters of the distribution, to be used as starting values for numerical opti mization procedures, are discussed. “Exact” moment estimation, effected via a numerical procedure, and maximum likelihood estimation, are considered. The quality of the results produced by these estimators is assessed via simulation experiments. Several examples are given of fitting instances of the new distribution to real and simulated data. It is noted that the new distribution is a member of the exponential family. Expressions for the gradient and Hessian of the log-likelihood of the new distribution are derived. The former facilitates the numerical maximization of the likelihood with optim(); the latter provides means of calculating or estimating the covariance matrix of of the parame ter estimates. A discrepancy between estimates of the covariance matrix obtained by inverting the Hessian and those obtained by Monte Carlo methods is discussed.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-067</guid>
      <pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-067/preview.png" medium="image" type="image/png" width="504" height="432"/>
    </item>
    <item>
      <title>BayesSPsurv: An R Package to Estimate Bayesian (Spatial) Split-Population Survival Models</title>
      <dc:creator>Brandon Bolte</dc:creator>
      <dc:creator>Nicolás Schmidt</dc:creator>
      <dc:creator>Sergio Béjar</dc:creator>
      <dc:creator>Nguyen Huynh</dc:creator>
      <dc:creator>Bumba Mukherjee</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-068</link>
      <description>Survival data often include a fraction of units that are susceptible to an event of interest
 as well as a fraction of “immune” units. In many applications, spatial clustering in unobserved risk
 factors across nearby units can also affect their survival rates and odds of becoming immune. To
 address these methodological challenges, this article introduces our BayesSPsurv R-package, which
 fits parametric Bayesian Spatial split-population survival (cure) models that can account for spatial
 autocorrelation in both subpopulations of the user’s time-to-event data. Spatial autocorrelation is
 modeled with spatially weighted frailties, which are estimated using a conditionally autoregressive
 prior. The user can also fit parametric cure models with or without nonspatial i.i.d. frailties, and
 each model can incorporate time-varying covariates. BayesSPsurv also includes various functions to
 conduct pre-estimation spatial autocorrelation tests, visualize results, and assess model performance,
 all of which are illustrated using data on post-civil war peace survival.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-068</guid>
      <pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A Method for Deriving Information from Running R Code</title>
      <dc:creator>Mark P.J. van der Loo</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-056</link>
      <description>It is often useful to tap information from a running R script. Obvious use cases include
 monitoring the consumption of resources (time, memory) and logging. Perhaps less obvious cases
 include tracking changes in R objects or collecting the output of unit tests. In this paper, we demonstrate
 an approach that abstracts the collection and processing of such secondary information from the
 running R script. Our approach is based on a combination of three elements. The first element is
 to build a customized way to evaluate code. The second is labeled local masking and it involves
 temporarily masking a user-facing function so an alternative version of it is called. The third element
 we label local side effect. This refers to the fact that the masking function exports information to the
 secondary information flow without altering a global state. The result is a method for building systems
 in pure R that lets users create and control secondary flows of information with minimal impact on
 their workflow and no global side effects.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-056</guid>
      <pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>RobustBF: An R Package for Robust Solution to the Behrens-Fisher Problem</title>
      <dc:creator>Gamze Güven</dc:creator>
      <dc:creator>Şükrü Acıtaş</dc:creator>
      <dc:creator>Hatice Şamkar</dc:creator>
      <dc:creator>Birdal Şenoğlu</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-107</link>
      <description>Welch’s two-sample t-test based on least squares (LS) estimators is generally used to test the equality of two normal means when the variances are not equal. However, this test loses its power when the underlying distribution is not normal. In this paper, two different tests are proposed to test the equality of two long-tailed symmetric (LTS) means under heterogeneous variances. Adaptive modified maximum likelihood (AMML) estimators are used in developing the proposed tests since they are highly efficient under LTS distribution. An R package called RobustBF is given to show the implementation of these tests. Simulated Type I error rates and powers of the proposed tests are also given and compared with Welch’s t-test based on LS estimators via an extensive Monte Carlo simulation study.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-107</guid>
      <pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Reproducible Summary Tables with the gtsummary Package</title>
      <dc:creator>Daniel D. Sjoberg</dc:creator>
      <dc:creator>Karissa Whiting</dc:creator>
      <dc:creator>Michael Curry</dc:creator>
      <dc:creator>Jessica A. Lavery</dc:creator>
      <dc:creator>Joseph Larmarange</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-053</link>
      <description>The gtsummary package provides an elegant and flexible way to create publication-ready
 summary tables in R. A critical part of the work of statisticians, data scientists, and analysts is
 summarizing data sets and regression models in R and publishing or sharing polished summary tables.
 The gtsummary package was created to streamline these everyday analysis tasks by allowing users
 to easily create reproducible summaries of data sets, regression models, survey data, and survival
 data with a simple interface and very little code. The package follows a tidy framework, making it
 easy to integrate with standard data workflows, and offers many table customization features through
 function arguments, helper functions, and custom themes.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-053</guid>
      <pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>garchx: Flexible and Robust GARCH-X Modeling</title>
      <dc:creator>Genaro Sucarrat</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-057</link>
      <description>The garchx package provides a user-friendly, fast, flexible, and robust framework for the
 estimation and inference of GARCH(p, q, r)-X models, where p is the ARCH order, q is the GARCH
 order, r is the asymmetry or leverage order, and ’X’ indicates that covariates can be included. Quasi
 Maximum Likelihood (QML) methods ensure estimates are consistent and standard errors valid,
 even when the standardized innovations are non-normal or dependent, or both. Zero-coefficient
 restrictions by omission enable parsimonious specifications, and functions to facilitate the non-standard
 inference associated with zero-restrictions in the null-hypothesis are provided. Finally, in the formal
 comparisons of precision and speed, the garchx package performs well relative to other prominent
 GARCH-packages on CRAN.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-057</guid>
      <pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>gofCopula: Goodness-of-Fit Tests for Copulae</title>
      <dc:creator>Ostap Okhrin</dc:creator>
      <dc:creator>Simon Trimborn</dc:creator>
      <dc:creator>Martin Waltz</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-060</link>
      <description>The last decades show an increased interest in modeling various types of data through
 copulae. Different copula models have been developed, which lead to the challenge of finding the
 best fitting model for a particular dataset. From the other side, a strand of literature developed a list
 of different Goodness-of-Fit (GoF) tests with different powers under different conditions. The usual
 practice is the selection of the best copula via the p-value of the GoF test. Although this method is not
 purely correct due to the fact that non-rejection does not imply acception, this strategy is favored by
 practitioners. Unfortunately, different GoF tests often provide contradicting outputs. The proposed
 R-package brings under one umbrella 13 most used copulae plus their rotated variants together
 with 16 GoF tests and a hybrid one. The package offers flexible margin modeling, automatized
 parallelization, parameter estimation, as well as a user-friendly interface, and pleasant visualizations
 of the results. To illustrate the functionality of the package, two exemplary applications are provided.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-060</guid>
      <pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The HBV.IANIGLA Hydrological Model</title>
      <dc:creator>Ezequiel Toum</dc:creator>
      <dc:creator>Mariano H. Masiokas</dc:creator>
      <dc:creator>Ricardo Villalba</dc:creator>
      <dc:creator>Pierre Pitte</dc:creator>
      <dc:creator>Lucas Ruiz</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-059</link>
      <description>Over the past 40 years, the HBV (Hydrologiska Byråns Vattenbalansavdelning) hydrological
 model has been one of the most used worldwide due to its robustness, simplicity, and reliable results.
 Despite these advantages, the available versions impose some limitations for research studies in
 mountain watersheds dominated by ice-snow melt runoff (i.e., no glacier module, a limited number of
 elevation bands, among other constraints). Here we present HBV.IANIGLA, a tool for hydroclimatic
 studies in regions with steep topography and/or cryospheric processes which provides a modular
 and extended implementation of the HBV model as an R package. To our knowledge, this is the first
 modular version of the original HBV model. This feature can be very useful for teaching hydrological
 modeling, as it offers the possibility to build a customized, open-source model that can be adjusted to
 different requirements of students and users.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-059</guid>
      <pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>penPHcure: Variable Selection in Proportional Hazards Cure Model with Time-Varying Covariates</title>
      <dc:creator>Alessandro Beretta</dc:creator>
      <dc:creator>Cédric Heuchenne</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-061</link>
      <description>We describe the penPHcure R package, which implements the semiparametric proportional
hazards (PH) cure model of Sy and Taylor (2000) extended to time-varying covariates and the variable
 selection technique based on its SCAD-penalized likelihood proposed by Beretta and Heuchenne
 (2019a). In survival analysis, cure models are a useful tool when a fraction of the population is likely to
 be immune from the event of interest. They can separate the effects of certain factors on the probability
 of being susceptible and on the time until the occurrence of the event. Moreover, the penPHcure
 package allows the user to simulate data from a PH cure model, where the event-times are generated
 on a continuous scale from a piecewise exponential distribution conditional on time-varying covariates,
 with a method similar to Hendry (2014). We present the results of a simulation study to assess the
 finite sample performance of the methodology and illustrate the functionalities of the penPHcure
 package using criminal recidivism data.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-061</guid>
      <pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Unidimensional and Multidimensional Methods for Recurrence Quantification Analysis with crqa</title>
      <dc:creator>Moreno I. Coco</dc:creator>
      <dc:creator>Dan Mønster</dc:creator>
      <dc:creator>Giuseppe Leonardi</dc:creator>
      <dc:creator>Rick Dale</dc:creator>
      <dc:creator>Sebastian Wallot</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-062</link>
      <description>Recurrence quantification analysis is a widely used method for characterizing patterns in
 time series. This article presents a comprehensive survey for conducting a wide range of recurrence
based analyses to quantify the dynamical structure of single and multivariate time series and capture
 coupling properties underlying leader-follower relationships. The basics of recurrence quantification
 analysis (RQA) and all its variants are formally introduced step-by-step from the simplest auto
recurrence to the most advanced multivariate case. Importantly, we show how such RQA methods can
 be deployed under a single computational framework in R using a substantially renewed version of
 our crqa 2.0 package. This package includes implementations of several recent advances in recurrence
based analysis, among them applications to multivariate data and improved entropy calculations
 for categorical data. We show concrete applications of our package to example data, together with a
 detailed description of its functions and some guidelines on their usage.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-062</guid>
      <pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>stratamatch: Prognostic Score Stratification Using a Pilot Design</title>
      <dc:creator>Rachael C. Aikens</dc:creator>
      <dc:creator>Joseph Rigdon</dc:creator>
      <dc:creator>Justin Lee</dc:creator>
      <dc:creator>Michael Baiocchi</dc:creator>
      <dc:creator>Andrew B. Goldstone</dc:creator>
      <dc:creator>Peter Chiu</dc:creator>
      <dc:creator>Y. Joseph Woo</dc:creator>
      <dc:creator>Jonathan H. Chen</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-063</link>
      <description>Optimal propensity score matching has emerged as one of the most ubiquitous approaches
 for causal inference studies on observational data. However, outstanding critiques of the statistical
 properties of propensity score matching have cast doubt on the statistical efficiency of this technique,
 and the poor scalability of optimal matching to large data sets makes this approach inconvenient
 if not infeasible for sample sizes that are increasingly commonplace in modern observational data.
 The stratamatch package provides implementation support and diagnostics for ‘stratified matching
 designs,’ an approach that addresses both of these issues with optimal propensity score matching for
 large-sample observational studies. First, stratifying the data enables more computationally efficient
 matching of large data sets. Second, stratamatch implements a ‘pilot design’ approach in order to
 stratify by a prognostic score, which may increase the precision of the effect estimate and increase
 power in sensitivity analyses of unmeasured confounding.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-063</guid>
      <pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>distr6: R6 Object-Oriented Probability Distributions Interface in R</title>
      <dc:creator>Raphael Sonabend</dc:creator>
      <dc:creator>Franz J. Király</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-055</link>
      <description>distr6 is an object-oriented (OO) probability distributions interface leveraging the extensibil
ity and scalability of R6 and the speed and efficiency of Rcpp. Over 50 probability distributions are
 currently implemented in the package with ‘core’ methods, including density, distribution, and gener
ating functions, and more ‘exotic’ ones, including hazards and distribution function anti-derivatives.
 In addition to simple distributions, distr6 supports compositions such as truncation, mixtures, and
 product distributions. This paper presents the core functionality of the package and demonstrates
 examples for key use-cases. In addition, this paper provides a critical review of the object-oriented
 programming paradigms in R and describes some novel implementations for design patterns and core
 object-oriented features introduced by the package for supporting distr6 components.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-055</guid>
      <pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>OneStep : Le Cam's One-step Estimation Procedure</title>
      <dc:creator>Alexandre Brouste</dc:creator>
      <dc:creator>Christophe Dutang</dc:creator>
      <dc:creator>Darel Noutsa Mieniedou</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-044</link>
      <description>The OneStep package proposes principally an eponymic function that numerically computes
 Le Cam’s one-step estimator, which is asymptotically efficient and can be computed faster than the
 maximum likelihood estimator for large datasets. Monte Carlo simulations are carried out for several
 examples (discrete and continuous probability distributions) in order to exhibit the performance of Le
 Cam’s one-step estimation procedure in terms of efficiency and computational cost on observation
 samples of finite size.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-044</guid>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-044/feature.png" medium="image" type="image/png" width="3000" height="2100"/>
    </item>
    <item>
      <title>The R Package smicd: Statistical Methods for Interval-Censored Data</title>
      <dc:creator>Paul Walter</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-045</link>
      <description>The package allows the use of two new statistical methods for the analysis of interval
censored data: 1) direct estimation/prediction of statistical indicators and 2) linear (mixed) regression
 analysis. Direct estimation of statistical indicators, for instance, poverty and inequality indicators,
 is facilitated by a non parametric kernel density algorithm. The algorithm is able to account for
 weights in the estimation of statistical indicators. The standard errors of the statistical indicators are
 estimated with a non parametric bootstrap. Furthermore, the package offers statistical methods for
 the estimation of linear and linear mixed regression models with an interval-censored dependent
 variable, particularly random slope and random intercept models. Parameter estimates are obtained
 through a stochastic expectation-maximization algorithm. Standard errors are estimated using a
 non parametric bootstrap in the linear regression model and by a parametric bootstrap in the linear
 mixed regression model. To handle departures from the model assumptions, fixed (logarithmic) and
 data-driven (Box-Cox) transformations are incorporated into the algorithm.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-045</guid>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-045/feature.png" medium="image" type="image/png" width="3000" height="1800"/>
    </item>
    <item>
      <title>krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff's Alpha Coefficient</title>
      <dc:creator>John Hughes</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-046</link>
      <description>R package krippendorffsalpha provides tools for measuring agreement using Krippendorff’s
 α coefficient, a well-known nonparametric measure of agreement (also called inter-rater reliability
 and various other names). This article first develops Krippendorff’s α in a natural way and situates
 α among statistical procedures. Then, the usage of package krippendorffsalpha is illustrated via
 analyses of two datasets, the latter of which was collected during an imaging study of hip cartilage.
 The package permits users to apply the α methodology using built-in distance functions for the
 nominal, ordinal, interval, or ratio levels of measurement. User-defined distance functions are also
 supported. The fitting function can accommodate any number of units, any number of coders, and
 missingness. Bootstrap inference is supported, and the bootstrap computation can be carried out in
 parallel.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-046</guid>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-046/feature.png" medium="image" type="image/png" width="4263" height="3324"/>
    </item>
    <item>
      <title>Working with CRSP/COMPUSTAT in R: Reproducible Empirical Asset Pricing</title>
      <dc:creator>Majeed Simaan</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-047</link>
      <description>It is common to come across SAS or Stata manuals while working on academic empirical
 finance research. Nonetheless, given the popularity of open-source programming languages such as R,
 there are fewer resources in R covering popular databases such as CRSP and COMPUSTAT. The aim
 of this article is to bridge the gap and illustrate how to leverage R in working with both datasets. As
 an application, we illustrate how to form size-value portfolios with respect to Fama and French (1993)
 and study the sensitivity of the results with respect to different inputs. Ultimately, the purpose of the
 article is to advocate reproducible finance research and contribute to the recent idea of “Open Source
 Cross-Sectional Asset Pricing”, proposed by Chen and Zimmermann (2020).</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-047</guid>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-047/feature.png" medium="image" type="image/png" width="1005" height="951"/>
    </item>
    <item>
      <title>Analyzing Dependence between Point Processes in Time Using IndTestPP</title>
      <dc:creator>Ana C. Cebrián</dc:creator>
      <dc:creator>Jesús Asín</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-049</link>
      <description>The need to analyze the dependence between two or more point processes in time appears in
 many modeling problems related to the occurrence of events, such as the occurrence of climate events
 at different spatial locations or synchrony detection in spike train analysis. The package IndTestPP
 provides a general framework for all the steps in this type of analysis, and one of its main features is the
 implementation of three families of tests to study independence given the intensities of the processes,
 which are not only useful to assess independence but also to identify factors causing dependence.
 The package also includes functions for generating different types of dependent point processes,
 and implements computational statistical inference tools using them. An application to characterize
 the dependence between the occurrence of extreme heat events in three Spanish locations using the
 package is shown.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-049</guid>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-049/feature.png" medium="image" type="image/png" width="1813" height="1808"/>
    </item>
    <item>
      <title>Conversations in Time: Interactive Visualization to Explore Structured Temporal Data</title>
      <dc:creator>Earo Wang</dc:creator>
      <dc:creator>Dianne Cook</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-050</link>
      <description>Temporal data often has a hierarchical structure, defined by categorical variables describing different levels, such as political regions or sales products. The nesting of categorical variables produces a hierarchical structure. The tsibbletalk package is developed to allow a user to interactively explore temporal data, relative to the nested or crossed structures. It can help to discover differences between category levels, and uncover interesting periodic or aperiodic slices. The package implements a shared `tsibble` object that allows for linked brushing between coordinated views, and a shiny module that aids in wrapping timelines for seasonal patterns. The tools are demonstrated using two data examples: domestic tourism in Australia and pedestrian traffic in Melbourne.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-050</guid>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-050/figure/highlight-retail-1.png" medium="image" type="image/png" width="1248" height="691"/>
    </item>
    <item>
      <title>Automating Reproducible, Collaborative Clinical Trial Document Generation with the listdown Package</title>
      <dc:creator>Michael Kane</dc:creator>
      <dc:creator>Xun Jiang</dc:creator>
      <dc:creator>Simon Urbanek</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-051</link>
      <description>The conveyance of clinical trial explorations and analysis results from a statistician to a clinical investigator is a critical component of the drug development and clinical research cycle. Automating the process of generating documents for data descriptions, summaries, exploration, and analysis allows the statistician to provide a more comprehensive view of the information captured by a clinical trial, and efficient generation of these documents allows the statistican to focus more on the conceptual development of a trial or trial analysis and less on the implementation of the summaries and results on which decisions are made. This paper explores the use of the listdown package for automating reproducible documents in clinical trials that facilitate the collaboration between statisticians and clinicians as well as defining an analysis pipeline for document generation.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-051</guid>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-051/waterfall.png" medium="image" type="image/png" width="1260" height="800"/>
    </item>
    <item>
      <title>Towards a Grammar for Processing Clinical Trial Data</title>
      <dc:creator>Michael J. Kane</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-052</link>
      <description>The goal of this paper is to help define a path toward a grammar for processing clinical trials by a) defining a format in which we would like to represent data from standardized clinical trial data b) describing a standard set of operations to transform clinical trial data into this format, and c) to identify a set of verbs and other functionality to facilitate data processing and encourage reproducibility in the processing of these data. It provides a background on standard clinical trial data and goes through a simple preprocessing example illustrating the value of the proposed approach through the use of the forceps package, which is currently being used for data of this kind.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-052</guid>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Regularized Transformation Models: The tramnet Package</title>
      <dc:creator>Lucas Kook</dc:creator>
      <dc:creator>Torsten Hothorn</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-054</link>
      <description>The tramnet package implements regularized linear transformation models by combining the
 flexible class of transformation models from tram with constrained convex optimization implemented
 in CVXR. Regularized transformation models unify many existing and novel regularized regression
 models under one theoretical and computational framework. Regularization strategies implemented
 for transformation models in tramnet include the Lasso, ridge regression, and the elastic net and
 follow the parameterization in glmnet. Several functionalities for optimizing the hyperparameters,
 including model-based optimization based on the mlrMBO package, are implemented. A multitude
 of S3 methods is deployed for visualization, handling, and simulation purposes. This work aims at
 illustrating all facets of tramnet in realistic settings and comparing regularized transformation models
 with existing implementations of similar models.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-054</guid>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-054/feature.png" medium="image" type="image/png" width="1950" height="1200"/>
    </item>
    <item>
      <title>SEEDCCA: An Integrated R-Package for Canonical Correlation Analysis and Partial Least Squares</title>
      <dc:creator>Bo-Young Kim</dc:creator>
      <dc:creator>Yunju Im</dc:creator>
      <dc:creator>Jae Keun Yoo</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-026</link>
      <description>Canonical correlation analysis (CCA) has a long history as an explanatory statistical method
 in high-dimensional data analysis and has been successfully applied in many scientific fields such as
 chemometrics, pattern recognition, genomic sequence analysis, and so on. The so-called seedCCA is a
 newly developed R package that implements not only the standard and seeded CCA but also partial
 least squares. The package enables us to fit CCA to large-p and small-n data. The paper provides a
 complete guide. Also, the seeded CCA application results are compared with the regularized CCA in
 the existing R package. It is believed that the package, along with the paper, will contribute to high
dimensional data analysis in various science field practitioners and that the statistical methodologies
 in multivariate analysis become more fruitful.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-026</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-026/feature.png" medium="image" type="image/png" width="1408" height="1404"/>
    </item>
    <item>
      <title>npcure: An R Package for Nonparametric Inference in Mixture Cure Models</title>
      <dc:creator>Ana López-Cheda</dc:creator>
      <dc:creator>M. Amalia Jácome</dc:creator>
      <dc:creator>Ignacio López-de-Ullibarri</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-027</link>
      <description>Mixture cure models have been widely used to analyze survival data with a cure fraction.
 They assume that a subgroup of the individuals under study will never experience the event (cured
 subjects). So, the goal is twofold: to study both the cure probability and the failure time of the
 uncured individuals through a proper survival function (latency). The R package npcure implements a
 completely nonparametric approach for estimating these functions in mixture cure models, considering
 right-censored survival times. Nonparametric estimators for the cure probability and the latency as
 functions of a covariate are provided. Bootstrap bandwidth selectors for the estimators are included.
 The package also implements a nonparametric covariate significance test for the cure probability,
 which can be applied with a continuous, discrete, or qualitative covariate.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-027</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-027/feature.png" medium="image" type="image/png" width="1800" height="1800"/>
    </item>
    <item>
      <title>JMcmprsk: An R Package for Joint Modelling of Longitudinal and Survival Data with Competing Risks</title>
      <dc:creator>Hong Wang</dc:creator>
      <dc:creator>Ning Li</dc:creator>
      <dc:creator>Shanpeng Li</dc:creator>
      <dc:creator>Gang Li</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-028</link>
      <description>In this paper, we describe an R package named JMcmprsk, for joint modelling of longitudinal
 and survival data with competing risks. The package in its current version implements two joint
 models of longitudinal and survival data proposed to handle competing risks survival data together
 with continuous and ordinal longitudinal outcomes respectively (Elashoff et al., 2008; Li et al., 2010).
 The corresponding R implementations are further illustrated with real examples. The package also
 provides simulation functions to simulate datasets for joint modelling with continuous or ordinal
 outcomes under the competing risks scenario, which provide useful tools to validate and evaluate
 new joint modelling methods.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-028</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-028/feature.png" medium="image" type="image/png" width="555" height="394"/>
    </item>
    <item>
      <title>Wide-to-tall Data Reshaping Using Regular Expressions and the nc Package</title>
      <dc:creator>Toby Dylan Hocking</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-029</link>
      <description>Regular expressions are powerful tools for extracting tables from non-tabular text data.
 Capturing regular expressions that describe the information to extract from column names can be
 especially useful when reshaping a data table from wide (few rows with many regularly named
 columns) to tall (fewer columns with more rows). We present the R package nc (short for named
 capture), which provides functions for wide-to-tall data reshaping using regular expressions. We
 describe the main new ideas of nc, and provide detailed comparisons with related R packages (stats,
 utils, data.table, tidyr, tidyfast, tidyfst, reshape2, cdata).</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-029</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-029/feature.png" medium="image" type="image/png" width="700" height="300"/>
    </item>
    <item>
      <title>Linear Regression with Stationary Errors: the R Package slm</title>
      <dc:creator>Emmanuel Caron</dc:creator>
      <dc:creator>Jérôme Dedecker</dc:creator>
      <dc:creator>Bertrand Michel</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-030</link>
      <description>This paper introduces the R package slm, which stands for Stationary Linear Models.
The package contains a set of statistical procedures for linear regression in the general context where
the error process is strictly stationary with a short memory. We work in the setting of Hannan (1973),
who proved the asymptotic normality of the (normalized) least squares estimators (LSE) under
very mild conditions on the error process. We propose different ways to estimate the asymptotic
covariance matrix of the LSE and then to correct the type I error rates of the usual tests on the
parameters (as well as confidence intervals). The procedures are evaluated through different sets of
simulations.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-030</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-030/feature.png" medium="image" type="image/png" width="581" height="657"/>
    </item>
    <item>
      <title>exPrior: An R Package for the Formulation of Ex-Situ Priors</title>
      <dc:creator>Falk Heße</dc:creator>
      <dc:creator>Karina Cucchi</dc:creator>
      <dc:creator>Nura Kawa</dc:creator>
      <dc:creator>Yoram Rubin</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-031</link>
      <description>The exPrior package implements a procedure for formulating informative priors of geo
statistical properties for a target field site, called ex-situ priors and introduced in Cucchi et al. (2019).
 The procedure uses a Bayesian hierarchical model to assimilate multiple types of data coming from
 multiple sites considered as similar to the target site. This prior summarizes the information contained
 in the data in the form of a probability density function that can be used to better inform further
 geostatistical investigations at the site. The formulation of the prior uses ex-situ data, where the data
 set can either be gathered by the user or come in the form of a structured database. The package is
 designed to be flexible in that regard. For illustration purposes and for easiness of use, the package is
 ready to be used with the worldwide hydrogeological parameter database (WWHYPDA) Comunian
 and Renard (2009).</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-031</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-031/feature.png" medium="image" type="image/png" width="3508" height="2480"/>
    </item>
    <item>
      <title>clustcurv: An R Package for Determining Groups in Multiple Curves </title>
      <dc:creator>Nora M. Villanueva</dc:creator>
      <dc:creator>Marta Sestelo</dc:creator>
      <dc:creator>Luis Meira-Machado</dc:creator>
      <dc:creator>Javier Roca-Pardiñas</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-032</link>
      <description>In many situations, it could be interesting to ascertain whether groups of curves can be
 performed, especially when confronted with a considerable number of curves. This paper introduces
 an R package, known as clustcurv, for determining clusters of curves with an automatic selection of
 their number. The package can be used for determining groups in multiple survival curves as well as
 for multiple regression curves. Moreover, it can be used with large numbers of curves. An illustration
 of the use of clustcurv is provided, using both real data examples and artificial data.
 Keywords: multiple curves, number of groups, nonparametric, survival analysis, regression models,
 cluster</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-032</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-032/feature.png" medium="image" type="image/png" width="2100" height="1200"/>
    </item>
    <item>
      <title>Benchmarking R packages for Calculation of Persistent Homology</title>
      <dc:creator>Eashwar V. Somasundaram</dc:creator>
      <dc:creator>Shael E. Brown</dc:creator>
      <dc:creator>Adam Litzler</dc:creator>
      <dc:creator>Jacob G. Scott</dc:creator>
      <dc:creator>Raoul R. Wadhwa</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-033</link>
      <description>Several persistent homology software libraries have been implemented in R. Specifically,
 the Dionysus, GUDHI, and Ripser libraries have been wrapped by the TDA and TDAstats CRAN
 packages. These software represent powerful analysis tools that are computationally expensive and, to
 our knowledge, have not been formally benchmarked. Here, we analyze runtime and memory growth
 for the 2 R packages and the 3 underlying libraries. We find that datasets with less than 3 dimensions
 can be evaluated with persistent homology fastest by the GUDHI library in the TDA package. For
 higher-dimensional datasets, the Ripser library in the TDAstats package is the fastest. Ripser and
 TDAstats are also the most memory-efficient tools to calculate persistent homology.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-033</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-033/feature.png" medium="image" type="image/png" width="2700" height="1800"/>
    </item>
    <item>
      <title>Statistical Quality Control with the qcr Package</title>
      <dc:creator>Miguel Flores</dc:creator>
      <dc:creator>Rubén Fernández-Casal</dc:creator>
      <dc:creator>Salvador Naya</dc:creator>
      <dc:creator>Javier Tarrío-Saavedra</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-034</link>
      <description>The R package qcr for Statistical Quality Control (SQC) is introduced and described. It
     includes a comprehensive set of univariate and multivariate SQC tools that completes and increases
     the SQC techniques available in R. Apart from integrating different R packages devoted to SQC (qcc,
     MSQC), qcr provides nonparametric tools that are highly useful when Gaussian assumption is not
     met. This package computes standard univariate control charts for individual measurements, x̄, S, R,
     p, np, c, u, EWMA, and CUSUM. In addition, it includes functions to perform multivariate control
     charts such as Hotelling T2 , MEWMA and MCUSUM. As representative features, multivariate
     nonparametric alternatives based on data depth are implemented in this package: r, Q and S control
     charts. The qcr library also estimates the most complete set of capability indices from first to
     the fourth generation, covering the nonparametric alternatives, and performing the corresponding
     capability analysis graphical outputs, including the process capability plots. Moreover, Phase I and
     II control charts for functional data are included.

    Prácticas de CEC con R</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-034</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-034/feature.png" medium="image" type="image/png" width="2479" height="3508"/>
    </item>
    <item>
      <title>pdynmc: A Package for Estimating Linear Dynamic Panel Data Models Based on Nonlinear Moment Conditions</title>
      <dc:creator>Markus Fritsch</dc:creator>
      <dc:creator>Andrew Adrian Yu Pua</dc:creator>
      <dc:creator>Joachim Schnurbus</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-035</link>
      <description>This paper introduces pdynmc, an R package that provides users sufficient flexibility
 and precise control over the estimation and inference in linear dynamic panel data models. The
 package primarily allows for the inclusion of nonlinear moment conditions and the use of iterated
 GMM; additionally, visualizations for data structure and estimation results are provided. The current
 implementation reflects recent developments in literature, uses sensible argument defaults, and
 aligns commercial and noncommercial estimation commands. Since the understanding of the model
 assumptions is vital for setting up plausible estimation routines, we provide a broad introduction
 of linear dynamic panel data models directed towards practitioners before concisely describing the
 functionality available in pdynmc regarding instrument type, covariate type, estimation methodology,
 and general configuration. We then demonstrate the functionality by revisiting the popular firm-level
 dataset of Arellano and Bond (1991).</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-035</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-035/feature.png" medium="image" type="image/png" width="2100" height="2100"/>
    </item>
    <item>
      <title>DChaos: An R Package for Chaotic Time Series Analysis</title>
      <dc:creator>Julio E. Sandubete</dc:creator>
      <dc:creator>Lorenzo Escot</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-036</link>
      <description>Chaos theory has been hailed as a revolution of thoughts and attracting ever-increasing
 attention of many scientists from diverse disciplines. Chaotic systems are non-linear deterministic
 dynamic systems which can behave like an erratic and apparently random motion. A relevant field
 inside chaos theory is the detection of chaotic behavior from empirical time-series data. One of the
 main features of chaos is the well-known initial-value sensitivity property. Methods and techniques
 related to testing the hypothesis of chaos try to quantify the initial-value sensitive property estimating
 the so-called Lyapunov exponents. This paper describes the main estimation methods of the Lyapunov
 exponent from time series data. At the same time, we present the DChaos library. R users may
 compute the delayed-coordinate embedding vector from time series data, estimates the best-fitted
 neural net model from the delayed-coordinate embedding vectors, calculates analytically the partial
 derivatives from the chosen neural nets model. They can also obtain the neural net estimator of the
 Lyapunov exponent from the partial derivatives computed previously by two different procedures
 and four ways of subsampling by blocks. To sum up, the DChaos package allows the R users to test
 robustly the hypothesis of chaos in order to know if the data-generating process behind time series
 behaves chaotically or not. The package’s functionality is illustrated by examples.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-036</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-036/feature.png" medium="image" type="image/png" width="828" height="513"/>
    </item>
    <item>
      <title>IndexNumber: An R Package for Measuring the Evolution of Magnitudes</title>
      <dc:creator>Alejandro Saavedra-Nieves</dc:creator>
      <dc:creator>Paula Saavedra-Nieves</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-038</link>
      <description>Index numbers are descriptive statistical measures useful in economic settings for comparing
 simple and complex magnitudes registered, usually in two time periods. Although this theory has
 a large history, it still plays an important role in modern today’s societies where big amounts of
 economic data are available and need to be analyzed. After a detailed revision on classical index
 numbers in literature, this paper is focused on the description of the R package IndexNumber with
 strong capabilities for calculating them. Two of the four real data sets contained in this library
 are used for illustrating the determination of the index numbers in this work. Graphical tools are
 also implemented in order to show the time evolution of considered magnitudes simplifying the
 interpretation of the results.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-038</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>StratigrapheR: Concepts for Litholog Generation in R </title>
      <dc:creator>Sébastien Wouters</dc:creator>
      <dc:creator>Anne-Christine Da Silva</dc:creator>
      <dc:creator>Frédéric Boulvain</dc:creator>
      <dc:creator>Xavier Devleeschouwer</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-039</link>
      <description>The StratigrapheR package proposes new concepts for the generation of lithological logs, or lithologs, in R. The generation of lithologs in a scripting environment opens new opportunities for the processing and analysis of stratified geological data. Among the new concepts presented: new plotting and data processing methodologies, new general R functions, and computer-oriented data conventions are provided. The package structure allows for these new concepts to be further improved, which can be done independently by any R user. The current limitations of the package are highlighted, along with the limitations in R for geological data processing, to help identify the best paths for improvements.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-039</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-039/preview.png" medium="image" type="image/png" width="720" height="720"/>
    </item>
    <item>
      <title>ROBustness In Network (robin): an R Package for Comparison and Validation of Communities </title>
      <dc:creator>Valeria Policastro</dc:creator>
      <dc:creator>Dario Righelli</dc:creator>
      <dc:creator>Annamaria Carissimo</dc:creator>
      <dc:creator>Luisa Cutillo</dc:creator>
      <dc:creator>Italia De Feis</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-040</link>
      <description>In network analysis, many community detection algorithms have been developed. However,
 their implementation leaves unaddressed the question of the statistical validation of the results. Here,
 we present robin (ROBustness In Network), an R package to assess the robustness of the community
 structure of a network found by one or more methods to give indications about their reliability. The
 procedure initially detects if the community structure found by a set of algorithms is statistically
 significant and then compares two selected detection algorithms on the same graph to choose the
 one that better fits the network of interest. We demonstrate the use of our package on the American
 College Football benchmark dataset.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-040</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-040/feature.png" medium="image" type="image/png" width="1280" height="720"/>
    </item>
    <item>
      <title>Finding Optimal Normalizing Transformations via bestNormalize</title>
      <dc:creator>Ryan A. Peterson</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-041</link>
      <description>The bestNormalize R package was designed to help users find a transformation that can effectively normalize a vector regardless of its actual distribution. Each of the many normalization techniques that have been developed has its own strengths and weaknesses, and deciding which to use until data are fully observed is difficult or impossible. This package facilitates choosing between a range of possible transformations and will automatically return  the best one, i.e., the one that makes data look the *most* normal. To evaluate and compare  the normalization efficacy across a suite of possible transformations, we developed a  statistic based on a goodness of fit test divided by its degrees of freedom.  Transformations can be seamlessly trained and applied to newly observed data  and can be implemented in conjunction with caret and recipes for  data preprocessing in machine learning workflows. Custom transformations and  normalization statistics are supported.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-041</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-041/feature.png" medium="image" type="image/png" width="2033" height="1442"/>
    </item>
    <item>
      <title>Package wsbackfit for Smooth Backfitting Estimation of Generalized Structured Models</title>
      <dc:creator>Javier Roca-Pardiñas</dc:creator>
      <dc:creator>María Xosé Rodríguez-Álvarez</dc:creator>
      <dc:creator>Stefan Sperlich</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-042</link>
      <description>A package is introduced that provides the weighted smooth backfitting estimator for
a large family of popular semiparametric regression models. This family is known as generalized
structured models, comprising, for example, generalized varying coefficient model, generalized
additive models, mixtures, potentially including parametric parts. The kernel-based weighted
smooth backfitting belongs to the statistically most efficient procedures for this model class. Its
asymptotic properties are well-understood thanks to the large body of literature about this estimator.
The introduced weights allow for the inclusion of sampling weights, trimming, and efficient estimation
under heteroscedasticity. Further options facilitate easy handling of aggregated data, prediction,
and the presentation of estimation results. Cross-validation methods are provided which can be used
for model and bandwidth selection.1</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-042</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-042/feature.png" medium="image" type="image/png" width="2400" height="1800"/>
    </item>
    <item>
      <title>RLumCarlo: Simulating Cold Light using Monte Carlo Methods</title>
      <dc:creator>Sebastian Kreutzer</dc:creator>
      <dc:creator>Johannes Friedrich</dc:creator>
      <dc:creator>Vasilis Pagonis</dc:creator>
      <dc:creator>Christian Laag</dc:creator>
      <dc:creator>Ena Rajovic</dc:creator>
      <dc:creator>Christoph Schmidt</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-043</link>
      <description>The luminescence phenomena of insulators and semiconductors (e.g., natural minerals such
 as quartz) have various application domains. For instance, Earth Sciences and archaeology exploit
 luminescence as a dating method. Herein, we present the R package RLumCarlo implementing sets of
 luminescence models to be simulated with Monte Carlo (MC) methods. MC methods make a powerful
 ally to all kinds of simulation attempts involving stochastic processes. Luminescence production
 is such a stochastic process in the form of charge (electron-hole pairs) interaction within insulators
 and semiconductors. To simulate luminescence-signal curves, we distribute single and independent
 MC processes to virtual MC clusters. RLumCarlo comes with a modularized design and consistent
 user interface: (1) C++ functions represent the modeling core and implement models for specific
 stimulations modes. (2) R functions give access to combinations of models and stimulation modes,
 start the simulation and render terminal and graphical feedback. The combination of MC clusters
 supports the simulation of complex luminescence phenomena.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-043</guid>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-043/feature.png" medium="image" type="image/png" width="1302" height="1117"/>
    </item>
    <item>
      <title>RPESE: Risk and Performance Estimators Standard Errors with Serially Dependent Data</title>
      <dc:creator>Anthony-Alexander Christidis</dc:creator>
      <dc:creator>R. Douglas Martin</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-106</link>
      <description>The R package RPESE (Risk and Performance Estimators Standard Errors) implements a new method for computing accurate standard errors of risk and performance estimators when returns are serially dependent. The new method makes use of the representation of a risk or performance estimator as a summation of a time series of influence-function (IF) transformed returns, and computes estimator standard errors using a sophisticated method of estimating the spectral density at frequency zero of the time series of IF-transformed returns. Two additional packages used by RPESE are introduced, namely RPEIF which computes and provides graphical displays of the IF of risk and performance estimators, and RPEGLMEN which implements a regularized Gamma generalized linear model polynomial fit to the periodogram of the time series of the IF-transformed returns. A Monte Carlo study shows that the new method provides more accurate estimates of standard errors for risk and performance estimators compared to well-known alternative methods in the presence of serial correlation.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-106</guid>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-106/preview.png" medium="image" type="image/png" width="2331" height="1287"/>
    </item>
    <item>
      <title>Software Engineering and R Programming: A Call for Research</title>
      <dc:creator>Melina Vidoni</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-108</link>
      <description>Although R programming has been a part of research since its origins in the 1990s, few studies address scientific software development from a Software Engineering (SE) perspective. The past few years have seen unparalleled growth in the R community, and it is time to push the boundaries of SE research and R programming forwards. This paper discusses relevant studies that close this gap Additionally, it proposes a set of good practices derived from those findings aiming to act as a call-to-arms for both the R and RSE (Research SE) community to explore specific, interdisciplinary paths of research.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-108</guid>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The vote Package: Single Transferable Vote and Other Electoral Systems in R</title>
      <dc:creator>Adrian E. Raftery</dc:creator>
      <dc:creator>Hana Ševčíková</dc:creator>
      <dc:creator>Bernard W. Silverman</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-086</link>
      <description>We describe the vote package in R, which implements the plurality (or first-past-the-post), two-round runoff, score, approval, and Single Transferable Vote (STV) electoral systems, as well as methods for selecting the Condorcet winner and loser. We emphasize the STV system, which we have found to work well in practice for multi-winner elections with small electorates, such as committee and council elections, and the selection of multiple job candidates. For single-winner elections, STV is also called Instant Runoff Voting (IRV), Ranked Choice Voting (RCV), or the alternative vote (AV) system. The package also implements the STV system with equal preferences, for the first time in a software package, to our knowledge. It also implements a new variant of STV, in which a minimum number of candidates from a specified group are required to be elected. We illustrate the package with several real examples.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-086</guid>
      <pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-086/preview.png" medium="image" type="image/png" width="504" height="504"/>
    </item>
    <item>
      <title>volesti: Volume Approximation and Sampling for Convex Polytopes in R</title>
      <dc:creator>Apostolos Chalkis</dc:creator>
      <dc:creator>Vissarion Fisikopoulos</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-077</link>
      <description>Sampling from high-dimensional distributions and volume approximation of convex bodies are fundamental operations that appear in optimization, finance, engineering, artificial intelligence, and machine learning. In this paper, we present volesti, an R package that provides efficient, scalable algorithms for volume estimation, uniform, and Gaussian sampling from convex polytopes. volesti scales to hundreds of dimensions, handles efficiently three different types of polyhedra and pro vides non existing sampling routines to R. We demonstrate the power of volesti by solving several challenging problems using the R language.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-077</guid>
      <pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-077/preview.png" medium="image" type="image/png" width="595" height="842"/>
    </item>
    <item>
      <title>Elliptical Symmetry Tests in R</title>
      <dc:creator>Slad̄ana Babić</dc:creator>
      <dc:creator>Christophe Ley</dc:creator>
      <dc:creator>Marko Palangetić</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-078</link>
      <description>The assumption of elliptical symmetry has an important role in many theoretical develop ments and applications. Hence, it is of primary importance to be able to test whether that assumption actually holds true or not. Various tests have been proposed in the literature for this problem. To the best of our knowledge, none of them has been implemented in R. This article describes the R package ellipticalsymmetry which implements several well-known tests for elliptical symmetry together with some recent tests. We demonstrate the testing procedures with a real data example.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-078</guid>
      <pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Visual Diagnostics for Constrained Optimisation with Application to Guided Tours</title>
      <dc:creator>H. Sherry Zhang</dc:creator>
      <dc:creator>Dianne Cook</dc:creator>
      <dc:creator>Ursula Laa</dc:creator>
      <dc:creator>Nicolas Langrené</dc:creator>
      <dc:creator>Patricia Menéndez</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-105</link>
      <description>A guided tour helps to visualise high-dimensional data by showing low-dimensional projections along a projection pursuit optimisation path. Projection pursuit is a generalisation of principal component analysis in the sense that different indexes are used to define the interestingness of the projected data. While much work has been done in developing new indexes in the literature, less has been done on understanding the optimisation. Index functions can be noisy, might have multiple local maxima as well as an optimal maximum, and are constrained to generate orthonormal projection frames, which complicates the optimization. In addition, projection pursuit is primarily used for exploratory data analysis, and finding the local maxima is also useful. The guided tour is especially useful for exploration because it conducts geodesic interpolation connecting steps in the optimisation and shows how the projected data changes as a maxima is approached. This work provides new visual diagnostics for examining a choice of optimisation procedure based on the provision of a new data object which collects information throughout the optimisation. It has helped to diagnose and fix several problems with projection pursuit guided tour. This work might be useful more broadly for diagnosing optimisers and comparing their performance. The diagnostics are implemented in the R package [ferrn](https://github.com/huizezhang-sherry/ferrn).</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-105</guid>
      <pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-105/preview.png" medium="image" type="image/png" width="720" height="360"/>
    </item>
    <item>
      <title>Generalized Linear Randomized Response Modeling using GLMMRR</title>
      <dc:creator>Jean-Paul Fox</dc:creator>
      <dc:creator>Konrad Klotzke</dc:creator>
      <dc:creator>Duco Veen</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-104</link>
      <description>Randomized response (RR) designs are used to collect response data about sensitive behaviors (e.g., criminal behavior, sexual desires). The modeling of RR data is more complex since it requires a description of the RR process. For the class of generalized linear mixed models (GLMMs), the RR process can be represented by an adjusted link function, which relates the expected RR to the linear predictor for most common RR designs. The package GLMMRR includes modified link functions for four different cumulative distributions (i.e., logistic, cumulative normal, Gumbel, Cauchy) for GLMs and GLMMs, where the package lme4 facilitates ML and REML estimation. The mixed modeling framework in GLMMRR can be used to jointly analyze data collected under different designs (e.g., dual questioning, multilevel, mixed mode, repeated measurements designs, multiple-group designs). Model-fit tests, tools for residual analyses, and plot functions to give support to a profound RR data analysis are added to the well-known features of the GLM and GLMM software (package lme4). Data of Höglinger and Jann (2018) and Höglinger, Jann, and Diekmann (2014) are used to illustrate the methodology and software.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-104</guid>
      <pubDate>Mon, 22 Mar 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-104/preview.png" medium="image" type="image/png" width="376" height="264"/>
    </item>
    <item>
      <title>openSkies - Integration of Aviation Data into the R Ecosystem</title>
      <dc:creator>Rafael Ayala</dc:creator>
      <dc:creator>Daniel Ayala</dc:creator>
      <dc:creator>Lara Sellés Vidal</dc:creator>
      <dc:creator>David Ruiz</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-095</link>
      <description>Aviation data has become increasingly more accessible to the public thanks to the adoption of technologies such as Automatic Dependent Surveillance-Broadcast (ADS-B) and Mode S, which provide aircraft information over publicly accessible radio channels. Furthermore, the OpenSky Network provides multiple public resources to access such air traffic data from a large network of ADS-B receivers. Here, we present openSkies, the first R package for processing public air traffic data. The package provides an interface to the OpenSky Network resources, standardized data structures to represent the different entities involved in air traffic data, and functionalities to analyze and visualize such data. Furthermore, the portability of the implemented data structures makes openSkies easily reusable by other packages, therefore laying the foundation of aviation data engineering in R.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-095</guid>
      <pubDate>Mon, 08 Mar 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-095/preview.png" medium="image" type="image/png" width="477" height="229"/>
    </item>
    <item>
      <title>bssm: Bayesian Inference of Non-linear and Non-Gaussian State Space Models in R</title>
      <dc:creator>Jouni Helske</dc:creator>
      <dc:creator>Matti Vihola</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-103</link>
      <description>We present an R package bssm for Bayesian non-linear/non-Gaussian state space modeling. Unlike the existing packages, bssm allows for easy-to-use approximate inference based on Gaussian approximations such as the Laplace approximation and the extended Kalman filter. The package also accommodates discretely observed latent diffusion processes. The inference is based on fully automatic, adaptive Markov chain Monte Carlo (MCMC) on the hyperparameters, with optional importance sampling post-correction to eliminate any approximation bias. The package also implements a direct pseudo-marginal MCMC and a delayed acceptance pseudo-marginal MCMC using intermediate approximations. The package offers an easy-to-use interface to define models with linear-Gaussian state dynamics with non-Gaussian observation models and has an Rcpp interface for specifying custom non-linear and diffusion models.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-103</guid>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-103/preview.png" medium="image" type="image/png" width="468" height="324"/>
    </item>
    <item>
      <title>CompModels: A Suite of Computer Model Test Functions for Bayesian Optimization</title>
      <dc:creator>Tony Pourmohamad</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-076</link>
      <description>The CompModels package for R provides a suite of computer model test functions that can be used for computer model prediction/emulation, uncertainty quantification, and calibration. Moreover, the CompModels package is especially well suited for the sequential optimization of computer models. The package is a mix of real-world physics problems, known mathematical functions, and black-box functions that have been converted into computer models with the goal of Bayesian (i.e., sequential) optimization in mind. Likewise, the package contains computer models that represent either the constrained or unconstrained optimization case, each with varying levels of difficulty. In this paper, we illustrate the use of the package with both real-world examples and black-box functions by solving constrained optimization problems via Bayesian optimization. Ultimately, the package is shown to provide users with a source of computer model test functions that are reproducible, shareable, and that can be used for benchmarking of novel optimization methods.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-076</guid>
      <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-076/preview.png" medium="image" type="image/png" width="504" height="504"/>
    </item>
    <item>
      <title>An R package for Non-Normal Multivariate Distributions: Simulation and Probability Calculations from Multivariate Lomax (Pareto Type II) and Other Related Distributions</title>
      <dc:creator>Zhixin Lun</dc:creator>
      <dc:creator>Ravindra Khattree</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-090</link>
      <description>Convenient and easy-to-use programs are readily available in R to simulate data from and probability calculations for several common multivariate distributions such as normal and t. However, functions for doing so from other less common multivariate distributions, especially those which are asymmetric, are not as readily available, either in R or otherwise. We introduce the R package NonNorMvtDist to generate random numbers from multivariate Lomax distribution, which constitutes a very flexible family of skewed multivariate distributions. Further, by applying certain useful properties of multivariate Lomax distribution, multivariate cases of generalized Lomax, Mardia’s Pareto of Type I, Logistic, Burr, Cook-Johnson’s uniform, F, and inverted beta can be also considered, and random numbers from these distributions can be generated. Methods for the probability and the equicoordinate quantile calculations for all these distributions are then provided. This work substantially enriches the existing R toolbox for nonnormal or nonsymmetric multivariate probability distributions.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-090</guid>
      <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-090/preview.png" medium="image" type="image/png" width="960" height="460"/>
    </item>
    <item>
      <title>spNetwork: A Package for Network Kernel Density Estimation</title>
      <dc:creator>Jeremy Gelb</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-102</link>
      <description>This paper introduces the new package spNetwork that provides functions to perform Network Kernel Density Estimate analysis (NKDE). This method is an extension of the classical Kernel Density Estimate (KDE), a non parametric approach to estimate the intensity of a spatial process. More specifically, it adapts the KDE for cases when the study area is a network, constraining the location of events (such as accidents on roads, leaks in pipes, fish in rivers, etc.). We present and discuss in this paper the three main versions of NKDE: simple, discontinuous, and continuous that are implemented in spNetwork. We illustrate how to apply the three methods and map their results using a sample from a real dataset representing bike accidents in a central neighborhood of Montreal. We also describe the optimization techniques used to reduce calculation time and investigate their impacts when applying the three NKDE to a city-wide dataset.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-102</guid>
      <pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-102/preview.png" medium="image" type="image/png" width="815" height="443"/>
    </item>
    <item>
      <title>PASSED: Calculate Power and Sample Size for Two Sample Tests</title>
      <dc:creator>Jinpu Li</dc:creator>
      <dc:creator>Ryan P. Knigge</dc:creator>
      <dc:creator>Kaiyi Chen</dc:creator>
      <dc:creator>Emily V. Leary</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-094</link>
      <description>Power and sample size estimation are critical aspects of study design to demonstrate minimized risk for subjects and justify the allocation of time, money, and other resources. Researchers often work with response variables that take the form of various distributions. Here, we present an R package, PASSED, that allows flexibility with seven common distributions and multiple options to accommodate sample size or power analysis. The relevant statistical theory, calculations, and examples for each distribution using PASSED are discussed in this paper.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-094</guid>
      <pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-094/preview.png" medium="image" type="image/png" width="762" height="394"/>
    </item>
    <item>
      <title>spfilteR: An R package for Semiparametric Spatial Filtering with Eigenvectors in (Generalized) Linear Models</title>
      <dc:creator>Sebastian Juhl</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-085</link>
      <description>Eigenvector-based Spatial filtering constitutes a highly flexible semiparametric approach to account for spatial autocorrelation in a regression framework. It combines judiciously selected eigenvectors from a transformed connectivity matrix to construct a synthetic spatial filter and remove spatial patterns from model residuals. This article introduces the spfilteR package that provides several useful and flexible tools to estimate spatially filtered linear and generalized linear models in R. While the package features functions to identify relevant eigenvectors based on different selection criteria in an unsupervised fashion, it also helps users to perform supervised spatial filtering and to select eigenvectors based on alternative user-defined criteria. Besides a brief discussion of the eigenvector-based spatial filtering approach, this article presents the main functions of the package and illustrates their usage. Comparison to alternative implementations in other R packages highlights the added value of the spfilteR package.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-085</guid>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-085/preview.png" medium="image" type="image/png" width="300" height="300"/>
    </item>
    <item>
      <title>SIQR: An R Package for Single-index Quantile Regression</title>
      <dc:creator>Tianhai Zu</dc:creator>
      <dc:creator>Yan Yu</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-092</link>
      <description>We develop an R package SIQR that implements the single-index quantile regression (SIQR) models via an efficient iterative local linear approach in Wu et al. (2010). Single-index quantile regression models are important tools in semiparametric regression to provide a comprehensive view of the conditional distributions of a response variable. It is especially useful when the data is heterogeneous or heavy-tailed. The package provides functions that allow users to fit SIQR models, predict, provide standard errors of the single-index coefficients via bootstrap, and visualize the estimated univariate function. We apply the R package SIQR to a well-known Boston Housing data.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-092</guid>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-092/preview.png" medium="image" type="image/png" width="468" height="324"/>
    </item>
    <item>
      <title>mgee2: An R package for marginal analysis of longitudinal ordinal data with misclassified responses and covariates</title>
      <dc:creator>Yuliang Xu</dc:creator>
      <dc:creator>Shuo Shuo Liu</dc:creator>
      <dc:creator>Grace Y. Yi</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-093</link>
      <description>Marginal methods have been widely used for analyzing longitudinal ordinal data due to their simplicity in model assumptions, robustness in inference results, and easiness in the implementation. However, they are often inapplicable in the presence of measurement errors in the variables. Under the setup of longitudinal studies with ordinal responses and covariates subject to misclassification, Chen et al. (2014) developed marginal methods for misclassification adjustments using the second-order estimating equations and proposed a two-stage estimation approach when the validation subsample is available. Parameter estimation is conducted through the Newton-Raphson algorithm, and the asymptotic distribution of the estimators is established. While the methods of Chen et al. (2014) can successfully correct the misclassification effects, its implementation is not accessible to general users due to the lack of a software package. In this paper, we develop an R package, mgee2, to implement the marginal methods proposed by Chen et al. (2014). To evaluate the performance and illustrate the features of the package, we conduct numerical studies.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-093</guid>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-093/preview.png" medium="image" type="image/png" width="9600" height="6000"/>
    </item>
    <item>
      <title>Automatic Time Series Forecasting with Ata Method in R: ATAforecasting Package</title>
      <dc:creator>Ali Sabri Taylan</dc:creator>
      <dc:creator>Güçkan Yapar</dc:creator>
      <dc:creator>Hanife Taylan Selamlar</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-101</link>
      <description>Ata method is a new univariate time series forecasting method that provides innovative solutions to issues faced during the initialization and optimization stages of existing methods. The Ata method’s forecasting performance is superior to existing methods in terms of easy implementation and accurate forecasting. It can be applied to non-seasonal or deseasonalized time series, where</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-101</guid>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-101/preview.png" medium="image" type="image/png" width="1024" height="768"/>
    </item>
    <item>
      <title>tramME: Mixed-Effects Transformation Models Using Template Model Builder</title>
      <dc:creator>Bálint Tamási</dc:creator>
      <dc:creator>Torsten Hothorn</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-075</link>
      <description>Linear transformation models constitute a general family of parametric regression models for discrete and continuous responses. To accommodate correlated responses, the model is extended by incorporating mixed effects. This article presents the R package tramME, which builds on existing implementations of transformation models (mlt and tram packages) as well as Laplace approximation and automatic differentiation (using the TMB package), to calculate estimates and perform likelihood inference in mixed-effects transformation models. The resulting framework can be readily applied to a wide range of regression problems with grouped data structures.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-075</guid>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-075/preview.png" medium="image" type="image/png" width="504" height="360"/>
    </item>
    <item>
      <title>DRHotNet: An R package for detecting differential risk hotspots on a linear network</title>
      <dc:creator>Álvaro Briz-Redón</dc:creator>
      <dc:creator>Francisco Martínez-Ruiz</dc:creator>
      <dc:creator>Francisco Montes</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-100</link>
      <description>One of the most common applications of spatial data analysis is detecting zones, at a certain scale, where a point-referenced event under study is especially concentrated. The detection of such zones, which are usually referred to as hotspots, is essential in certain fields such as criminology, epidemiology, or traffic safety. Traditionally, hotspot detection procedures have been developed over areal units of analysis. Although working at this spatial scale can be suitable enough for many research or practical purposes, detecting hotspots at a more accurate level (for instance, at the road segment level) may be more convenient sometimes. Furthermore, it is typical that hotspot detection procedures are entirely focused on the determination of zones where an event is (overall) highly concentrated. It is less common, by far, that such procedures focus on detecting zones where a specific type of event is overrepresented in comparison with the other types observed, which have been denoted as differential risk hotspots. The R package DRHotNet provides several functionalities to facilitate the detection of differential risk hotspots within a linear network. In this paper, DRHotNet is depicted, and its usage in the R console is shown through a detailed analysis of a crime dataset.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-100</guid>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-100/preview.png" medium="image" type="image/png" width="504" height="504"/>
    </item>
    <item>
      <title>miRecSurv Package: Prentice-Williams-Peterson Models with Multiple Imputation of Unknown Number of Previous Episodes</title>
      <dc:creator>David Moriña</dc:creator>
      <dc:creator>Gilma Hernández-Herrera</dc:creator>
      <dc:creator>Albert Navarro</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-082</link>
      <description>Left censoring can occur with relative frequency when analyzing recurrent events in epi demiological studies, especially observational ones. Concretely, the inclusion of individuals that were already at risk before the effective initiation in a cohort study may cause the unawareness of prior episodes that have already been experienced, and this will easily lead to biased and inefficient estimates. The miRecSurv package is based on the use of models with specific baseline hazard, with multiple imputation of the number of prior episodes when unknown by means of the COMPoisson distribution, a very flexible count distribution that can handle over, sub, and equidispersion, with a stratified model depending on whether the individual had or had not previously been at risk, and the use of a frailty term. The usage of the package is illustrated by means of a real data example based on an occupational cohort study and a simulation study.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-082</guid>
      <pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-082/preview.png" medium="image" type="image/png" width="1073" height="589"/>
    </item>
    <item>
      <title>diproperm: An R Package for the DiProPerm Test</title>
      <dc:creator>Andrew G. Allmon</dc:creator>
      <dc:creator>J.S. Marron</dc:creator>
      <dc:creator>Michael G. Hudgens</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-072</link>
      <description>High-dimensional low sample size (HDLSS) data sets frequently emerge in many biomedical applications. The direction-projection-permutation (DiProPerm) test is a two-sample hypothesis test for comparing two high-dimensional distributions. The DiProPerm test is exact, i.e., the type I error is guaranteed to be controlled at the nominal level for any sample size, and thus is applicable in the HDLSS setting. This paper discusses the key components of the DiProPerm test, introduces the diproperm R package, and demonstrates the package on a real-world data set.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-072</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-072/preview.png" medium="image" type="image/png" width="1956" height="1437"/>
    </item>
    <item>
      <title>MatchThem:: Matching and Weighting after Multiple Imputation</title>
      <dc:creator>Farhad Pishgar</dc:creator>
      <dc:creator>Noah Greifer</dc:creator>
      <dc:creator>Clémence Leyrat</dc:creator>
      <dc:creator>Elizabeth Stuart</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-073</link>
      <description>Balancing the distributions of the confounders across the exposure levels in an observational study through matching or weighting is an accepted method to control for confounding due to these variables when estimating the association between an exposure and outcome and reducing the degree of dependence on certain modeling assumptions. Despite the increasing popularity in practice, these procedures cannot be immediately applied to datasets with missing values. Multiple imputation of the missing data is a popular approach to account for missing values while preserving the number of units in the dataset and accounting for the uncertainty in the missing values. However, to the best of our knowledge, there is no comprehensive matching and weighting software that can be easily implemented with multiply imputed datasets. In this paper, we review this problem and suggest a framework to map out the matching and weighting of multiply imputed datasets to 5 actions as well as the best practices to assess balance in these datasets after matching and weighting. We also illustrate these approaches using a companion package for R, MatchThem.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-073</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-073/preview.png" medium="image" type="image/png" width="2400" height="540"/>
    </item>
    <item>
      <title>MAINT.Data: Modelling and Analysing Interval Data in R</title>
      <dc:creator>A. Pedro Duarte Silva</dc:creator>
      <dc:creator>Paula Brito</dc:creator>
      <dc:creator>Peter Filzmoser</dc:creator>
      <dc:creator>José G. Dias</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-074</link>
      <description>We present the CRAN R package MAINT.Data for the modelling and analysis of multivariate interval data, i.e., where units are described by variables whose values are intervals of IR, representing intrinsic variability. Parametric inference methodologies based on probabilistic models for interval variables have been developed, where each interval is represented by its midpoint and log-range, for</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-074</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-074/preview.png" medium="image" type="image/png" width="720" height="540"/>
    </item>
    <item>
      <title>Robust and Efficient Optimization Using a Marquardt-Levenberg Algorithm with R Package marqLevAlg</title>
      <dc:creator>Viviane Philipps</dc:creator>
      <dc:creator>Boris P. Hejblum</dc:creator>
      <dc:creator>Mélanie Prague</dc:creator>
      <dc:creator>Daniel Commenges</dc:creator>
      <dc:creator>Cécile Proust-Lima</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-089</link>
      <description>Implementations in R of classical general-purpose algorithms for local optimization generally have two major limitations which cause difficulties in applications to complex problems: too loose convergence criteria and too long calculation time. By relying on a Marquardt-Levenberg algorithm (MLA), a Newton-like method particularly robust for solving local optimization problems, we provide with marqLevAlg package an efficient and general-purpose local optimizer which (i) prevents con vergence to saddle points by using a stringent convergence criterion based on the relative distance to minimum/maximum in addition to the stability of the parameters and of the objective function; and (ii) reduces the computation time in complex settings by allowing parallel calculations at each iteration. We demonstrate through a variety of cases from the literature that our implementation reli ably and consistently reaches the optimum (even when other optimizers fail) and also largely reduces computational time in complex settings through the example of maximum likelihood estimation of different sophisticated statistical models.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-089</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-089/preview.png" medium="image" type="image/png" width="772" height="366"/>
    </item>
    <item>
      <title>EMSS: New EM-type algorithms for the Heckman selection model in R</title>
      <dc:creator>Kexuan Yang</dc:creator>
      <dc:creator>Sang Kyu Lee</dc:creator>
      <dc:creator>Jun Zhao</dc:creator>
      <dc:creator>Hyoung-Moon Kim</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-098</link>
      <description>When investigators observe non-random samples from populations, sample selectivity problems may occur. The Heckman selection model is widely used to deal with selectivity problems. Based on the EM algorithm, Zhao et al. (2020) developed three algorithms, namely, ECM, ECM(NR), and ECME(NR), which also have the EM algorithm’s main advantages: stability and ease of imple mentation. This paper provides the implementation of these three new EM-type algorithms in the package EMSS and illustrates the usage of the package on several simulated and real data examples. The comparison between the maximum likelihood estimation method (MLE) and three new EM-type algorithms in robustness issues is further discussed.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-098</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Analysis of Corneal Data in R with the rPACI Package</title>
      <dc:creator>Darío Ramos-López</dc:creator>
      <dc:creator>Ana D. Maldonado</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-099</link>
      <description>In ophthalmology, the early detection of keratoconus is still a crucial problem. Placido disk corneal topographers are essential in clinical practice, and many indices for diagnosing corneal irregularities exist. The main goal of this work is to present the R package rPACI, providing several functions to handle and analyze corneal data. This package implements primary indices of corneal irregularity (based on geometrical properties) and compound indices built from the primary ones, either using a generalized linear model or as a Bayesian classifier using a hybrid Bayesian network and performing approximate inference. rPACI aims to make the analysis of corneal data accessible for practitioners and researchers in the field. Moreover, a shiny app was developed to use rPACI in any web browser in a truly user-friendly graphical interface without installing R or writing any R code. It is openly deployed at https://admaldonado.shinyapps.io/rPACI/.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-099</guid>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-099/preview.png" medium="image" type="image/png" width="980" height="500"/>
    </item>
    <item>
      <title>PAsso: an R Package for Assessing Partial Association between Ordinal Variables</title>
      <dc:creator>Shaobo Li</dc:creator>
      <dc:creator>Xiaorui Zhu</dc:creator>
      <dc:creator>Yuejie Chen</dc:creator>
      <dc:creator>Dungang Liu</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-088</link>
      <description>Partial association, the dependency between variables after adjusting for a set of covariates, is an important statistical notion for scientific research. However, if the variables of interest are ordered categorical data, the development of statistical methods and software for assessing their partial association is limited. Following the framework established by Liu et al. (2021), we develop an R package PAsso for assessing Partial Associations between ordinal variables. The package provides various functions that allow users to perform a wide spectrum of assessments, including quantification, visualization, and hypothesis testing. In this paper, we discuss the implementation of PAsso in detail and demonstrate its utility through an analysis of the 2016 American National Election Study.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-088</guid>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-088/preview.png" medium="image" type="image/png" width="1193" height="576"/>
    </item>
    <item>
      <title>dad: an R Package for Visualisation, Classification and Discrimination of Multivariate Groups Modelled by their Densities</title>
      <dc:creator>Rachid Boumaza</dc:creator>
      <dc:creator>Pierre Santagostini</dc:creator>
      <dc:creator>Smail Yousfi</dc:creator>
      <dc:creator>Sabine Demotes-Mainard</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-071</link>
      <description>Multidimensional scaling (MDS), hierarchical cluster analysis (HCA), and discriminant analysis (DA) are classical techniques which deal with data made of n individuals and p variables. When the individuals are divided into T groups, the R package dad associates with each group a multivariate probability density function and then carries out these techniques on the densities, which are estimated by the data under consideration. These techniques are based on distance measures between densities: chi-square, Hellinger, Jeffreys, Jensen-Shannon, and L p for discrete densities, Hellinger , Jeffreys, L2 , and 2-Wasserstein for Gaussian densities, and L2 for numeric non-Gaussian densities estimated by the Gaussian kernel method. Practical methods help the user to give meaning to the outputs in the context of MDS and HCA and to look for an optimal prediction in the context of DA based on the one-leave-out misclassification ratio. Some functions for data management or basic statistics calculations on groups are annexed.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-071</guid>
      <pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-071/preview.png" medium="image" type="image/png" width="504" height="504"/>
    </item>
    <item>
      <title>bcmixed: A Package for Median Inference on Longitudinal Data with the Box–Cox Transformation</title>
      <dc:creator>Kazushi Maruo</dc:creator>
      <dc:creator>Ryota Ishii</dc:creator>
      <dc:creator>Yusuke Yamaguchi</dc:creator>
      <dc:creator>Masahiko Gosho</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-083</link>
      <description>This article illustrates the use of the bcmixed package and focuses on the two main functions: bcmarg and bcmmrm. The bcmarg function provides inference results for a marginal model of a mixed ef fect model using the Box–Cox transformation. The bcmmrm function provides model median inferences based on the mixed effect models for repeated measures analysis using the Box–Cox transformation for longitudinal randomized clinical trials. Using the bcmmrm function, analysis results with high power and high interpretability for treatment effects can be obtained for longitudinal randomized clinical trials with skewed outcomes. Further, the bcmixed package provides summarizing and visualization tools, which would be helpful to write clinical trial reports.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-083</guid>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-083/preview.png" medium="image" type="image/png" width="576" height="360"/>
    </item>
    <item>
      <title>NGSSEML: Non-Gaussian State Space with Exact Marginal Likelihood</title>
      <dc:creator>Thiago R. Santos</dc:creator>
      <dc:creator>Glaura C. Franco</dc:creator>
      <dc:creator>Dani Gamerman</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-087</link>
      <description>The number of packages/software for Gaussian State Space models has increased over recent decades. However, there are very few codes available for non-Gaussian State Space (NGSS) models due to analytical intractability that prevents exact calculations. One of the few tractable exceptions is the family of NGSS with exact marginal likelihood, named NGSSEML. In this work, we present the wide range of data formats and distributions handled by NGSSEML and a package in the R language to perform classical and Bayesian inference for them. Special functions for filtering, forecasting, and smoothing procedures and the exact calculation of the marginal likelihood function are provided. The methods implemented in the package are illustrated for count and volatility time series and some reliability/survival models, showing that the codes are easy to handle. Therefore, the NGSSEML family emerges as a simple and interesting option/alternative for modeling non-Gaussian time-varying structures commonly encountered in time series and reliability/survival studies. Keywords: Bayesian, classical inference, reliability, smoothing, time series, software R</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-087</guid>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-087/preview.png" medium="image" type="image/png" width="498" height="497"/>
    </item>
    <item>
      <title>BayesSenMC: an R package for Bayesian Sensitivity Analysis of Misclassification</title>
      <dc:creator>Jinhui Yang</dc:creator>
      <dc:creator>Lifeng Lin</dc:creator>
      <dc:creator>Haitao Chu</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-097</link>
      <description>In case–control studies, the odds ratio is commonly used to summarize the association be tween a binary exposure and a dichotomous outcome. However, exposure misclassification frequently appears in case–control studies due to inaccurate data reporting, which can produce bias in measures of association. In this article, we implement a Bayesian sensitivity analysis of misclassification to provide a full posterior inference on the corrected odds ratio under both non-differential and differen tial misclassification. We present an R (R Core Team, 2018) package BayesSenMC, which provides user-friendly functions for its implementation. The usage is illustrated by a real data analysis on the association between bipolar disorder and rheumatoid arthritis.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-097</guid>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-097/preview.png" medium="image" type="image/png" width="754" height="521"/>
    </item>
    <item>
      <title>A GUIded tour of Bayesian regression</title>
      <dc:creator>Andrés Ramírez–Hassan</dc:creator>
      <dc:creator>Mateo Graciano-Londoño</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-081</link>
      <description>This paper presents a Graphical User Interface (GUI) to carry out a Bayesian regression analysis in a very friendly environment without any programming skills (drag and drop). This paper is designed for teaching and applied purposes at an introductory level. Our GUI is based on an interactive web application using shiny and libraries from R. We carry out some applications to highlight the potential of our GUI for applied researchers and practitioners. In addition, the Help option in the main tap panel has an extended version of this paper, where we present the basic theory underlying all regression models that we developed in our GUI and more applications associated with each model.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-081</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-081/preview.png" medium="image" type="image/png" width="2351" height="1017"/>
    </item>
    <item>
      <title>cat.dt: An R package for fast construction of accurate Computerized Adaptive Tests using Decision Trees</title>
      <dc:creator>Javier Rodríguez-Cuadrado</dc:creator>
      <dc:creator>Juan C. Laria</dc:creator>
      <dc:creator>David Delgado-Gómez</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-091</link>
      <description>This article introduces the cat.dt package for the creation of Computerized Adaptive Tests (CATs). Unlike existing packages, the cat.dt package represents the CAT in a Decision Tree (DT) structure. This allows building the test before its administration, ensuring that the creation time of the test is independent of the number of participants. Moreover, to accelerate the construction of the tree, the package controls its growth by joining nodes with similar estimations or distributions of the ability level and uses techniques such as message passing and pre-calculations. The constructed tree, as well as the estimation procedure, can be visualized using the graphical tools included in the package. An experiment designed to evaluate its performance shows that the cat.dt package drastically reduces computational time in the creation of CATs without compromising accuracy.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-091</guid>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-091/preview.png" medium="image" type="image/png" width="442" height="272"/>
    </item>
    <item>
      <title>Multiple Imputation and Synthetic Data Generation with NPBayesImputeCat</title>
      <dc:creator>Jingchen Hu</dc:creator>
      <dc:creator>Olanrewaju Akande</dc:creator>
      <dc:creator>Quanli Wang</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-080</link>
      <description>In many contexts, missing data and disclosure control are ubiquitous and challenging issues. In particular, at statistical agencies, the respondent-level data they collect from surveys and censuses can suffer from high rates of missingness. Furthermore, agencies are obliged to protect respondents’ privacy when publishing the collected data for public use. The NPBayesImputeCat R package, introduced in this paper, provides routines to i) create multiple imputations for missing data and ii) create synthetic data for statistical disclosure control, for multivariate categorical data, with or without structural zeros. We describe the Dirichlet process mixture of products of the multinomial distributions model used in the package and illustrate various uses of the package using data samples from the American Community Survey (ACS). We also compare results of the missing data imputation to the mice R package and those of the synthetic data generation to the synthpop R package.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-080</guid>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-080/preview.png" medium="image" type="image/png" width="432" height="288"/>
    </item>
    <item>
      <title>msae: An R Package of Multivariate Fay-Herriot Models for Small Area Estimation</title>
      <dc:creator>Novia Permatasari</dc:creator>
      <dc:creator>Azka Ubaidillah</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-096</link>
      <description>The paper introduces an R Package of multivariate Fay-Herriot models for small area estimation named msae. This package implements four types of Fay-Herriot models, including univariate Fay-Herriot model (model 0), multivariate Fay-Herriot model (model 1), autoregressive multivariate Fay-Herriot model (model 2), and heteroskedastic autoregressive multivariate Fay-Herriot model (model 3). It also contains some datasets generated based on multivariate Fay-Herriot models. We describe and implement functions through various practical examples. Multivariate Fay-Herriot models produce a more efficient parameter estimation than direct estimation and univariate model.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-096</guid>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-096/preview.png" medium="image" type="image/png" width="1153" height="379"/>
    </item>
    <item>
      <title>lg: An R package for Local Gaussian Approximations</title>
      <dc:creator>Håkon Otneim</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-079</link>
      <description>The package lg for the R programming language provides implementations of recent methodological advances on applications of the local Gaussian correlation. This includes the estimation of the local Gaussian correlation itself, multivariate density estimation, conditional density estimation, various tests for independence and conditional independence, as well as a graphical module for creating dependence maps. This paper describes the lg package, its principles, and its practical use.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-079</guid>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-079/preview.png" medium="image" type="image/png" width="360" height="288"/>
    </item>
    <item>
      <title>survidm: An R package for Inference and Prediction in an Illness-Death Model</title>
      <dc:creator>Gustavo Soutinho</dc:creator>
      <dc:creator>Marta Sestelo</dc:creator>
      <dc:creator>Luís Meira-Machado</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-070</link>
      <description>Multi-state models are a useful way of describing a process in which an individual moves through a number of finite states in continuous time. The illness-death model plays a central role in the theory and practice of these models, describing the dynamics of healthy subjects who may move to an intermediate "diseased" state before entering into a terminal absorbing state. In these models, one important goal is the modeling of transition rates which is usually done by studying the relationship between covariates and disease evolution. However, biomedical researchers are also interested in reporting other interpretable results in a simple and summarized manner. These include estimates of predictive probabilities, such as the transition probabilities, occupation probabilities, cumulative incidence functions, and the sojourn time distributions. The development of survidm package has been motivated by recent contribution that provides answers to all these topics. An illustration of the software usage is included using real data.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-070</guid>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-070/preview.png" medium="image" type="image/png" width="456" height="317"/>
    </item>
    <item>
      <title>Estimating Social Influence Effects in Networks Using A Latent Space Adjusted Approach in R </title>
      <dc:creator>Ran Xu</dc:creator>
      <link>https://rjournal-distill.netlify.app/articles/RJ-2021-069</link>
      <description>Social influence effects have been extensively studied in various empirical network research. However, many challenges remain in estimating social influence effects in networks, as influence effects are often entangled with other factors, such as homophily in the selection process and the common social-environmental factors that individuals are embedded in. Methods currently available either do not solve these problems or require stringent assumptions. Recent works by Xu (2018) and others have shown that a latent space adjusted approach based on the latent space model has the potential to disentangle the influence effects from other processes, and the simulation evidence has shown that this approach outperforms other state-of-the-art approaches in terms of recovering the true social influence effect when there is an unobserved trait co-determining influence and selection. In this paper, I will further illustrate how the latent space adjusted approach can account for bias in the estimation of social influence effects and how this approach can be easily implemented in R.</description>
      <guid>https://rjournal-distill.netlify.app/articles/RJ-2021-069</guid>
      <pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate>
      <media:content url="https://rjournal-distill.netlify.app/articles/RJ-2021-069/preview.png" medium="image" type="image/png" width="1113" height="522"/>
    </item>
  </channel>
</rss>
