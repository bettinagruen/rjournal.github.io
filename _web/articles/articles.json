[
  {
    "path": "articles/RJ-2021-063/",
    "title": "stratamatch: Prognostic Score Stratification Using a Pilot Design",
    "description": "Optimal propensity score matching has emerged as one of the most ubiquitous approaches\n for causal inference studies on observational data. However, outstanding critiques of the statistical\n properties of propensity score matching have cast doubt on the statistical efficiency of this technique,\n and the poor scalability of optimal matching to large data sets makes this approach inconvenient\n if not infeasible for sample sizes that are increasingly commonplace in modern observational data.\n The stratamatch package provides implementation support and diagnostics for ‘stratified matching\n designs,’ an approach that addresses both of these issues with optimal propensity score matching for\n large-sample observational studies. First, stratifying the data enables more computationally efficient\n matching of large data sets. Second, stratamatch implements a ‘pilot design’ approach in order to\n stratify by a prognostic score, which may increase the precision of the effect estimate and increase\n power in sensitivity analyses of unmeasured confounding.",
    "author": [
      {
        "name": "Rachael C. Aikens",
        "url": {}
      },
      {
        "name": "Joseph Rigdon",
        "url": {}
      },
      {
        "name": "Justin Lee",
        "url": {}
      },
      {
        "name": "Michael Baiocchi",
        "url": {}
      },
      {
        "name": "Andrew B. Goldstone",
        "url": {}
      },
      {
        "name": "Peter Chiu",
        "url": {}
      },
      {
        "name": "          Y. Joseph Woo",
        "url": {}
      },
      {
        "name": "Jonathan H. Chen",
        "url": {}
      }
    ],
    "date": "2021-03-01",
    "categories": [],
    "contents": "\ntitle: ‘stratamatch: Prognostic Score Stratification Using a Pilot Design’ abstract: | Optimal propensity score matching has emerged as one of the most ubiquitous approaches for causal inference studies on observational data. However, outstanding critiques of the statistical properties of propensity score matching have cast doubt on the statistical efficiency of this technique, and the poor scalability of optimal matching to large data sets makes this approach inconvenient if not infeasible for sample sizes that are increasingly commonplace in modern observational data. The stratamatch package provides implementation support and diagnostics for ‘stratified matching designs,’ an approach that addresses both of these issues with optimal propensity score matching for large-sample observational studies. First, stratifying the data enables more computationally efficient matching of large data sets. Second, stratamatch implements a ‘pilot design’ approach in order to stratify by a prognostic score, which may increase the precision of the effect estimate and increase power in sensitivity analyses of unmeasured confounding. draft: no type: package journal: title: ‘{The R Journal}’ firstpage: ‘614’ lastpage: ‘630’ volume: 13 issue: 1 citation_url: https://doi.org/10.32614/RJ-2021-063 doi: 10.32614/RJ-2021-063 slug: RJ-2021-063 author: - name: Rachael C. Aikens - name: Joseph Rigdon - name: Justin Lee - name: Michael Baiocchi - name: Andrew B. Goldstone - name: Peter Chiu - name: ’ Y. Joseph Woo’ - name: Jonathan H. Chen date: ‘2021-03-01’ date_published: ‘2021-06-21’ packages: cran: - stratamatch - MatchIt - optmatch - DOS2 - nearfar - sensitivitymw - sensitivityfull - glmnet bioc: ~ CTV: - SocialSciences - MachineLearning - OfficialStatistics - Optimization - Survival creative_commons: CC BY output: rjdistill::rjournal_web_article: self_contained: no toc: no\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:40+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-068/",
    "title": "BayesSPsurv: An R Package to Estimate Bayesian (Spatial) Split-Population Survival Models",
    "description": "Survival data often include a fraction of units that are susceptible to an event of interest\n as well as a fraction of “immune” units. In many applications, spatial clustering in unobserved risk\n factors across nearby units can also affect their survival rates and odds of becoming immune. To\n address these methodological challenges, this article introduces our BayesSPsurv R-package, which\n fits parametric Bayesian Spatial split-population survival (cure) models that can account for spatial\n autocorrelation in both subpopulations of the user’s time-to-event data. Spatial autocorrelation is\n modeled with spatially weighted frailties, which are estimated using a conditionally autoregressive\n prior. The user can also fit parametric cure models with or without nonspatial i.i.d. frailties, and\n each model can incorporate time-varying covariates. BayesSPsurv also includes various functions to\n conduct pre-estimation spatial autocorrelation tests, visualize results, and assess model performance,\n all of which are illustrated using data on post-civil war peace survival.",
    "author": [
      {
        "name": "Brandon Bolte",
        "url": {}
      },
      {
        "name": "Nicolás Schmidt",
        "url": {}
      },
      {
        "name": "Sergio Béjar",
        "url": {}
      },
      {
        "name": "Nguyen Huynh",
        "url": {}
      },
      {
        "name": "Bumba Mukherjee",
        "url": {}
      }
    ],
    "date": "2021-02-01",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:54+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-053/",
    "title": "Reproducible Summary Tables with the gtsummary Package",
    "description": "The gtsummary package provides an elegant and flexible way to create publication-ready\n summary tables in R. A critical part of the work of statisticians, data scientists, and analysts is\n summarizing data sets and regression models in R and publishing or sharing polished summary tables.\n The gtsummary package was created to streamline these everyday analysis tasks by allowing users\n to easily create reproducible summaries of data sets, regression models, survey data, and survival\n data with a simple interface and very little code. The package follows a tidy framework, making it\n easy to integrate with standard data workflows, and offers many table customization features through\n function arguments, helper functions, and custom themes.",
    "author": [
      {
        "name": "Daniel D. Sjoberg",
        "url": {}
      },
      {
        "name": "Karissa Whiting",
        "url": {}
      },
      {
        "name": "Michael Curry",
        "url": {}
      },
      {
        "name": "Jessica A. Lavery",
        "url": {}
      },
      {
        "name": "Joseph Larmarange",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:05+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-054/",
    "title": "Regularized Transformation Models: The tramnet Package",
    "description": "The tramnet package implements regularized linear transformation models by combining the\n flexible class of transformation models from tram with constrained convex optimization implemented\n in CVXR. Regularized transformation models unify many existing and novel regularized regression\n models under one theoretical and computational framework. Regularization strategies implemented\n for transformation models in tramnet include the Lasso, ridge regression, and the elastic net and\n follow the parameterization in glmnet. Several functionalities for optimizing the hyperparameters,\n including model-based optimization based on the mlrMBO package, are implemented. A multitude\n of S3 methods is deployed for visualization, handling, and simulation purposes. This work aims at\n illustrating all facets of tramnet in realistic settings and comparing regularized transformation models\n with existing implementations of similar models.",
    "author": [
      {
        "name": "Lucas Kook",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:08+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-051/",
    "title": "Automating Reproducible, Collaborative Clinical Trial Document Generation with the listdown Package",
    "description": "The conveyance of clinical trial explorations and analysis results from a statistician to a clinical\n investigator is a critical component of the drug development and clinical research cycle. Automating\n the process of generating documents for data descriptions, summaries, exploration, and analysis\n allows the statistician to provide a more comprehensive view of the information captured by a clinical\n trial, and efficient generation of these documents allows the statistican to focus more on the conceptual\n development of a trial or trial analysis and less on the implementation of the summaries and results\n on which decisions are made. This paper explores the use of the listdown package for automating\n reproducible documents in clinical trials that facilitate the collaboration between statisticians and\n clinicians as well as defining an analysis pipeline for document generation.",
    "author": [
      {
        "name": "Michael Kane",
        "url": {}
      },
      {
        "name": "Xun Jiang",
        "url": {}
      },
      {
        "name": "Simon Urbanek",
        "url": {}
      }
    ],
    "date": "2020-11-02",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:58+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-052/",
    "title": "Towards a Grammar for Processing Clinical Trial Data",
    "description": "The goal of this paper is to help define a path toward a grammar for processing clinical\n trials by a) defining a format in which we would like to represent data from standardized clinical\n trial data b) describing a standard set of operations to transform clinical trial data into this format,\n and c) to identify a set of verbs and other functionality to facilitate data processing and encourage\n reproducibility in the processing of these data. It provides a background on standard clinical trial data\n and goes through a simple preprocessing example illustrating the value of the proposed approach\n through the use of the forceps package, which is currently being used for data of this kind.",
    "author": [
      {
        "name": "Michael J. Kane",
        "url": {}
      }
    ],
    "date": "2020-11-02",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:01+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-046/",
    "title": "krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff's Alpha Coefficient",
    "description": "R package krippendorffsalpha provides tools for measuring agreement using Krippendorff’s\n α coefficient, a well-known nonparametric measure of agreement (also called inter-rater reliability\n and various other names). This article first develops Krippendorff’s α in a natural way and situates\n α among statistical procedures. Then, the usage of package krippendorffsalpha is illustrated via\n analyses of two datasets, the latter of which was collected during an imaging study of hip cartilage.\n The package permits users to apply the α methodology using built-in distance functions for the\n nominal, ordinal, interval, or ratio levels of measurement. User-defined distance functions are also\n supported. The fitting function can accommodate any number of units, any number of coders, and\n missingness. Bootstrap inference is supported, and the bootstrap computation can be carried out in\n parallel.",
    "author": [
      {
        "name": "John Hughes",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:46+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-047/",
    "title": "Working with CRSP/COMPUSTAT in R: Reproducible Empirical Asset Pricing",
    "description": "It is common to come across SAS or Stata manuals while working on academic empirical\n finance research. Nonetheless, given the popularity of open-source programming languages such as R,\n there are fewer resources in R covering popular databases such as CRSP and COMPUSTAT. The aim\n of this article is to bridge the gap and illustrate how to leverage R in working with both datasets. As\n an application, we illustrate how to form size-value portfolios with respect to Fama and French (1993)\n and study the sensitivity of the results with respect to different inputs. Ultimately, the purpose of the\n article is to advocate reproducible finance research and contribute to the recent idea of “Open Source\n Cross-Sectional Asset Pricing”, proposed by Chen and Zimmermann (2020).",
    "author": [
      {
        "name": "Majeed Simaan",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:49+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-049/",
    "title": "Analyzing Dependence between Point Processes in Time Using IndTestPP",
    "description": "The need to analyze the dependence between two or more point processes in time appears in\n many modeling problems related to the occurrence of events, such as the occurrence of climate events\n at different spatial locations or synchrony detection in spike train analysis. The package IndTestPP\n provides a general framework for all the steps in this type of analysis, and one of its main features is the\n implementation of three families of tests to study independence given the intensities of the processes,\n which are not only useful to assess independence but also to identify factors causing dependence.\n The package also includes functions for generating different types of dependent point processes,\n and implements computational statistical inference tools using them. An application to characterize\n the dependence between the occurrence of extreme heat events in three Spanish locations using the\n package is shown.",
    "author": [
      {
        "name": "Ana C. Cebrián",
        "url": {}
      },
      {
        "name": "Jesús Asín",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:52+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-050/",
    "title": "Conversations in Time: Interactive Visualization to Explore Structured Temporal Data",
    "description": "Temporal data often has a hierarchical structure, defined by categorical variables describing\n different levels, such as political regions or sales products. The nesting of categorical variables\n produces a hierarchical structure. The tsibbletalk package is developed to allow a user to interactively\n explore temporal data, relative to the nested or crossed structures. It can help to discover differences\n between category levels, and uncover interesting periodic or aperiodic slices. The package implements\n a shared tsibble object that allows for linked brushing between coordinated views, and a shiny\n module that aids in wrapping timelines for seasonal patterns. The tools are demonstrated using two\n data examples: domestic tourism in Australia and pedestrian traffic in Melbourne.",
    "author": [
      {
        "name": "Earo Wang",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:55+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-055/",
    "title": "distr6: R6 Object-Oriented Probability Distributions Interface in R",
    "description": "distr6 is an object-oriented (OO) probability distributions interface leveraging the extensibil\nity and scalability of R6 and the speed and efficiency of Rcpp. Over 50 probability distributions are\n currently implemented in the package with ‘core’ methods, including density, distribution, and gener\nating functions, and more ‘exotic’ ones, including hazards and distribution function anti-derivatives.\n In addition to simple distributions, distr6 supports compositions such as truncation, mixtures, and\n product distributions. This paper presents the core functionality of the package and demonstrates\n examples for key use-cases. In addition, this paper provides a critical review of the object-oriented\n programming paradigms in R and describes some novel implementations for design patterns and core\n object-oriented features introduced by the package for supporting distr6 components.",
    "author": [
      {
        "name": "Raphael Sonabend",
        "url": {}
      },
      {
        "name": "Franz J. Király",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:12+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-060/",
    "title": "gofCopula: Goodness-of-Fit Tests for Copulae",
    "description": "The last decades show an increased interest in modeling various types of data through\n copulae. Different copula models have been developed, which lead to the challenge of finding the\n best fitting model for a particular dataset. From the other side, a strand of literature developed a list\n of different Goodness-of-Fit (GoF) tests with different powers under different conditions. The usual\n practice is the selection of the best copula via the p-value of the GoF test. Although this method is not\n purely correct due to the fact that non-rejection does not imply acception, this strategy is favored by\n practitioners. Unfortunately, different GoF tests often provide contradicting outputs. The proposed\n R-package brings under one umbrella 13 most used copulae plus their rotated variants together\n with 16 GoF tests and a hybrid one. The package offers flexible margin modeling, automatized\n parallelization, parameter estimation, as well as a user-friendly interface, and pleasant visualizations\n of the results. To illustrate the functionality of the package, two exemplary applications are provided.",
    "author": [
      {
        "name": "Ostap Okhrin",
        "url": {}
      },
      {
        "name": "Simon Trimborn",
        "url": {}
      },
      {
        "name": "Martin Waltz",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:27+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-066/",
    "title": "ROCnReg: An R Package for Receiver Operating Characteristic Curve Inference With and Without Covariates",
    "description": "This paper introduces the package ROCnReg that allows estimating the pooled ROC\n curve, the covariate-specific ROC curve, and the covariate-adjusted ROC curve by different methods,\n both from (semi) parametric and nonparametric perspectives and within Bayesian and frequentist\n paradigms. From the estimated ROC curve (pooled, covariate-specific, or covariate-adjusted), several\n summary measures of discriminatory accuracy, such as the (partial) area under the ROC curve and the\n Youden index, can be obtained. The package also provides functions to obtain ROC-based optimal\n threshold values using several criteria, namely, the Youden index criterion and the criterion that\n sets a target value for the false positive fraction. For the Bayesian methods, we provide tools for\n assessing model fit via posterior predictive checks, while the model choice can be carried out via\n several information criteria. Numerical and graphical outputs are provided for all methods. This is\n the only package implementing Bayesian procedures for ROC curves.",
    "author": [
      {
        "name": "María Xosé Rodríguez-Álvarez",
        "url": {}
      },
      {
        "name": "Vanda Inácio",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:49+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-044/",
    "title": "OneStep : Le Cam's One-step Estimation Procedure",
    "description": "The OneStep package proposes principally an eponymic function that numerically computes\n Le Cam’s one-step estimator, which is asymptotically efficient and can be computed faster than the\n maximum likelihood estimator for large datasets. Monte Carlo simulations are carried out for several\n examples (discrete and continuous probability distributions) in order to exhibit the performance of Le\n Cam’s one-step estimation procedure in terms of efficiency and computational cost on observation\n samples of finite size.",
    "author": [
      {
        "name": "Alexandre Brouste",
        "url": {}
      },
      {
        "name": "Christophe Dutang",
        "url": {}
      },
      {
        "name": "Darel Noutsa Mieniedou",
        "url": {}
      }
    ],
    "date": "2020-10-27",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:41+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-059/",
    "title": "The HBV.IANIGLA Hydrological Model",
    "description": "Over the past 40 years, the HBV (Hydrologiska Byråns Vattenbalansavdelning) hydrological\n model has been one of the most used worldwide due to its robustness, simplicity, and reliable results.\n Despite these advantages, the available versions impose some limitations for research studies in\n mountain watersheds dominated by ice-snow melt runoff (i.e., no glacier module, a limited number of\n elevation bands, among other constraints). Here we present HBV.IANIGLA, a tool for hydroclimatic\n studies in regions with steep topography and/or cryospheric processes which provides a modular\n and extended implementation of the HBV model as an R package. To our knowledge, this is the first\n modular version of the original HBV model. This feature can be very useful for teaching hydrological\n modeling, as it offers the possibility to build a customized, open-source model that can be adjusted to\n different requirements of students and users.",
    "author": [
      {
        "name": "Ezequiel Toum",
        "url": {}
      },
      {
        "name": "Mariano H. Masiokas",
        "url": {}
      },
      {
        "name": "Ricardo Villalba",
        "url": {}
      },
      {
        "name": "Pierre Pitte",
        "url": {}
      },
      {
        "name": "Lucas Ruiz",
        "url": {}
      }
    ],
    "date": "2020-10-27",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:23+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-045/",
    "title": "The R Package smicd: Statistical Methods for Interval-Censored Data",
    "description": "The package allows the use of two new statistical methods for the analysis of interval\ncensored data: 1) direct estimation/prediction of statistical indicators and 2) linear (mixed) regression\n analysis. Direct estimation of statistical indicators, for instance, poverty and inequality indicators,\n is facilitated by a non parametric kernel density algorithm. The algorithm is able to account for\n weights in the estimation of statistical indicators. The standard errors of the statistical indicators are\n estimated with a non parametric bootstrap. Furthermore, the package offers statistical methods for\n the estimation of linear and linear mixed regression models with an interval-censored dependent\n variable, particularly random slope and random intercept models. Parameter estimates are obtained\n through a stochastic expectation-maximization algorithm. Standard errors are estimated using a\n non parametric bootstrap in the linear regression model and by a parametric bootstrap in the linear\n mixed regression model. To handle departures from the model assumptions, fixed (logarithmic) and\n data-driven (Box-Cox) transformations are incorporated into the algorithm.",
    "author": [
      {
        "name": "Paul Walter",
        "url": {}
      }
    ],
    "date": "2020-07-23",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:43+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-043/",
    "title": "RLumCarlo: Simulating Cold Light using Monte Carlo Methods",
    "description": "The luminescence phenomena of insulators and semiconductors (e.g., natural minerals such\n as quartz) have various application domains. For instance, Earth Sciences and archaeology exploit\n luminescence as a dating method. Herein, we present the R package RLumCarlo implementing sets of\n luminescence models to be simulated with Monte Carlo (MC) methods. MC methods make a powerful\n ally to all kinds of simulation attempts involving stochastic processes. Luminescence production\n is such a stochastic process in the form of charge (electron-hole pairs) interaction within insulators\n and semiconductors. To simulate luminescence-signal curves, we distribute single and independent\n MC processes to virtual MC clusters. RLumCarlo comes with a modularized design and consistent\n user interface: (1) C++ functions represent the modeling core and implement models for specific\n stimulations modes. (2) R functions give access to combinations of models and stimulation modes,\n start the simulation and render terminal and graphical feedback. The combination of MC clusters\n supports the simulation of complex luminescence phenomena.",
    "author": [
      {
        "name": "Sebastian Kreutzer",
        "url": {}
      },
      {
        "name": "Johannes Friedrich",
        "url": {}
      },
      {
        "name": "Vasilis Pagonis",
        "url": {}
      },
      {
        "name": "Christian Laag",
        "url": {}
      },
      {
        "name": "Ena Rajovic",
        "url": {}
      },
      {
        "name": "           Christoph Schmidt",
        "url": {}
      }
    ],
    "date": "2020-07-06",
    "categories": [],
    "contents": "\ntitle: ‘RLumCarlo: Simulating Cold Light using Monte Carlo Methods’ abstract: | The luminescence phenomena of insulators and semiconductors (e.g., natural minerals such as quartz) have various application domains. For instance, Earth Sciences and archaeology exploit luminescence as a dating method. Herein, we present the R package RLumCarlo implementing sets of luminescence models to be simulated with Monte Carlo (MC) methods. MC methods make a powerful ally to all kinds of simulation attempts involving stochastic processes. Luminescence production is such a stochastic process in the form of charge (electron-hole pairs) interaction within insulators and semiconductors. To simulate luminescence-signal curves, we distribute single and independent MC processes to virtual MC clusters. RLumCarlo comes with a modularized design and consistent user interface: (1) C++ functions represent the modeling core and implement models for specific stimulations modes. (2) R functions give access to combinations of models and stimulation modes, start the simulation and render terminal and graphical feedback. The combination of MC clusters supports the simulation of complex luminescence phenomena. draft: no type: package journal: title: ‘{The R Journal}’ firstpage: ‘351’ lastpage: ‘365’ volume: 13 issue: 1 citation_url: https://doi.org/10.32614/RJ-2021-043 doi: 10.32614/RJ-2021-043 slug: RJ-2021-043 author: - name: Sebastian Kreutzer - name: Johannes Friedrich - name: Vasilis Pagonis - name: Christian Laag - name: Ena Rajovic - name: ’ Christoph Schmidt’ date: ‘2020-07-06’ date_published: ‘2021-06-07’ packages: cran: - RLumCarlo - BayLum - Luminescence - numOSL - RLumModel - RLumShiny - tgcd - scales - ggplot2 - Rcpp - parallel - doParallel - foreach bioc: ~ CTV: - HighPerformanceComputing - Graphics - NumericalMathematics - Phylogenetics - TeachingStatistics creative_commons: CC BY output: rjdistill::rjournal_web_article: self_contained: no toc: no\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:37+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-035/",
    "title": "pdynmc: A Package for Estimating Linear Dynamic Panel Data Models Based on Nonlinear Moment Conditions",
    "description": "This paper introduces pdynmc, an R package that provides users sufficient flexibility\n and precise control over the estimation and inference in linear dynamic panel data models. The\n package primarily allows for the inclusion of nonlinear moment conditions and the use of iterated\n GMM; additionally, visualizations for data structure and estimation results are provided. The current\n implementation reflects recent developments in literature, uses sensible argument defaults, and\n aligns commercial and noncommercial estimation commands. Since the understanding of the model\n assumptions is vital for setting up plausible estimation routines, we provide a broad introduction\n of linear dynamic panel data models directed towards practitioners before concisely describing the\n functionality available in pdynmc regarding instrument type, covariate type, estimation methodology,\n and general configuration. We then demonstrate the functionality by revisiting the popular firm-level\n dataset of Arellano and Bond (1991).",
    "author": [
      {
        "name": "Markus Fritsch",
        "url": {}
      },
      {
        "name": "Andrew Adrian Yu Pua",
        "url": {}
      },
      {
        "name": "Joachim Schnurbus",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:23+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-036/",
    "title": "DChaos: An R Package for Chaotic Time Series Analysis",
    "description": "Chaos theory has been hailed as a revolution of thoughts and attracting ever-increasing\n attention of many scientists from diverse disciplines. Chaotic systems are non-linear deterministic\n dynamic systems which can behave like an erratic and apparently random motion. A relevant field\n inside chaos theory is the detection of chaotic behavior from empirical time-series data. One of the\n main features of chaos is the well-known initial-value sensitivity property. Methods and techniques\n related to testing the hypothesis of chaos try to quantify the initial-value sensitive property estimating\n the so-called Lyapunov exponents. This paper describes the main estimation methods of the Lyapunov\n exponent from time series data. At the same time, we present the DChaos library. R users may\n compute the delayed-coordinate embedding vector from time series data, estimates the best-fitted\n neural net model from the delayed-coordinate embedding vectors, calculates analytically the partial\n derivatives from the chosen neural nets model. They can also obtain the neural net estimator of the\n Lyapunov exponent from the partial derivatives computed previously by two different procedures\n and four ways of subsampling by blocks. To sum up, the DChaos package allows the R users to test\n robustly the hypothesis of chaos in order to know if the data-generating process behind time series\n behaves chaotically or not. The package’s functionality is illustrated by examples.",
    "author": [
      {
        "name": "Julio E. Sandubete",
        "url": {}
      },
      {
        "name": "Lorenzo Escot",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:25+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-038/",
    "title": "IndexNumber: An R Package for Measuring the Evolution of Magnitudes",
    "description": "Index numbers are descriptive statistical measures useful in economic settings for comparing\n simple and complex magnitudes registered, usually in two time periods. Although this theory has\n a large history, it still plays an important role in modern today’s societies where big amounts of\n economic data are available and need to be analyzed. After a detailed revision on classical index\n numbers in literature, this paper is focused on the description of the R package IndexNumber with\n strong capabilities for calculating them. Two of the four real data sets contained in this library\n are used for illustrating the determination of the index numbers in this work. Graphical tools are\n also implemented in order to show the time evolution of considered magnitudes simplifying the\n interpretation of the results.",
    "author": [
      {
        "name": "Alejandro Saavedra-Nieves",
        "url": {}
      },
      {
        "name": "Paula Saavedra-Nieves",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:27+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-040/",
    "title": "ROBustness In Network (robin): an R Package for Comparison and Validation of Communities ",
    "description": "In network analysis, many community detection algorithms have been developed. However,\n their implementation leaves unaddressed the question of the statistical validation of the results. Here,\n we present robin (ROBustness In Network), an R package to assess the robustness of the community\n structure of a network found by one or more methods to give indications about their reliability. The\n procedure initially detects if the community structure found by a set of algorithms is statistically\n significant and then compares two selected detection algorithms on the same graph to choose the\n one that better fits the network of interest. We demonstrate the use of our package on the American\n College Football benchmark dataset.",
    "author": [
      {
        "name": "Valeria Policastro",
        "url": {}
      },
      {
        "name": "Dario Righelli",
        "url": {}
      },
      {
        "name": "Annamaria Carissimo",
        "url": {}
      },
      {
        "name": "Luisa Cutillo",
        "url": {}
      },
      {
        "name": "Italia De Feis",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:29+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-041/",
    "title": "Finding Optimal Normalizing Transformations via bestNormalize",
    "description": "The bestNormalize R package was designed to help users find a transformation that can\neffectively normalize a vector regardless of its actual distribution. Each of the many normalization\ntechniques that have been developed has its own strengths and weaknesses, and deciding which to\nuse until data are fully observed is difficult or impossible. This package facilitates choosing between\na range of possible transformations and will automatically return the best one, i.e., the one that\nmakes data look the most normal. To evaluate and compare the normalization efficacy across a suite\nof possible transformations, we developed a statistic based on a goodness of fit test divided by its\ndegrees of freedom. Transformations can be seamlessly trained and applied to newly observed data\nand can be implemented in conjunction with caret and recipes for data preprocessing in machine\nlearning workflows. Custom transformations and normalization statistics are supported.",
    "author": [
      {
        "name": "Ryan A. Peterson",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:32+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-042/",
    "title": "Package wsbackfit for Smooth Backfitting Estimation of Generalized Structured Models",
    "description": "A package is introduced that provides the weighted smooth backfitting estimator for\na large family of popular semiparametric regression models. This family is known as generalized\nstructured models, comprising, for example, generalized varying coefficient model, generalized\nadditive models, mixtures, potentially including parametric parts. The kernel-based weighted\nsmooth backfitting belongs to the statistically most efficient procedures for this model class. Its\nasymptotic properties are well-understood thanks to the large body of literature about this estimator.\nThe introduced weights allow for the inclusion of sampling weights, trimming, and efficient estimation\nunder heteroscedasticity. Further options facilitate easy handling of aggregated data, prediction,\nand the presentation of estimation results. Cross-validation methods are provided which can be used\nfor model and bandwidth selection.1",
    "author": [
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      },
      {
        "name": "María Xosé Rodríguez-Álvarez",
        "url": {}
      },
      {
        "name": "Stefan Sperlich",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:34+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-057/",
    "title": "garchx: Flexible and Robust GARCH-X Modeling",
    "description": "The garchx package provides a user-friendly, fast, flexible, and robust framework for the\n estimation and inference of GARCH(p, q, r)-X models, where p is the ARCH order, q is the GARCH\n order, r is the asymmetry or leverage order, and ’X’ indicates that covariates can be included. Quasi\n Maximum Likelihood (QML) methods ensure estimates are consistent and standard errors valid,\n even when the standardized innovations are non-normal or dependent, or both. Zero-coefficient\n restrictions by omission enable parsimonious specifications, and functions to facilitate the non-standard\n inference associated with zero-restrictions in the null-hypothesis are provided. Finally, in the formal\n comparisons of precision and speed, the garchx package performs well relative to other prominent\n GARCH-packages on CRAN.",
    "author": [
      {
        "name": "Genaro Sucarrat",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:19+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-034/",
    "title": "Statistical Quality Control with the qcr Package",
    "description": "The R package qcr for Statistical Quality Control (SQC) is introduced and described. It\n     includes a comprehensive set of univariate and multivariate SQC tools that completes and increases\n     the SQC techniques available in R. Apart from integrating different R packages devoted to SQC (qcc,\n     MSQC), qcr provides nonparametric tools that are highly useful when Gaussian assumption is not\n     met. This package computes standard univariate control charts for individual measurements, x̄, S, R,\n     p, np, c, u, EWMA, and CUSUM. In addition, it includes functions to perform multivariate control\n     charts such as Hotelling T2 , MEWMA and MCUSUM. As representative features, multivariate\n     nonparametric alternatives based on data depth are implemented in this package: r, Q and S control\n     charts. The qcr library also estimates the most complete set of capability indices from first to\n     the fourth generation, covering the nonparametric alternatives, and performing the corresponding\n     capability analysis graphical outputs, including the process capability plots. Moreover, Phase I and\n     II control charts for functional data are included.\n\n    Prácticas de CEC con R",
    "author": [
      {
        "name": "Miguel Flores",
        "url": {}
      },
      {
        "name": "Rubén Fernández-Casal",
        "url": {}
      },
      {
        "name": "Salvador Naya",
        "url": {}
      },
      {
        "name": "Javier Tarrío-Saavedra",
        "url": {}
      }
    ],
    "date": "2020-05-07",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:21+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-030/",
    "title": "Linear Regression with Stationary Errors: the R Package slm",
    "description": "This paper introduces the R package slm, which stands for Stationary Linear Models.\nThe package contains a set of statistical procedures for linear regression in the general context where\nthe error process is strictly stationary with a short memory. We work in the setting of Hannan (1973),\nwho proved the asymptotic normality of the (normalized) least squares estimators (LSE) under\nvery mild conditions on the error process. We propose different ways to estimate the asymptotic\ncovariance matrix of the LSE and then to correct the type I error rates of the usual tests on the\nparameters (as well as confidence intervals). The procedures are evaluated through different sets of\nsimulations.",
    "author": [
      {
        "name": "Emmanuel Caron",
        "url": {}
      },
      {
        "name": "Jérôme Dedecker",
        "url": {}
      },
      {
        "name": "Bertrand Michel",
        "url": {}
      }
    ],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:15+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-032/",
    "title": "clustcurv: An R Package for Determining Groups in Multiple Curves ",
    "description": "In many situations, it could be interesting to ascertain whether groups of curves can be\n performed, especially when confronted with a considerable number of curves. This paper introduces\n an R package, known as clustcurv, for determining clusters of curves with an automatic selection of\n their number. The package can be used for determining groups in multiple survival curves as well as\n for multiple regression curves. Moreover, it can be used with large numbers of curves. An illustration\n of the use of clustcurv is provided, using both real data examples and artificial data.\n Keywords: multiple curves, number of groups, nonparametric, survival analysis, regression models,\n cluster",
    "author": [
      {
        "name": "Nora M. Villanueva",
        "url": {}
      },
      {
        "name": "Marta Sestelo",
        "url": {}
      },
      {
        "name": "Luis Meira-Machado",
        "url": {}
      },
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      }
    ],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:18+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-033/",
    "title": "Benchmarking R packages for Calculation of Persistent Homology",
    "description": "Several persistent homology software libraries have been implemented in R. Specifically,\n the Dionysus, GUDHI, and Ripser libraries have been wrapped by the TDA and TDAstats CRAN\n packages. These software represent powerful analysis tools that are computationally expensive and, to\n our knowledge, have not been formally benchmarked. Here, we analyze runtime and memory growth\n for the 2 R packages and the 3 underlying libraries. We find that datasets with less than 3 dimensions\n can be evaluated with persistent homology fastest by the GUDHI library in the TDA package. For\n higher-dimensional datasets, the Ripser library in the TDAstats package is the fastest. Ripser and\n TDAstats are also the most memory-efficient tools to calculate persistent homology.",
    "author": [
      {
        "name": "Eashwar V. Somasundaram",
        "url": {}
      },
      {
        "name": "Shael E. Brown",
        "url": {}
      },
      {
        "name": "Adam Litzler",
        "url": {}
      },
      {
        "name": "Jacob G. Scott",
        "url": {}
      },
      {
        "name": "Raoul R. Wadhwa",
        "url": {}
      }
    ],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:20+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-029/",
    "title": "Wide-to-tall Data Reshaping Using Regular Expressions and the nc Package",
    "description": "Regular expressions are powerful tools for extracting tables from non-tabular text data.\n Capturing regular expressions that describe the information to extract from column names can be\n especially useful when reshaping a data table from wide (few rows with many regularly named\n columns) to tall (fewer columns with more rows). We present the R package nc (short for named\n capture), which provides functions for wide-to-tall data reshaping using regular expressions. We\n describe the main new ideas of nc, and provide detailed comparisons with related R packages (stats,\n utils, data.table, tidyr, tidyfast, tidyfst, reshape2, cdata).",
    "author": [
      {
        "name": "Toby Dylan Hocking",
        "url": {}
      }
    ],
    "date": "2020-04-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:13+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-062/",
    "title": "Unidimensional and Multidimensional Methods for Recurrence Quantification Analysis with crqa",
    "description": "Recurrence quantification analysis is a widely used method for characterizing patterns in\n time series. This article presents a comprehensive survey for conducting a wide range of recurrence\nbased analyses to quantify the dynamical structure of single and multivariate time series and capture\n coupling properties underlying leader-follower relationships. The basics of recurrence quantification\n analysis (RQA) and all its variants are formally introduced step-by-step from the simplest auto\nrecurrence to the most advanced multivariate case. Importantly, we show how such RQA methods can\n be deployed under a single computational framework in R using a substantially renewed version of\n our crqa 2.0 package. This package includes implementations of several recent advances in recurrence\nbased analysis, among them applications to multivariate data and improved entropy calculations\n for categorical data. We show concrete applications of our package to example data, together with a\n detailed description of its functions and some guidelines on their usage.",
    "author": [
      {
        "name": "Moreno I. Coco",
        "url": {}
      },
      {
        "name": "Dan Mønster",
        "url": {}
      },
      {
        "name": "Giuseppe Leonardi",
        "url": {}
      },
      {
        "name": "Rick Dale",
        "url": {}
      },
      {
        "name": "Sebastian Wallot",
        "url": {}
      }
    ],
    "date": "2020-04-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:35+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-065/",
    "title": "The bdpar Package: Big Data Pipelining Architecture for R",
    "description": "In the last years, big data has become a useful paradigm for taking advantage of multiple\n sources to find relevant knowledge in real domains (such as the design of personalized marketing\n campaigns or helping to palliate the effects of several fatal diseases). Big data programming tools and\n methods have evolved over time from a MapReduce to a pipeline-based archetype. Concretely the use\n of pipelining schemes has become the most reliable way of processing and analyzing large amounts of\n data. To this end, this work introduces bdpar, a new highly customizable pipeline-based framework\n (using the OOP paradigm provided by R6 package) able to execute multiple preprocessing tasks over\n heterogeneous data sources. Moreover, to increase the flexibility and performance, bdpar provides\n helpful features such as (i) the definition of a novel object-based pipe operator (%>|%), (ii) the ability to\n easily design and deploy new (and customized) input data parsers, tasks, and pipelines, (iii) only-once\n execution which avoids the execution of previously processed information (instances), guaranteeing\n that only new both input data and pipelines are executed, (iv) the capability to perform serial or\n parallel operations according to the user needs, (v) the inclusion of a debugging mechanism which\n allows users to check the status of each instance (and find possible errors) throughout the process.",
    "author": [
      {
        "name": "Miguel Ferreiro-Díaz",
        "url": {}
      },
      {
        "name": "Tomás R. Cotos-Yáñez",
        "url": {}
      },
      {
        "name": "José R. Méndez",
        "url": {}
      },
      {
        "name": "David Ruano-Ordás",
        "url": {}
      }
    ],
    "date": "2020-04-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:44+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-028/",
    "title": "JMcmprsk: An R Package for Joint Modelling of Longitudinal and Survival Data with Competing Risks",
    "description": "In this paper, we describe an R package named JMcmprsk, for joint modelling of longitudinal\n and survival data with competing risks. The package in its current version implements two joint\n models of longitudinal and survival data proposed to handle competing risks survival data together\n with continuous and ordinal longitudinal outcomes respectively (Elashoff et al., 2008; Li et al., 2010).\n The corresponding R implementations are further illustrated with real examples. The package also\n provides simulation functions to simulate datasets for joint modelling with continuous or ordinal\n outcomes under the competing risks scenario, which provide useful tools to validate and evaluate\n new joint modelling methods.",
    "author": [
      {
        "name": "Hong Wang",
        "url": {}
      },
      {
        "name": "Ning Li",
        "url": {}
      },
      {
        "name": "Shanpeng Li",
        "url": {}
      },
      {
        "name": "Gang Li",
        "url": {}
      }
    ],
    "date": "2020-04-04",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:12+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-031/",
    "title": "exPrior: An R Package for the Formulation of Ex-Situ Priors",
    "description": "The exPrior package implements a procedure for formulating informative priors of geo\nstatistical properties for a target field site, called ex-situ priors and introduced in Cucchi et al. (2019).\n The procedure uses a Bayesian hierarchical model to assimilate multiple types of data coming from\n multiple sites considered as similar to the target site. This prior summarizes the information contained\n in the data in the form of a probability density function that can be used to better inform further\n geostatistical investigations at the site. The formulation of the prior uses ex-situ data, where the data\n set can either be gathered by the user or come in the form of a structured database. The package is\n designed to be flexible in that regard. For illustration purposes and for easiness of use, the package is\n ready to be used with the worldwide hydrogeological parameter database (WWHYPDA) Comunian\n and Renard (2009).",
    "author": [
      {
        "name": "Falk Heße",
        "url": {}
      },
      {
        "name": "Karina Cucchi",
        "url": {}
      },
      {
        "name": "Nura Kawa",
        "url": {}
      },
      {
        "name": "Yoram Rubin",
        "url": {}
      }
    ],
    "date": "2020-04-04",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:16+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-061/",
    "title": "penPHcure: Variable Selection in Proportional Hazards Cure Model with Time-Varying Covariates",
    "description": "We describe the penPHcure R package, which implements the semiparametric proportional\nhazards (PH) cure model of Sy and Taylor (2000) extended to time-varying covariates and the variable\n selection technique based on its SCAD-penalized likelihood proposed by Beretta and Heuchenne\n (2019a). In survival analysis, cure models are a useful tool when a fraction of the population is likely to\n be immune from the event of interest. They can separate the effects of certain factors on the probability\n of being susceptible and on the time until the occurrence of the event. Moreover, the penPHcure\n package allows the user to simulate data from a PH cure model, where the event-times are generated\n on a continuous scale from a piecewise exponential distribution conditional on time-varying covariates,\n with a method similar to Hendry (2014). We present the results of a simulation study to assess the\n finite sample performance of the methodology and illustrate the functionalities of the penPHcure\n package using criminal recidivism data.",
    "author": [
      {
        "name": "Alessandro Beretta",
        "url": {}
      },
      {
        "name": "Cédric Heuchenne",
        "url": {}
      }
    ],
    "date": "2020-02-14",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:31+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-056/",
    "title": "A Method for Deriving Information from Running R Code",
    "description": "It is often useful to tap information from a running R script. Obvious use cases include\n monitoring the consumption of resources (time, memory) and logging. Perhaps less obvious cases\n include tracking changes in R objects or collecting the output of unit tests. In this paper, we demonstrate\n an approach that abstracts the collection and processing of such secondary information from the\n running R script. Our approach is based on a combination of three elements. The first element is\n to build a customized way to evaluate code. The second is labeled local masking and it involves\n temporarily masking a user-facing function so an alternative version of it is called. The third element\n we label local side effect. This refers to the fact that the masking function exports information to the\n secondary information flow without altering a global state. The result is a method for building systems\n in pure R that lets users create and control secondary flows of information with minimal impact on\n their workflow and no global side effects.",
    "author": [
      {
        "name": "Mark P.J. van der Loo",
        "url": {}
      }
    ],
    "date": "2019-10-20",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:12:16+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-027/",
    "title": "npcure: An R Package for Nonparametric Inference in Mixture Cure Models",
    "description": "Mixture cure models have been widely used to analyze survival data with a cure fraction.\n They assume that a subgroup of the individuals under study will never experience the event (cured\n subjects). So, the goal is twofold: to study both the cure probability and the failure time of the\n uncured individuals through a proper survival function (latency). The R package npcure implements a\n completely nonparametric approach for estimating these functions in mixture cure models, considering\n right-censored survival times. Nonparametric estimators for the cure probability and the latency as\n functions of a covariate are provided. Bootstrap bandwidth selectors for the estimators are included.\n The package also implements a nonparametric covariate significance test for the cure probability,\n which can be applied with a continuous, discrete, or qualitative covariate.",
    "author": [
      {
        "name": "Ana López-Cheda",
        "url": {}
      },
      {
        "name": "M. Amalia Jácome",
        "url": {}
      },
      {
        "name": "Ignacio López-de-Ullibarri",
        "url": {}
      }
    ],
    "date": "2019-07-19",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:11+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-026/",
    "title": "SEEDCCA: An Integrated R-Package for Canonical Correlation Analysis and Partial Least Squares",
    "description": "Canonical correlation analysis (CCA) has a long history as an explanatory statistical method\n in high-dimensional data analysis and has been successfully applied in many scientific fields such as\n chemometrics, pattern recognition, genomic sequence analysis, and so on. The so-called seedCCA is a\n newly developed R package that implements not only the standard and seeded CCA but also partial\n least squares. The package enables us to fit CCA to large-p and small-n data. The paper provides a\n complete guide. Also, the seeded CCA application results are compared with the regularized CCA in\n the existing R package. It is believed that the package, along with the paper, will contribute to high\ndimensional data analysis in various science field practitioners and that the statistical methodologies\n in multivariate analysis become more fruitful.",
    "author": [
      {
        "name": "Bo-Young Kim",
        "url": {}
      },
      {
        "name": "Yunju Im",
        "url": {}
      },
      {
        "name": "Jae Keun Yoo",
        "url": {}
      }
    ],
    "date": "2019-07-18",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T02:11:09+10:00",
    "input_file": {}
  }
]
