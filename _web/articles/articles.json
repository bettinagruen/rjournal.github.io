[
  {
    "path": "articles/RJ-2021-063/",
    "title": "stratamatch: Prognostic Score Stratification Using a Pilot Design",
    "description": "Optimal propensity score matching has emerged as one of the most ubiquitous approaches\n for causal inference studies on observational data. However, outstanding critiques of the statistical\n properties of propensity score matching have cast doubt on the statistical efficiency of this technique,\n and the poor scalability of optimal matching to large data sets makes this approach inconvenient\n if not infeasible for sample sizes that are increasingly commonplace in modern observational data.\n The stratamatch package provides implementation support and diagnostics for ‘stratified matching\n designs,’ an approach that addresses both of these issues with optimal propensity score matching for\n large-sample observational studies. First, stratifying the data enables more computationally efficient\n matching of large data sets. Second, stratamatch implements a ‘pilot design’ approach in order to\n stratify by a prognostic score, which may increase the precision of the effect estimate and increase\n power in sensitivity analyses of unmeasured confounding.",
    "author": [
      {
        "name": "Rachael C. Aikens",
        "url": {}
      },
      {
        "name": "Joseph Rigdon",
        "url": {}
      },
      {
        "name": "Justin Lee",
        "url": {}
      },
      {
        "name": "Michael Baiocchi",
        "url": {}
      },
      {
        "name": "Andrew B. Goldstone",
        "url": {}
      },
      {
        "name": "Peter Chiu",
        "url": {}
      },
      {
        "name": "Y. Joseph Woo",
        "url": {}
      },
      {
        "name": "Jonathan H. Chen",
        "url": {}
      }
    ],
    "date": "2021-03-01",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-068/",
    "title": "BayesSPsurv: An R Package to Estimate Bayesian (Spatial) Split-Population Survival Models",
    "description": "Survival data often include a fraction of units that are susceptible to an event of interest\n as well as a fraction of “immune” units. In many applications, spatial clustering in unobserved risk\n factors across nearby units can also affect their survival rates and odds of becoming immune. To\n address these methodological challenges, this article introduces our BayesSPsurv R-package, which\n fits parametric Bayesian Spatial split-population survival (cure) models that can account for spatial\n autocorrelation in both subpopulations of the user’s time-to-event data. Spatial autocorrelation is\n modeled with spatially weighted frailties, which are estimated using a conditionally autoregressive\n prior. The user can also fit parametric cure models with or without nonspatial i.i.d. frailties, and\n each model can incorporate time-varying covariates. BayesSPsurv also includes various functions to\n conduct pre-estimation spatial autocorrelation tests, visualize results, and assess model performance,\n all of which are illustrated using data on post-civil war peace survival.",
    "author": [
      {
        "name": "Brandon Bolte",
        "url": {}
      },
      {
        "name": "Nicolás Schmidt",
        "url": {}
      },
      {
        "name": "Sergio Béjar",
        "url": {}
      },
      {
        "name": "Nguyen Huynh",
        "url": {}
      },
      {
        "name": "Bumba Mukherjee",
        "url": {}
      }
    ],
    "date": "2021-02-01",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-053/",
    "title": "Reproducible Summary Tables with the gtsummary Package",
    "description": "The gtsummary package provides an elegant and flexible way to create publication-ready\n summary tables in R. A critical part of the work of statisticians, data scientists, and analysts is\n summarizing data sets and regression models in R and publishing or sharing polished summary tables.\n The gtsummary package was created to streamline these everyday analysis tasks by allowing users\n to easily create reproducible summaries of data sets, regression models, survey data, and survival\n data with a simple interface and very little code. The package follows a tidy framework, making it\n easy to integrate with standard data workflows, and offers many table customization features through\n function arguments, helper functions, and custom themes.",
    "author": [
      {
        "name": "Daniel D. Sjoberg",
        "url": {}
      },
      {
        "name": "Karissa Whiting",
        "url": {}
      },
      {
        "name": "Michael Curry",
        "url": {}
      },
      {
        "name": "Jessica A. Lavery",
        "url": {}
      },
      {
        "name": "Joseph Larmarange",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-054/",
    "title": "Regularized Transformation Models: The tramnet Package",
    "description": "The tramnet package implements regularized linear transformation models by combining the\n flexible class of transformation models from tram with constrained convex optimization implemented\n in CVXR. Regularized transformation models unify many existing and novel regularized regression\n models under one theoretical and computational framework. Regularization strategies implemented\n for transformation models in tramnet include the Lasso, ridge regression, and the elastic net and\n follow the parameterization in glmnet. Several functionalities for optimizing the hyperparameters,\n including model-based optimization based on the mlrMBO package, are implemented. A multitude\n of S3 methods is deployed for visualization, handling, and simulation purposes. This work aims at\n illustrating all facets of tramnet in realistic settings and comparing regularized transformation models\n with existing implementations of similar models.",
    "author": [
      {
        "name": "Lucas Kook",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-052/",
    "title": "Towards a Grammar for Processing Clinical Trial Data",
    "description": "The goal of this paper is to help define a path toward a grammar for processing clinical\n trials by a) defining a format in which we would like to represent data from standardized clinical\n trial data b) describing a standard set of operations to transform clinical trial data into this format,\n and c) to identify a set of verbs and other functionality to facilitate data processing and encourage\n reproducibility in the processing of these data. It provides a background on standard clinical trial data\n and goes through a simple preprocessing example illustrating the value of the proposed approach\n through the use of the forceps package, which is currently being used for data of this kind.",
    "author": [
      {
        "name": "Michael J. Kane",
        "url": {}
      }
    ],
    "date": "2020-11-02",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-051/",
    "title": "Automating Reproducible, Collaborative Clinical Trial Document Generation with the \\pkg{listdown} Package",
    "description": "The conveyance of clinical trial explorations and analysis results from a statistician to a clinical investigator is a critical component of the drug development and clinical research cycle. Automating the process of generating documents for data descriptions, summaries, exploration, and analysis allows the statistician to provide a more comprehensive view of the information captured by a clinical trial, and efficient generation of these documents allows the statistican to focus more on the conceptual development of a trial or trial analysis and less on the implementation of the summaries and results on which decisions are made. This paper explores the use of the \\pkg{listdown} package for automating reproducible documents in clinical trials that facilitate the collaboration between statisticians and clinicians as well as defining an analysis pipeline for document generation.",
    "author": [
      {
        "name": "Michael Kane",
        "url": {}
      },
      {
        "name": "Xun Jiang",
        "url": {}
      },
      {
        "name": "Simon Urbanek",
        "url": {}
      }
    ],
    "date": "2020-11-02",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\nBackground and Introduction\nThe conveyance of clinical trial explorations and analysis results from a statistician to a clinical investigator is an often overlooked but critical component to the drug development and clinical research cycle. Graphs, tables, and other analysis artifacts are at the nexus of these collaborations. They facilitate identifying problems and bugs in the data preparation and processing stage, they help to build an intuitive understanding of mechanisms of disease and their treatment, they elucidate prognostic and predictive relationships, they provide insight that results in new hypotheses, and they convince researchers of analyses testing hypotheses.\nDespite their importance, the process of generating these artifacts is usually done in an ad-hoc manner. This is partially because of the nuance and diversity of the hypotheses and scientific questions being interrogated and, to a lesser degree, the variation in clinical data formatting. The usual process usually has a statistician providing a standard set of artifacts, receiving feedback, and providing updates based on feedback. Work performed for one trial is rarely leveraged on others, and as a result, a large amount of work needs to be reproduced for each trial.\nThere are two glaring problems with this approach. First, each analysis of a trial requires a substantial amount of error-prone work. While the variation between trials means some work needs to be done for preparation, exploration, and analysis, many aspects of these trials could be better automated resulting in greater efficiency and accuracy. Second, because this work is challenging, it often occupies the majority of the statisticians’ effort. Less time is spent on trial design and analysis, and then this portion is taken up by a clinician who often has less expertise with the statistical aspects of the trial. As a result, the extra effort spent on processing data undermines statisticians’ role as a collaborator and relegates them to service provider. Need tools leveraging existing work to more efficiently provide holistic views on trials will result in less effort and more accurate and comprehensive trial design and analysis.\nThe richness of the R Core Team (2012) package ecosystem, particularly with its emphasis on analysis, visualization, reproducibility, and dissemination makes the goal of creating these tools for clinical trials feasible. Generation of tables is supported by packages including tableone (Yoshida and Bartel 2020), gt (Iannone, Cheng, and Schloerke 2020), gtsummary (Sjoberg et al. 2020). Visualization is achieved using package including ggplot2 (Wickham 2016) and survminer (Kassambara, Kosinski, and Biecek 2020). We can even provide interactive presentations of data with DT (Xie, Cheng, and Tan 2020), plotly (Sievert 2020), and trelliscopejs (Hafen and Schloerke 2020).\nIt should also be realized that work building on these tools for clinical trial data is already in process. The greport (Harrell Jr 2020) package provides graphical summaries for clinical trials and has been used in conjunction with rmarkdown (Allaire et al. 2020) to produce specific trial report types with a specified format.\nUsing listdown for programmatic, collaborative clinical trial document generation\nThe listdown package (Kane, Jiang, and Urbanek 2020) was recently released to automate the process of generating reproducible (RMarkdown) documents. Objects derived from a summary, exploration, or analysis are stored hierarchically in an R list, which defines the structure of the document. These objects are referred to as computational components since they are derived from computation, as opposed to prose, which makes up the narrative components of a document.\nThe computational components capture and structure the objects to be presented. Describing how the objects will be presented and how the document will be rendered is handled through the creation of a listdown object. The separation between how computational components are created and how they are shown to a user provides two advantages. First, it decouples the data processing and analysis from its exploration and visualization. For compute-intensive analyses, this separation is critical for avoiding redundant computations for small changes in the presentation. It also discourages putting compute-intensive code into RMarkdown documents. Second, it provides the flexibility to quickly change how a computational component is visualized or summarized or even how a document is rendered. This makes transitioning from an interactive .html document to a static .pdf document significantly easier than substituting functions and parameters in an R Markdown document.\nThe package has been found to be particularly useful in the reporting and research of clinical trial data. In particular, the package has been used for server collaborations focusing on either the analysis of past trial data to formulate a new trial or in trial monitoring where trial telemetry (enrollment, responses, etc.) is reported, and initial analyses are conveyed to a clinician. The associated presentations require very little context since clinicians often have as good an understanding of the data collected as that of the statistician’s meaning narrative components are not needed. At the same time, a large number of hierarchical, heterogeneous artifacts (tables and multiple types of plots) can be automated where manual creation of RMarkdown documents would be inconvenient and inefficient.\nThe rest of this document describes concepts implemented in the listdown package for automated, reproducible document generation and shows its use with a simplified, synthetic clinical trial data set whose variables are typical of a non-small cell lung cancer trial. The data set comes from the forceps (Kane 2020) package. As of the time this document was written, the package is under development and is not available on CRAN. However, it can be installed as follows.\n\n\ndevtools::install_github(\"kaneplusplus/forceps\")\n\n\n\nThe following section uses the trial data to construct a pipeline for document generation. We note that both the data and the pipeline are simple when compared to most analyses of this type. However, it is sufficient to illustrate accompanying concepts, and both the analyses and concepts translate readily to real-world applications. A final section discusses the use of the package and its current direction.\nConstructing a pipeline for document generation\nThe process of analyzing data can be described using the classic waterfall model of Benington (1983) where the output (the analysis presentation or service) is dependent on a sequence of tasks that come before it. This dependency structure means that if a problem is detected in a given stage of the production of the analysis, all downstream parts must be rerun to reflect the change. A graphical depiction of the waterfall model, specific to data analyses (clinical or otherwise) is shown in Figure . Note that data exploration and visualization are an integral part of all stages of the production and are often the means for identifying issues and refining analyses.\n\n\n\nFigure 1: The data analysis waterfall.\n\n\n\nAs explained in the previous section, we are going to implement a simple analysis pipeline. The data acquisition and preprocessing steps are handled by importing data sets from the forceps package and using some of the functions implemented in the package to create a single trial data set, thereby de-emphasizing these components in the pipeline. While these steps are critical, the emphasis of this paper is the incorporation of the listdown package into the later stages.\nData acquisision and preprocessing\nData acquisition refers to the portion of the analysis pipeline where the data is retrieved from some managed data store for integration into the pipeline. These data sets may be retrieved as tables from a database, case reports, Analysis Data Model (ADaM) data formatted according to the Clinical Data Interchange Standards Consortium (CDISC) (“Clinical Data Interchange Standards Consortium” 2020), Electronic Health Records, or other clinical Real World Data (RWD) formats. These data are then transformed to a format appropriate for analysis.\nIn our simple example, this is accomplished by loading data corresponding to trial outcomes, patient adverse events, patient biomarkers, and patient demography and transforming them into a single data set with one row per patient and one variable per column using the forceps and dplyr (Wickham et al. 2020) packages. The data also includes longitudinal adverse event information, which will is stored as a nested data frame in the ae_long column of the resulting data set.\n\n\nlibrary(forceps)\nlibrary(dplyr)\n\ndata(lc_adsl, lc_adverse_events, lc_biomarkers, lc_demography)\n\nlc_trial <- consolidate(\n  list(adsl = lc_adsl,\n       adverse_events = lc_adverse_events %>% \n         cohort(on = \"usubjid\", name = \"ae_long\"),\n       biomarkers = lc_biomarkers,\n       demography = lc_demography %>% \n         select(-chemo_stop)\n  ),\n  on = \"usubjid\")\n\nlc_trial\n\n\n# A tibble: 558 × 18\n   usubjid best_response       pfs_days pfs_censor os_days os_censor\n     <dbl> <chr>                  <dbl>      <dbl>   <dbl>     <dbl>\n 1    1003 Stable Disease           101          1     233         1\n 2    1005 Complete Response         78          0     184         1\n 3    1006 Progressive Disease      253          1     333         1\n 4    1009 Partial Response         130          0     643         0\n 5    1014 Complete Response         41          1     116         1\n 6    1018 Partial Response         194          1     423         1\n 7    1023 Stable Disease            49          1     337         1\n 8    1025 Stable Disease            95          1     589         1\n 9    1030 Complete Response        351          1     688         1\n10    1033 Complete Response         33          1     125         1\n# … with 548 more rows, and 12 more variables: chemo_stop <chr>,\n#   arm <chr>, ae_count <int>, ae_long <list>, egfr_mutation <chr>,\n#   smoking <chr>, ecog <chr>, prior_resp <chr>, site_id <int>,\n#   sex <chr>, refractory <lgl>, age <dbl>\n\nAnalysis defining and structuring the computational components\nThe next step is to define the computational components as a hierarchical, named list of objects that will be presented. These objects are generally derived from the exploratory, descriptive, and analysis stages and are presented as visualizations and tables. This example will focus on the exploratory and descriptive portions. We will start by defining a new function class_and_tag(), which takes an object and prepends a class designation to the objects along with optionally associating the object with a set of attributes. This extra information will be used later in order to dispatch to the proper functions responsible for presenting the objects in a resulting R Markdown document. If no such information is given, then the object will be presented as it usually would in the document. It should also be noted that class_and_tag() is provided for convenience here and will be available in subsequent package versions.\n\n\n\nThe next step is to define the computational components that will be presented. Our simple example will contain three sections: Outcome Information, Adverse Events, Biomarkers. The Adverse Events and Biomarkers sections will each show summary tables of one of the variables from those data. The Outcome Information section will be composed of two subsections, with the first (Best Response) providing a summary of the best response by the arm and the second (Overall Survival) showing a survival plot by arm of the overall survival. The call to ld_cc_dendro() at the end of the example provides of dendrogram of the hierarchical structure.\n\n\nlibrary(listdown)\n\ntrial_summary <- list(\n  `Outcome Information` = list(\n    `Best Response` = lc_trial %>% \n      select(best_response, arm) %>% \n      class_and_tag(\"summary_table\", by = \"arm\"),\n    `Overall Survival` = lc_trial %>% \n      select(os_days, os_censor, arm) %>% \n      class_and_tag(\"survival_plot\", \n                    time = \"os_days\", \n                    censor = \"os_censor\", \n                    x = \"arm\")),\n  `Adverse Events` = lc_trial %>%\n    select(best_response) %>%\n    class_and_tag(\"summary_table\"),\n  `Biomarkers` = list(\n    `EGFR` = lc_trial %>%\n      select(egfr_mutation) %>%\n      class_and_tag(\"summary_table\")\n  )\n)\n\nld_cc_dendro(trial_summary)\n\n\n\ntrial_summary\n  |-- Outcome Information\n   |-- Best Response\n   |  o-- object of type(s):summary_table tbl_df tbl data.frame\n   o-- Overall Survival\n      o-- object of type(s):survival_plot tbl_df tbl data.frame\n  |-- Adverse Events\n  |  o-- object of type(s):summary_table tbl_df tbl data.frame\n  o-- Biomarkers\n   o-- EGFR\n      o-- object of type(s):summary_table tbl_df tbl data.frame\n\nCommunicating results\nThe objects have been constructed and they have been structured. The next step is to define how they will be presented. This is done by creating two functions make_summary and make_survival_plot. The former takes a data.frame and uses the gtsummary package to summarize the results. If the data.frame includes an attribute named by and denoting a valid variable in the data set, then the summary is created conditionally on that variable. The latter function also takes a data.frame and uses attributes to denote variables on which a Kaplan-Meier plot can be constructed using the survival (Terry M. Therneau and Patricia M. Grambsch 2000) and survminer packages. The functions are written to a file called decorators.r and will be used by the R Markdown document to render the final document.\n\n\nlibrary(gtsummary)\nlibrary(survival)\nlibrary(survminer)\nlibrary(dplyr)\n\nmake_summary <- function(x) {\n  by <- attributes(x)$by\n  tbl_summary(x, by = by)\n}\n\nmake_survival_plot <- function(.x) {\n  att <- attributes(.x)\n  x <- ifelse(is.null(att$x), \"1\", att$x)\n  form <- sprintf(\"Surv(%s, %s) ~ %s\", att$time, att$censor, x) %>%\n    as.formula()\n  fit <- surv_fit(form, data = as.data.frame(.x))\n  ggsurvplot(fit, data = as.data.frame(.x))\n}\n\n\n\nThe next step is to connect the computational components to the functions that will present them using the listdown() function. First, the computational components are stored in the trial_summary object are written to the disk. The resulting R Markdown document will read it in based on the load_cc_expr argument. Along with reading in the data, initialization in the R Markdown document will include sourcing the decorators.r file previously written to disk. This is handled with the init_expr argument. The decorators argument takes a list where the name specifies the class and the list element corresponds to a function that will present objects of the specified class. For example, summary_table objects are sent to the make_summary() function for presentation. This allows us to connect objects to their appropriate function to process and present them. Finally, echo and message chunk options are set to FALSE so that the code and associated messages will not appear in the final rendered document. The last line of code below displays the first 12 lines of R Markdown code chunks as they will appear in the corresponding document.\n\n\nsaveRDS(trial_summary, \"cc.rds\")\n\nld <- listdown(load_cc_expr = readRDS(\"cc.rds\"),\n               init_expr = source(\"decorators.r\"),\n               decorator = list(summary_table = make_summary,\n                                survival_plot = make_survival_plot),\n               echo = FALSE,\n               message = FALSE)\n\nld_make_chunks(ld)[1:12]\n\n\n [1] \"\"                                    \n [2] \"```{r echo = FALSE, message = FALSE}\"\n [3] \"source(\\\"decorators.r\\\")\"            \n [4] \"\"                                    \n [5] \"cc_list <- readRDS(\\\"cc.rds\\\")\"      \n [6] \"```\"                                 \n [7] \"\"                                    \n [8] \"# Outcome Information\"               \n [9] \"\"                                    \n[10] \"## Best Response\"                    \n[11] \"\"                                    \n[12] \"```{r echo = FALSE, message = FALSE}\"\n\nThe last step creates the R Markdown header, writes it along with the R code chunks to a file named \"simple-data-trial-summary.rmd\" and knits the file with the knitr package (Xie 2020) to a pdf document, per the output argument of the ld_rmarkdown_header() function. md_header is a yml object making it easy to control how the document is rendered. The code to generate the document along with the resulting R Markdown and output file constitute a reproducible workflow that can be quickly iterated upon and adapted for different types of explorations, summaries, and analyses.\n\n\nlibrary(knitr)\n\nmd_header <- ld_rmarkdown_header(\"Fake Data Trial Summary\",\n                                 output = \"pdf_document\")\n\nld_write_file(\n  rmd_header = as.character(md_header),\n  ld = ld_make_chunks(ld),\n  file_name = \"simple-data-trial-summary.rmd\")\n\nknit(\"simple-data-trial-summary.rmd\")\n\n\n\nDirection\nThe listdown package has been successfully used for collaborations on several clinical trial analyses. The package works particularly well when creating large, navigable sets of summaries and visualizations. Current work focuses on two areas. First is the construction of standard decorators. By packaging decorators and associated functionality, presentations can be made rich as they are developed over time. This standardization also makes it easier to leverage work in configuring chunks so that they can be made more aesthetic by default. In addition, we have been working on abstract and formalize the notion of document composition. In the example presented, the aggregation of the header and R code chunks into a file was sufficient for generating a document. However, the composition of other outputs is also supported. For example, the top-level names of the trial summary could just as easily designate tabs on a web page or other target. The notion of a composer would allow a user to target a greater variety of output types, better suiting the application under consideration and the target audience for the presentation.\n\n\n\nAllaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2020. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown.\n\n\nBenington, H. D. 1983. “Production of Large Computer Programs.” Annals of the History of Computing 5 (4): 350–61. https://doi.org/10.1109/MAHC.1983.10102.\n\n\n“Clinical Data Interchange Standards Consortium.” 2020. https://www.cdisc.org/.\n\n\nHafen, Ryan, and Barret Schloerke. 2020. Trelliscopejs: Create Interactive Trelliscope Displays. https://CRAN.R-project.org/package=trelliscopejs.\n\n\nHarrell Jr, Frank E. 2020. Greport: Graphical Reporting for Clinical Trials. https://CRAN.R-project.org/package=greport.\n\n\nIannone, Richard, Joe Cheng, and Barret Schloerke. 2020. Gt: Easily Create Presentation-Ready Display Tables. https://CRAN.R-project.org/package=gt.\n\n\nKane, Michael J. 2020. Forceps: A Grammar for Manipulating Clinical Trial Data. https://github.com/kaneplusplus/forceps.\n\n\nKane, Michael J., Xun (Tony) Jiang, and Simon Urbanek. 2020. “On the Programmatic Generation of Reproducible Documents.” http://arxiv.org/abs/2007.12631.\n\n\nKassambara, Alboukadel, Marcin Kosinski, and Przemyslaw Biecek. 2020. Survminer: Drawing Survival Curves Using ’Ggplot2’. https://CRAN.R-project.org/package=survminer.\n\n\nR Core Team. 2012. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. http://www.R-project.org/.\n\n\nSievert, Carson. 2020. Interactive Web-Based Data Visualization with r, Plotly, and Shiny. Chapman; Hall/CRC. https://plotly-r.com.\n\n\nSjoberg, Daniel D., Michael Curry, Margie Hannum, Karissa Whiting, and Emily C. Zabor. 2020. Gtsummary: Presentation-Ready Data Summary and Analytic Result Tables. https://CRAN.R-project.org/package=gtsummary.\n\n\nTerry M. Therneau, and Patricia M. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. New York: Springer.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nXie, Yihui. 2020. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nXie, Yihui, Joe Cheng, and Xianying Tan. 2020. DT: A Wrapper of the JavaScript Library ’DataTables’. https://CRAN.R-project.org/package=DT.\n\n\nYoshida, Kazuki, and Alexander Bartel. 2020. Tableone: Create ’Table 1’ to Describe Baseline Characteristics with or Without Propensity Score Weights. https://CRAN.R-project.org/package=tableone.\n\n\n\n\n",
    "preview": "articles/RJ-2021-051/waterfall.png",
    "last_modified": "2021-08-10T02:53:39+10:00",
    "input_file": "RJ-2021-051.knit.md",
    "preview_width": 1260,
    "preview_height": 800
  },
  {
    "path": "articles/RJ-2021-046/",
    "title": "krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff's Alpha Coefficient",
    "description": "R package krippendorffsalpha provides tools for measuring agreement using Krippendorff’s\n α coefficient, a well-known nonparametric measure of agreement (also called inter-rater reliability\n and various other names). This article first develops Krippendorff’s α in a natural way and situates\n α among statistical procedures. Then, the usage of package krippendorffsalpha is illustrated via\n analyses of two datasets, the latter of which was collected during an imaging study of hip cartilage.\n The package permits users to apply the α methodology using built-in distance functions for the\n nominal, ordinal, interval, or ratio levels of measurement. User-defined distance functions are also\n supported. The fitting function can accommodate any number of units, any number of coders, and\n missingness. Bootstrap inference is supported, and the bootstrap computation can be carried out in\n parallel.",
    "author": [
      {
        "name": "John Hughes",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-047/",
    "title": "Working with CRSP/COMPUSTAT in R: Reproducible Empirical Asset Pricing",
    "description": "It is common to come across SAS or Stata manuals while working on academic empirical\n finance research. Nonetheless, given the popularity of open-source programming languages such as R,\n there are fewer resources in R covering popular databases such as CRSP and COMPUSTAT. The aim\n of this article is to bridge the gap and illustrate how to leverage R in working with both datasets. As\n an application, we illustrate how to form size-value portfolios with respect to Fama and French (1993)\n and study the sensitivity of the results with respect to different inputs. Ultimately, the purpose of the\n article is to advocate reproducible finance research and contribute to the recent idea of “Open Source\n Cross-Sectional Asset Pricing”, proposed by Chen and Zimmermann (2020).",
    "author": [
      {
        "name": "Majeed Simaan",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-049/",
    "title": "Analyzing Dependence between Point Processes in Time Using IndTestPP",
    "description": "The need to analyze the dependence between two or more point processes in time appears in\n many modeling problems related to the occurrence of events, such as the occurrence of climate events\n at different spatial locations or synchrony detection in spike train analysis. The package IndTestPP\n provides a general framework for all the steps in this type of analysis, and one of its main features is the\n implementation of three families of tests to study independence given the intensities of the processes,\n which are not only useful to assess independence but also to identify factors causing dependence.\n The package also includes functions for generating different types of dependent point processes,\n and implements computational statistical inference tools using them. An application to characterize\n the dependence between the occurrence of extreme heat events in three Spanish locations using the\n package is shown.",
    "author": [
      {
        "name": "Ana C. Cebrián",
        "url": {}
      },
      {
        "name": "Jesús Asín",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-050/",
    "title": "Conversations in Time: Interactive Visualization to Explore Structured Temporal Data",
    "description": "Temporal data often has a hierarchical structure, defined by categorical variables describing\n different levels, such as political regions or sales products. The nesting of categorical variables\n produces a hierarchical structure. The tsibbletalk package is developed to allow a user to interactively\n explore temporal data, relative to the nested or crossed structures. It can help to discover differences\n between category levels, and uncover interesting periodic or aperiodic slices. The package implements\n a shared tsibble object that allows for linked brushing between coordinated views, and a shiny\n module that aids in wrapping timelines for seasonal patterns. The tools are demonstrated using two\n data examples: domestic tourism in Australia and pedestrian traffic in Melbourne.",
    "author": [
      {
        "name": "Earo Wang",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-055/",
    "title": "distr6: R6 Object-Oriented Probability Distributions Interface in R",
    "description": "distr6 is an object-oriented (OO) probability distributions interface leveraging the extensibil\nity and scalability of R6 and the speed and efficiency of Rcpp. Over 50 probability distributions are\n currently implemented in the package with ‘core’ methods, including density, distribution, and gener\nating functions, and more ‘exotic’ ones, including hazards and distribution function anti-derivatives.\n In addition to simple distributions, distr6 supports compositions such as truncation, mixtures, and\n product distributions. This paper presents the core functionality of the package and demonstrates\n examples for key use-cases. In addition, this paper provides a critical review of the object-oriented\n programming paradigms in R and describes some novel implementations for design patterns and core\n object-oriented features introduced by the package for supporting distr6 components.",
    "author": [
      {
        "name": "Raphael Sonabend",
        "url": {}
      },
      {
        "name": "Franz J. Király",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-060/",
    "title": "gofCopula: Goodness-of-Fit Tests for Copulae",
    "description": "The last decades show an increased interest in modeling various types of data through\n copulae. Different copula models have been developed, which lead to the challenge of finding the\n best fitting model for a particular dataset. From the other side, a strand of literature developed a list\n of different Goodness-of-Fit (GoF) tests with different powers under different conditions. The usual\n practice is the selection of the best copula via the p-value of the GoF test. Although this method is not\n purely correct due to the fact that non-rejection does not imply acception, this strategy is favored by\n practitioners. Unfortunately, different GoF tests often provide contradicting outputs. The proposed\n R-package brings under one umbrella 13 most used copulae plus their rotated variants together\n with 16 GoF tests and a hybrid one. The package offers flexible margin modeling, automatized\n parallelization, parameter estimation, as well as a user-friendly interface, and pleasant visualizations\n of the results. To illustrate the functionality of the package, two exemplary applications are provided.",
    "author": [
      {
        "name": "Ostap Okhrin",
        "url": {}
      },
      {
        "name": "Simon Trimborn",
        "url": {}
      },
      {
        "name": "Martin Waltz",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-066/",
    "title": "ROCnReg: An R Package for Receiver Operating Characteristic Curve Inference With and Without Covariates",
    "description": "This paper introduces the package ROCnReg that allows estimating the pooled ROC\n curve, the covariate-specific ROC curve, and the covariate-adjusted ROC curve by different methods,\n both from (semi) parametric and nonparametric perspectives and within Bayesian and frequentist\n paradigms. From the estimated ROC curve (pooled, covariate-specific, or covariate-adjusted), several\n summary measures of discriminatory accuracy, such as the (partial) area under the ROC curve and the\n Youden index, can be obtained. The package also provides functions to obtain ROC-based optimal\n threshold values using several criteria, namely, the Youden index criterion and the criterion that\n sets a target value for the false positive fraction. For the Bayesian methods, we provide tools for\n assessing model fit via posterior predictive checks, while the model choice can be carried out via\n several information criteria. Numerical and graphical outputs are provided for all methods. This is\n the only package implementing Bayesian procedures for ROC curves.",
    "author": [
      {
        "name": "María Xosé Rodríguez-Álvarez",
        "url": {}
      },
      {
        "name": "Vanda Inácio",
        "url": {}
      }
    ],
    "date": "2020-10-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-044/",
    "title": "OneStep : Le Cam's One-step Estimation Procedure",
    "description": "The OneStep package proposes principally an eponymic function that numerically computes\n Le Cam’s one-step estimator, which is asymptotically efficient and can be computed faster than the\n maximum likelihood estimator for large datasets. Monte Carlo simulations are carried out for several\n examples (discrete and continuous probability distributions) in order to exhibit the performance of Le\n Cam’s one-step estimation procedure in terms of efficiency and computational cost on observation\n samples of finite size.",
    "author": [
      {
        "name": "Alexandre Brouste",
        "url": {}
      },
      {
        "name": "Christophe Dutang",
        "url": {}
      },
      {
        "name": "Darel Noutsa Mieniedou",
        "url": {}
      }
    ],
    "date": "2020-10-27",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-059/",
    "title": "The HBV.IANIGLA Hydrological Model",
    "description": "Over the past 40 years, the HBV (Hydrologiska Byråns Vattenbalansavdelning) hydrological\n model has been one of the most used worldwide due to its robustness, simplicity, and reliable results.\n Despite these advantages, the available versions impose some limitations for research studies in\n mountain watersheds dominated by ice-snow melt runoff (i.e., no glacier module, a limited number of\n elevation bands, among other constraints). Here we present HBV.IANIGLA, a tool for hydroclimatic\n studies in regions with steep topography and/or cryospheric processes which provides a modular\n and extended implementation of the HBV model as an R package. To our knowledge, this is the first\n modular version of the original HBV model. This feature can be very useful for teaching hydrological\n modeling, as it offers the possibility to build a customized, open-source model that can be adjusted to\n different requirements of students and users.",
    "author": [
      {
        "name": "Ezequiel Toum",
        "url": {}
      },
      {
        "name": "Mariano H. Masiokas",
        "url": {}
      },
      {
        "name": "Ricardo Villalba",
        "url": {}
      },
      {
        "name": "Pierre Pitte",
        "url": {}
      },
      {
        "name": "Lucas Ruiz",
        "url": {}
      }
    ],
    "date": "2020-10-27",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-045/",
    "title": "The R Package smicd: Statistical Methods for Interval-Censored Data",
    "description": "The package allows the use of two new statistical methods for the analysis of interval\ncensored data: 1) direct estimation/prediction of statistical indicators and 2) linear (mixed) regression\n analysis. Direct estimation of statistical indicators, for instance, poverty and inequality indicators,\n is facilitated by a non parametric kernel density algorithm. The algorithm is able to account for\n weights in the estimation of statistical indicators. The standard errors of the statistical indicators are\n estimated with a non parametric bootstrap. Furthermore, the package offers statistical methods for\n the estimation of linear and linear mixed regression models with an interval-censored dependent\n variable, particularly random slope and random intercept models. Parameter estimates are obtained\n through a stochastic expectation-maximization algorithm. Standard errors are estimated using a\n non parametric bootstrap in the linear regression model and by a parametric bootstrap in the linear\n mixed regression model. To handle departures from the model assumptions, fixed (logarithmic) and\n data-driven (Box-Cox) transformations are incorporated into the algorithm.",
    "author": [
      {
        "name": "Paul Walter",
        "url": {}
      }
    ],
    "date": "2020-07-23",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-043/",
    "title": "RLumCarlo: Simulating Cold Light using Monte Carlo Methods",
    "description": "The luminescence phenomena of insulators and semiconductors (e.g., natural minerals such\n as quartz) have various application domains. For instance, Earth Sciences and archaeology exploit\n luminescence as a dating method. Herein, we present the R package RLumCarlo implementing sets of\n luminescence models to be simulated with Monte Carlo (MC) methods. MC methods make a powerful\n ally to all kinds of simulation attempts involving stochastic processes. Luminescence production\n is such a stochastic process in the form of charge (electron-hole pairs) interaction within insulators\n and semiconductors. To simulate luminescence-signal curves, we distribute single and independent\n MC processes to virtual MC clusters. RLumCarlo comes with a modularized design and consistent\n user interface: (1) C++ functions represent the modeling core and implement models for specific\n stimulations modes. (2) R functions give access to combinations of models and stimulation modes,\n start the simulation and render terminal and graphical feedback. The combination of MC clusters\n supports the simulation of complex luminescence phenomena.",
    "author": [
      {
        "name": "Sebastian Kreutzer",
        "url": {}
      },
      {
        "name": "Johannes Friedrich",
        "url": {}
      },
      {
        "name": "Vasilis Pagonis",
        "url": {}
      },
      {
        "name": "Christian Laag",
        "url": {}
      },
      {
        "name": "Ena Rajovic",
        "url": {}
      },
      {
        "name": "Christoph Schmidt",
        "url": {}
      }
    ],
    "date": "2020-07-06",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-035/",
    "title": "pdynmc: A Package for Estimating Linear Dynamic Panel Data Models Based on Nonlinear Moment Conditions",
    "description": "This paper introduces pdynmc, an R package that provides users sufficient flexibility\n and precise control over the estimation and inference in linear dynamic panel data models. The\n package primarily allows for the inclusion of nonlinear moment conditions and the use of iterated\n GMM; additionally, visualizations for data structure and estimation results are provided. The current\n implementation reflects recent developments in literature, uses sensible argument defaults, and\n aligns commercial and noncommercial estimation commands. Since the understanding of the model\n assumptions is vital for setting up plausible estimation routines, we provide a broad introduction\n of linear dynamic panel data models directed towards practitioners before concisely describing the\n functionality available in pdynmc regarding instrument type, covariate type, estimation methodology,\n and general configuration. We then demonstrate the functionality by revisiting the popular firm-level\n dataset of Arellano and Bond (1991).",
    "author": [
      {
        "name": "Markus Fritsch",
        "url": {}
      },
      {
        "name": "Andrew Adrian Yu Pua",
        "url": {}
      },
      {
        "name": "Joachim Schnurbus",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-036/",
    "title": "DChaos: An R Package for Chaotic Time Series Analysis",
    "description": "Chaos theory has been hailed as a revolution of thoughts and attracting ever-increasing\n attention of many scientists from diverse disciplines. Chaotic systems are non-linear deterministic\n dynamic systems which can behave like an erratic and apparently random motion. A relevant field\n inside chaos theory is the detection of chaotic behavior from empirical time-series data. One of the\n main features of chaos is the well-known initial-value sensitivity property. Methods and techniques\n related to testing the hypothesis of chaos try to quantify the initial-value sensitive property estimating\n the so-called Lyapunov exponents. This paper describes the main estimation methods of the Lyapunov\n exponent from time series data. At the same time, we present the DChaos library. R users may\n compute the delayed-coordinate embedding vector from time series data, estimates the best-fitted\n neural net model from the delayed-coordinate embedding vectors, calculates analytically the partial\n derivatives from the chosen neural nets model. They can also obtain the neural net estimator of the\n Lyapunov exponent from the partial derivatives computed previously by two different procedures\n and four ways of subsampling by blocks. To sum up, the DChaos package allows the R users to test\n robustly the hypothesis of chaos in order to know if the data-generating process behind time series\n behaves chaotically or not. The package’s functionality is illustrated by examples.",
    "author": [
      {
        "name": "Julio E. Sandubete",
        "url": {}
      },
      {
        "name": "Lorenzo Escot",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-038/",
    "title": "IndexNumber: An R Package for Measuring the Evolution of Magnitudes",
    "description": "Index numbers are descriptive statistical measures useful in economic settings for comparing\n simple and complex magnitudes registered, usually in two time periods. Although this theory has\n a large history, it still plays an important role in modern today’s societies where big amounts of\n economic data are available and need to be analyzed. After a detailed revision on classical index\n numbers in literature, this paper is focused on the description of the R package IndexNumber with\n strong capabilities for calculating them. Two of the four real data sets contained in this library\n are used for illustrating the determination of the index numbers in this work. Graphical tools are\n also implemented in order to show the time evolution of considered magnitudes simplifying the\n interpretation of the results.",
    "author": [
      {
        "name": "Alejandro Saavedra-Nieves",
        "url": {}
      },
      {
        "name": "Paula Saavedra-Nieves",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-040/",
    "title": "ROBustness In Network (robin): an R Package for Comparison and Validation of Communities ",
    "description": "In network analysis, many community detection algorithms have been developed. However,\n their implementation leaves unaddressed the question of the statistical validation of the results. Here,\n we present robin (ROBustness In Network), an R package to assess the robustness of the community\n structure of a network found by one or more methods to give indications about their reliability. The\n procedure initially detects if the community structure found by a set of algorithms is statistically\n significant and then compares two selected detection algorithms on the same graph to choose the\n one that better fits the network of interest. We demonstrate the use of our package on the American\n College Football benchmark dataset.",
    "author": [
      {
        "name": "Valeria Policastro",
        "url": {}
      },
      {
        "name": "Dario Righelli",
        "url": {}
      },
      {
        "name": "Annamaria Carissimo",
        "url": {}
      },
      {
        "name": "Luisa Cutillo",
        "url": {}
      },
      {
        "name": "Italia De Feis",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-041/",
    "title": "Finding Optimal Normalizing Transformations via \\pkg{bestNormalize}",
    "description": "The \\pkg{bestNormalize} R package was designed to help users find a transformation that can effectively normalize a vector regardless of its actual distribution. Each of the many normalization techniques that have been developed has its own strengths and weaknesses, and deciding which to use until data are fully observed is difficult or impossible. This package facilitates choosing between a range of possible transformations and will automatically return  the best one, i.e., the one that makes data look the *most* normal. To evaluate and compare  the normalization efficacy across a suite of possible transformations, we developed a  statistic based on a goodness of fit test divided by its degrees of freedom.  Transformations can be seamlessly trained and applied to newly observed data  and can be implemented in conjunction with \\pkg{caret} and \\pkg{recipes} for  data preprocessing in machine learning workflows. Custom transformations and  normalization statistics are supported.",
    "author": [
      {
        "name": "Ryan A. Peterson",
        "url": "https://petersonr.github.io/"
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\nIntroduction\nThe  package contains a suite of transformation-estimating functions that can be used to normalize data. The function of the same name attempts to find and execute the best of all of these potential normalizing transformations. In this package, we define “normalize” as in “to render data Gaussian,” rather than transform them to a specific scale.\nThere are many instances where researchers may want to normalize a variable. First, there is the (often problematic) assumption of normality of the outcome (conditional on the covariates) in the classical linear regression problem. Over the years, many methods have been used to relax this assumption: generalized linear models, quantile regression, survival models, etc. One technique that is still somewhat popular in this context is to “beat the data” to look normal via some kind of normalizing transformation. This could be something as simple as a log transformation or something as complex as a Yeo-Johnson transformation (Yeo and Johnson 2000). In fact, many complex normalization methods were designed expressly to find a transformation that could render regression residuals Gaussian. While perhaps not the most elegant solution to the problem, often, this technique works well as a quick solution. Another increasingly popular application of normalization occurs in applied regression settings with highly skewed distributions of the covariates (Kuhn and Johnson 2013). In these settings, there exists the tendency to have high leverage points (and highly influential points), even when one centers and scales the covariates. When examining interactions, these influential points can become especially problematic since the leverage of that point gets amplified for every interaction in which it is involved. Normalization of such covariates can mitigate their leverage and influence, thereby allowing for easier model selection and more robust downstream predictor manipulations (such as principal components analysis), which can otherwise be sensitive to skew or outliers. As a result, popular model selection packages such as  (Kuhn 2017) and  (Kuhn and Wickham 2018) have built-in mechanisms to normalize the predictor variables (they call this “preprocessing”). This concept is unique in that it forgoes the assumption of linearity between the outcome (Y) and the covariate, opting instead for a linear relationship between Y and the transformed value of the covariate (which in many cases may be more plausible).\nThis package is designed to make normalization effortless and consistent. We have also introduced Ordered Quantile (ORQ) normalization via the  function, which uses a rank mapping of the observed data to the normal distribution in order to guarantee normally distributed transformed data (if ties are not present). We have shown how ORQ normalization performs very consistently across different distributions, successfully normalizing left- or right-skewed data, multi-modal data, and even data generated from a Cauchy distribution (Peterson and Cavanaugh 2019).\nIn this paper, we describe our R package , which is available via the Comprehensive R Archive Network (CRAN). First, we describe normalization methods that have been developed and that we implement in the package. Second, we describe the novel cross-validation-based estimation procedure, which we utilize to judge the normalization efficacy of our suite of normalization transformations. Third, we go through some basic examples of  functionality and a simple implementation of our methods within the  package. We illustrate a more in-depth use-case in a car pricing application, performing a transform-both-sides regression as well as comparing the performance of several predictive models fit via . Finally, we conclude by discussing the pros and cons of normalization in general and future directions for the package.\nNormalization methods\nMany normalization transformation functions exist, and though some can be implemented well in existing R packages,  puts them all under the same umbrella syntax. This section describes each transformation contained in the  suite.\nThe Box-Cox transformation\nThe Box-Cox transformation was famously proposed in Box and Cox (1964) and can be implemented with differing syntax and methods in many existing packages in R (e.g., ,  (Venables and Ripley 2002), and more). It is a straightforward transformation that typically only involves one parameter, \\(\\lambda\\):\n\\[\ng(x; \\lambda) = \\boldsymbol 1 _{(\\lambda \\neq 0)} \\frac{x^\\lambda-1}{\\lambda} \n+ \\boldsymbol 1_{(\\lambda = 0)} \\log x\\text{ ,}\n\\]\nwhere \\(x\\) refers to the datum in its original unit (pre-transformation). Given multiple observations, the \\(\\lambda\\) parameter can be estimated via maximum likelihood, and \\(x\\) must be greater than zero.\nThe Yeo-Johnson transformation\nThe Yeo-Johnson transformation (Yeo and Johnson 2000) attempts to find the value of \\(\\lambda\\) in the following equation that minimizes the Kullback-Leibler distance between the normal distribution and the transformed distribution.\n\\[\n\\begin{aligned}\ng(x;\\lambda) &= \n\\boldsymbol 1 _{(\\lambda \\neq 0, x \\geq 0)} \\frac{(x+1)^\\lambda-1}{\\lambda} \\\\\n&+ \\boldsymbol 1_{(\\lambda = 0, x \\geq 0)} \\log (x+1) \\\\\n&+ \\boldsymbol 1_{(\\lambda \\neq 2, x < 0)} \\frac{(1-x)^{2-\\lambda}-1}{\\lambda - 2} \\\\\n&+ \\boldsymbol 1_{(\\lambda = 2, x < 0)} -\\log (1-x) \\\\\n\\end{aligned}\n\\]\nThis method has the advantage of working without having to worry about the domain of \\(x\\). As with the Box-Cox \\(\\lambda\\), this \\(\\lambda\\) parameter can be estimated via maximum likelihood.\nThe Lambert W x F transformation\nThe Lambert W x F transformation, proposed in Goerg (2011) and implemented in the  package, is essentially a mechanism that de-skews a random variable \\(X\\) using moments. The method is motivated by a system theory and is alleged to be able to transform any random variable into any other kind of random variable, thus being applicable to a large number of contexts. One of the package’s main functions is , which is similar in spirit to the purpose of this package. However, this method may not perform as well on certain shapes of distributions as other candidate transformations; see Peterson and Cavanaugh (2019) for some examples.\nThe  transformation can handle three types of transformations: skewed, heavy-tailed, and skewed heavy-tailed. For more details on this transformation, consult the  documentation. While the transformations contained and implemented by  are reversible (i.e., 1-1), in rare circumstances, we have observed that the  function can yield non-reversible transformations.\nThe Ordered Quantile technique\nThe ORQ normalization technique () is based on the following transformation (originally discussed, as far as we can find, in Bartlett (1947) and further developed in Van der Waerden (1952)):\nLet \\(\\underline x\\) refer to the original data. Then the transformation is:\n\\[\ng(\\underline x) = \\Phi ^{-1} \\left(\\frac{\\text{rank} (\\underline x) - 1/2}{\\text{length}(\\underline x) }\\right)\n\\]\nThis nonparametric transformation as defined works well on the observed data, but it is not trivial to implement in modern settings where the transformation needs to be applied on new data; we discussed this issue and our solution to it in Peterson and Cavanaugh (2019). Basically, on new data within the range of the original data, ORQ normalization will linearly interpolate between two of the original data points. On new data outside the range of the original data, the transformation extrapolates using a shifted logit approximation of the ranks to the original data. This is visualized below via the  data set on the  variable.\n\n\n\n(#fig:orq_vis)ORQ normalization visualization on Fisher’s iris data.\n\n\n\nThe shifted logit extrapolation ensures that the function is 1-1 and can handle data outside the original (observed) domain. The effects of the approximation will usually be relatively minimal since we should not expect to see many observations outside the observed range if the training set sample size is large relative to the test set. The ORQ technique will not guarantee a normal distribution in the presence of ties, but it still could yield the best normalizing transformation when compared to the other possible approaches. More information on ORQ normalization can be found in Peterson and Cavanaugh (2019) or in the  documentation.\nOther included transformations\nIn addition to the techniques above, the  package performs and evaluates:\n\\(\\log_b(x + a)\\) where \\(a = \\max(0, -\\min(x) + \\epsilon)\\) and \\(b = 10\\) by default\n\\(\\sqrt{x + a}\\) where \\(a = \\max(0, -\\min(x))\\) by default\n\\(\\exp(x)\\)\n\\(\\text {arcsinh}(x) = log(x + \\sqrt{x^2 + 1})\\)\nOther not-included transformations\nA range of other normalization techniques has been proposed that are not included in this package (at the time of writing). These include (but are not limited to): Modified Box-Cox (Box and Cox 1964), Manly’s Exponential (Manly 1976), John/Draper’s Modulus (John and Draper 1980), and Bickel/Doksum’s Modified Box-Cox (Bickel and Doksum 1981). However, it is straightforward to add new transformations into the same framework as other included transformations; each one is treated as its own S3 class, so in order to add other transformations, all one must do is define a new S3 class and provide the requisite S3 methods. To this end, we encourage readers to submit a pull request to the package’s GitHub page with new transformation techniques that could be then added as a default in . Otherwise, in a later section, we show how users can implement custom transformations alongside the default ones described above.\nWhich transformation “best normalizes” the data?\nThe  function selects the best transformation according to an extra-sample estimate of the Pearson P statistic divided by its degrees of freedom (\\(DF\\)). This P statistic is defined as\n\\[\nP = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}\\text{ ,}\n\\]\nwhere \\(O_i\\) is the number observed, and \\(E_i\\) is the number of expected (under the hypothesis of normality) to fall into “bin” \\(i\\). The bins (or “classes”) are built such that observations will fall into each one with equal probability under the hypothesis of normality. A variety of alternative normality tests exist, but this particular one is relatively interpretable as a goodness of fit test, and the ratio \\(P/DF\\) can be compared between transformations as an absolute measure of departure from normality. Specifically, if the data in question follow a normal distribution, this ratio will be close to 1 or lower. The transformation which produces data with the lowest normality statistic is thus the most effective at normalizing the data, and gets selected by . The  package utilizes  (Gross and Ligges 2015) to compute this statistic; more information on its computation and degrees of freedom can be found in D’Agostino (1986) and Thode (2002).\nNormality statistics for all candidate transformations can be estimated and compared with one simple call to , whose output makes it easy to see which transformations are viable and which are not. We have found that while complicated transformations are often most effective and therefore selected automatically, sometimes a simple transformation (e.g., the log or identity transforms) may be almost as effective, and ultimately the latter type will yield more interpretable results.\nIt is worth noting that when the normality statistic is estimated on in-sample data, the ORQ technique is predestined to be most effective since it is forcing its transformed data to follow a normal distribution exactly (Peterson and Cavanaugh 2019). For this reason, by default, the  function calculates an out-of-sample estimate for the \\(P/DF\\) statistic. Since this method necessitates cross-validation, it can be computationally frustrating for three reasons: (1) the results and the chosen transformation can depend on the seed, (2) it takes considerably longer to estimate than the in-sample statistic, and (3) it is unclear how to choose the number of folds and repeats.\nIn order to mediate these issues, we have built several features into . Issue (1) is only important for small sample sizes, and when it is a concern, the best transformations should look similar to one another. We address two solutions to (2) in the next section. In short, we have methods to parallelize or simplify the estimation of the statistic. For (3), we recommend 10-fold cross-validation with 5 repeats as the default, but if the sample is small, we suggest using 5 (or fewer) folds instead with more repeats; accurate estimation of \\(P/DF\\) requires a relatively large fold size (as a rule of thumb, 20 observations per fold seems to be enough for most cases, but this unfortunately depends on the distribution of the observed data).\nSimple examples\nIn this section, we illustrate a simple use-case of the functions provided in .\nBasic implementation\nFirst, we will generate and plot some skewed data:\n\n\nx <- rgamma(250, 1, 1)\n\n\n\n\n\n\n(#fig:simple_hist)Simulated skewed data for simple example.\n\n\n\nTo perform a suite of potential transformations and see how effectively they normalized this vector, simply call :\n\n\n(BNobject <- bestNormalize(x))\n\n\nBest Normalizing transformation with 250 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.7917\n - Box-Cox: 1.0442\n - Center+scale: 3.0102\n - Exp(x): 9.5306\n - Log_b(x+a): 1.7072\n - orderNorm (ORQ): 1.1773\n - sqrt(x + a): 1.144\n - Yeo-Johnson: 1.1875\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\nStandardized Box Cox Transformation with 250 nonmissing obs.:\n Estimated statistics:\n - lambda = 0.3254863 \n - mean (before standardization) = -0.3659267 \n - sd (before standardization) = 0.9807881 \n\nEvidently, the Box-Cox transformation performed the best, though many other transformations performed similarly. We can visualize the suite of transformations using the built-in  method:\n\n\nplot(BNobject, leg_loc = \"topleft\")\n\n\n\n\n(#fig:bn_plot)The suite of transformations estimated by default in  (trained on simulated right-skewed data).\n\n\n\nFinally, we can execute the best performing normalization on new data with  or reverse the transformation with . Note that normalized values can either be obtained using  or by extracting  from the object. The best transformation, in this case, is plotted in Figure 4.\n\n\n\n(#fig:hist_best)Summary of transformations performed on simulated right-skewed data.\n\n\n\nPerforming transformations individually\nEach method can be performed (and stored) individually:\n\n\n(arcsinh_obj <- arcsinh_x(x))\n\n\nStandardized asinh(x) Transformation with 250 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 0.7383146 \n - sd (before standardization) = 0.5458515 \n\n(boxcox_obj <- boxcox(x))\n\n\nStandardized Box Cox Transformation with 250 nonmissing obs.:\n Estimated statistics:\n - lambda = 0.3254863 \n - mean (before standardization) = -0.3659267 \n - sd (before standardization) = 0.9807881 \n\n(yeojohnson_obj <- yeojohnson(x))\n\n\nStandardized Yeo-Johnson Transformation with 250 nonmissing obs.:\n Estimated statistics:\n - lambda = -0.7080476 \n - mean (before standardization) = 0.4405464 \n - sd (before standardization) = 0.2592004 \n\n(lambert_obj <- lambert(x, type = \"s\"))\n\n\nStandardized Lambert WxF Transformation of type s with 250 nonmissing obs.:\n Estimated statistics:\n - gamma = 0.3729\n - mean (before standardization) = 0.6781864 \n - sd (before standardization) = 0.7123011 \n\n(orderNorm_obj <- orderNorm(x))\n\n\norderNorm Transformation with 250 nonmissing obs and no ties \n - Original quantiles:\n   0%   25%   50%   75%  100% \n0.001 0.268 0.721 1.299 4.161 \n\nAll normalization techniques in  have their own class with convenient S3 methods and documentation. For instance, we can use the  method to perform the transformation on new values using the objects we have just created, visualizing them in a plot:\n\n\nxx <- seq(min(x), max(x), length = 100)\nplot(xx, predict(arcsinh_obj, newdata = xx), type = \"l\", col = 1)\nlines(xx, predict(boxcox_obj, newdata = xx), col = 2)\nlines(xx, predict(yeojohnson_obj, newdata = xx), col = 3)\nlines(xx, predict(orderNorm_obj, newdata = xx), col = 4)\n\n\n\n\n\n\n(#fig:vis_data2)Manually plotting transformations trained on simulated right-skewed data.\n\n\n\nIn-sample normalization efficacy\nTo examine how each of the normalization methods performed (in-sample), we can visualize the transformed values in histograms (Figure 6), which plot the transformed data, , stored in the transformation objects we created previously.\n\n\n\n(#fig:hist_trans)Normalized values for trained transformations on simulated right-skewed data.\n\n\n\nEvidently, ORQ normalization appears to have worked perfectly to normalize the data (as expected), and the Box-Cox method seemed to do quite well too.\nOut-of-sample normalization efficacy\nThe  function performs repeated (r=5) 10-fold cross-validation (CV) by default and stores the estimated normality statistic for each left-out fold/repeat into . Users can access and visualize these results via a boxplot (see below), which may give some insight into whether the transformation is truly preferred by the normality statistic or if another (possibly simpler) transformation can be applied that would achieve the approximately the same results. In this example, Box-Cox, square-root, Yeo-Johnson, and ORQ seem to do similarly well, whereas the identity transform, hyperbolic arc-sine, logging, and exponentiation are performing worse.\n\n\nboxplot(BNobject$oos_preds, log = 'y')\nabline(h = 1, col = \"green3\")\n\n\n\n\nFigure 1: Cross-validation results for each normalization method, where our estimated normality statistic is plotted on the y-axis.\n\n\n\nLeave-one-out CV can be optionally performed in  via the  argument, which, if set to , will compute the leave-one-out CV transformations for each observation and method. Specifically,  will be run \\(n\\) separate times where each observation is individually left out of the fitting process and subsequently plugged back in to get a “leave-one-out transformed value.” Instead of taking the mean across repeats and folds, in this case, we estimate normalization efficacy using the full distribution of leave-one-out transformed values. This option is computationally intensive. Note that as with the “in-sample” normality statistics, the leave-one-out CV approach tends to select the ORQ transformation since ORQ’s performance improves as the number of points in the training set relative to the testing set increases.\n\n\nbestNormalize(x, loo = TRUE)\n\n\nBest Normalizing transformation with 250 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 4.42\n - Box-Cox: 0.7055\n - Center+scale: 8.258\n - Exp(x): 62.085\n - Log_b(x+a): 3.546\n - orderNorm (ORQ): 0.012\n - sqrt(x + a): 0.9145\n - Yeo-Johnson: 1.608\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 250 nonmissing obs and no ties \n - Original quantiles:\n   0%   25%   50%   75%  100% \n0.001 0.268 0.721 1.299 4.161 \n\nImportant features\nImproving speed of estimation\nBecause  uses repeated CV by default to estimate the out-of-sample normalization efficacy, it can be quite slow for larger objects. There are several means of speeding up the process. Each comes with some pros and cons. The first option is to specify . This will highly speed up the process. However, for reasons previously discussed, ORQ normalization will always be chosen unless . Therefore, a user might as well use the  function directly as opposed to only setting  since the end result will be the same (and  will run much faster). Note below that the in-sample normality results may differ slightly from the leave-one-out even when this may be unexpected (i.e., for the log transformation); this is due to slight differences in the standardization statistics.\n\n\nbestNormalize(x, allow_orderNorm = FALSE, out_of_sample = FALSE)\n\n\nBest Normalizing transformation with 250 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 4.401\n - Box-Cox: 0.7435\n - Center+scale: 8.087\n - Exp(x): 64.6975\n - Log_b(x+a): 3.47\n - sqrt(x + a): 0.9145\n - Yeo-Johnson: 1.7125\nEstimation method: In-sample\n \nBased off these, bestNormalize chose:\nStandardized Box Cox Transformation with 250 nonmissing obs.:\n Estimated statistics:\n - lambda = 0.3254863 \n - mean (before standardization) = -0.3659267 \n - sd (before standardization) = 0.9807881 \n\nAnother option to improve estimation efficiency is to use the built-in parallelization functionality. The repeated CV process can be parallelized via the  argument and the  and  (Gaujoux 2020) packages. A cluster can be set up with  and passed to  via the  argument.\n\n\ncl <- parallel::makeCluster(5)\nb <- bestNormalize(x, cluster = cl, r = 10, quiet = TRUE)\nparallel::stopCluster(cl)\n\n\n\nThe amount by which this parallelization will speed up the estimation of out-of-sample estimates depends (for the most part) on the number of repeats, the number of cores, and the sample size of the vector to be normalized. The plot below shows the estimation time for a run of  with 15 repeats of 10-fold CV on a gamma-distributed random variable with various sample sizes and numbers of cores.\n\n\n\n(#fig:parallel_timings)Potential speedup using parallelization functionality.\n\n\n\nImplementation with caret, recipes\nThe  and the  functions can be utilized in conjunction with the  package to preprocess data in machine learning workflows with  (Kuhn and Wickham 2020) or in combination with . The basic usage within  is shown below; for implementation with , refer to this paper’s application.\n\n\nrec <- recipe( ~ ., data = iris)  %>%                       # Initialize recipe\n  step_best_normalize(all_predictors(), -all_nominal()) %>% # Transform predictors\n  prep(iris) %>%                                            # Prep (train) recipe\n  bake(iris)                                                # Bake (apply) recipe\n\n\n\nOptions can be supplied to  to speed up or alter performance via the  argument, which passes a list of options to .\nAdditional customization\nTwo important means of customization are available: 1) users may add custom transformation functions to be assessed alongside the default suite of normalization methods, and 2) users may change the statistic used “under the hood” by  to estimate the departure from normality of the transformed data. This section contains examples and guidance for both extensions.\n1) Adding user-defined functions\nVia the  argument, users can use ’s machinery to compare custom, user-defined transformation functions to those included in the package. Below, I consider an example where a user may wish to compare the cube-root function with those provided in the package.  requires two functions to implement this: the transformation function and an associated  method. The custom cube-root transformation shown below is simple, but its skeleton can readily be made arbitrarily more complex.\n\n\n## Define custom function\ncuberoot_x <- function(x, ...) {\n  x.t <- (x)^(1/3)\n  \n  # Get in-sample normality statistic results\n  ptest <- nortest::pearson.test(x.t)\n  \n  val <- list(\n    x.t = x.t,\n    x = x,\n    n = length(x.t) - sum(is.na(x)), \n    norm_stat = unname(ptest$statistic / ptest$df)\n  )\n  \n  # Assign class, return\n  class(val) <- c('cuberoot_x')\n  val\n}\n\n# S3 method that is used to apply the transformation to newly observed data\npredict.cuberoot_x <- function(object, newdata = NULL, inverse = FALSE, ...) {\n  \n  # If no data supplied and not inverse\n  if (is.null(newdata) & !inverse)\n    newdata <- object$x\n  \n  # If no data supplied and inverse transformation is requested\n  if (is.null(newdata) & inverse)\n    newdata <- object$x.t\n  \n  # Perform inverse transformation\n  if (inverse) {\n    # Reverse-cube-root (cube)\n    val <-  newdata^3\n    \n    # Otherwise, perform transformation as estimated\n  } else if (!inverse) {\n    val <- (newdata)^(1/3)\n  }\n  \n  # Return transformed data\n  unname(val)\n}\n\n## Optional: print S3 method\nprint.cuberoot_x <- function(x, ...) {\n  cat('cuberoot(x) Transformation with', x$n, 'nonmissing obs.\\n')\n}\n\n\n\nThese functions can then be passed as a named list to :\n\n\ncustom_transform <- list(\n  cuberoot_x = cuberoot_x,\n  predict.cuberoot_x = predict.cuberoot_x,\n  print.cuberoot_x = print.cuberoot_x\n)\n\nset.seed(123129)\nx <- rgamma(100, 1, 1)\n(b <- bestNormalize(x = x, new_transforms = custom_transform))\n\n\nBest Normalizing transformation with 100 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.2347\n - Box-Cox: 1.0267\n - Center+scale: 2.0027\n - cuberoot_x: 0.9787\n - Exp(x): 4.7947\n - Log_b(x+a): 1.3547\n - orderNorm (ORQ): 1.1627\n - sqrt(x + a): 1.0907\n - Yeo-Johnson: 1.0987\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\ncuberoot(x) Transformation with 100 nonmissing obs.\n\nEvidently, the cube-root was the best normalizing transformation for this gamma-distributed random variable, performing comparably to the Box-Cox transformation.\n2) Re-defining normality\nThe question “what is normal?” outside of a statistical discussion is quite loaded and subjective. Even in statistical discussions, many authors have contributed to the question of how to best detect departures from normality; these solutions are diverse, and several have been implemented well in  already. In order to accommodate those with varying opinions on the best definition of normality, we have included a feature that allows users to specify a custom definition of a normality statistic. This customization can be accomplished via the  argument, which takes a function that will then be applied in lieu of the Pearson test statistic divided by its degree of freedom to assess normality.\nThe user-defined function must take an argument , which indicates the data on which a user wants to evaluate the statistic.\nHere is an example using the Lilliefors (Kolmogorov-Smirnov) normality test statistic:\n\n\nbestNormalize(x, norm_stat_fn = function(x) nortest::lillie.test(x)$stat)\n\n\nBest Normalizing transformation with 100 Observations\n Estimated Normality Statistics (using custom normalization statistic)\n - arcsinh(x): 0.1958\n - Box-Cox: 0.1785\n - Center+scale: 0.2219\n - Exp(x): 0.3299\n - Log_b(x+a): 0.1959\n - orderNorm (ORQ): 0.186\n - sqrt(x + a): 0.1829\n - Yeo-Johnson: 0.1872\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\nStandardized Box Cox Transformation with 100 nonmissing obs.:\n Estimated statistics:\n - lambda = 0.3281193 \n - mean (before standardization) = -0.1263882 \n - sd (before standardization) = 0.9913552 \n\nHere is an example using the Lillifors (Kolmogorov-Smirnov) normality test’s -value:\n\n\n(dont_do_this <- bestNormalize(x, norm_stat_fn = function(x) nortest::lillie.test(x)$p))\n\n\nBest Normalizing transformation with 100 Observations\n Estimated Normality Statistics (using custom normalization statistic)\n - arcsinh(x): 0.4327\n - Box-Cox: 0.4831\n - Center+scale: 0.2958\n - Exp(x): 0.0675\n - Log_b(x+a): 0.3589\n - orderNorm (ORQ): 0.4492\n - sqrt(x + a): 0.4899\n - Yeo-Johnson: 0.4531\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\nStandardized exp(x) Transformation with 100 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 6.885396 \n - sd (before standardization) = 13.66084 \n\nNote:  will attempt to minimize this statistic by default, which is definitely not what you want to do when calculating the -value. This is seen in the example above, where the worst normalization transformation, exponentiation, is chosen. In this case, a user is advised to either manually select the best one or reverse their defined normalization statistic (in this case by subtracting it from 1):\n\n\nbest_transform <- names(which.max(dont_do_this$norm_stats))\ndo_this <- dont_do_this$other_transforms[[best_transform]]\nor_this <- bestNormalize(x, norm_stat_fn = function(x) 1-nortest::lillie.test(x)$p)\n\n\n\nA -value for normality should not be routinely used as the sole selector of a normalizing transformation. A normality test’s -value, as a measure of the departure from normality, is confounded by the sample size (a high sample size may yield strong evidence of a practically insignificant departure from normality). Therefore, we suggest the statistic used should estimate the departure from normality rather the strength of evidence against normality (e.g., Royston 1991).\nApplication to Autotrader data\nBackground\nThe  data set was scraped from the autotrader website as part of this package (and because at the time of data collection in 2017, the package author needed to purchase a car). We apply the  functionality to de-skew mileage, age, and price in a pricing model. See  for more information on this data set.\n\n\ndata(\"autotrader\")\nautotrader$yearsold <- 2017 - autotrader$Year\n\n\n\n\n\nTable 1: Sample characteristics of autotrader data.\n\n\n\n\nOverall (N=6,283)\n\n\nMake\n\n\n\n\nAcura\n\n\n185 (2.9%)\n\n\nBuick\n\n\n252 (4.0%)\n\n\nChevrolet\n\n\n1,257 (20.0%)\n\n\nGMC\n\n\n492 (7.8%)\n\n\nHonda\n\n\n1,029 (16.4%)\n\n\nHyundai\n\n\n381 (6.1%)\n\n\nMazda\n\n\n272 (4.3%)\n\n\nNissan\n\n\n735 (11.7%)\n\n\nPontiac\n\n\n63 (1.0%)\n\n\nToyota\n\n\n1,202 (19.1%)\n\n\nVolkswagen\n\n\n415 (6.6%)\n\n\nPrice ($)\n\n\n\n\nMean (SD)\n\n\n17,145 (8,346)\n\n\nRange\n\n\n722 - 64,998\n\n\nMileage\n\n\n\n\nMean (SD)\n\n\n63,638 (49,125)\n\n\nRange\n\n\n2 - 325,556\n\n\nYear\n\n\n\n\nMean (SD)\n\n\n2011.9 (3.5)\n\n\nRange\n\n\n2000.0 - 2016.0\n\n\nAge (years old)\n\n\n\n\nMean (SD)\n\n\n5.1 (3.5)\n\n\nRange\n\n\n1.0 - 17.0\n\n\nTransform-both-sides regression\nTransform-both-sides (TBS) regression has several benefits that have been explored thoroughly elsewhere (see Harrell (2015) for an overview). Importantly, TBS regression can often (though not always) yield models that better satisfy assumptions of linear regression and mitigate the influence of outliers/skew. This approach has been shown to be useful in shrinking the size of prediction intervals while maintaining closer to nominal coverage in this data set (Peterson and Cavanaugh 2019).\nFirst, we will normalize the outcome (price).\n\n\n(priceBN <- bestNormalize(autotrader$price))\n\n\nBest Normalizing transformation with 6283 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 3.8573\n - Box-Cox: 2.2291\n - Center+scale: 3.5532\n - Log_b(x+a): 3.8573\n - orderNorm (ORQ): 1.1384\n - sqrt(x + a): 2.1977\n - Yeo-Johnson: 2.2291\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6283 nonmissing obs and ties\n - 2465 unique values \n - Original quantiles:\n   0%   25%   50%   75%  100% \n  722 11499 15998 21497 64998 \n\nWe can see that the estimated normality statistic for the ORQ transformation is close to 1, so we know it is performing quite well despite the ties in the data. It is also performing considerably better than all of the other transformations.\n\n\n(mileageBN <- bestNormalize(autotrader$mileage))\n\n\nBest Normalizing transformation with 6283 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 3.4332\n - Box-Cox: 3.0903\n - Center+scale: 14.7488\n - Log_b(x+a): 3.4354\n - orderNorm (ORQ): 1.1514\n - sqrt(x + a): 5.1041\n - Yeo-Johnson: 3.0891\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6283 nonmissing obs and ties\n - 6077 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n     2  29099  44800  88950 325556 \n\nSimilarly, the ORQ normalization performed best for mileage.\n\n\n(yearsoldBN <- bestNormalize(autotrader$yearsold))\n\n\nBest Normalizing transformation with 6283 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 83.2706\n - Box-Cox: 83.2909\n - Center+scale: 83.4324\n - Exp(x): 574.3318\n - Log_b(x+a): 83.0756\n - orderNorm (ORQ): 81.3615\n - sqrt(x + a): 83.4373\n - Yeo-Johnson: 84.0028\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6283 nonmissing obs and ties\n - 17 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n   1    3    4    7   17 \n\nFor age, we see something peculiar; none of the normalizing transformations performed well according to the normality statistics. By plotting the data, it becomes evident that the frequency of ties in age makes it very difficult to find a normalizing transformation (see figure below). Even so,  is chosen as it has the lowest estimated \\(P/DF\\) statistic.\n\n\n\n(#fig:hist_app)Distributions of car variables before and after normalization.\n\n\n\nNext, we will fit a linear model on the transformed values of each variable for our TBS regression. The reverse-transformation functions will allow us to visualize how these variables affect model predictions in terms of their original units.\n\n\np.t <- priceBN$x.t; m.t <- mileageBN$x.t; yo.t <- yearsoldBN$x.t\nfit <- lm(p.t ~ m.t + yo.t)\n\n\n\n\n\nTable 2: TBS regression results for autotrader data.\n\n\nVariable\n\n\nEstimate\n\n\nStd. Error\n\n\nt value\n\n\nPr(>|t|)\n\n\nIntercept\n\n\n0.005\n\n\n0.010\n\n\n0.553\n\n\n0.58\n\n\ng(Mileage)\n\n\n-0.234\n\n\n0.016\n\n\n-14.966\n\n\n< 0.001\n\n\ng(Age)\n\n\n-0.441\n\n\n0.016\n\n\n-27.134\n\n\n< 0.001\n\n\nUnsurprisingly, we find that there are very significant relationships between transformed car price, mileage, and age. However, to interpret these values, we must resort to visualizations since there is no inherent meaning of a “one-unit increase” in the ORQ normalized measurements. We utilize the  package (Breheny and Burchett 2017) to perform our visualizations, using  in conjunction with ’s  and  options to view the relationship in terms of the original unit for the response and covariate respectively (formatting omitted). For the sake of illustration, we have also plotted the estimated effect of a generalized additive (spline) model fit with  (Wood 2011).\n\n\nvisreg(fit, \"m.t\")\nvisreg(fit, \"m.t\", \n       partial = TRUE,\n       trans = function(price.t) \n         predict(priceBN, newdata = price.t, inverse = TRUE)/1000, \n       xtrans = function(mileage.t) \n         predict(mileageBN, newdata = mileage.t, inverse = TRUE)\n       )\n\n\n\n\n\n\n(#fig:linear_visreg)TBS regression visualized on transformed units (left) and original units (right).\n\n\n\nBelow, we visualize the age effect, demonstrating how one might visualize the effect outside of  (plot formatting is omitted).\n\n\n# Set up data for plotting line\nnew_yo <- seq(min(autotrader$yearsold), max(autotrader$yearsold), len = 100)\nnewX <- data.frame(yearsold = new_yo, mileage = median(autotrader$mileage))\nnewXt <- data.frame(yo.t = predict(yearsoldBN, newX$yearsold), \n                    m.t = predict(mileageBN, newX$mileage))\n\nline_vals_t <- predict(fit, newdata = newXt) # Calculate line (transformed)\nline_vals <- predict(priceBN, newdata = line_vals_t, inverse = TRUE)\nplot(autotrader$yearsold, autotrader$price)\nlines(new_yo, line_vals)\n\n\n\n\n\n\n(#fig:gam_tbs_model)Age effect on car price (re-transformed to original unit).\n\n\n\nImplementation with recipes\nTo build a predictive model for the price variable that uses each vehicle’s model and make in addition to its mileage and age, we can utilize the  and  functionality to do so. This section outlines how to use  in conjunction with these other popular ML packages. Price is logged instead of ORQ transformed in order to facilitate the interpretation of measures for prediction accuracy.\n\n\nlibrary(tidymodels)\nlibrary(caret)\nlibrary(recipes)\n\nset.seed(321)\ndf_split <- initial_split(autotrader, prop = .9)\ndf_train <- training(df_split)\ndf_test <- testing(df_split)\n\nrec <- recipe(price ~ Make + model +  mileage + status + Year, df_train) %>% \n  step_mutate(years_old = 2017 - Year) %>% \n  step_rm(Year) %>% \n  step_log(price) %>% \n  step_best_normalize(all_predictors(), -all_nominal()) %>% \n  step_other(all_nominal(), threshold = 10) %>% \n  step_dummy(all_nominal()) %>% \n  prep()\n\nfit1 <- train(price ~ ., bake(rec, NULL), method = 'glmnet')\nfit2 <- train(price ~ ., bake(rec, NULL), method = 'earth')\nfit3 <- train(price ~ ., bake(rec, NULL), method = 'rf')\n\nr <- resamples(fits <- list(glmnet = fit1, earth = fit2, rf = fit3))\nsummary(r) # Extra-sample CV results\n\n\n\n\n\nTable 3: CV prediction accuracy of various ML methods.\n\n\n\n\nMin.\n\n\n1st Qu.\n\n\nMedian\n\n\nMean\n\n\n3rd Qu.\n\n\nMax.\n\n\nNA’s\n\n\nMAE\n\n\nglmnet\n\n\n0.181\n\n\n0.184\n\n\n0.186\n\n\n0.189\n\n\n0.194\n\n\n0.198\n\n\n0\n\n\nearth\n\n\n0.147\n\n\n0.151\n\n\n0.154\n\n\n0.155\n\n\n0.158\n\n\n0.163\n\n\n0\n\n\nrf\n\n\n0.136\n\n\n0.141\n\n\n0.143\n\n\n0.144\n\n\n0.147\n\n\n0.157\n\n\n0\n\n\nRMSE\n\n\nglmnet\n\n\n0.242\n\n\n0.247\n\n\n0.252\n\n\n0.256\n\n\n0.264\n\n\n0.276\n\n\n0\n\n\nearth\n\n\n0.203\n\n\n0.209\n\n\n0.214\n\n\n0.217\n\n\n0.226\n\n\n0.235\n\n\n0\n\n\nrf\n\n\n0.193\n\n\n0.208\n\n\n0.213\n\n\n0.210\n\n\n0.215\n\n\n0.217\n\n\n0\n\n\nRSQ\n\n\nglmnet\n\n\n0.767\n\n\n0.772\n\n\n0.785\n\n\n0.782\n\n\n0.789\n\n\n0.801\n\n\n0\n\n\nearth\n\n\n0.807\n\n\n0.833\n\n\n0.845\n\n\n0.842\n\n\n0.855\n\n\n0.864\n\n\n0\n\n\nrf\n\n\n0.835\n\n\n0.845\n\n\n0.855\n\n\n0.854\n\n\n0.860\n\n\n0.873\n\n\n0\n\n\nEvidently, the random forest generally performed better in cross-validated prediction metrics, achieving a higher R-squared (RSQ), lower root-mean-squared error (RMSE), and lower mean absolute error (MAE). Since price was logged, RMSE and MAE are on the log scale. For the test set, we calculate these quantities in price’s original unit (2017 US dollars) using the  package (Kuhn and Vaughan 2020).\n\n\n# Out of sample prediction accuracy\nresults <- lapply(fits, function(x) {\n  p <- c(predict(x, newdata = bake(rec, df_test)))\n  yardstick::metrics(data.frame(est = exp(p), truth = df_test$price), \n                     truth = truth, estimate = est)\n})\nresults\n\n\n\n\n\nTable 4: Test data prediction accuracy of various ML methods. RMSE and MAE can be interpreted in terms of 2017 US dollars.\n\n\nMethod\n\n\nRMSE\n\n\nRSQ\n\n\nMAE\n\n\nglmnet\n\n\n4076\n\n\n0.772\n\n\n2847\n\n\nearth\n\n\n3619\n\n\n0.814\n\n\n2500\n\n\nrf\n\n\n3257\n\n\n0.853\n\n\n2294\n\n\nAfter normalization of mileage and age, a random forest had the optimal predictive performance on car price given a car’s make, model, age, and mileage compared to other ML models, achieving out-of-sample R-squared 0.853 on a left-out test data set. We conjecture that the random forest performs best because it can better capture differential depreciation by make and model than the other methods.\nDiscussion\nWe have shown how the  package can effectively and efficiently find the best normalizing transformation for a vector or set of vectors. However, normalization is by no means something that should be applied universally and without motivation. In situations where units have meaning, normalizing prior to analysis can contaminate the relationships suspected in the data and/or reduce predictive accuracy. Further, depending on the type of transformations used, interpreting regression coefficients post-transformation can be difficult or impossible without using a figure since the transformation function itself will look completely different for different distributions. So, while normalization transformations may well be able to increase the robustness of results and mitigate violations to the classical linear regression assumption of Gaussian residuals, it is by no means a universal solution.\nOn the other hand, when hypotheses are exploratory or when data is of poor quality with high amounts of skew/outliers, normalization can be an effective means of mitigating downstream issues this can cause in the analyses. For example, in machine learning contexts, some predictor manipulations rely on second-order statistics (e.g., principal components analysis or partial least squares), for which the variance calculation can be sensitive to skew and outliers. Normalizing transformations can improve the quality and stability of these calculations. Similarly, predictor normalization reduces the tendency for high-leverage points to have their leverage propagated into engineered features such as interactions or polynomials. Ultimately, these benefits can often produce predictive models that are more robust and stable.\nWe focused on making this package useful in a variety of machine learning workflows. We are enthusiastic in our support of , and will continue to maintain the package while it is found to be useful by R users. We hope to continue to build up the repertoire of candidate transformations using the same infrastructure so that additional ones can be considered by default in the future.\nReferences\n\n\n\nBartlett, M. S. 1947. “The Use of Transformations.” Biometrics 3 (1): 39–52. https://doi.org/10.2307/3001536.\n\n\nBickel, Peter J., and Kjell A. Doksum. 1981. “An Analysis of Transformations Revisited.” Journal of the American Statistical Association 76 (374): 296–311. https://doi.org/10.1080/01621459.1981.10477649.\n\n\nBox, G. E. P., and D. R. Cox. 1964. “An Analysis of Transformations.” Journal of the Royal Statistical Society. Series B (Methodological) 26 (2): 211–52. https://doi.org/10.2307/2984418.\n\n\nBreheny, Patrick, and Woodrow Burchett. 2017. “Visualization of Regression Models Using Visreg.” The R Journal 9 (2): 56–71.\n\n\nD’Agostino, Ralph B. 1986. Goodness-of-Fit-Techniques. Vol. 68. CRC press.\n\n\nGaujoux, Renaud. 2020. doRNG: Generic Reproducible Parallel Backend for ’Foreach’ Loops. https://CRAN.R-project.org/package=doRNG.\n\n\nGoerg, Georg M. 2011. “Lambert w Random Variables-a New Family of Generalized Skewed Distributions with Applications to Risk Estimation.” The Annals of Applied Statistics 5 (3): 2197–2230. https://doi.org/10.1214/11-AOAS457.\n\n\nGross, Juergen, and Uwe Ligges. 2015. Nortest: Tests for Normality. https://CRAN.R-project.org/package=nortest.\n\n\nHarrell, Frank E. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. Springer.\n\n\nJohn, J. A., and N. R. Draper. 1980. “An Alternative Family of Transformations.” Journal of the Royal Statistical Society. Series C (Applied Statistics) 29 (2): 190–97. https://doi.org/10.2307/2986305.\n\n\nKuhn, Max. 2017. Caret: Classification and Regression Training. https://CRAN.R-project.org/package=caret.\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. Springer. https://doi.org/10.1007/978-1-4614-6849-3.\n\n\nKuhn, Max, and Davis Vaughan. 2020. Yardstick: Tidy Characterizations of Model Performance. https://CRAN.R-project.org/package=yardstick.\n\n\nKuhn, Max, and Hadley Wickham. 2018. Recipes: Preprocessing Tools to Create Design Matrices. https://CRAN.R-project.org/package=recipes.\n\n\n———. 2020. Tidymodels: Easily Install and Load the ’Tidymodels’ Packages. https://CRAN.R-project.org/package=tidymodels.\n\n\nManly, B. F. J. 1976. “Exponential Data Transformations.” Journal of the Royal Statistical Society. Series D (The Statistician) 25 (1): 37–42. https://doi.org/10.2307/2988129.\n\n\nPeterson, Ryan A., and Joseph E. Cavanaugh. 2019. “Ordered Quantile Normalization: A Semiparametric Transformation Built for the Cross-Validation Era.” Journal of Applied Statistics, 1–16. https://doi.org/10.1080/02664763.2019.1630372.\n\n\nRoyston, Patrick. 1991. “Estimating Departure from Normality.” Statistics in Medicine 10 (8): 1283–93. https://doi.org/https://doi.org/10.1002/sim.4780100811.\n\n\nThode, Henry C. 2002. Testing for Normality. Vol. 164. CRC press.\n\n\nVan der Waerden, BL. 1952. “Order Tests for the Two-Sample Problem and Their Power.” In Indagationes Mathematicae (Proceedings), 55:453–58. Elsevier.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. http://www.stats.ox.ac.uk/pub/MASS4.\n\n\nWood, S. N. 2011. “Fast Stable Restricted Maximum Likelihood and Marginal Likelihood Estimation of Semiparametric Generalized Linear Models.” Journal of the Royal Statistical Society (B) 73 (1): 3–36.\n\n\nYeo, In‐Kwon, and Richard A. Johnson. 2000. “A New Family of Power Transformations to Improve Normality or Symmetry.” Biometrika 87 (4): 954–59. https://doi.org/10.1093/biomet/87.4.954.\n\n\n\n\n",
    "preview": "articles/RJ-2021-041/figs/orq_vis-1.png",
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 960
  },
  {
    "path": "articles/RJ-2021-042/",
    "title": "Package wsbackfit for Smooth Backfitting Estimation of Generalized Structured Models",
    "description": "A package is introduced that provides the weighted smooth backfitting estimator for\na large family of popular semiparametric regression models. This family is known as generalized\nstructured models, comprising, for example, generalized varying coefficient model, generalized\nadditive models, mixtures, potentially including parametric parts. The kernel-based weighted\nsmooth backfitting belongs to the statistically most efficient procedures for this model class. Its\nasymptotic properties are well-understood thanks to the large body of literature about this estimator.\nThe introduced weights allow for the inclusion of sampling weights, trimming, and efficient estimation\nunder heteroscedasticity. Further options facilitate easy handling of aggregated data, prediction,\nand the presentation of estimation results. Cross-validation methods are provided which can be used\nfor model and bandwidth selection.1",
    "author": [
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      },
      {
        "name": "María Xosé Rodríguez-Álvarez",
        "url": {}
      },
      {
        "name": "Stefan Sperlich",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-057/",
    "title": "garchx: Flexible and Robust GARCH-X Modeling",
    "description": "The garchx package provides a user-friendly, fast, flexible, and robust framework for the\n estimation and inference of GARCH(p, q, r)-X models, where p is the ARCH order, q is the GARCH\n order, r is the asymmetry or leverage order, and ’X’ indicates that covariates can be included. Quasi\n Maximum Likelihood (QML) methods ensure estimates are consistent and standard errors valid,\n even when the standardized innovations are non-normal or dependent, or both. Zero-coefficient\n restrictions by omission enable parsimonious specifications, and functions to facilitate the non-standard\n inference associated with zero-restrictions in the null-hypothesis are provided. Finally, in the formal\n comparisons of precision and speed, the garchx package performs well relative to other prominent\n GARCH-packages on CRAN.",
    "author": [
      {
        "name": "Genaro Sucarrat",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-034/",
    "title": "Statistical Quality Control with the qcr Package",
    "description": "The R package qcr for Statistical Quality Control (SQC) is introduced and described. It\n     includes a comprehensive set of univariate and multivariate SQC tools that completes and increases\n     the SQC techniques available in R. Apart from integrating different R packages devoted to SQC (qcc,\n     MSQC), qcr provides nonparametric tools that are highly useful when Gaussian assumption is not\n     met. This package computes standard univariate control charts for individual measurements, x̄, S, R,\n     p, np, c, u, EWMA, and CUSUM. In addition, it includes functions to perform multivariate control\n     charts such as Hotelling T2 , MEWMA and MCUSUM. As representative features, multivariate\n     nonparametric alternatives based on data depth are implemented in this package: r, Q and S control\n     charts. The qcr library also estimates the most complete set of capability indices from first to\n     the fourth generation, covering the nonparametric alternatives, and performing the corresponding\n     capability analysis graphical outputs, including the process capability plots. Moreover, Phase I and\n     II control charts for functional data are included.\n\n    Prácticas de CEC con R",
    "author": [
      {
        "name": "Miguel Flores",
        "url": {}
      },
      {
        "name": "Rubén Fernández-Casal",
        "url": {}
      },
      {
        "name": "Salvador Naya",
        "url": {}
      },
      {
        "name": "Javier Tarrío-Saavedra",
        "url": {}
      }
    ],
    "date": "2020-05-07",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-030/",
    "title": "Linear Regression with Stationary Errors: the R Package slm",
    "description": "This paper introduces the R package slm, which stands for Stationary Linear Models.\nThe package contains a set of statistical procedures for linear regression in the general context where\nthe error process is strictly stationary with a short memory. We work in the setting of Hannan (1973),\nwho proved the asymptotic normality of the (normalized) least squares estimators (LSE) under\nvery mild conditions on the error process. We propose different ways to estimate the asymptotic\ncovariance matrix of the LSE and then to correct the type I error rates of the usual tests on the\nparameters (as well as confidence intervals). The procedures are evaluated through different sets of\nsimulations.",
    "author": [
      {
        "name": "Emmanuel Caron",
        "url": {}
      },
      {
        "name": "Jérôme Dedecker",
        "url": {}
      },
      {
        "name": "Bertrand Michel",
        "url": {}
      }
    ],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-032/",
    "title": "clustcurv: An R Package for Determining Groups in Multiple Curves ",
    "description": "In many situations, it could be interesting to ascertain whether groups of curves can be\n performed, especially when confronted with a considerable number of curves. This paper introduces\n an R package, known as clustcurv, for determining clusters of curves with an automatic selection of\n their number. The package can be used for determining groups in multiple survival curves as well as\n for multiple regression curves. Moreover, it can be used with large numbers of curves. An illustration\n of the use of clustcurv is provided, using both real data examples and artificial data.\n Keywords: multiple curves, number of groups, nonparametric, survival analysis, regression models,\n cluster",
    "author": [
      {
        "name": "Nora M. Villanueva",
        "url": {}
      },
      {
        "name": "Marta Sestelo",
        "url": {}
      },
      {
        "name": "Luis Meira-Machado",
        "url": {}
      },
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      }
    ],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-033/",
    "title": "Benchmarking R packages for Calculation of Persistent Homology",
    "description": "Several persistent homology software libraries have been implemented in R. Specifically,\n the Dionysus, GUDHI, and Ripser libraries have been wrapped by the TDA and TDAstats CRAN\n packages. These software represent powerful analysis tools that are computationally expensive and, to\n our knowledge, have not been formally benchmarked. Here, we analyze runtime and memory growth\n for the 2 R packages and the 3 underlying libraries. We find that datasets with less than 3 dimensions\n can be evaluated with persistent homology fastest by the GUDHI library in the TDA package. For\n higher-dimensional datasets, the Ripser library in the TDAstats package is the fastest. Ripser and\n TDAstats are also the most memory-efficient tools to calculate persistent homology.",
    "author": [
      {
        "name": "Eashwar V. Somasundaram",
        "url": {}
      },
      {
        "name": "Shael E. Brown",
        "url": {}
      },
      {
        "name": "Adam Litzler",
        "url": {}
      },
      {
        "name": "Jacob G. Scott",
        "url": {}
      },
      {
        "name": "Raoul R. Wadhwa",
        "url": {}
      }
    ],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-029/",
    "title": "Wide-to-tall Data Reshaping Using Regular Expressions and the nc Package",
    "description": "Regular expressions are powerful tools for extracting tables from non-tabular text data.\n Capturing regular expressions that describe the information to extract from column names can be\n especially useful when reshaping a data table from wide (few rows with many regularly named\n columns) to tall (fewer columns with more rows). We present the R package nc (short for named\n capture), which provides functions for wide-to-tall data reshaping using regular expressions. We\n describe the main new ideas of nc, and provide detailed comparisons with related R packages (stats,\n utils, data.table, tidyr, tidyfast, tidyfst, reshape2, cdata).",
    "author": [
      {
        "name": "Toby Dylan Hocking",
        "url": {}
      }
    ],
    "date": "2020-04-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-062/",
    "title": "Unidimensional and Multidimensional Methods for Recurrence Quantification Analysis with crqa",
    "description": "Recurrence quantification analysis is a widely used method for characterizing patterns in\n time series. This article presents a comprehensive survey for conducting a wide range of recurrence\nbased analyses to quantify the dynamical structure of single and multivariate time series and capture\n coupling properties underlying leader-follower relationships. The basics of recurrence quantification\n analysis (RQA) and all its variants are formally introduced step-by-step from the simplest auto\nrecurrence to the most advanced multivariate case. Importantly, we show how such RQA methods can\n be deployed under a single computational framework in R using a substantially renewed version of\n our crqa 2.0 package. This package includes implementations of several recent advances in recurrence\nbased analysis, among them applications to multivariate data and improved entropy calculations\n for categorical data. We show concrete applications of our package to example data, together with a\n detailed description of its functions and some guidelines on their usage.",
    "author": [
      {
        "name": "Moreno I. Coco",
        "url": {}
      },
      {
        "name": "Dan Mønster",
        "url": {}
      },
      {
        "name": "Giuseppe Leonardi",
        "url": {}
      },
      {
        "name": "Rick Dale",
        "url": {}
      },
      {
        "name": "Sebastian Wallot",
        "url": {}
      }
    ],
    "date": "2020-04-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-065/",
    "title": "The bdpar Package: Big Data Pipelining Architecture for R",
    "description": "In the last years, big data has become a useful paradigm for taking advantage of multiple\n sources to find relevant knowledge in real domains (such as the design of personalized marketing\n campaigns or helping to palliate the effects of several fatal diseases). Big data programming tools and\n methods have evolved over time from a MapReduce to a pipeline-based archetype. Concretely the use\n of pipelining schemes has become the most reliable way of processing and analyzing large amounts of\n data. To this end, this work introduces bdpar, a new highly customizable pipeline-based framework\n (using the OOP paradigm provided by R6 package) able to execute multiple preprocessing tasks over\n heterogeneous data sources. Moreover, to increase the flexibility and performance, bdpar provides\n helpful features such as (i) the definition of a novel object-based pipe operator (%>|%), (ii) the ability to\n easily design and deploy new (and customized) input data parsers, tasks, and pipelines, (iii) only-once\n execution which avoids the execution of previously processed information (instances), guaranteeing\n that only new both input data and pipelines are executed, (iv) the capability to perform serial or\n parallel operations according to the user needs, (v) the inclusion of a debugging mechanism which\n allows users to check the status of each instance (and find possible errors) throughout the process.",
    "author": [
      {
        "name": "Miguel Ferreiro-Díaz",
        "url": {}
      },
      {
        "name": "Tomás R. Cotos-Yáñez",
        "url": {}
      },
      {
        "name": "José R. Méndez",
        "url": {}
      },
      {
        "name": "David Ruano-Ordás",
        "url": {}
      }
    ],
    "date": "2020-04-30",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-028/",
    "title": "JMcmprsk: An R Package for Joint Modelling of Longitudinal and Survival Data with Competing Risks",
    "description": "In this paper, we describe an R package named JMcmprsk, for joint modelling of longitudinal\n and survival data with competing risks. The package in its current version implements two joint\n models of longitudinal and survival data proposed to handle competing risks survival data together\n with continuous and ordinal longitudinal outcomes respectively (Elashoff et al., 2008; Li et al., 2010).\n The corresponding R implementations are further illustrated with real examples. The package also\n provides simulation functions to simulate datasets for joint modelling with continuous or ordinal\n outcomes under the competing risks scenario, which provide useful tools to validate and evaluate\n new joint modelling methods.",
    "author": [
      {
        "name": "Hong Wang",
        "url": {}
      },
      {
        "name": "Ning Li",
        "url": {}
      },
      {
        "name": "Shanpeng Li",
        "url": {}
      },
      {
        "name": "Gang Li",
        "url": {}
      }
    ],
    "date": "2020-04-04",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-031/",
    "title": "exPrior: An R Package for the Formulation of Ex-Situ Priors",
    "description": "The exPrior package implements a procedure for formulating informative priors of geo\nstatistical properties for a target field site, called ex-situ priors and introduced in Cucchi et al. (2019).\n The procedure uses a Bayesian hierarchical model to assimilate multiple types of data coming from\n multiple sites considered as similar to the target site. This prior summarizes the information contained\n in the data in the form of a probability density function that can be used to better inform further\n geostatistical investigations at the site. The formulation of the prior uses ex-situ data, where the data\n set can either be gathered by the user or come in the form of a structured database. The package is\n designed to be flexible in that regard. For illustration purposes and for easiness of use, the package is\n ready to be used with the worldwide hydrogeological parameter database (WWHYPDA) Comunian\n and Renard (2009).",
    "author": [
      {
        "name": "Falk Heße",
        "url": {}
      },
      {
        "name": "Karina Cucchi",
        "url": {}
      },
      {
        "name": "Nura Kawa",
        "url": {}
      },
      {
        "name": "Yoram Rubin",
        "url": {}
      }
    ],
    "date": "2020-04-04",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-061/",
    "title": "penPHcure: Variable Selection in Proportional Hazards Cure Model with Time-Varying Covariates",
    "description": "We describe the penPHcure R package, which implements the semiparametric proportional\nhazards (PH) cure model of Sy and Taylor (2000) extended to time-varying covariates and the variable\n selection technique based on its SCAD-penalized likelihood proposed by Beretta and Heuchenne\n (2019a). In survival analysis, cure models are a useful tool when a fraction of the population is likely to\n be immune from the event of interest. They can separate the effects of certain factors on the probability\n of being susceptible and on the time until the occurrence of the event. Moreover, the penPHcure\n package allows the user to simulate data from a PH cure model, where the event-times are generated\n on a continuous scale from a piecewise exponential distribution conditional on time-varying covariates,\n with a method similar to Hendry (2014). We present the results of a simulation study to assess the\n finite sample performance of the methodology and illustrate the functionalities of the penPHcure\n package using criminal recidivism data.",
    "author": [
      {
        "name": "Alessandro Beretta",
        "url": {}
      },
      {
        "name": "Cédric Heuchenne",
        "url": {}
      }
    ],
    "date": "2020-02-14",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-056/",
    "title": "A Method for Deriving Information from Running R Code",
    "description": "It is often useful to tap information from a running R script. Obvious use cases include\n monitoring the consumption of resources (time, memory) and logging. Perhaps less obvious cases\n include tracking changes in R objects or collecting the output of unit tests. In this paper, we demonstrate\n an approach that abstracts the collection and processing of such secondary information from the\n running R script. Our approach is based on a combination of three elements. The first element is\n to build a customized way to evaluate code. The second is labeled local masking and it involves\n temporarily masking a user-facing function so an alternative version of it is called. The third element\n we label local side effect. This refers to the fact that the masking function exports information to the\n secondary information flow without altering a global state. The result is a method for building systems\n in pure R that lets users create and control secondary flows of information with minimal impact on\n their workflow and no global side effects.",
    "author": [
      {
        "name": "Mark P.J. van der Loo",
        "url": {}
      }
    ],
    "date": "2019-10-20",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-027/",
    "title": "npcure: An R Package for Nonparametric Inference in Mixture Cure Models",
    "description": "Mixture cure models have been widely used to analyze survival data with a cure fraction.\n They assume that a subgroup of the individuals under study will never experience the event (cured\n subjects). So, the goal is twofold: to study both the cure probability and the failure time of the\n uncured individuals through a proper survival function (latency). The R package npcure implements a\n completely nonparametric approach for estimating these functions in mixture cure models, considering\n right-censored survival times. Nonparametric estimators for the cure probability and the latency as\n functions of a covariate are provided. Bootstrap bandwidth selectors for the estimators are included.\n The package also implements a nonparametric covariate significance test for the cure probability,\n which can be applied with a continuous, discrete, or qualitative covariate.",
    "author": [
      {
        "name": "Ana López-Cheda",
        "url": {}
      },
      {
        "name": "M. Amalia Jácome",
        "url": {}
      },
      {
        "name": "Ignacio López-de-Ullibarri",
        "url": {}
      }
    ],
    "date": "2019-07-19",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-026/",
    "title": "SEEDCCA: An Integrated R-Package for Canonical Correlation Analysis and Partial Least Squares",
    "description": "Canonical correlation analysis (CCA) has a long history as an explanatory statistical method\n in high-dimensional data analysis and has been successfully applied in many scientific fields such as\n chemometrics, pattern recognition, genomic sequence analysis, and so on. The so-called seedCCA is a\n newly developed R package that implements not only the standard and seeded CCA but also partial\n least squares. The package enables us to fit CCA to large-p and small-n data. The paper provides a\n complete guide. Also, the seeded CCA application results are compared with the regularized CCA in\n the existing R package. It is believed that the package, along with the paper, will contribute to high\ndimensional data analysis in various science field practitioners and that the statistical methodologies\n in multivariate analysis become more fruitful.",
    "author": [
      {
        "name": "Bo-Young Kim",
        "url": {}
      },
      {
        "name": "Yunju Im",
        "url": {}
      },
      {
        "name": "Jae Keun Yoo",
        "url": {}
      }
    ],
    "date": "2019-07-18",
    "categories": [],
    "contents": "\n\n\nd-title h1, d-title p, d-title figure {\n  grid-column: page;\n}\n\n\n  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:46:30+10:00",
    "input_file": {}
  }
]
