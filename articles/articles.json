[
  {
    "path": "articles/RJ-2022-023/",
    "title": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R",
    "description": "Longitudinal (panel) data provide the opportunity to examine temporal patterns of individuals, because measurements are collected on the same person at different, and often irregular, time points. The data is typically visualised using a \"spaghetti plot\", where a line plot is drawn for each individual. When overlaid in one plot, it can have the appearance of a bowl of spaghetti. With even a small number of subjects, these plots are too overloaded to be read easily. The interesting aspects of individual differences are lost in the noise. Longitudinal data is often modelled with a hierarchical linear model to capture the overall trends, and variation among individuals, while accounting for various levels of dependence. However, these models can be difficult to fit, and can miss unusual individual patterns. Better visual tools can help to diagnose longitudinal models, and better capture the individual experiences. This paper introduces the R package, brolgar (BRowse over Longitudinal data Graphically and Analytically in R), which provides tools to identify and summarise interesting individual patterns in longitudinal data.",
    "author": [
      {
        "name": "Nicholas Tierney",
        "url": "https://njtierney.com"
      },
      {
        "name": "Dianne Cook",
        "url": "https://dicook.org"
      },
      {
        "name": "Tania Prvan",
        "url": {}
      }
    ],
    "date": "2022-10-12",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\nThis paper is about exploring longitudinal data effectively. By “longitudinal data” we specifically mean individuals repeatedly measured through time. This could include panel data, where possibly different samples from a key variable (e.g. country), are aggregated at each time collection. The important component is a key variable with repeated measurements regularly, or irregularly over time. The inherent structure allows us to examine temporal patterns of individuals, shown in Figure 1, of the average height of Australian males over years. The individual component is country, and the time component is year. The variable country along with other variables is measured repeatedly from 1900 to 1970, with irregular intervals between years.\n\n\n\nFigure 1: Example of longitudinal data: average height of men in Australia for 1900-1970. The height increase over time, and are measured at irregular intervals.\n\n\n\nThe full dataset of Figure 1 is shown in Figure 2, showing 144 countries from the year 1700. This plot is challenging to understand because there is overplotting, making it hard to see the individuals. Solutions to this are not always obvious. Showing separate individual plots of each country does not help, as 144 plots is too many to comprehend. Making the lines transparent or fitting a simple model to all the data Figure 2B, might be a common first step to see common trends. However, all this seems to clarify is: 1) There is a set of some countries that are similar, and they are distributed around the center of the countries, and 2) there is a general upward trend in heights over time. We learn about the collective, but lose sight of the individuals.\n\n\n\nFigure 2: The full dataset shown as a spaghetti plot (A), with transparency (B), and with a linear model overlayed (C). It is still hard to see the individuals.\n\n\n\nThis paper demonstrates how to effectively and efficiently explore longitudinal data, using the R package, brolgar. We examine four problems in exploring longitudinal data:\nHow to sample the data\nFinding interesting individuals\nFinding representative individuals\nUnderstanding a model\nThis paper proceeds in the following way: first, a brief review of existing approaches to longitudinal data, then the definition of longitudinal data, then approaches to these four problems are discussed, followed by a summary.\nBackground\nR provides basic time series, ts, objects, which are vectors or matrices that represent data sampled at equally spaced points in time. These have been extended through packages such as xts, and zoo (Zeileis and Grothendieck 2005; Ryan and Ulrich 2020), which only consider data in a wide format with a regular implied time series. These are not appropriate for longitudinal data, which can have indexes that are not time unit oriented, such as “Wave 1…n”, or may contain irregular intervals.\nOther packages focus more directly on panel data in R, focussing on data operations and model interfaces. The pmdplyr package provides “Panel Manoeuvres” in dplyr(Huntington-Klein and Khor 2020). It defines the data structure in as a pibble object (panel tibble), requiring an id and group column being defined to identify the unique identifier and grouping. The pmdplyr package focuses on efficient and custom joins and functions, such as inexact_left_join(). It does not implement tidyverse equivalent tools, but instead extends their usecase with a new function, for example mutate_cascade and mutate_subset. The panelr package provides an interface for data reshaping on panel data, providing widening and lengthening functions (widen_panel() and long_panel() (Long 2020)). It also provides model facilitating functions by providing its own interface for mixed effects models. The plm package (Millo 2017) for panel data econometrics provides methods for estimating models such as GMM for panel data, and testing, for example for model specification or serial correlation. It also provides a data structure, the pdata.frame, which stores the index attribute of the individual and time dimensions, for use within the package’s functions.\nThese software generally re-implement their own custom panel data class object, as well as custom data cleaning tasks, such as reshaping into long and wide form. They all share similar features, providing some identifying or index variable, and some grouping or key.\nLongitudinal Data Structures\nLongitudinal data is a sibling of many other temporal data forms, including panel data, repeated measures, and time series. The differences are many, and can be in data collection, context and even the field of research. Time series are usually long and regularly spaced in time. Panel data may measure different units at each time point and aggregate these values by a categorical or key variable. Repeated measures typically measure before and after treatment effects. We like to think of longitudinal as measuring the same individual (e.g. wage earner) over time, but this definition is not universally agreed on. Despite the differences, they all share a fundamental similarity: they are measurements over a time period.\nThis time period has structure - the time component (dates, times, waves, seconds, etc), and the spacing between measurements - unequal or equal. This data structure needs to be respected during analysis to preserve the lowest level of granularity, to avoid for example, collapsing across month when the data is collected every second, or assuming measurements occur at fixed time intervals. These mistakes can be avoided by encoding the data structure into the data itself. This information can then be accessed by analysis tools, providing a consistent way to understand and summarise the data. This ensures the different types of longitudinal data previously mentioned can be handled in the same way.\nBuilding on a tsibble\nSince longitudinal data can be thought of as “individuals repeatedly measured through time”, they can be considered as a type of time series, as defined in Hyndman and Athanasopoulos (2018): “Anything that is observed sequentially over time is a time series”. This definition has been realised as a time series tsibble in (Wang et al. 2020). These objects are defined as data meeting these conditions:\nThe index: the time variable\nThe key: variable(s) defining individual groups (or series)\nThe index and key (1 + 2) together determine a distinct row\nIf the specified key and index pair do not define a distinct row - for example, if there are duplicates in the data, the tsibble will not be created. This helps ensure the data is properly understood and cleaned before analysis is conducted, removing avoidable errors that might have impacted downstream decisions.\nWe can formally define our heights data from Figure 1 as a tsibble using, as_tsibble:\n\n\nheights_brolgar <- as_tsibble(heights_brolgar,\n                      index = year,\n                      key = country,\n                      regular = FALSE)\n\n\nThe index is year, the key is country, and regular = FALSE since the intervals in the years measured are not regular. Using a tsibble means that the index and key time series information is recorded only once, and can be referred to many times in other parts of the data analysis by time-aware tools.\nIn addition to providing consistent ways to manipulate time series data, further benefits to building on tsibble are how it works within the tidyverse ecosystem, as well as the tidy time series packages called “tidyverts”, containing fable (O’Hara-Wild et al. 2020a), feasts, (O’Hara-Wild et al. 2020b). For example, tsibble provides modified tidyverse functions to explore implicit missing values in the index (e.g., has_gaps() and fill_gaps()), as well as grouping and partitioning based on the index with index_by(). For full details and examples of use with the tidyverts time series packages, see Wang et al. (2020).\nThe brolgar package uses tsibble so users can take advantage of these tools, learning one way of operating a data analysis that will work and have overlap with other contexts.\nCharacterising Individual Series\nCalculating a feature\nWe can summarise the individual series by collapsing their many measurements into a single statistic, such as the minimum, maximum, or median, with one row per key. We do this with the features function from the fabletools package, made available in brolgar. This provides a summary of a given variable, accounting for the time series structure, and returning one row per key specified. It can be thought of as a time-series aware variant of the summarise function from dplyr. The feature function works by specifying the data, the variable to summarise, and the feature to calculate. A template is shown below\nfeatures(<DATA>, <VARIABLE>, <FEATURE>)\nor, with the pipe:\n<DATA> %>% features(<VARIABLE>, <FEATURE>)\nFor example, to calculate the minimum height for each key (country), in heights, we specify the heights data, then the variable to calculate features on, height_cm, then the feature to calculate, min (we write c(min = min) so the column calculated gets the name “min”):\n\n\nheights_min <- features(.tbl = heights_brolgar, \n                        .var = height_cm, \n                        features = c(min = min))\n\nheights_min\n\n# A tibble: 119 × 2\n   country       min\n   <chr>       <dbl>\n 1 Afghanistan  161.\n 2 Algeria      166.\n 3 Angola       159.\n 4 Argentina    167.\n 5 Armenia      164.\n 6 Australia    170 \n 7 Austria      162.\n 8 Azerbaijan   170.\n 9 Bangladesh   160.\n10 Belgium      163.\n# … with 109 more rows\n\nWe call these summaries features of the data. We can use this information to summarise these features of the data, for example, visualising the distribution of minimum values (Figure 3A).\nWe are not limited to one feature at a time, many features can also be calculated, for example:\n\n\nheights_three <- heights_brolgar %>%\n  features(height_cm, c(\n    min = min,\n    median = median,\n    max = max\n  ))\n\nheights_three\n\n# A tibble: 119 × 4\n   country       min median   max\n   <chr>       <dbl>  <dbl> <dbl>\n 1 Afghanistan  161.   167.  168.\n 2 Algeria      166.   169   171.\n 3 Angola       159.   167.  169.\n 4 Argentina    167.   168.  174.\n 5 Armenia      164.   169.  172.\n 6 Australia    170    172.  178.\n 7 Austria      162.   167.  179.\n 8 Azerbaijan   170.   172.  172.\n 9 Bangladesh   160.   162.  164.\n10 Belgium      163.   166.  177.\n# … with 109 more rows\n\nThese can then be visualised together (Figure 3).\n\n\n\nFigure 3: Three plots showing the distribution of minimum, median, and maximum values of height in centimeters. Part A shows just the distribution of minimum, part B shows the distribution of minimum, median, and maximum, and part C shows these three values plotted together as a line graph. We see that there is overlap amongst all three statistics. That is, some countries minimum heights are taller than some countries maximum heights.\n\n\n\nThese sets of features can be pre-specified, for example, brolgar provides a five number summary (minimum, 25th quantile, median, mean, 75th quantile, and maximum) of the data with feat_five_num:\n\n\nheights_five <- heights_brolgar %>%\n  features(height_cm, feat_five_num)\n\nheights_five\n\n# A tibble: 119 × 6\n   country       min   q25   med   q75   max\n   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Afghanistan  161.  164.  167.  168.  168.\n 2 Algeria      166.  168.  169   170.  171.\n 3 Angola       159.  160.  167.  168.  169.\n 4 Argentina    167.  168.  168.  170.  174.\n 5 Armenia      164.  166.  169.  172.  172.\n 6 Australia    170   171.  172.  173.  178.\n 7 Austria      162.  164.  167.  169.  179.\n 8 Azerbaijan   170.  171.  172.  172.  172.\n 9 Bangladesh   160.  162.  162.  163.  164.\n10 Belgium      163.  164.  166.  168.  177.\n# … with 109 more rows\n\nThis takes the heights data, pipes it to features, and then instructs it to summarise the height_cm variable, using feat_five_num. There are several handy functions for calculating features of the data that\nbrolgar provides. These all start with feat_, and include:\nfeat_ranges(): min, max, range difference, interquartile range;\nfeat_spread(): variance, standard deviation, median absolute distance, and interquartile range;\nfeat_monotonic(): is it always increasing, decreasing, or unvarying?;\nfeat_diff_summary(): the summary statistics of the differences amongst a value, including the five number summary, as well as the standard deviation and variance;\nfeat_brolgar(), which will calculate all features available in the brolgar package.\nOther examples of features from the feasts package.\nFeature sets\nIf you want to run many or all features from a package on your data you can collect them all with feature_set. For example:\n\n\nlibrary(fabletools)\nfeat_set_brolgar <- feature_set(pkgs = \"brolgar\")\nlength(feat_set_brolgar)\n\n[1] 6\n\nYou could then run these like so:\n\n\nheights_brolgar %>%\n  features(height_cm, feat_set_brolgar)\n\n# A tibble: 119 × 46\n   country     min...1 med...2 max...3 min...4 q25...5 med...6 q75...7\n   <chr>         <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Afghanistan    161.    167.    168.    161.    164.    167.    168.\n 2 Algeria        166.    169     171.    166.    168.    169     170.\n 3 Angola         159.    167.    169.    159.    160.    167.    168.\n 4 Argentina      167.    168.    174.    167.    168.    168.    170.\n 5 Armenia        164.    169.    172.    164.    166.    169.    172.\n 6 Australia      170     172.    178.    170     171.    172.    173.\n 7 Austria        162.    167.    179.    162.    164.    167.    169.\n 8 Azerbaijan     170.    172.    172.    170.    171.    172.    172.\n 9 Bangladesh     160.    162.    164.    160.    162.    162.    163.\n10 Belgium        163.    166.    177.    163.    164.    166.    168.\n# … with 109 more rows, and 38 more variables: max...8 <dbl>,\n#   min...9 <dbl>, max...10 <dbl>, range_diff...11 <dbl>,\n#   iqr...12 <dbl>, var...13 <dbl>, sd...14 <dbl>, mad...15 <dbl>,\n#   iqr...16 <dbl>, min...17 <dbl>, max...18 <dbl>, median <dbl>,\n#   mean <dbl>, q25...21 <dbl>, q75...22 <dbl>, range1 <dbl>,\n#   range2 <dbl>, range_diff...25 <dbl>, sd...26 <dbl>,\n#   var...27 <dbl>, mad...28 <dbl>, iqr...29 <dbl>, …\n\nTo see other features available in the feasts R package run library(feasts) then ?fabletools::feature_set.\nCreating your own feature\nTo create your own features or summaries to pass to features, you provide a named vector of functions. These can include functions that you have written yourself. For example, returning the first three elements of a series, by writing our own second and third functions.\n\n\nsecond <- function(x) nth(x, n = 2)\nthird <- function(x) nth(x, n = 3)\n\nfeat_first_three <- c(first = first,\n                      second = second,\n                      third = third)\n\n\nThese are then passed to features like so:\n\n\nheights_brolgar %>%\n  features(height_cm, feat_first_three)\n\n# A tibble: 119 × 4\n   country     first second third\n   <chr>       <dbl>  <dbl> <dbl>\n 1 Afghanistan  168.   166.  167.\n 2 Algeria      169.   166.  169 \n 3 Angola       160.   159.  160.\n 4 Argentina    170.   168.  168 \n 5 Armenia      169.   168.  166.\n 6 Australia    170    171.  170.\n 7 Austria      165.   163.  162.\n 8 Azerbaijan   170.   171.  171.\n 9 Bangladesh   162.   162.  164.\n10 Belgium      163.   164.  164 \n# … with 109 more rows\n\nAs well, brolgar provides some useful additional features for the five number summary, feat_five_num, whether keys are monotonically increasing feat_monotonic, and measures of spread or variation, feat_spread. Inside brolgar, the features are created with the following syntax:\n\n\nfeat_five_num <- function(x, ...) {\n  c(\n    min = b_min(x, ...),\n    q25 = b_q25(x, ...),\n    med = b_median(x, ...),\n    q75 = b_q75(x, ...),\n    max = b_max(x, ...)\n  )\n}\n\n\nHere the functions b_ are functions with a default of na.rm = TRUE, and in\nthe cases of quantiles, they use type = 8, and names = FALSE. What is particularly useful is that these will work on any type of time series data, and you can use other more typical time series features from the feasts package, such as autocorrelation, feat_acf() and Seasonal and Trend decomposition using Loess feat_stl() (O’Hara-Wild et al. 2020b).\nThis demonstrates a workflow that can be used to understand and explore your longitudinal data. The brolgar package builds upon this workflow made available by feasts and fabletools. Users can also create their own features to summarise the data.\nBreaking up the Spaghetti\nPlots like Figure 2 are often called, “spaghetti plots”, and can be useful for a high level understanding as a whole. However, we cannot process and understand the individuals when the data is presented like this.\nSampling\nJust how spaghetti is portioned out for consumption, we can sample some of the data by randomly sampling the data into sub-plots with the facet_sample() function (Figure 4).\n\n\nggplot(heights_brolgar,\n       aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() + \n  facet_sample() + \n  scale_x_continuous(breaks = c(1750, 1850, 1950))\n\n\n\nFigure 4: Twelve facets with three keys per facet shown. This allows us to quickly view a random sample of the data.\n\n\n\nThis defaults to 12 facets and 3 samples per facet, and provides options for the number of facets, and the number of samples per facet. This means the user only needs to consider the most relevant questions: “How many keys per facet?” and “How many facets to look at?”. The code to change the figure from Figure 2 into 4 requires only one line of code, shown below:\nggplot(heights_brolgar,\n       aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() + \n  facet_sample()\nStratifying\nExtending this idea of samples, we can instead look at all of the data, spread out equally over facets, using facet_strata(). It uses 12 facets by default, controllable with n_strata. The code to do so is shown below, creating Figure 5.\n\n\nggplot(heights_brolgar,\n       aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() + \n  facet_strata() +\n  scale_x_continuous(breaks = c(1750, 1850, 1950))\n\n\n\nFigure 5: All of the data is shown by spreading out each key across twelve facets. Each key is only shown once, and is randomly allocated to a facet.\n\n\n\nFeaturing\nFigure 4 and Figure 5 only show each key once, being randomly assigned to a facet. We can meaningfully place the keys into facets, by arranging the heights “along” a variable, like year, using the along argument in facet_strata to produce Figure 6:\n\n\nggplot(heights_brolgar,\n       aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() + \n  facet_strata(along = -year) + \n  scale_x_continuous(breaks = c(1750, 1850, 1950))\n\n\n\nFigure 6: Displaying all the data across twelve facets. Instead of each key being randomly in a facet, each facet displays a specified range of values of year. In this case, the top left facet shows the keys with the earliest starting year, and the bottom right shows the facet with the latest starting year.\n\n\n\nWe have not lost any of the data, only the order in which they are presented has changed. We learn the distribution and changes in heights over time, and those measured from the earliest times appear to be more similar, but there is much wider variation in the middle years, and then for more recent heights measured from the early 1900s, the heights are more similar. The starting point of each of these years seems to increase at roughly the same interval. This informs us that the starting times of the years is approximately uniform.\nTogether facet_sample() and facet_strata() allow for rapid exploration, by focusing on relevant questions instead of the minutiae. This is achieved by appropriately randomly assigning while maintaining key structure, keeping the correct number of keys per plot, and so on. For example, facet_sample() the questions are: “How many lines per facet” and “How many facets?”, and for facet_strata() the questions are: “How many facets / strata?” and “What to arrange plots along?”.\nAnswering these questions keeps the analysis in line with the analytic goals of exploring the data, rather than distracting to minutiae. This is a key theme of improving tools for data analysis. Abstracting away the parts that are not needed, so the analyst can focus on the task at hand.\nUnder the hood, facet_sample() and facet_strata() are powered with sample_n_keys() and stratify_keys(). These can be used to create data structures used in facet_sample() and facet_strata(), and extend them for other purposes.\nUsing a tsibble stores important key and index components, in turn allowing for better ways to break up spaghetti plots so we can look at many and all sub-samples using facet_sample() and facet_strata().\nBook-keeping\nLongitudinal data is not always measured at the same time and at the same frequency. When exploring longitudinal data, a useful first step is to explore the frequency of measurements of the index. We can check if the index is regular using index_regular() and summarise the spacing of the index with index_summary(). These are S3 methods, so for data.frame objects, the index must be specified, however for the tsibble objects, the defined index is used.\n\n\nindex_summary(heights_brolgar)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1710    1782    1855    1855    1928    2000 \n\nindex_regular(heights_brolgar)\n\n[1] TRUE\n\nWe can explore how many observations per country by counting the number of observations with features, like so:\n\n\nheights_brolgar %>% features(year, n_obs)\n\n# A tibble: 119 × 2\n   country     n_obs\n   <chr>       <int>\n 1 Afghanistan     5\n 2 Algeria         5\n 3 Angola          9\n 4 Argentina      20\n 5 Armenia        11\n 6 Australia      10\n 7 Austria        18\n 8 Azerbaijan      7\n 9 Bangladesh      9\n10 Belgium        10\n# … with 109 more rows\n\nThis can be further summarised by counting the number of times there are a given number of observations:\n\n\nheights_brolgar %>% features(year, n_obs) %>% count(n_obs)\n\n# A tibble: 24 × 2\n   n_obs     n\n   <int> <int>\n 1     5    11\n 2     6    11\n 3     7    13\n 4     8     5\n 5     9    12\n 6    10    12\n 7    11     9\n 8    12     4\n 9    13     7\n10    14     6\n# … with 14 more rows\n\nBecause we are exploring the temporal patterns, we cannot reliably say anything about those individuals with few measurements. The data used, heights_brolgar has less than 5 measurements. This was done using add_n_obs(), which adds the number of observations to the existing data. Overall this drops 25 countries, leaves us with 119 out of the original 144 countries.\n\n\nheights_brolgar <- heights %>% \n  add_n_obs() %>% \n  filter(n_obs >= 5)\n\n\nWe can further explore when countries are first being measured using features to find the first year for each country number of starting years with the first function from dplyr, and explore this with a visualisation (Figure 7).\n\n\nheights_brolgar %>% \n  features(year, c(first = first))\n\n# A tibble: 119 × 2\n   country     first\n   <chr>       <dbl>\n 1 Afghanistan  1870\n 2 Algeria      1910\n 3 Angola       1790\n 4 Argentina    1770\n 5 Armenia      1850\n 6 Australia    1850\n 7 Austria      1750\n 8 Azerbaijan   1850\n 9 Bangladesh   1850\n10 Belgium      1810\n# … with 109 more rows\n\n\n\nheights_brolgar %>% \n  features(year, c(first = first)) %>% \n  ggplot(aes(x = first)) +\n  geom_bar()\n\n\n\nFigure 7: Distribution of starting years of measurement. The data is already binned into 10 year blocks. Most of the years start between 1840 and 1900.\n\n\n\nWe can explore the variation in first year using feat_diff_summary. This combines many summaries of the differences in year.\n\n\nheights_diffs <- heights_brolgar %>% \n  features(year, feat_diff_summary)\n\nheights_diffs\n\n# A tibble: 119 × 10\n   country     diff_…¹ diff_…² diff_…³ diff_…⁴ diff_…⁵ diff_…⁶ diff_…⁷\n   <chr>         <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Afghanistan      10      10      30    32.5    55.8      60   692. \n 2 Algeria          10      10      10    22.5    39.2      60   625  \n 3 Angola           10      10      10    17.5    10        70   450  \n 4 Argentina        10      10      10    11.6    10        40    47.4\n 5 Armenia          10      10      10    15      20.8      30    72.2\n 6 Australia        10      10      10    13.3    10        40   100  \n 7 Austria          10      10      10    13.5    10        40    74.3\n 8 Azerbaijan       10      10      10    25      25.8      90  1030  \n 9 Bangladesh       10      10      10    18.8    15.8      70   441. \n10 Belgium          10      10      10    16.7    23.3      40   125  \n# … with 109 more rows, 2 more variables: diff_sd <dbl>,\n#   diff_iqr <dbl>, and abbreviated variable names ¹​diff_min,\n#   ²​diff_q25, ³​diff_median, ⁴​diff_mean, ⁵​diff_q75, ⁶​diff_max,\n#   ⁷​diff_var\n\nThis is particularly useful as using diff on year would return a very wide dataset that is hard to explore:\n\n\nheights_brolgar %>% \n  features(year, diff)\n\n# A tibble: 119 × 30\n   country  ...1  ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 ...10\n   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Afghan…    10    50    60    10    NA    NA    NA    NA    NA    NA\n 2 Algeria    10    10    60    10    NA    NA    NA    NA    NA    NA\n 3 Angola     10    10    70    10    10    10    10    10    NA    NA\n 4 Argent…    10    10    10    10    10    10    10    10    10    10\n 5 Armenia    10    30    10    10    30    20    10    10    10    10\n 6 Austra…    10    10    10    10    10    10    10    40    10    NA\n 7 Austria    20    10    10    30    10    10    10    10    10    10\n 8 Azerba…    10    90    10    10    10    20    NA    NA    NA    NA\n 9 Bangla…    10    10    10    70    10    20    10    10    NA    NA\n10 Belgium    10    10    10    10    10    10    30    40    20    NA\n# … with 109 more rows, and 19 more variables: ...11 <dbl>,\n#   ...12 <dbl>, ...13 <dbl>, ...14 <dbl>, ...15 <dbl>, ...16 <dbl>,\n#   ...17 <dbl>, ...18 <dbl>, ...19 <dbl>, ...20 <dbl>, ...21 <dbl>,\n#   ...22 <dbl>, ...23 <dbl>, ...24 <dbl>, ...25 <dbl>, ...26 <dbl>,\n#   ...27 <dbl>, ...28 <dbl>, ...29 <dbl>\n\nWe can then look at the summaries of the differences in year by changing to long form and facetting (Figure 8), we learn about the range of intervals between measurements, the smallest being 10 years, the largest being 125, and that most of the data is measured between 10 and 30 years.\n\n\n\nFigure 8: Exploring the different summary statistics of the differences amongst the years. We learn that the smallest interval between measurements is 10 years, and the largest interval is between 10 and 125 years, and that most of the data is measured between 10 and 30 or so years.\n\n\n\nFinding Waldo\nLooking at a spaghetti plot, it can be hard to identify which lines are the most interesting, or unusual. A workflow to identify interesting individuals to start with is given below:\nDecide upon an interesting feature (e.g., maximum)\nThis feature produces one value per key\nExamine the distribution of the feature\nJoin this table back to the data to get all observations for those keys\nArrange the keys or filter, using the feature\nDisplay the data for selected keys\nThis workflow is now demonstrated. Firstly, we decide on an interesting feature, “maximum height”, and whether height is always increasing. We calculate our own “feature”, calculating maximum height, and whether a value is increasing (with brolgar’s increasing function) as follows:\n\n\nheights_max_in <- heights_brolgar %>% \n  features(height_cm, list(max = max,\n                           increase = increasing))\n\nheights_max_in\n\n# A tibble: 119 × 3\n   country       max increase\n   <chr>       <dbl> <lgl>   \n 1 Afghanistan  168. FALSE   \n 2 Algeria      171. FALSE   \n 3 Angola       169. FALSE   \n 4 Argentina    174. FALSE   \n 5 Armenia      172. FALSE   \n 6 Australia    178. FALSE   \n 7 Austria      179. FALSE   \n 8 Azerbaijan   172. FALSE   \n 9 Bangladesh   164. FALSE   \n10 Belgium      177. FALSE   \n# … with 109 more rows\n\nThis returns a dataset of one value per key. Figure 9 examines the distribution of the features, showing us the distribution of maximum height, and the number of countries that are always increasing.\n\n\n\nFigure 9: The different distributions of the features - A is depicting the distribution of maximum height, and B displays the number of countries that are always increasing (FALSE), and always increasing (TRUE). We note that the average maximum heights range from about 160cm to 185cm, with most being around 170cm. We also learn that the vast majority of countries are not always increasing in height through time.\n\n\n\nWe can now join this table back to the data to get all observations for those keys to move from one key per row to all many rows per key.\n\n\nheights_max_in_full <- heights_max_in %>% \n  left_join(heights_brolgar,\n            by = \"country\")\n\nheights_max_in_full\n\n# A tibble: 1,406 × 9\n   country       max incre…¹  year n_obs conti…² heigh…³ year0 count…⁴\n   <chr>       <dbl> <lgl>   <dbl> <int> <chr>     <dbl> <dbl> <fct>  \n 1 Afghanistan  168. FALSE    1870     5 Asia       168.   160 Afghan…\n 2 Afghanistan  168. FALSE    1880     5 Asia       166.   170 Afghan…\n 3 Afghanistan  168. FALSE    1930     5 Asia       167.   220 Afghan…\n 4 Afghanistan  168. FALSE    1990     5 Asia       167.   280 Afghan…\n 5 Afghanistan  168. FALSE    2000     5 Asia       161.   290 Afghan…\n 6 Algeria      171. FALSE    1910     5 Africa     169.   200 Algeria\n 7 Algeria      171. FALSE    1920     5 Africa     166.   210 Algeria\n 8 Algeria      171. FALSE    1930     5 Africa     169    220 Algeria\n 9 Algeria      171. FALSE    1990     5 Africa     171.   280 Algeria\n10 Algeria      171. FALSE    2000     5 Africa     170.   290 Algeria\n# … with 1,396 more rows, and abbreviated variable names ¹​increase,\n#   ²​continent, ³​height_cm, ⁴​country_fct\n\nWe can then arrange the keys or filter, using the feature, for example, filtering only those countries that are only increasing:\n\n\nheights_increase <- heights_max_in_full %>% filter(increase)\nheights_increase\n\n# A tibble: 22 × 9\n   country    max increase  year n_obs continent heigh…¹ year0 count…²\n   <chr>    <dbl> <lgl>    <dbl> <int> <chr>       <dbl> <dbl> <fct>  \n 1 Honduras  168. TRUE      1950     6 Americas     164.   240 Hondur…\n 2 Honduras  168. TRUE      1960     6 Americas     164.   250 Hondur…\n 3 Honduras  168. TRUE      1970     6 Americas     165.   260 Hondur…\n 4 Honduras  168. TRUE      1980     6 Americas     165.   270 Hondur…\n 5 Honduras  168. TRUE      1990     6 Americas     165.   280 Hondur…\n 6 Honduras  168. TRUE      2000     6 Americas     168.   290 Hondur…\n 7 Moldova   174. TRUE      1840     5 Europe       165.   130 Moldova\n 8 Moldova   174. TRUE      1950     5 Europe       172.   240 Moldova\n 9 Moldova   174. TRUE      1960     5 Europe       173.   250 Moldova\n10 Moldova   174. TRUE      1970     5 Europe       174.   260 Moldova\n# … with 12 more rows, and abbreviated variable names ¹​height_cm,\n#   ²​country_fct\n\nOr tallest country\n\n\nheights_top <- heights_max_in_full %>% top_n(n = 1, wt = max)\nheights_top\n\n# A tibble: 16 × 9\n   country   max increase  year n_obs continent height…¹ year0 count…²\n   <chr>   <dbl> <lgl>    <dbl> <int> <chr>        <dbl> <dbl> <fct>  \n 1 Denmark  183. FALSE     1820    16 Europe        167.   110 Denmark\n 2 Denmark  183. FALSE     1830    16 Europe        165.   120 Denmark\n 3 Denmark  183. FALSE     1850    16 Europe        167.   140 Denmark\n 4 Denmark  183. FALSE     1860    16 Europe        168.   150 Denmark\n 5 Denmark  183. FALSE     1870    16 Europe        168.   160 Denmark\n 6 Denmark  183. FALSE     1880    16 Europe        170.   170 Denmark\n 7 Denmark  183. FALSE     1890    16 Europe        169.   180 Denmark\n 8 Denmark  183. FALSE     1900    16 Europe        170.   190 Denmark\n 9 Denmark  183. FALSE     1910    16 Europe        170    200 Denmark\n10 Denmark  183. FALSE     1920    16 Europe        174.   210 Denmark\n11 Denmark  183. FALSE     1930    16 Europe        174.   220 Denmark\n12 Denmark  183. FALSE     1940    16 Europe        176.   230 Denmark\n13 Denmark  183. FALSE     1950    16 Europe        180.   240 Denmark\n14 Denmark  183. FALSE     1960    16 Europe        180.   250 Denmark\n15 Denmark  183. FALSE     1970    16 Europe        181.   260 Denmark\n16 Denmark  183. FALSE     1980    16 Europe        183.   270 Denmark\n# … with abbreviated variable names ¹​height_cm, ²​country_fct\n\nWe can then display the data by highlighting it in the background, first creating a background plot and overlaying the plots on top of this as an additional ggplot layer, in Figure\n10.\n\n\n\n\n\n\n\n\n\n\nFigure 10: Interactive plot to explore longnostics of maximum height and slope from a simple linear fit, relative to the profiles. Click on either plot to select countries. For example, the country with the most negative slope, that is people are getting shorter is Eritrea.\n\n\n\nDancing with Models\nThese same workflows can be used to interpret and explore a model. As the data tends to follow a non linear trajectory, we use a general additive model (gam) with the mgcv R package (Wood 2017) using the code below:\n\n\nheights_gam <- gam(\n    height_cm ~ s(year0, by = country_fct) + country_fct,\n    data = heights_brolgar,\n    method = \"REML\"\n  )\n\n\nThis fits height in centimetres with a smooth effect for year for each country, with a different intercept for each country. It is roughly equivalent to a random intercept varying slope model. Note that this gam model took approximately 8074 seconds to fit. We add the predicted and residual values for the model below, as well as the residual sums of squares for each country.\n\n\n\n\n\nlibrary(mgcv)\nlibrary(modelr)\nheights_aug <- heights_brolgar %>%\n  add_predictions(heights_gam, var = \"pred\") %>%\n  add_residuals(heights_gam, var = \"res\") %>% \n  group_by_key() %>% \n  mutate(rss = sum(res^2)) %>% \n  ungroup()\n\n\nWe can use the previous approach to explore the model results. We can take a look at a sample of the predictions along with the data, by using sample_n_keys. This provides a useful way to explore some set of the model predictions. In order to find those predictions that best summarise the best, and worst, and in between, we need to use the methods in the next section, “Stereotyping”.\n\n\nheights_aug %>% \n  sample_n_keys(12) %>% \n  ggplot(aes(x = year,\n             y = pred,\n             group = country)) + \n  geom_line(colour = \"steelblue\") +\n  geom_point(aes(y = height_cm)) + \n  facet_wrap(~country)\n\n\n\nFigure 11: Exploration of a random sample of the data. This shows the data points of 12 countries, with the model fit in blue.\n\n\n\nStereotyping\nTo help understand a population of measurements over time, it can be useful to understand which individual measurements are typical (or “stereotypical”) of a measurement. For example, to understand which individuals are stereotypical of a statistic such as the minimum, median, and maximum height. This section discusses how to find these stereotypes in the data.\nFigure 12 shows the residuals of the simple model fit to the data in the previous section. There is an overlaid five number summary, showing the minimum, 1st quantile, median, 3rd quantile, and maximum residual value residuals, as well as a rug plot to show the data. We can use these residuals to understand the stereotypes of the data - those individuals in the model that best match to this five number summary.\n\n\n\nFigure 12: Five number summary of residual values from the model fit. The residuals are centered around zero with some variation.\n\n\n\nWe can do this using keys_near() from brolgar. By default this uses the 5 number summary, but any function can be used. You specify the variable you want to find the keys nearest, in this case rss, residual sums of squares for each key:\n\n\nkeys_near(heights_aug, var = rss)\n\n# A tibble: 62 × 5\n   country   rss stat  stat_value stat_diff\n   <chr>   <dbl> <fct>      <dbl>     <dbl>\n 1 Denmark  9.54 med         9.54         0\n 2 Denmark  9.54 med         9.54         0\n 3 Denmark  9.54 med         9.54         0\n 4 Denmark  9.54 med         9.54         0\n 5 Denmark  9.54 med         9.54         0\n 6 Denmark  9.54 med         9.54         0\n 7 Denmark  9.54 med         9.54         0\n 8 Denmark  9.54 med         9.54         0\n 9 Denmark  9.54 med         9.54         0\n10 Denmark  9.54 med         9.54         0\n# … with 52 more rows\n\nTo plot the data, they need to be joined back to the original data, we use a left join, joining by country.\n\n\nheights_near_aug <- heights_aug %>% \n  keys_near(var = rss) %>% \n  left_join(heights_aug, \n            by = c(\"country\"))\n\n\nFigure 13 shows those countries closest to the five number summary. Observing this, we see that the minimum RSS for Moldova fits a nearly perfectly straight line, and the maximum residuals for Myanmar have wide spread of values.\n\n\nggplot(heights_near_aug,\n       aes(x = year,\n           y = pred,\n           group = country,\n           colour = country)) + \n  geom_line(colour = \"orange\") + \n  geom_point(aes(y = height_cm)) + \n  scale_x_continuous(breaks = c(1780, 1880, 1980)) +\n  facet_wrap(~stat + country,\n             labeller = label_glue(\"Country: {country} \\nNearest to \\n{stat} RSS\"),\n             nrow = 1) + \n  theme(legend.position = \"none\",\n        aspect.ratio = 1)\n\n\n\nFigure 13: The keys nearest to the five number summary of the residual sums of squares. Moldova and Madagascar are well fit by the model, and are fit by a straight line. The remaining countries with poorer fit have greater variation in height. It is not clear how a better model fit could be achieved.\n\n\n\nWe can also look at the highest and lowest 3 residual sums of squares:\n\n\nheights_near_aug_top_3 <- heights_aug %>% \n  distinct(country, rss) %>% \n  top_n(n = 3,\n        wt = rss)\n\nheights_near_aug_bottom_3 <- heights_aug %>% \n  distinct(country, rss) %>% \n  top_n(n = -3,\n        wt = rss)\n\nheights_near_top_bot_3 <- bind_rows(highest_3 = heights_near_aug_top_3,\n                                    lowest_3 = heights_near_aug_bottom_3,\n                                    .id = \"rank\") %>% \n  left_join(heights_aug,\n            by = c(\"country\", \"rss\"))\n\n\nFigure 14 shows the same information as the previous plot, but with the 3 representative countries for each statistic. This gives us more data on what the stereotypically “good” and “poor” fitting countries to this model.\n\n\n\nFigure 14: Figure of stereotypes for those keys with the three highest and lowest RSS values. Those that fit best tend to be linear, but those that fit worst have wider variation in heights.\n\n\n\nGetting Started\nThe brolgar R package can be installed from CRAN using\n\n\n# From CRAN\ninstall.packages(\"brolgar\")\n# Development version\nremotes::install_github(\"njtierney/brolgar\")\n\n\nThe functions are all designed to build upon existing packages, but are predicated on working with tsibble. The package extends upon ggplot2 to provide facets for exploration: facet_sample() and facet_strata(). Extending dplyr’s sample_n() and sample_frac() functions by providing sampling and stratifying based around keys: sample_n_keys(), sample_frac_keys(), and stratify_keys(). New functions are focussed around the use of key, for example key_slope() to find the slope of each key, and keys_near() to find those keys near a summary statistic. Finally, feature calculation is provided by building upon the existing time series feature package, feasts.\nTo get started with brolgar you must first ensure your data is specified as a tsibble - discussed earlier in the paper, there is also a vignette “Longitudinal Data Structures”, which discusses these ideas. The next step we recommend is sampling some of your data with facet_sample(), and facet_strata(). When using facet_strata(), facets can be arranged in order of a variable, using the along argument, which can reveal interesting features.\nTo further explore longitudinal data, we recommend finding summary features of each variable with features, and identifying variables that are near summary statistics, using keys_near to find individuals stereotypical of a statistical value.\nConcluding Remarks\nThe brolgar package facilitates exploring longitudinal data in R. It builds upon existing infrastructure from tsibble, and feasts, which work within the tidyverse family of R packages, as well as the newer, tidyverts, time series packages. Users familiar with either of these package families will find a lot of similarity in their use, and first time users will be able to easily transition from brolgar to the tidyverse or tidyverts.\nVisualizing categorical or binary data over a time period can be difficult as the limited number of values on the y axis leads to overplotting. This can conceal the number of values present at a given value. The tools discussed in brolgar facilitate this in the form of facet_sample, and facet_strata. Some special methods could be developed to add jitter or noise around these values on the y axis, while still maintaining the graphical axis and tick marks.\nFuture work will explore more features and stratifications, and stereotypes, and generalise the tools to work for data without time components, and other data types.\nAcknowledgements\nWe would like to thank Stuart Lee, Mitchell O’Hara Wild, Earo Wang, and Miles McBain for their discussion on the design of brolgar. We would also like to thank Rob Hyndman, Monash University and ACEMS for their support of this research.\nPaper Source\nThe complete source files for the paper can be found at https://github.com/njtierney/rjournal-brolgar. The paper is built using rmarkdown, targets and capsule to ensure R package versions are the same. See the README file on the github repository for details on recreating the paper.\n\nCRAN packages used\nbrolgar, ts, xts, zoo, pmdplyr, dplyr, panelr, plm, tsibble, fable, feasts, fabletools, mgcv, ggplot2, targets, capsule\nCRAN Task Views implied by cited packages\nBayesian, CausalInference, Databases, Econometrics, Environmetrics, Epidemiology, Finance, HighPerformanceComputing, MissingData, ModelDeployment, ReproducibleResearch, Spatial, SpatioTemporal, TeachingStatistics, TimeSeries\n\n\nN. Huntington-Klein and P. Khor. Pmdplyr: ’Dplyr’ extension for common panel data maneuvers. 2020. URL https://CRAN.R-project.org/package=pmdplyr. R package version 0.3.3.\n\n\nR. J. Hyndman and G. Athanasopoulos. Forecasting: Principles and practice. 2nd ed Australia: OTexts, 2018.\n\n\nJ. A. Long. Panelr: Regression models and utilities for repeated measures and panel data. 2020. URL https://cran.r-project.org/package=panelr. R package version 0.7.3.\n\n\nG. Millo. Robust standard error estimators for panel models: A unifying approach. Journal of Statistical Software, 82(3): 1–27, 2017. DOI 10.18637/jss.v082.i03.\n\n\nM. O’Hara-Wild, R. Hyndman and E. Wang. Fable: Forecasting models for tidy time series. 2020a. URL https://CRAN.R-project.org/package=fable. R package version 0.2.1.\n\n\nM. O’Hara-Wild, R. Hyndman and E. Wang. Feasts: Feature extraction and statistics for time series. 2020b. URL https://CRAN.R-project.org/package=feasts. R package version 0.1.5.\n\n\nJ. A. Ryan and J. M. Ulrich. Xts: eXtensible time series. 2020. URL https://CRAN.R-project.org/package=xts. R package version 0.12.1.\n\n\nE. Wang, D. Cook and R. J. Hyndman. A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data. Journal of Computational and Graphical Statistics, 29(3): 466–478, 2020. URL https://doi.org/10.1080/10618600.2019.1695624 [online; last accessed November 26, 2020]. Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10618600.2019.1695624.\n\n\nS. N. Wood. Generalized additive models: An introduction with r. 2nd ed Chapman; Hall/CRC, 2017.\n\n\nA. Zeileis and G. Grothendieck. Zoo: S3 infrastructure for regular and irregular time series. Journal of Statistical Software, 14(6): 1–27, 2005. DOI 10.18637/jss.v014.i06.\n\n\n\n\n",
    "preview": "articles/RJ-2022-023/brolgar-paper_files/figure-html5/heights-sample-plot-1.png",
    "last_modified": "2022-10-18T09:09:59+00:00",
    "input_file": {},
    "preview_width": 7200,
    "preview_height": 2400
  },
  {
    "path": "articles/RJ-2022-035/",
    "title": "An Open-Source Implementation of the CMPS Algorithm for Assessing Similarity of Bullets",
    "description": "In this paper, we introduce the R package cmpsR, an open-source implementation of the Congruent Matching Profile Segments (CMPS) method developed at the National Institute of Standards and Technology (NIST) for objective comparison of striated tool marks. The functionality of the package is showcased by examples of bullet signatures that come with the package. Graphing tools are implemented in the package as well for users to assess and understand the CMPS results. Initial tests were performed on bullet signatures generated from two sets of 3D scans in the Hamby study under the framework suggested by the R package `bulletxtrctr`. New metrics based on CMPS scores are introduced and compared with existing metrics. A measure called sum of squares ratio is included, and how it can be used for evaluating different scans, metrics, or parameters is showcased with the Hamby study data sets. An open-source implementation of the CMPS algorithm makes the algorithm more accessible, generates reproducible results, and facilitates further studies of the algorithm such as method comparisons.",
    "author": [
      {
        "name": "Wangqian Ju",
        "url": "https://github.com/willju-wangqian"
      },
      {
        "name": "Heike Hofmann",
        "url": "https://github.com/heike"
      }
    ],
    "date": "2022-10-12",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nIntroduction\nIn this paper, we present an open-source implementation of the algorithm of the Congruent Matching Profile Segments (CMPS) method.\nChen et al. (2019) developed the CMPS method for “objective comparison of striated tool marks” and demonstrated its use in some examples of comparing bullet signature correlations.\nAlthough Chen et al. (2019) conceptually describe the CMPS algorithm in their paper, the authors did not release an implementation of their method.\nThus, our effort here is to introduce the CMPS method to the R community and provide an open-source, publicly available implementation of the algorithm to use, review, and improve.\nOur implementation is made available as part of the R package cmpsR on CRAN.\nAccording to the Uniform Crime Reporting Program of the FBI (Federal Bureau of Investigation), “more than 76 percent (76.7) of the homicides for which the FBI received weapons data in 2020 involved the use of firearms” (United States Department of Justice, Federal Bureau of Investigation.). At the same time, the number of murder victims jumped between 2019 and 2020 by more than 23 percent (23.4) to 17754 (United States Department of Justice, Federal Bureau of Investigation.). This increase is unprecedented and highlights the important role that firearm examination plays in all these cases.\n\nAn important task of firearm examination is to answer the question of whether two pieces of evidence come from the same source or whether a piece of evidence matches a sample obtained from a specific firearm.\nHere, in particular, we are interested to determine whether two bullets were fired from the same gun barrel.\nAssessing the similarity between two bullets is based on a comparison of striation marks acquired during the firing process as bullets are propelled through the barrel.\nThe current state of the art sees firearms examiners make an assessment of similarity based on a visual comparison, generally, using a comparison microscope (AFTE Criteria for Identification Committee 1992).\nThis practice has been criticized for its lack of objectivity and the associated problem of determining valid error rates (President’s Council of Advisors on Science and Technology 2016).\nA report published by the Committee on Identifying the Needs of the Forensic Sciences of the National Research Council (2009) states that “[m]uch forensic evidence-including, for example, bite marks and firearm and toolmark identification—is introduced in criminal trials without any meaningful scientific validation, determination of error rates, or reliability testing to explain the limits of the discipline.” To overcome those criticisms and concerns, researchers have been making an effort to build databases and develop frameworks and algorithms that bring an objective and quantitative assessment into the field.\nThe database of Zheng (2016) with digital 3D topographic scans from various studies by Brundage (1998), Hamby et al. (2009), Hamby et al. (2019) and others, provides a great resource for researchers. Algorithms that can be validated and tested and generate quantitative results are developed for 2D and 3D surface texture (Song et al. 2005), tool marks (Chumbley et al. 2010), and striae on Land Engraved Areas (LEAs) of bullets Chen et al. (2019).\n\nThe notion of bullet signatures is one of the results of these efforts, and we will use it to explain the CMPS algorithm and showcase the cmpsR implementation.\nBullet signatures can be extracted from bullet LEAs, typically one for each land engraved area, and how those bullet signatures are extracted from scans will be discussed in the Background section.\nBullet signatures capture striation marks on the bullet in a numeric format and therefore serve as the foundation for algorithms. These scans played an important role in bringing objectivity into the field.\nIn the following sections, we will: review the background of bullet signature comparisons, discuss how we followed the idea described by Chen et al. (2019) for the implementation of the CMPS algorithm, propose new metrics based on the CMPS score and a principled evaluation framework for algorithmic results comparison, and present results of applying our implementation to real data.\nBackground\nHamby data set\n\nThe datasets we worked with come from the James Hamby Consecutively Rifled Ruger Barrel Study (Brundage 1998; Hamby et al. 2009, 2019), in particular, Hamby set 252 and Hamby set 44.\nFor each Hamby set, a total of 35 bullets is fired from (the same) ten consecutively manufactured Ruger P-85 pistol barrels.\nTwo bullets are fired from each barrel, making up a set of 20 reference bullets.\nAn additional 15 bullets are fired from these ten barrels in a fashion unknown to the study participant.\nThe aim of the Hamby Study was to have firearms examiners identify which barrel each of the 15 questioned bullets was fired from.\nThe Ruger P-85 barrels are traditionally rifled barrels with six grooves and lands as shown in Figure 1.\nDuring the firing process, grooves and lands are engraved on a bullet.\nFirearms examiners use striation marks on land engraved areas (LEAs) for their visual comparison.\nFor algorithmic purposes, 3D topographical images of land engraved areas were obtained and stored in x3p format (XML 3-D Surface Profile).\nThe x3p format provides a standard way of exchanging 2D and 3D profile data.\nIt conforms to the ISO5436-2 standard adopted by the OpenFMC (Open Forensic Metrology Consortium), a group of firearm forensics researchers who contributes to the establishment of best practices of using metrology in forensic science.\nHamby set 252 was scanned using a NanoFocus lens at 20x magnification with the scan resolution being 1.5625 m \\(\\times\\) 1.5625 m per pixel.\nHamby set 44 was scanned at the Roy J Carver High-Resolution microscopy lab at Iowa State.\nThese scans were acquired with a Sensofar Confocal Light Microscope at 20x magnification for a nominal resolution of 0.645 m \\(\\times\\) 0.645 m per pixel.\nBoth Hamby set 252 and Hamby set 44 are publicly available from the NIST Ballistics Database Project (Zheng 2016).\nExtracting signatures from LEA scans\n\nThe automated framework for extracting signatures from x3p files used in this paper was proposed by Hare et al. (2017) and is implemented in the R packages x3ptools (Hofmann et al. 2020) and bulletxtrctr (Hofmann et al. 2019).\nx3ptools is a package to read, write, and generally, process x3p files.\nThe bulletxtrctr package implements a pipeline for extracting and comparing signatures from scans of land-engraved areas.\nFigure 2 gives an overview of all of the steps in the process from scan to signatures.\nFigure 2(a) shows a rendering of a 3D scan of a bullet land engraved area.\nThe raised portion of the surface on the left and right of the scan are parts of the adjacent groove engraved areas (GEAs), the middle area shows a land engraved area with well expressed striation marks.\nThe first step of obtaining the bullet signature is to extract a cross-section at a fixed height on the land engraving.\nThe thin white horizontal line in Figure 2(a) indicates which cross-section was identified by the algorithm to represent the LEA; Figure 2(b) shows the corresponding cross-sectional view.\nGroove engraved areas are removed from the analysis as indicated by the vertical blue lines in Figure 2(c) and Figure 2(d).\nA non-parametric LOESS smooth (Cleveland et al. 1991) is fitted to capture the bullet curvature (Figure 2(e)) and, finally, the bullet signature (Figure 2(f)) is obtained as residuals of the cross-section and the fitted smooth.\nNote that in Chen et al. (2019) bullet signatures are referred to as bullet profiles.\nHowever, to avoid confusion, we distinguish the notion of bullet signatures from bullet profiles.\nBullet profiles are shown in panels (b), (c), and (d) of Figure 2, while Figure 2(f) shows the corresponding bullet signature.\nIdentifying the groove engraved areas correctly is fundamental for a correct downstream analysis of the signatures.\nWe have provided an interactive web application to allow for a human-in-the-middle inspection and intervention. It is implemented as an R Shiny App (Chang et al. 2021) named bulletinspectR for identifying and correcting those errors.\nAn example of the extraction process with corresponding code and parameter settings can be found in the “Supplementary materials”. Note that the process of extracting signatures might be different from the one used in Chen et al. (2019) because no code or parameter settings are made available publicly.\n\n\n\nFigure 1: Photo of a traditionally rifled gun barrel (left) and a fired bullet (right).\n\n\n\n\n\n\nFigure 2: A framework of obtaining a bullet signature. (a) rendering from the 3D topographic scan of a land engraved area (LEA). The selected crosscut location is indicated by a thin white horizontal line. (b) view of the cross-section of the land engraved area at the white line in (a). (c) the crosscut data plotted in 2D; blue vertical lines indicate the position of the left and right grooves. (d) the crosscut data after chopping the left and right grooves. (e) the fitted curvature using LOESS. (f) after removing the curvature from the crosscut data, the bullet signature is obtained.\n\n\n\nConceptual idea of CMPS\n\nMost algorithms for comparing striation marks are based on the digitized signatures and produce a similarity score Krishnan and Hofmann (2019).\nThe congruent matching profile segments (CMPS) algorithm, developed by Chen et al. (2019) for “objective comparison of striated tool marks”, is one such algorithm.\nThe algorithm’s main idea is to take a set of consecutive and non-overlapping basis segments from the comparison and for each segment find the “best” registration position on the reference (the other bullet signature) with respect to their cross-correlation values.\nFrom a comparison of these registration positions, a congruent registration position is identified, and the number of basis segments taking the congruent registration position is the CMPS score.\nNote that researchers in Chen et al. (2019) refer to the origin of basis segments as the reference, but in this paper we refer to it as the comparison signature.\nHigh CMPS scores are achieved between more similar signatures and are therefore indicative of a same-source pair.\nLow scores between pairs of signatures are attributed to different source pairs.\nHowever, a specific threshold of the CMPS score to distinguish between same-source and different-source comparisons is not provided in Chen et al. (2019), instead the threshold depends on the underlying structure of the data and the choice of parameters. In a legal setting this variability is problematic because it allows for situations in which experts could choose parameters based on whether they are witnesses for the defense or the prosecution.\nFurther research is needed to understand how to determine optimal threshold settings.\nThe CMPS algorithm can assist firearms examiners with drawing a conclusion about the source of a comparison pair.\nThus, in this paper we present an open-source implementation of the CMPS algorithm in the R package cmpsR available from both CRAN and Github. This publicly available implementation calculates the CMPS score of a comparison using the following code:\n\n\n# install.packages(\"cmpsR\")\n\nlibrary(cmpsR)\ndata(bullets)\n\nsig1 <- bullets$sigs[[2]]$sig\nsig2 <- bullets$sigs[[9]]$sig\nsig3 <- bullets$sigs[[10]]$sig\n\ncmps_result_KM <- extract_feature_cmps(sig1, sig2)\ncmps_result_KNM <- extract_feature_cmps(sig1, sig3)\n\n\nIn this example, the comparison between sig1 and sig2, two signatures coming from the same source (a known-match comparison), gets a CMPS score of 17; the comparison between sig1 and sig3, two signatures coming from different sources (a known non-match comparison), gets a CMPS score of 1.\nWe also implemented graphing tools for users to better understand these results as well as the algorithm itself.\nThe section “Implementation” will go through the algorithm and show how to use the cmpsR package.\nA further example that illustrates the main points is also included.\nIn the section on “Evaluation metrics”, we introduce new CMPS metrics that summarize land-level CMPS scores and a sum of squares ratio that can be used to evaluate algorithmic results.\nThe section “Results” presents the results of evaluating the cmpsR package using Hamby set 252 and Hamby set 44. The results from Hamby set 252 are used to verify that our implementation is, at least qualitatively, comparable to the algorithm described in Chen et al. (2019). Results from Hamby 44 show the need for a further investigation of the parameter choices even in the case of bullets fired from the same barrels.\nThe last section covers some final discussion and conclusions.\nImplementation\nAlgorithm\nConceptually, the CMPS algorithm consists of three main steps:\ncut the comparison signature into consecutive, non-overlapping, and equal-length basis segments: The command get_segs(x, len=50) implements this step: it takes bullet signature x in the format of a numeric vector and cuts it into consecutive, non-overlapping and equal-length segments of length len, which are referred to as “basis segments”. Note that the parameter len determines the length of a basis segment and thus affects the total number of basis segments, which is the upper limit of the CMPS score of a comparison. The default value of len is 50, which will result in about 25 basis segments for the example data of the package.\nidentify candidate positions: For each basis segment a set of candidate registration positions on the comparison signature is identified based on the segment’s similarity to the reference signature.\nIn the first step, the cross-correlation function of the segment to the reference is calculated, then a number of positions with high correlation values are identified as candidate positions.\nIn case multiple segment lengths are considered, the length of each basis segment is expanded (by default it is doubled) and these two steps are repeated.\nOnly when candidate positions coincide (or are similar enough), they are considered further.\nFigure 5 and Figure 6 illustrate these ideas.\nCalculate the cross-correlation curve: Calculate the cross-correlation curve between a basis segment x and the reference signature y using the function get_ccf4(x, y, ...) as shown in Figure 5(b). The position indicates the lag by which a basis segment is moved with respect to its original placement. A position is considered “good” if it results in a peak in the cross-correlation between the basis segment and the reference.\nCorrelation peaks: Two strategies referred to as “multi-peak inspection” and “multi-peak inspection at different segment lengths” in Chen et al. (2019) are used for identifying positions of correlation peaks as candidate positions. The latter is also called the “multi-segment lengths strategy”. The parameter npeaks_set in extract_feature_cmps(...) determines which strategy to use and the number of candidate positions:\nIf npeaks_set is an integer vector of length 1, for example, npeaks_set = 5, the positions of the top five peaks in the cross-correlation curve are identified as candidate positions for registration.\nIf npeaks_set is an integer vector of length more than 1, for example, npeaks_set = c(5, 3, 1), the multi-segment lengths strategy will be used: calculate the cross-correlation function between a basis segment and the reference and identify positions of the top five peaks; double the segment length to a specified value, re-calculate the cross-correlation function, and identify three peaks; repeat this process and identify a single peak in the newly computed cross-correlation function. Figure 6 shows an example of three levels of basis segment 6 and their corresponding cross-correlation curves and identified peaks. Note that in Chen et al. (2019) the segment length is doubled at each level of a basis segment, but in the present implementation users are allowed to choose the segment length at each level.\nget_ccr_peaks(comp, segments, seg_outlength, nseg = 1, npeaks = 5) computes the cross-correlation curve between a basis or increased segment and the reference signature and finds peaks in the cross-correlation curve. The number of peaks detected is equal to npeaks, which is an integer. segments, seg_outlength, and nseg determine the segment in the cross-correlation computation, and comp gives the reference signature. If the multi-segment lengths strategy is used, then get_ccr_peaks(...) is called in a lapply() for each level of the basis segment. The resulting list is called ccr_list.\n\nmulti-segment lengths strategy: with the multi-segment lengths strategy being used, a position is identified as a candidate position for registration and is called a “consistent correlation peak” if it results in a top peak in the cross-correlation curve with a tolerance zone determined by Tx in all segment levels. Note that in Chen et al. (2019), a segment at its largest scale (highest level) always identifies one peak, but we do not have this requirement in our implementation.\nthe function get_seg_scale(segments, nseg, out_length) is used to obtain the (potentially increased) version of a basis segment. segments, which is a list containing all basis segments generated by the function get_segs(...) in step 1, and nseg are used to determine the basis segment to be increased. out_length specifies the length of the output segment.\nget_ccp(ccr_list, Tx = 25) tries to identify the “consistent correlation peak”. ccr_list is the result of lapply() and get_ccr_peaks(...), and Tx determines the size of a tolerance zone used in identifying the consistent correlation peak. get_ccp(...) returns NULL if there is no consistent correlation peak.\n\n\ndetermine the congruent registration position: A candidate position “receives” votes from basis segments that identify it or a close position within a tolerance zone of Tx as a candidate position in step 2.\nVotes for all candidate positions are tallied, and the position with the highest number of votes gets chosen as the congruent registration position, indicating that most of the basis segments find their highly similar counterpart in the reference signature in terms of correlation at this registration position.\nIn the case of ties, the middle position is taken as the congruent registration position.\nBasis segments with a congruent registration position are called “congruent matching profile segments” (CMPS).\nThe total number of CMPS is the CMPS score of the comparison.\nget_CMPS(input_ccp, Tx = 25) is the function that tallies the votes and determines the congruent registration position and congruent matching profile segments (CMPS).\nNote that there are several parameters in the CMPS algorithm that will affect the final results and are left to the users to decide, such as the length of basis segments seg_length in step 1, the number of peaks npeaks_set identified on each level in step 2, and the length of the tolerance zone Tx in both step 2 and 3.\nIn our implementation of the CMPS algorithm, we used the parameters given in the original CMPS paper (Chen et al. 2019) as the default values for these parameters.\nHowever, the authors state that no cross-validation has been done - there might also be issues with respect to the resolution of the scans.\nFurther research is needed, until then users are advised to think of default values as starting values and consider alternatives. However, the evaluation framework based on the sum of squares ratio introduced in the later section could be used to evaluate the choices of these parameters.\nThe main function that combines all steps in the CMPS algorithm described above is called extract_feature_cmps(...).\nHere we present it with its default parameters.\n\n\nextract_feature_cmps(\n  x,\n  y,\n  seg_length = 50,\n  Tx = 25,\n  npeaks_set = c(5, 3, 1),\n  include = NULL,\n  outlength = NULL\n)\n\n\nThe function extract_feature_cmps allows for the following input from users besides the previously discussed parameters seg_length, npeaks_set, and Tx:\nx and y are two signatures: x serves as the comparison signature (which will be divided into basis segments) and y is the reference signature;\ninclude determines the format of the function result.\nBesides the CMPS_score, other aspects of the comparison help in understanding how the CMPS_score is computed.\nBy default include is set to NULL and only the CMPS_score is returned; further results are included when include is (an abbreviation of) one of or a vector of the following strings: \"nseg\", \"congruent_pos\", \"congruent_seg_idx\", \"segments\", \"parameters\", and \"full_result\".\nIf include is specified as \"full_result\" (or its abbreviation), the output includes everything listed below.\nnseg: the number of basis segments from the comparison signature; this is also the highest possible CMPS score of the comparison;\ncongruent_pos: the congruent registration position;\ncongruent_seg_idx: the indices of all congruent matching profile segments;\nccp_list: a list showing identified candidate positions of all basis segments;\npos_df: a data frame containing all candidate positions and their respective number of votes;\nsegments: a list containing all basis segments;\nparameters: a list containing all input arguments of extract_feature_cmps;\n\noutlength specifies the segment length of a basis segment at each level under the multi-segment lengths strategy.\nBy default outlength is set to NULL, indicating that a basis segment should double its segment length at the next level and conforming to the description in Chen et al. (2019) .\nIn the remainder of the paper, we showcase the use of the cmpsR functionality on some examples and present the results of applying it to two datasets.\nInstallation\nThe cmpsR package is publicly available from CRAN and can be installed by\n\n\ninstall.packages(\"cmpsR\")\n\n\nMoreover, its development version is also available from Github and can be installed by\n\n\n# install.packages(\"remotes\")\nremotes::install_github(\"willju-wangqian/cmpsR\")\n\n\nAn example\nThe cmpsR package contains a simple example to illustrate the basic usage of the package.\nThe data in this example are twelve bullet signatures obtained from two bullets in Hamby set 252 (Hamby et al. 2009).\nThe procedure for generating signatures from high-resolution 3D topographic scans of bullet lands used here follows the methodology described in Hare et al. (2017) (as discussed above).\nThe two bullets under consideration are known to have been fired from the same gun barrel, so for the 36 pairwise land-by-land comparisons, six comparisons are from same-source pairs (known matches) while thirty are from different-source pairs (known non-matches).\nTo access the example data, we use\n\n\nlibrary(cmpsR)\ndata(bullets)\n\n\nbullets$sigs is a list of twelve numeric vectors corresponding to the twelve bullet signatures shown in Figure 3.\nbullets$source contains the URLs to the corresponding x3p file containing the topographic scan from the NIST Ballistics Toolmark Research Database (Zheng 2016).\n\n\n\nFigure 3: Signatures of all lands of bullet 1 in the top row, and of bullet 2 in the bottom row. Signatures in the second row are ordered to be in phase with the signatures above, i.e. matching signatures are displayed on top of each other. On the x-axis is the length of the scan in millimeter, and on the y-axis is the relative height in micron.\n\n\n\nThe signatures of Land 4 of Bullet 1 and Land 5 of Bullet 2 are stored in objects sigs2 and sigs1, respectively.\nThis comparison consists of a pair of signatures that are known to be a match – a KM (known match) comparison.\nWe compute the CMPS score using two versions of the CMPS algorithm:\n\n\nsigs1 <- bullets$sigs[bullets$bulletland == \"2-5\"][[1]]\nsigs2 <- bullets$sigs[bullets$bulletland == \"1-4\"][[1]]\n\n# compute cmps\n\n# algorithm with multi-peak insepction at three different segment levels\ncmps_with_multi_scale <-\n  extract_feature_cmps(sigs1$sig, sigs2$sig,\n    npeaks_set = c(5, 3, 1), include = \"full_result\"\n  )\n\n# algorithm with multi-peak inspection at the basis scale only\ncmps_without_multi_scale <-\n  extract_feature_cmps(sigs1$sig, sigs2$sig,\n    npeaks_set = 5, include = \"full_result\"\n  )\n\n\nIn the first example, npeaks_set is a vector of three integers, i.e. the algorithm uses the multi-segment lengths strategy to create the result object cmps_with_multi_scale.\nFor cmps_without_multi_scale each basis segment is linked to the top 5 candidate positions.\nWe use include = \"full_result\" to capture all results.\nIn this example, the CMPS is 9 when using multiple segments, and 12 when using a single segment.\nAs discussed in Chen et al. (2019), using multi-segment lengths strategy can reduce the number of false positives when identifying candidate positions; however, any score-based method is walking the line between false positives and false negatives.\nAs the number of false positives is reduced the number of false negatives might rise.\nMore discussion and comparisons between the two versions of the CMPS algorithm will be presented in later sections.\nNote that the multi-segment lengths method is slower because the algorithm is run once for each segment length.\n\n\n\nVisualize and understand CMPS results\nWe also implemented graphing tools for visualizing the results of the CMPS algorithm.\nThe goal is to provide users with tools to inspect each of the basis segments and to help them have a better understanding of how the algorithm works.\nFigure 4 shows the plots generated by the first graphing function, cmps_signature_plot(), and continues with the example above.\ncmps_signature_plot() takes the output of extract_feature_cmps(..., include = \"full_result\") and returns a list of 5 elements.\nIt creates an overall impression of how the comparison signature aligns with the reference signature at the congruent registration position.\nThe first element is a plot called segment_shift_plot, shown in Figure 4(a). On this plot the reference signature is drawn as a black line, congruent matching profile segments from the comparison signature are overlaid in red at the congruent registration position.\n\n\nsig_plot <- cmps_signature_plot(\n  cmps_with_multi_scale\n)\n\n# (a)\nsig_plot$segment_shift_plot\n\n\nThe second plot is called signature_shift_plot, shown in Figure 4(b). This visual presents both the comparison signature and the reference signature. The comparison signature is aligned with the reference signature based on the congruent registration position. Congruent matching profile segments are highlighted by solid red lines.\n\n\n# (b)\nsig_plot$signature_shift_plot\n\n\n\n\n\nFigure 4: In (a) the black line shows the comparison signature; each red line segment shows one congruent matching profile segment. Each grey rectangle highlights one congruent matching profile segment. In (b) the black line shows the reference signature; the red line shows the comparison signature. Solid part shows the congruent matching profile segments, and the dashed part shows segments that do not agree with the congruent registration position.\n\n\n\nOther elements of this list are seg_shift and sig_shift. sig_shift gives the congruent registration position, while seg_shift is a data frame showing the congruent matching profile segments and their identified candidate position closest to the congruent registration position.\n\n\nsig_plot$seg_shift\n\n#>    seg_idx seg_shift\n#> 7        7         0\n#> 8        8        -1\n#> 14      14         5\n#> 16      16         8\n#> 17      17         8\n#> 18      18         8\n#> 19      19         9\n#> 20      20         9\n#> 22      22        12\n\nWhile cmps_signature_plot() focuses on the signature level, cmps_segment_plot() focuses on the segment level.\nIt provides the “full result” of extract_feature_cmps(), but also takes an argument, seg_idx, indicating which segment should be inspected.\nWhen checking sig_plot$seg_shift we notice that segment number 6 is not one of the congruent matching profile segments.\nWe can therefore set seg_idx = 6 in cmps_segment_plot() and investigate the reason why this segment disagrees with the congruent registration position.\nFor each segment scale, we have two plots: segment_plot and scale_ccf_plot, as shown in Figure 5 for the example of segment number 6:\nFigure 5(a) is the segment_plot for basis segment 6 at level one (in its original length). We used npeaks_set = c(5, 3, 1) in extract_feature_cmps() when calculating the CMPS score. Therefore the top five peaks are identified in the cross-correlation curve at level one. Segment 6 is plotted at the positions where these five peaks are identified with dashed lines in the segment_plot. The solid thick black line shows the segment at its original position (which in this example is very close to the actual registration position).\n\n\nseg_plot <- cmps_segment_plot(\n  cmps_with_multi_scale,\n  seg_idx = 6\n)\n\n# (a)\nseg_plot[[1]]$segment_plot\n\n\nFigure 5(b) is the scale_ccf_plot of basis segment 6 at level one. It shows the cross-correlation curve computed by the reference signature and the level-one basis segment 6. The five highest peaks are marked by dots on the curve.\n\n\n# (b)\nseg_plot[[1]]$scale_ccf_plot\n\n\n\n\n\nFigure 5: Plot (a) shows segment_plot for segment 6 at level one. The original position of segment 6 is indicated by the solid black line. Positions, where the segment achieves the 5 highest cross-correlations, are indicated by the dashed line segments. The scale_ccf_plot in plot (b) shows the cross-correlation curve between the reference signature and segment 6 at level one. The five highest peaks are marked by dots. The vertical red dashed line indicates the congruent registration position; the green dashed line shows a peak position in the highest segment level; the blue dashed lines show the tolerance zone around the green dashed line. We can see that none of the five highest peaks at level one falls within the tolerance zone, indicating that there is no consistent correlation peak or a candidate position identified by basis segment 6 under the multi-segment lengths strategy. Thus, the basis segment 6 doesn’t vote for the congruent registration position and is not a cmps.\n\n\n\nAdditionally, users can have more insights about why segment 6 is not a congruent matching profile segment if we put the segment_plot and scale_ccf_plot of all three segment levels together, as shown in Figure 6 with the help of ggpubr::ggarrange().\n\n\nlibrary(ggpubr)\n\nggarrange(\n  plotlist =\n    unlist(seg_plot,\n      recursive = FALSE\n    ),\n  ncol = 2,\n  nrow = 3\n)\n\n\n\nFigure 6: Put segment_plot and scale_ccf_plot of all three levels together. We are identifying the five highest peaks at level one, three peaks at level two, and one peak at level three since npeaks_set = c(5, 3, 1). The highest peak position at level three is marked by the green dashed line across all segment levels. However, the highest peak on level three does not coincide with any of the top five highest peaks at level one. This indicates that there is no consistent correlation peak or a candidate position for basis segment 6 under the multi-segment lengths strategy.\n\n\n\nIn Figure 6, the red vertical dashed line indicates the congruent registration position.\nWe can see that the basis segment 6 does obtain a peak near the congruent registration position at level two and level three, respectively; however, this position doesn’t give one of the five highest peaks at level one.\nAs a result, segment 6 fails to identify the consistent correlation peak (ccp) and fails to become one of the congruent matching profile segments according to the multi-segment lengths strategy.\nThe identified top five peaks at level one are also examples of “false positive” peaks.\nThe “true positive” peak (the peak within the tolerance zone of the congruent registration position) is identified at level two and three by increasing the segment length, which justifies the usage of the multi-segment lengths strategy.\nEvaluation metrics\nMetrics based on CMPS scores\nThe CMPS algorithm measures the similarity between two signatures resulting in a similarity score of a land-to-level comparison.\nBullets fired from traditionally rifled barrels have multiple land and groove engraved areas.\nHere, we are working with bullets fired from Ruger P85 barrels with six lands and grooves.\nA comparison of two bullets, therefore, involves 36 land-to-land comparisons, resulting in 36 CMPS scores (as shown in Figure 7).\nIn order to obtain a single similarity score of a bullet-level comparison, we need to summarize these 36 CMPS scores.\nTwo similarity metrics for bullet-level comparisons have been introduced in the literature (Chen et al. 2019): \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\).\n\\(\\mathrm{CMPS_{max}}\\) is the highest CMPS score obtained among all land-level comparisons, while \\(\\mathrm{\\overline{CMPS}_{max}}\\) is the highest possible mean CMPS score of land-level comparisons that are in the same phase:\nIn general, we assume each bullet has \\(n\\) land engravings (in our case \\(n=6\\)).\nLet \\(c_{ij}\\) denote the CMPS score of a comparison between bullet 1 land \\(i\\) and bullet 2 land \\(j\\), for \\(i,j = 1, \\dots, n\\).\nLet \\(\\mathcal{P}_k\\) denote bullet land pairs in phase \\(k\\) for \\(k = 0, \\dots, n-1\\), and\n\\[\\begin{align}\n\\mathcal{P}_k = \\{ \\left(i,j\\right): i = 1, \\dots, n ; \\; j = \\left(i + k\\right) \\;\\mathrm{mod}\\; n \\}\n\\end{align}\\]\n, where mod denotes the modulo operation.\nFor example, \\(\\mathcal{P}_1 = \\{ \\left(1,2\\right), \\left(2,3\\right), \\left(3,4\\right), \\left(4,5\\right), \\left(5,6\\right), \\left(6,1\\right) \\}\\) when \\(n = 6\\).\nLet \\(k^*\\) denote the index of the highest phase.\nWith that, the two measures to evaluate accuracy used in Chen et al. (2019) are defined as\n\\[\\begin{align}\n\\mathrm{CMPS_{max}} &= \\max_{i,j} c_{ij} \\text{ , and} \\\\\n\\mathrm{\\overline{CMPS}_{max}} &= \\frac{1}{n} \\sum_{(i,j) \\in \\mathcal{P}_{k^*}} c_{ij} \\text{ , where} \\\\\nk^* &= \\text{arg}\\max\\limits_{k} \\left[  \\frac{1}{n} \\sum_{(i,j) \\in \\mathcal{P}_k} c_{ij}\\right]\n\\end{align}\\]\nWe can continue with the example used in previous sections.\nbullets contains bullet signatures of two bullets, bullet1 and bullet2.\nAs mentioned before, each bullet has six land engravings, resulting in six bullet signatures.\nThus, there are 36 pairwise bullet signature comparisons, resulting in 36 \\(c_{ij}\\) values in total.\nWe use multi-segment lengths strategy with default parameters to compute these CMPS scores, and the result is shown in Figure 7.\nWe can see that in this example,\n\\[\n\\mathrm{CMPS_{max}} =  \\max_{i,j} c_{ij} = 17\n\\]\nand since bullet lands in phase \\(\\mathcal{P}_1\\) gives the highest mean CMPS score (\\(k^* = 1\\)), we have\n\\[\n\\begin{aligned}\n\\mathrm{\\overline{CMPS}_{max}} &= \\frac{1}{6} \\sum_{(i,j) \\in \\mathcal{P}_1} c_{ij} \\\\\n                        &= \\frac{1}{6} \\left(c_{12} + c_{23} + c_{34} + c_{45} + c_{56} + c_{61}\\right) \\\\\n                        &= \\frac{1}{6} \\left(3+17+14+10+15+16\\right) \\\\\n                        &= 12.5\n\\end{aligned}\n\\]\n\n\n\nFigure 7: CMPS scores of all 36 pairwise bullet signature comparisons for two bullets. Land engraving pairs generated by the same land (KM comparisons) are highlighted. Note that in this example the axis along Bullet 2 starts with Land 2. This corresponds to Phase 1 in equation (1).\n\n\n\nHowever, both \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\) consider only relatively high CMPS scores and ignore the rest.\nSo we introduce a new metric based on CMPS scores called \\(\\mathrm{\\overline{CMPS}_{diff}}\\), which is the difference between \\(\\mathrm{\\overline{CMPS}_{max}}\\) and the mean of all other CMPS scores.\nWith our notation above, we have:\n\\[\\begin{align}\n\\mathrm{\\overline{CMPS}_{diff}} = \\left[  \\frac{1}{n} \\sum_{(i,j) \\in \\mathcal{P}_{k^*}} c_{ij}\\right] - \\left[  \\frac{1}{n\\left(n-1\\right)} \\sum_{(i,j) \\notin \\mathcal{P}_{k^*}} c_{ij}\\right]\n\\end{align}\\]\n\\(\\mathrm{\\overline{CMPS}_{diff}}\\) highlights the difference between CMPS scores of matching and non-matching comparisons.\nIf two bullets are non-matching, all 36 CMPS scores are expected to be small with relatively the same values, resulting in a \\(\\mathrm{\\overline{CMPS}_{diff}}\\) value close to 0.\nFor the example above, \\(\\mathrm{\\overline{CMPS}_{diff}} = 12.5 - 1.53 = 10.97\\) \nScaled CMPS scores\nAnother issue with the CMPS score is that the highest possible CMPS score (the total number of basis segments) might differ across comparisons (as shown in Figure 8(a)) due to different lengths of bullet signatures and different lengths of basis segments specified by the parameter.\nA CMPS score of 5 might indicate a non-match if the highest possible CMPS score is 30 but indicate a match if the highest possible CMPS score is 6.\nThus, we introduce the scaled CMPS score, denoted as \\(c^*_{ij}\\).\nLet \\(s_{ij}\\) denote the highest possible CMPS score or the total number of basis segments, then the scaled CMPS score \\(c^*_{ij}\\) is defined as the ratio of raw score and maximum score:\n\\[\\begin{align}\nc^*_{ij} = \\frac{c_{ij}}{s_{ij}}\n\\end{align}\\]\nThe scaled CMPS scores of the above example are shown in Figure 8(b).\nCompared to the original CMPS scores, scaled scores have values within the interval \\([0, 1]\\) regardless of the length of the basis segments and therefore make a comparison of values possible across different parameter choices.\nSimilar to the original CMPS scores, we will denote the scaled CMPS scores adjusted for out-of-phase background values by \\(\\mathrm{\\overline{CMPS^*}_{diff}}\\).\nFor example, \\(\\mathrm{\\overline{CMPS^*}_{diff}} = 0.498\\) for Figure 8(b)\n\n\n\nFigure 8: Plot (a) shows the highest possible CMPS scores (the total number of basis segments) for the 36 comparisons. (b) shows the scaled CMPS scores for the 36 comparisons.\n\n\n\nSum of squares ratio\nThe “sum of squares ratio” quantifies how well two groups of values separate.\nLet \\(n_T\\) denote the total number of observations, \\(n_k\\) denote the number of observations in group \\(k\\), and \\(y_{kl}\\) denote the \\(l\\)-th observation in group \\(k\\), for \\(k = 1,2\\) and \\(l = 1, \\dots, n_k\\).\nLet \\(\\bar{y}_{k.} = \\frac{1}{n_k} \\sum_{l=1}^{n_k} y_{kl}\\) denote the mean value in group \\(k\\) and \\(\\bar{y}_{..} = \\frac{1}{n_T} \\left( \\sum_{k} \\sum_{l = 1}^{n_k} y_{kl} \\right)\\) denote the mean value of all observations.\nConsider the following model:\n\\[\\begin{align}\ny_{kl} = \\mu_k + e_{kl}\n\\end{align}\\]\nwhere \\(\\mu_k\\) is the true mean of group \\(k\\) and \\(e_{kl}\\) is a random effect of different observations.\nThen we can define the sum of squares ratio \\(V\\) as:\n\\[\\begin{align}\nV = \\frac{\\sum_k n_k \\left(\\bar{y}_{k.} - \\bar{y}_{..} \\right)^2}{\\sum_k \\sum_l^{n_k} \\left(y_{kl} - \\bar{y}_{k.} \\right)^2 }.\n\\end{align}\\]\nThe numerator of the sum of squares ratio \\(V\\) quantifies the variation between the two groups, while the denominator quantifies the variation within each group.\nThe sum of squares ratio \\(V\\) can be used as an index for evaluating scans, metrics, and different sets of parameters if the same data set is being used.\nSome examples will be presented in the following section.\nIf we impose the normality and independence assumptions on the random effects \\(e_{kl}\\), the sum of squares ratio \\(V\\) becomes a scaled F-statistic with degrees of freedom of \\(k-1\\) and \\(n_T - k\\) and \\(F = \\frac{n_T - k}{k- 1} V\\).\nIf we want to compare different data sets, stating that a certain setup can achieve better separation on one data set than another, we can scale the sum of squares ratio \\(V\\) and obtain the F-statistic and obtain the corresponding p-value as an index for comparison.\nUsing the sum of squares ratio \\(V\\) as an evaluation metric, we are able to construct a pipeline that aims to find the optimal parameter values for the CMPS algorithm by maximizing the sum of squares ratio.\nNote that other measures of an algorithm, such as the accuracy and the AUC (Area Under the Curve), are also important and useful. But when algorithms achieve 100 percent accuracy and AUC value of 1, we need other measures such as the sum of squares ratio to further distinguish the performance of algorithms. In the following section, we will use the sum of squares ratio to compare the CMPS metrics introduced earlier and investigate the effects of different parameter settings.\nResults\n\n\n\n\n\n\n\n\n\nAs presented in the work of Chen et al. (2019), researchers applied the CMPS method to scans of one of the Hamby sets.\nWhile it is not explicitly stated in the paper, we presume this to be Hamby 252, as only those scans were publicly available at the time.\nIn order to show that our implementation of the CMPS algorithm is able to reproduce the results in Chen et al. (2019) and be used for other data sets, we applied our implementation to both Hamby set 252 and Hamby set 44.\nHere we present how we obtained bullet signatures from the Hamby set data: for both Hamby 252 and Hamby 44, we started with scans in the form of x3p files in the database.\nFollowing the framework proposed by Hare et al. (2017), we used the same set of parameters, removed damaged bullet scans, obtained bullet signatures for each bullet land engraving, and removed outliers in bullet signatures.\nNote that researchers of Chen et al. (2019) applied the CMPS algorithm to bullet signatures as well but used a framework different from ours.\nHowever, since their work is not open-source, we were not able to follow their framework and were only able to reproduce the results for Hamby set 252 qualitatively.\nHamby 252\nFigure 9 shows the distribution of \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\) after we applied the CMPS algorithm to Hamby set 252 with the multi-segment lengths strategy.\nThe parameters we used in extract_feature_cmps for the CMPS algorithm are:\n\n\nextract_feature_cmps(\n  x, y,\n  seg_length = 50,\n  Tx = 25,\n  npeaks_set = c(5, 3, 1),\n  include = \"nseg\"\n)\n\n\nAs noted above, the CMPS scores we found here are not exactly the same as those presented in Chen et al. (2019) since we were not able to follow their framework, but the results presented in Figure 9 are qualitatively equivalent to those presented in Chen et al. (2019), showing a clear separation between scores based on comparisons from known matches (KM) and scores from comparisons of known non-matches (KNM) for both \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\).\nAdditionally, to mimic the parameters used in Chen et al. (2019), we set seg_length = 50 and Tx = 25 to make sure that each basis segment has a length of 78.125 m and the tolerance zone is \\(\\pm 39.0625\\) m (one unit represents 1.5625 m for Hamby set 252).\nThe sum of squares ratios are 20.64 and 28.96 for \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\), respectively.\nThis indicates that even though scores from \\(\\mathrm{CMPS_{max}}\\) for known-match comparisons are larger than scores from the averaged version of \\(\\mathrm{\\overline{CMPS}_{max}}\\), these scores achieve a better separation between the two groups of comparisons.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Distribution of \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\) for Hamby 252; outliers are removed in bullet signatures; seg_length = 50, Tx = 25, npeaks_set = c(5,3,1); instead of showing the counts on the y-axis, we present the observed proportions conditioned on KM group and KNM group to enhance the visibility of the bars.\n\n\n\nHamby 44\nSimilar procedures are applied to Hamby set 44, and Figure 10 shows the distribution of \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\), respectively.\nThe parameters used in extract_feature_cmps are:\n\n\nextract_feature_cmps(\n  x, y,\n  seg_length = 61,\n  Tx = 30,\n  npeaks_set = c(5, 3, 1),\n  include = \"nseg\"\n)\n\n\nSince the resolution of Hamby 44 is set to 1.29 m per unit, we make seg_length = 61 and Tx = 30 to ensure that the setup of Hamby set 44 is similar to that of Hamby set 252, resulting in basis segments of 78.69 m and the tolerance zone of \\(\\pm 38.7\\) m.\nAs shown in Figure 10, again, we are able to see a clear separation between the known match comparisons and the known non-match comparisons, even though the separation is relatively small compared with that of Hamby set 252, which is also indicated by the sum of squares ratios.\nFor this specific set of parameters, the sum of squares ratios are 8.87 and 10.64 for \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\), respectively.\nThis might suggest that we could enlarge the separation in terms of the sum of squares ratio by using other CMPS metrics and other sets of parameters.\n\n\n\nFigure 10: Distribution of \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\) for Hamby 44; outliers are removed in bullet signatures; seg_length = 61, Tx = 30, npeaks_set = c(5,3,1); instead of showing the counts on the y-axis, we present the observed proportions conditioned on KM group and KNM group to enhance the visibility of the bars.\n\n\n\nComparing CMPS metrics and parameters\n\n\n\nFigure 11: Comparison of results from the CMPS algorithm based on different basis segment lengths (parameter seg_length). Only the \\(\\mathrm{CMPS_{max}}\\) metric suggests that the default values for the basis segment length result in the best separation. Better separation is achieved based on the modified CMPS metrics, including the newly suggested ones. For Hamby 252 these metrics agree on a segment length of 75, and a segment length of 122 for Hamby 44 yields better results.\n\n\n\n\n\n\nFigure 12: Comparison of CMPS results based on different strategies of number of peak selections. Starred results compare CMPS performance with results published in the literature. Results for the random forest score are represented with circles because the metrics are computed not based on the CMPS scores, but on the random forest scores with the same logic. Since random forest scores lie within the interval \\([0, 1]\\), scaling the random forest scores will not change the results.\n\n\n\nWe investigated the effects of different sets of parameters in the example of both Hamby sets 252 and 44.\nMore specifically, we investigated the separation achieved using the cmpsR implementation under various segment lengths (controlled by the parameter seg_length). Specifically, we fixed the parameter npeaks_set that controls the number of peaks at each segment level to be npeaks_set = c(5, 3, 1) and modified the parameter seg_length.\nFigure 11 shows that the default values of seg_length for Hamby 252 and Hamby 44 (50 for Hamby 252 and 61 for Hamby 44, which represent 78.125 m and 78.69 m, respectively) result in high values of the sum of squares ratio, no matter which CMPS metrics are used for an evaluation.\nHowever, we also see, that for Hamby set 252 a basis segment length of 75 is a better choice than the default segment length; a basis segment length of 122 results in a higher value of the sum of squares ratio for Hamby 44.\nFigure 12 shows the results of the CMPS algorithm using different strategies for identifying peaks in the correlation structure between signatures. Basis segment length is fixed to the default level for this evaluation.\nAs can be seen in Figure 12, the default value of npeaks_set (npeaks_set = c(5, 3, 1)) leads to promising results in terms of the sum of squares ratio; however, other choices of npeaks_set match and exceed this sum of squares ratio value.\nAs seen before, the results suggest that the \\(\\mathrm{CMPS_{max}}\\) metric produces the least amount of separation compared with the other three CMPS metrics.\nThey also suggest that the two newly proposed metrics, \\(\\mathrm{\\overline{CMPS}_{diff}}\\) and \\(\\mathrm{\\overline{CMPS^*}_{diff}}\\), lead to equally good or even better results as \\(\\mathrm{\\overline{CMPS}_{max}}\\).\nBecause \\(\\mathrm{\\overline{CMPS^*}_{diff}}\\) relies on a scaled version of CMPS scores, it is more comparable to other similarity scores and summarizes the out-of-phase background CMPS scores, making it superior to the other CMPS metrics.\nWhat we can also observe in Figure 11 and Figure 12 is that the values of the sum of squares ratio for Hamby 44 is lower than those for Hamby 252.\nThis might be because determining the source is a harder task for Hamby 44 than for Hamby 252, but also suggests that the choice of parameters also depends on the resolution or the scanning process of the data set.\nThe same set of parameters might work for one data set, but not work equally well for another.\nThe purpose of the results shown in Figure 11 and Figure 12 is not to determine the “best” parameters for the CMPS algorithm, but to show that the sum of squares ratio can be used as an evaluation measure to compare different parameters, metrics, or scans.\nA pipeline that maximizes the sum of squares ratio might help researchers determine the set of parameters that work best for their data.\nBut a large database that is representative is what we really need in order to fully understand and cross-validate the parameters of the CMPS algorithm.\nComparing with original results and the random forest model\nChen et al. (2019) present histograms of \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\) for npeaks_set = 5 and npeaks_set = c(5, 3, 1) with (presumably) Hamby 252.\nThe values in these histograms allow us to calculate the sum of squares ratios and include the results in Figure 12 as well.\nThey are marked by an asterisk at the top right corner in Figure 12.\nThe sum of squares ratios we obtained for npeaks_set = 5 and npeaks_set = c(5, 3, 1) is slightly higher than those obtained from the histograms of Chen et al. (2019).\nIt’s curious to see that for the Hamby 252 results published in Chen et al. (2019) the \\(\\mathrm{{CMPS}_{max}}\\) metric achieves values of the sum of squares ratio higher than those achieved by the \\(\\mathrm{\\overline{CMPS}_{max}}\\) metric.\nSince the researchers of Chen et al. (2019) did not use \\(\\mathrm{\\overline{CMPS}_{diff}}\\) or \\(\\mathrm{\\overline{CMPS^*}_{diff}}\\), and they did not apply the CMPS algorithm to the Hamby set 44, we were not able to compare those results.\nIn Figure 12, we also included the sum of squares ratios computed from the random forest scores (Hare et al. 2017) for different metrics.\nThe random forest model presented in Hare et al. (2017) was trained at the Center for Statistics and Applications in Forensic Evidence (CSAFE) and is publicly available in the R package bulletxtrctr (Hofmann et al. 2019).\nSimilar to the CMPS algorithm, this random forest model produces a score to quantify the similarity of a land-by-land comparison.\nThe RF scores lie in an interval of [0,1] making them most comparable to the scaled CMPS scores.\nWe applied the logic of \\(\\mathrm{{CMPS}_{max}}\\), \\(\\mathrm{\\overline{CMPS}_{max}}\\), and \\(\\mathrm{\\overline{CMPS}_{diff}}\\) to the random forest scores and obtained results of the random forest model for different metrics.\nAs shown in Figure 12, the random forest model does not achieve the sum of squares ratio as high as those achieved by the top CMPS algorithms for Hamby 252, but it does achieve better results for Hamby 44.\nThis might suggest that inclusion of the CMPS score as an additional feature in the random forest model training might be beneficial for an overall separation to determine the source.\nFor details about the implementation, examples, and results, please refer to the “Supplementary materials”.\nConclusion\nIn this paper, we present the cmpsR package, an open-source implementation of the Congruent Matching Profile Segments (CMPS) algorithm (Chen et al. 2019), and apply it to two datasets in Hamby study (Hamby et al. 2009) to show its potential for further research.\nThe CMPS algorithm was proposed by NIST in 2019 and was made for objective tool marks comparisons.\nWe introduce the basic logic of the CMPS algorithm and layout its implementation in the cmpsR package.\nWe also showcase the functionality of the cmpsR package with a small dataset example that is included in the package.\nIn the cmpsR package we implement some graphing tools for users to visualize results and to gain a better understanding of both the algorithm and the results.\nAdditionally, we propose two new metrics based on the CMPS scores and compare the new metrics with the existing metrics.\nWe also introduce a principled evaluation framework of algorithmic results using a measure based on the sum of squares ratio.\nWe showcase the implementation with two datasets in the Hamby study (Hamby set 252 and Hamby set 44) and compare the CMPS algorithm using different sets of parameters and different metrics, following the evaluation framework based on the sum of squares ratio.\nThe results obtained are promising: we were both able to reproduce the results in Chen et al. (2019) qualitatively and achieve a clear separation between the known match (KM) comparisons and known non-match (KNM) comparisons in another bullet study.\nHowever, the comparisons among different sets of parameters suggest that the optimal choice for parameter settings varies between different datasets.\nNote, that the main difference between Hamby sets 44 and 252 is that they were taken with different resolution scanning devices. The bullets for Hamby 44 and Hamby 252 are fired from the same ten consecutively rifled P-85 Ruger barrels. The difference in parameter settings for optimal differentiation between same-source comparisons and different-source comparisons is therefore quite surprising. In the next steps, we need validation studies similar to Vanderplas et al. (2020) - trying out the CMPS algorithm on different firearms and ammunition combinations to test the limits of the algorithm. The evaluation framework we proposed based on the sum of squares ratio will facilitate such studies and other validation of the algorithm.\nComparisons of the R implementation of the CMPS algorithm with the random forest model proposed by Hare et al. (2017) suggest that adding the CMPS score as an additional feature in the random forest model might add further separation between known match (KM) comparisons and known non-match (KNM) comparisons.\nThe open-source implementation of the CMPS algorithm provided by the cmpsR package is just one step towards to the framework of open science. Open science is particularly important to fields like forensic science where transparency and accuracy are critical to fair justice.\nOpen-source implementations not only allow a peer-review but also facilitate further research, such as parameter cross-validations, method comparisons, and the development of statistical methods for modeling KM and KNM CMPS score distributions, which can then be used for error-rate estimations and fair applications that were called for by the PCAST (President’s Council of Advisors on Science and Technology 2016) report.\nHowever, having open-source implementations is not enough for open science.\nIn order to actually build the world of open science, many other efforts, such as open evaluation, open access publication, open educational resources, etc., are still needed.\nThe database built and maintained by the National Institute of Standards and Technology is a good example of open data.\nWhat we are aiming for is an open system that is able to collect open results, compare multiple algorithms using multiple datasets, and evaluate algorithmic variation and accuracy.\nThe evaluation metric we proposed in this paper can be used to compare different algorithms or even different datasets and is a snippet of this open system.\nBut the core of this open system is the open science culture and contributions of the community.\nAcknowledgement\nThis work was funded (or partially funded) by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreements 70NANB15H176 and 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.\nSupplementary materials\nThe zip file “supplementary-files.zip” contains R scripts for reproducing all aspects of this paper.\nData not included in “supplementary-files.zip”\nThe processed data of Hamby set 252 and Hamby set 44 are stored in two .rds files:\nBulletSignatures252.rds\nBulletSignatures44.rds\nAnd the ground truth of Hamby set 252 is provided in StudyInfo.xlsx\nDue to the size of the file, these processed data are not included in the “supplementary-files.zip”, but the links are provided for download.\nIn order to reproduce the results in the paper, please download “BulletSignatures252.rds”, “BulletSignatures44.rds”, and “StudyInfo.xlsx” and save them in a folder named “bullet_signatures_etc”. And place the folder “bullet_signatures_etc” into the folder of all the R scripts of “supplementary-files.zip”.\nPlease check out the folder structure of the reproducible folder for reference.\nFile description\nhamby*_result_generator.R: these R scripts take the processed data of Hamby set 252 and Hamby set 44, generate preliminary results used in the paper, and save the results in the .csv format in the folder data-csv. Please make sure that the package versions of bulletxtrctr and cmpsR meet the requirements.\nrds_generator.R: this R script takes the generated .csv files and produce CMPSpaper_results.rds. CMPSpaper_results.rds is identical to data/CMPSpaper_results.rds and is used generate figures and other results presented in the paper.\nData included in “supplementary-files.zip”\ndata-csv: this folder contains .csv files we generated using R scripts hamby*_result_generator.R\nCMPSpaper_results.rds: this is the .rds file we generated using R script rds_generator.R\nThese data can be used as reference or example results of the reproducible codes\ncsafe_rf2.rds: this .rds file contains the random forest model used in this paper\n\nCRAN packages used\ncmpsR, x3ptools\nCRAN Task Views implied by cited packages\n\n\nAFTE Criteria for Identification Committee. Theory of identification, range striae comparison reports and modified glossary definitions. AFTE Journal, 24(3): 336–340, 1992.\n\n\nD. J. Brundage. The Identification of Consecutively Rifled Gun Barrels. AFTE Journal, 30(3): 438–444, 1998.\n\n\nW. Chang, J. Cheng, J. Allaire, C. Sievert, B. Schloerke, Y. Xie, J. Allen, J. McPherson, A. Dipert and B. Borges. shiny: Web Application Framework for R. 2021. URL https://CRAN.R-project.org/package=shiny. R package version 1.6.0.\n\n\nZ. Chen, W. Chu, J. A. Soons, R. M. Thompson, J. Song and X. Zhao. Fired bullet signature correlation using the Congruent Matching Profile Segments (CMPS) method. Forensic Science International, 305: Article 109964, (10 pages), 2019. URL https://www.sciencedirect.com/science/article/pii/S0379073819303767.\n\n\nL. S. Chumbley, M. D. Morris, M. J. Kreiser, C. Fisher, J. Craft, L. J. Genalo, S. Davis, D. Faden and J. Kidd. Validation of tool mark comparisons obtained using a quantitative, comparative, statistical algorithm: VALIDATION OF TOOL MARK COMPARISONS. Journal of forensic sciences, 55(4): 953–961, 2010. DOI https://doi.org/10.1111/j.1556-4029.2010.01424.x.\n\n\nW. S. Cleveland, E. Grosse and W. M. Shyu. Local regression models. In Statistical models in s, Eds J. M. Chambers and T. J. Hastie pages. 309–376 1991. Boca Raton, Florida: Chapman; Hall/CRC.\n\n\nCommittee on Identifying the Needs of the Forensic Sciences of the National Research Council. Strengthening Forensic Science in the United States: A Path Forward. 2009.\n\n\nJ. E. Hamby, D. J. Brundage, N. D. K. Petraco and J. W. Thorpe. A Worldwide Study of Bullets Fired From 10 Consecutively Rifled 9MM RUGER Pistol Barrels—Analysis of Examiner Error Rate. Journal of Forensic Sciences, 64(2): 551–557, 2019. DOI 10.1111/1556-4029.13916.\n\n\nJ. E. Hamby, D. J. Brundage and J. W. Thorpe. The Identification of Bullets Fired from 10 Consecutively Rifled 9mm Ruger Pistol Barrels: A Research Project Involving 507 Participants from 20 Countries. AFTE Journal, 41(2): 99–110, 2009.\n\n\nE. Hare, H. Hofmann and A. Carriquiry. Automatic matching of bullet land impressions. Ann. Appl. Stat., 11(4): 2332–2356, 2017. DOI 10.1214/17-AOAS1080.\n\n\nH. Hofmann, S. Vanderplas and G. Krishnan. Bulletxtrctr: Automatic matching of bullet striae. 2019. URL https://heike.github.io/bulletxtrctr/. R package version 0.2.0.\n\n\nH. Hofmann, S. Vanderplas, G. Krishnan and E. Hare. x3ptools: Tools for Working with 3D Surface Measurements. 2020. URL https://github.com/heike/x3ptools. R package version 0.0.3.\n\n\nG. Krishnan and H. Hofmann. Adapting the Chumbley Score to Match Striae on Land Engraved Areas (LEAs) of Bullets. J Forensic Sci, 64(3): 728–740, 2019.\n\n\nPresident’s Council of Advisors on Science and Technology. Report on forensic science in criminal courts: Ensuring scientific validity of feature-comparison methods. 2016.\n\n\nJ. Song, L. Ma, E. Whitenton and T. Vorburger. 2D and 3D surface texture comparisons using autocorrelation functions. In Measurement technology and intelligent instruments VI, pages. 437–440 2005. Trans Tech Publications Ltd. DOI 10.4028/www.scientific.net/KEM.295-296.437.\n\n\nUnited States Department of Justice, Federal Bureau of Investigation. Crime in the United States, 2019.URL https://ucr.fbi.gov/crime-in-the-u.s/2019/crime-in-the-u.s.-2019/tables/expanded-homicide-data-table-8.xls. Expanded Homicide Data Table 8.\n\n\nUnited States Department of Justice, Federal Bureau of Investigation. Expanded Homicide Tables, 2020.URL https://s3-us-gov-west-1.amazonaws.com/cg-d4b776d0-d898-4153-90c8-8336f86bdfec/CIUS/downloads/2020/expanded-homicide-2020.zip. Data available through the link, not published to the website yet.\n\n\nS. Vanderplas, M. Nally, T. Klep, C. Cadevall and H. Hofmann. Comparison of three similarity scores for bullet LEA matching. Forensic Science International, 308: 110167, 2020. URL https://www.sciencedirect.com/science/article/pii/S0379073820300293.\n\n\nX. A. Zheng. NIST Ballistics Toolmark Research Database (NBTRB). 2016. URL https://tsapps.nist.gov/NRBTD.\n\n\n\n\n",
    "preview": "articles/RJ-2022-035/img/barrel_bullet_ps.png",
    "last_modified": "2022-10-18T09:09:59+00:00",
    "input_file": {},
    "preview_width": 836,
    "preview_height": 516
  },
  {
    "path": "articles/RJ-2022-024/",
    "title": "htestClust: A Package for Marginal Inference of Clustered Data Under Informative Cluster Size",
    "description": "When observations are collected in/organized into observational units, within which observations may be dependent, those observational units are often referred to as \\\"clustered\\\" and the data as \\\"clustered data\\\". Examples of clustered data include repeated measures or hierarchical shared association (e.g., individuals within families). This paper provides an overview of the R package [htestClust](https://CRAN.R-project.org/package=htestClust), a tool for the marginal analysis of such clustered data with potentially informative cluster and/or group sizes. Contained in htestClust are clustered data analogues to the following classical hypothesis tests: rank-sum, signed rank, $t$-, one-way ANOVA, F, Levene, Pearson/Spearman/Kendall correlation, proportion, goodness-of-fit, independence, and McNemar. Additional functions allow users to visualize and test for informative cluster size. This package has an easy-to-use interface mimicking that of classical hypothesis-testing functions in the R environment. Various features of this package are illustrated through simple examples.",
    "author": [
      {
        "name": "Mary Gregg",
        "url": {}
      },
      {
        "name": "Somnath Datta",
        "url": {}
      },
      {
        "name": "Douglas Lorenz",
        "url": {}
      }
    ],
    "date": "2022-10-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-024.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:59+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-026/",
    "title": "APCI: An R and Stata Package for Visualizing and Analyzing Age-Period-Cohort Data",
    "description": "Social scientists have frequently attempted to assess the relative contribution of age, period, and cohort variables to the overall trend in an outcome. We develop an R package [APCI](https://CRAN.R-project.org/package=APCI) (and Stata command apci) to implement the age-period-cohort-interaction (APC-I) model for estimating and testing age, period, and cohort patterns in various types of outcomes for pooled cross-sectional data and multi-cohort panel data. Package [APCI](https://CRAN.R-project.org/package=APCI) also provides a set of functions for visualizing the data and modeling results. We demonstrate the usage of package [APCI](https://CRAN.R-project.org/package=APCI) with empirical data from the Current Population Survey. We show that package [APCI](https://CRAN.R-project.org/package=APCI) provides useful visualization and analytical tools for understanding age, period, and cohort trends in various types of outcomes.",
    "author": [
      {
        "name": "Jiahui Xu",
        "url": {}
      },
      {
        "name": "Liying Luo",
        "url": {}
      }
    ],
    "date": "2022-10-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-026.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:59+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-027/",
    "title": "shinybrms: Fitting Bayesian Regression Models Using a Graphical User Interface for the R Package brms",
    "description": "Despite their advantages, the application of Bayesian regression models is still the exception compared to frequentist regression models. Here, we present our R package [shinybrms](https://CRAN.R-project.org/package=shinybrms) which provides a graphical user interface for fitting Bayesian regression models, with the frontend consisting of a [shiny](https://CRAN.R-project.org/package=shiny) app and the backend relying on the R package [brms](https://CRAN.R-project.org/package=brms) which in turn relies on Stan. With shinybrms, we hope that Bayesian regression models (and regression models in general) will become more popular in applied research, data analyses, and teaching. Here, we illustrate our graphical user interface by the help of an example from medical research.",
    "author": [
      {
        "name": "Frank Weber",
        "url": {}
      },
      {
        "name": "Katja Ickstadt",
        "url": {}
      },
      {
        "name": "Änne Glass",
        "url": {}
      }
    ],
    "date": "2022-10-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-027.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:59+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-030/",
    "title": "TensorTest2D: Fitting Generalized Linear Models with Matrix Covariates",
    "description": "The [TensorTest2D](https://CRAN.R-project.org/package=TensorTest2D) package provides the means to fit generalized linear models on second-order tensor type data. Functions within this package can be used for parameter estimation (e.g., estimating regression coefficients and their standard deviations) and hypothesis testing. We use two examples to illustrate the utility of our package in analyzing data from different disciplines. In the first example, a tensor regression model is used to study the effect of multi-omics predictors on a continuous outcome variable which is associated with drug sensitivity. In the second example, we draw a subset of the MNIST handwritten images and fit to them a logistic tensor regression model. A significance test characterizes the image pattern that tells the difference between two handwritten digits. We also provide a function to visualize the areas as effective classifiers based on a tensor regression model. The visualization tool can also be used together with other variable selection techniques, such as the LASSO, to inform the selection results.",
    "author": [
      {
        "name": "Ping-Yang Chen",
        "url": {}
      },
      {
        "name": "Hsing-Ming Chang",
        "url": {}
      },
      {
        "name": "Yu-Ting Chen",
        "url": {}
      },
      {
        "name": "Jung-Ying Tzeng",
        "url": {}
      },
      {
        "name": "Sheng-Mao Chang",
        "url": {}
      }
    ],
    "date": "2022-10-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-030.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:59+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-031/",
    "title": "wavScalogram: An R Package with Wavelet Scalogram Tools for Time Series Analysis",
    "description": "In this work we present the wavScalogram R package, which contains methods based on wavelet scalograms for time series analysis. These methods are related to two main wavelet tools: the windowed scalogram difference and the scale index. The windowed scalogram difference compares two time series, identifying if their scalograms follow similar patterns at different scales and times, and it is thus a useful complement to other comparison tools such as the squared wavelet coherence. On the other hand, the scale index provides a numerical estimation of the degree of non-periodicity of a time series and it is widely used in many scientific areas.",
    "author": [
      {
        "name": "Vicente J. Bolós",
        "url": {}
      },
      {
        "name": "Rafael Benı́tez",
        "url": {}
      }
    ],
    "date": "2022-10-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-031.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:59+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-032/",
    "title": "ClusTorus: An R Package for Prediction and Clustering on the Torus by Conformal Prediction",
    "description": "Protein structure data consist of several dihedral angles, lying on a multidimensional torus. Analyzing such data has been and continues to be key in understanding functional properties of proteins. However, most of the existing statistical methods assume that data are on Euclidean spaces, and thus they are improper to deal with angular data. In this paper, we introduce the package ClusTorus specialized to analyzing multivariate angular data. The package collects some tools and routines to perform algorithmic clustering and model-based clustering for data on the torus. In particular, the package enables the construction of conformal prediction sets and predictive clustering, based on kernel density estimates and mixture model estimates. A novel hyperparameter selection strategy for predictive clustering is also implemented, with improved stability and computational efficiency. We demonstrate the use of the package in clustering protein dihedral angles from two real data sets.",
    "author": [
      {
        "name": "Seungki Hong",
        "url": {}
      },
      {
        "name": "Sungkyu Jung",
        "url": {}
      }
    ],
    "date": "2022-10-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-032.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:59+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-034/",
    "title": "iccCounts: An R Package to Estimate the Intraclass Correlation Coefficient for Assessing Agreement with Count Data",
    "description": "The intraclass correlation coefficient (ICC) is a widely used index to assess agreement with continuous data. The common approach for estimating the ICC involves estimating the variance components of a linear mixed model under assumptions such as linearity and normality of effects. However, if the outcomes are counts these assumptions are not met and the ICC estimates are biased and inefficient. In this situation, it is necessary to use alternative approaches that are better suited for count data. Here, the iccCounts R package is introduced for estimating the ICC under the Poisson, Negative Binomial, Zero-Inflated Poisson and Zero-Inflated Negative Binomial distributions. The utility of the iccCounts package is illustrated by three examples that involve the assessment of repeatability and concordance with count data.",
    "author": [
      {
        "name": "Josep L. Carrasco",
        "url": "https://webgrec.ub.edu/webpages/personal/ang/005037_jlcarrasco.ub.edu.html"
      }
    ],
    "date": "2022-10-10",
    "categories": [],
    "contents": "\n1 Introduction\nRepeated measurements are often collected and hierarchically structured in clusters (commonly, in subjects). These repeated measurements can be interchangeable within subjects (i.e. they are replicates). This case is often known as the evaluation of repeatability (Nakagawa and Schielzeth 2010) or intra-rater reliability (DeVet et al. 2011). Moreover, the repeated measurements may be structured (not interchangeable) because they were obtained under different experimental conditions, involving different methods or observers. In this case the analysis of agreement is often known as concordance analysis, method comparison analysis (Choudhary and Nagaraja 2017) or inter-rater reliability (DeVet et al. 2011).\nWhatever the structure of the repeated measurements, the intraclass correlation coefficient (ICC) is a common index used to assess agreement with continuous data (Fleiss 1986; Carrasco and Jover 2003). The general definition of the ICC is the ratio of the between-clusters variance to the total variance. However, the appropriate ICC has to be defined to afford the different variance components that are involved in the total variance besides the between-clusters variance. These variance components are typically estimated by means of a linear mixed model with common assumptions: that there is linearity between the outcome expectation and the effects (cluster, method,…); and that the random effects and the random error follow normal distributions. Currently, there are several R packages that estimate the ICC under the Normality assumption for assessing repeatability or concordance (Wolak et al. 2012; Carrasco et al. 2013; Stoffel et al. 2017).\nHowever, if the outcomes are counts, such assumptions are not met and the ICC estimates are biased and inefficient (Carrasco and Jover 2005). In this situation, it is necessary to use alternative approaches that are better suited to the properties of count data. The methodology for estimating the ICC for non-normal distributions using generalized linear mixed models (GLMM), and in particular for count data, was developed in Carrasco (2010). The ICC is therefore estimated by the variance components from the appropriate GLMM. The cluster random effect is still distributed as a Normal distribution but the within-cluster variability is assumed to follow a probability distribution function for counts. Stoffel et al. (2017) introduced the rptR package which can be used to estimate the ICC assuming a Poisson model for the within-subjects variability.\nIn the iccCounts package introduced here, besides the Poisson distribution, other models as the Negative Binomial, the Zero-Inflated Poisson and the Zero-Inflated Negative Binomial are also considered. These models are useful when overdispersion arises in the Poisson model. Overdispersion means that the variability assumed by the model is lower than that from the data. Therefore, the within-subjects variability and, by extension, the total variance are underestimated and the ICC and its standard error are biased. Thus, the validity of the ICC estimate is closely linked to the validity of the model, so that a goodness-of-fit (GOF) analysis of the model must be performed.\nThe article is structured as follows: the section introduces the definition of the ICC, their expressions depending on the model GLLM chosen, some inferential aspects and the validation approach of the GLMM; the issues of the package are described in section; in section three examples are introduced. Two of them are cases of the repeatability setting whereas the remaining one shows the case of a concordance setting. Finally, the main contributions are summarized in the section.\n2 Methodology background\nExperimental design\nAs mentioned in the introduction, the experimental design depends on the aim of the study: concordance or repeatability. In the case of a concordance study, a sample of n subjects are measured m times by k methods. In this setting, the aim is to analyse the degree of concordance of the measurement methods when assessing the subjects. Note that within-subjects repeated measurements are not interchangeable because they belong to one specific method. It is worthy to noting that the term “methods” is the conventional way to describe the experimental condition of the repeated measurements across subjects in this context, but it might be referred to differently name depending on the context. In this setting, \\(Y_{ijk}\\) stands for the k-th reading made by the j-th method on the i-th subject, with \\(i=1,\\ldots,n\\), \\(j=1,\\ldots,m\\) and \\(k=1,\\ldots,s\\). Hence, there will be three variance components to consider when assessing agreement among the repeated measurements: between-subjects, between-methods and random error variabilities.\nIn a repeatability study, a sample of n subjects are measured m times. In this case the repeated measurements share the same experimental condition across subjects, therefore they can be considered as interchangeable. Thus, in this setting \\(Y_{ik}\\) stands for the k-th reading made on the i-th subject, with \\(i=1,\\ldots,n\\), and \\(k=1,\\ldots,s\\). In this case, only two variance components are involved in the evaluation of the agreement: between-subjects and random error variabilities.\nGeneralized linear mixed model\nThe estimation of the variance components is carried out by means of generalized linear mixed models (GLMM). The GLMM for the concordance setting (considering subjects and methods effects) is defined as follows:\nLet \\(\\alpha_i\\) and \\(\\beta_j\\) be the subjects and methods random effects respectively, with \\(i=1,\\ldots,n\\), \\(j=1,\\ldots,m\\), that follow Normal distributions with mean 0 and variance \\(\\sigma_{\\alpha}^2\\) and \\(\\sigma_{\\beta}^2\\). Although the method effect could be a fixed effect by design, when defining the agreement index it is convenient to consider it as a random effect to account for the systematic differences between observers as a source of disagreement (Fleiss 1986; Carrasco and Jover 2003).\nThe conditional distribution of \\(Y_{ijk}\\) given \\(\\alpha_i\\) and \\(\\beta_j\\), \\(f\\left(Y_{ijk}|\\alpha_i,\\beta_j\\right)\\), is a probability density function from the exponential family.\nThe conditional mean of \\(Y_{ijk}\\) given \\(\\alpha_i\\) and \\(\\beta_j\\) is\n\\[\\begin{equation}\n\\mu_{ij}=E\\left(Y_{ijk}|\\alpha_i,\\beta_j\\right)=g^{-1}\\left(\\lambda_i+\\alpha_i+\\beta_j\\right).\n\\tag{1}\n\\end{equation}\\]\nwhere \\(g\\) is called the link function. Here, \\(\\lambda_i\\) is the linear combination of the mean modifying covariates for the i-th subject.\nFurthermore, the conditional variance of \\(Y_{ijk}\\) given \\(\\alpha_i\\) and \\(\\beta_j\\) is defined as \\(Var\\left(Y_{ijk}|\\alpha_i,\\beta_j\\right)=\\phi h\\left(\\mu_{ij}\\right)\\) where \\(\\phi\\) is the dispersion parameter and \\(h\\left(\\right)\\) is variance function.\nIf the methods effect is removed, the GLMM for the repeatability setting is obtained.\nThus, depending on the nature of the data the appropriate conditional probability model and link function must be chosen. When analysing count data, the logarithm is commonly used as a link function and models such as Poisson or Negative Binomial are considered.\nIntraclass correlation coefficient for count data\nThe intraclass correlation coefficient (ICC) is calculated as:\n\\[\\begin{equation}\nICC=\\frac{Cov\\left(Y_{ijk},Y_{ij'k'}\\right)}{Var\\left(Y_{ijk}\\right)}.\n\\tag{2}\n\\end{equation}\\]\nwhere the \\(Cov\\left(Y_{ijk},Y_{ij'k'}\\right)\\) is the marginal (over subjects and observers) covariance between any pair of data from the same subject, whereas \\(Var\\left(Y_{ijk}\\right)\\) is the marginal variance of data.\nFurthermore, the marginal variance and covariance can be developed as functions of the GLMM parameters (Carrasco 2010):\n\\[\\begin{equation}\nICC=\\frac{Cov\\left(\\mu_{ij},\\mu_{ij'}\\right)}{Var\\left(\\mu_{ij}\\right)+E\\left(\\phi h\\left(\\mu_{ij}\\right)\\right)}\n\\tag{3}\n\\end{equation}\\]\nThis result allows the ICC to be generalized to any distribution fitted with a GLMM.\nCarrasco (2010) developed the ICC for Poisson and Negative Binomial distributions, the latter with variance increasing quadratically with the mean (NegBin2), \\(Var\\left(Y_{ijk}|\\alpha_i,\\beta_j\\right)=\\mu_{ij} \\left(1+r\\mu_{ij}\\right)\\) (Table ??). A Negative Binomial model with variance increasing linearly with the mean is also considered (NegBin1) (Hardin and Hilbe 2007; Brooks et al. 2017), \\(Var\\left(Y_{ijk}|\\alpha_i,\\beta_j\\right)=\\mu_{ij} \\left(1+r\\right)\\). It is worth to noting that NegBin1 does not belong to the exponential family (Hardin and Hilbe 2007), therefore this model would not be a proper GLMM. However, it can still be useful to model count data that show overdispersion in a Poisson model.\nAdditionally, it is possible to define the ICC for the cases of zero inflated models (Table ??). Let’s define \\(B_{ijk}\\) as a Bernoulli variable that takes a value of 1 if the reading \\(k\\) on subject \\(i\\) and method \\(j\\) is a structural zero with probability \\(\\pi\\) and 0 otherwise. The observed data, \\(X_{ijk}\\) is the result of \\(X_{ijk}=Y_{ijk}\\left(1-B_{ijk}\\right)\\), where \\(Y_{ijk}\\) is the count variable as defined before. The marginal covariance and variance of \\(X_{ijk}\\) are:\n\\[\\begin{equation}\nCov\\left(X_{ijk},X_{ij'k'}\\right)=Cov\\left(Y_{ijk},Y_{ij'k'}\\right)\\left(1-\\pi\\right)^2\n\\tag{4}\n\\end{equation}\\]\n\\[\\begin{equation}\nVar\\left(X_{ijk}\\right)=Var\\left(Y_{ijk}\\right)\\left(1-\\pi\\right)+E^2\\left(Y_{ijk}\\right)\\pi\\left(1-\\pi\\right)\n\\tag{5}\n\\end{equation}\\]\nand the general expression of the ICC for zero-inflated data becomes:\n\\[\\begin{equation}\nICC=\\frac{Cov\\left(Y_{ijk},Y_{ij'k'}\\right)\\left(1-\\pi\\right)}{Var\\left(Y_{ijk}\\right)+E^2\\left(Y_{ijk}\\right)\\pi}\n\\tag{6}\n\\end{equation}\\]\nwhere \\(\\pi\\) stands for the probability of excess of zeros.\nNotice that ICCs appearing in Table ?? are for the concordance setting where \\(\\sigma^2_\\beta\\) stands for the variability between methods. The ICCs for the repeatability setting are obtained just removing \\(\\sigma^2_\\beta\\) from the equations, i.e. by setting \\(\\sigma^2_\\beta=0\\).\nEstimation of ICC\nThe estimation of the ICC involves estimating the GLMM parameters. However, maximum likelihood approach is not straightforward because there is no closed analytical expression for the marginal likelihood besides the Normal case (linear mixed model). Thus, it is necessary to apply numerical methods to approximate the marginal likelihood and to obtain maximum likelihood estimates (Bolker et al. 2009).\nWith regards to the standard error of the ICC, let \\(\\theta\\) be the GLMM parameters involved in the ICC expression (see Table ??) and \\(\\Sigma\\) the variance-covariance matrix of \\(\\theta\\). The asymptotic standard error can be estimated by applying the delta method (Hoef 2012):\n\\[\\begin{equation}\nVar\\left(ICC\\right) \\approx \\Delta'\\Sigma\\Delta\n\\tag{7}\n\\end{equation}\\]\nwhere \\(\\Delta\\) stand for the vector containing the derivatives of ICC respect to \\(\\theta\\). Confidence intervals for the ICC are based on asymptotic Normal distribution using the inverse hyperbolic tangent function or Fisher’s Z-transformation (Carrasco and Jover 2003).\nValidation\nThe goodness-of-fit (GOF) analysis can be carried out by the computation of randomized quantile residuals (RQR) (Dunn and Smyth 1996; Feng et al. 2020). Briefly, the GOF analysis involve the comparison of the RQR from the original data to those obtained by simulation under the fitted model. Simulations of counts based on the fitted model are generated and the model is refitted to each simulated dataset. Using the simulated RQR, envelopes are built as the appropriate quantiles (in relation to the level of significance) of RQR from the refitted models. If the model fits correctly the data it is expected that the original RQR will completely lie within the simulated envelopes.\nAdditionally, dispersion as well as zero-inflation can be checked by comparing the dispersion and proportion of zeros from the simulated data to those from the original data. Thus, tests for dispersion and zero inflation are carried out by comparing the RQR dispersion and the number of zeros from the original model and data to those from the refitted models and simulated data.\n3 Package description\nThe main function in the iccCounts package is which estimates the ICC under different models for count data. The argument identifies the data set to be analysed. This data set has to be a object with at at least two columns: outcome and subject identifier (arguments and respectively).\nIn the case of estimating the ICC for the concordance setting, a third column with the method identifier must be provided (the argument ). The argument is used to identify the setting in which the ICC should be estimated. Valid values are: (default) for the repeatability setting; and for the concordance setting. The repeatability setting requires that repeated measurements are interchangeable within subjects. This means the experimental conditions of the measurements are the same (replicates), and they come from the same probability distribution function (conditioned to subjects). On the other hand, in the concordance setting the repeated measurements are not interchangeable because their experimental conditions are different, and therefore their probability distribution function, conditioned to subjects, is different (commonly in the mean).\nThe argument is used to identify the within-subjects probability model. Valid options are: (default) for Poisson model; and for Negative Binomial model with variance increasing linearly and quadratically with the mean respectively; for zero-inflated Poisson model; and for zero-inflated Negative Binomial model with variance increasing linearly and quadratically with the mean.\nOnce the appropriate setting and model have been chosen, the GLMM is estimated by maximum likelihood via Laplace approximation using the glmmTMB package (Brooks et al. 2017). The output of the function is an object of class which is a list with the following components: which contains the generalized linear mixed model estimates; which includes the ICC estimate and its standard error and confidence interval; and with the variance components and parameters related to the ICC. Finally, the function runs the goodness of fit (GOF) analysis of the GLMM fitted to data. This function has three arguments: to denote the object to apply the GOF analysis; the argument that stands for the number of simulations to run which default value is set to 100; and the \\(\\alpha\\) argument to set the level of significance.\nThe output of is an object of class which is a list with the following components: , a plot of RQR envelopes with the original RQR; , a plot of the simulated RQR dispersion; , a plot of the count of zeros in the simulated datasets; , the dispersion of RQR from the original sample; , the proportion of simulated RQR dispersion that are greater than the original dispersion that can be interpreted as a simulated P-value to check the goodness of fit on dispersion; , the count of zeros in the original sample; and , the proportion of simulated zero count that are greater than that of the original sample. It can be interpreted as a simulated P-value to check the hypothesis of zero-inflation. The plots in the list are objects of class , hence users may change the plot themes or add modifications to the components.\nAdditionally, to describe the differences among the repeated measurements from the same subjects, the function draws the Bland-Altman plot (Bland and Altman 1995). The difference between each pair of data from the same subject is represented on the y-axis. The mean of data from the same subject is represented on the x-axis. Additionally, a bar plot with the proportions of differences can be drawn. This plot is a useful way to describe the differences when the range of observed values is small relative to the number of observations (Smith et al. 2010).\nThe arguments of function are: , a data frame containing at least the columns of the outcome and subject’s identifier; , a character string indicating the name of the outcome column in the data set; a character string indicating the name of the subjects column in the data set; , a character string indicating the name of the column that stands for the repeated measurements in the data set. This argument is only needed to identify the differences; , argument used to choose the plot to be drawn. Valid values are: (default) for the Bland-Altman plot; and for the bar plot of the differences. Besides the plots, the function provides a dataframe object that contains the data used to generate the plot.\n4 Examples\nThe package includes three real data sets as examples that covers the repeatability and concordance settings.\nSparrow fledglings paternity example\nIn the Sparrow fledglings paternity example, the incidence of extra-pair paternity (EPP) was monitored over 3 breeding seasons in a sparrow colony in Lundy, an island off the southwest coast of England (Schroeder et al. 2012). One of the aims of the study was to assess the repeatability of counts of fledglings that a male had in every breeding season. Thus, the repeated measurements are assumed to be exchangeable replicates. However, the means of the Social variable by year seem to differ:\n\n\nlibrary(iccCounts)\nlibrary(dplyr)\nEPP %>% group_by(Year) %>% summarize(Mean=mean(Social),SD=sd(Social))\n\n# A tibble: 3 × 3\n   Year  Mean    SD\n  <int> <dbl> <dbl>\n1  2003  3.19  3.10\n2  2004  2.53  2.21\n3  2005  4.5   2.79\n\nIn case these means were significantly different the repeated measurements could not be considered as exchangeable, and consequently the differences among the means of the repeated measurements should be included in the agreement index. This led us to the concordance setting considering Year as the methods effect.\nThe first model to consider is the Poisson model. The default option in the function is the Poisson model, and so it is necessary to specify the name of the data set, the count variable (Social), the subjects identifier (id), the methods variable (Year), and the concordance setting (type=“con”).\n\n\nEPP_P<-icc_counts(EPP,y=\"Social\",id=\"id\",met=\"Year\",type=\"con\")\nICC(EPP_P)\n\n           ICC    SE ICC 95% CI LL 95% CI UL\n[1,] 0.5284404 0.0866857 0.3383706 0.6770822\n\nVarComp(EPP_P)\n\n       mu     BSVar     BMVar\n 3.172783 0.4002965 0.0798841\n\nThe function applied to the object shows that the ICC estimate is 0.53 (95% confidence interval: 0.34 - 0.68). Moreover, the function gives the parameters involved in the ICC estimator, which in this case are the overall mean, the between-subjects variance and the between-methods variability (the two latter in log-scale).\nHowever, as mentioned in the previous section, the validity of the ICC estimate is linked to the validity of the model. The function is applied to the object to run the simulations and to compute the RQR. The random seed is set for the sake of the reproducibility of the example.\n\n\nset.seed(100)\nEPP_P.gof <- GOF_check(EPP_P)\n\n\n\n\nplot(EPP_P.gof)\n\n\nFigure 1a shows the plot of RQR with envelopes generated by simulation. Points on the plot stand for the RQR from the original sample. Notice that a substantial number of points lie outside the envelopes, indicating the fit of the model is unsuitable. The next plot (Figure 1c) shows the density of the RQR variances computed in the simulated samples. The RQR variance from the initial sample is 2.15 (shown inside the square) which is extreme compared to those from the simulations. Indeed, the proportion of simulated variances that are higher than that from the initial sample can be interpreted as a p-value generated by Monte Carlo simulation. This p-value is shown by applying the function to the object.\n\n\nDispersionTest(EPP_P.gof)\n\n        S    P_value\n 2.077549 0.00990099\n\nAdditionally, the Social variable has a considerable proportion of zero values (\\(26.4\\%\\)), and so the excess of zeros could be the cause of the unsuitable fitting of the Poisson model. To check this hypothesis, the third plot generated shows the proportion of zeros in the simulated data sets (Figure 1e). The count of zeros in the sample is 51, which exceeds the expected count under the Poisson model. Again, the proportion of simulated zero counts that are higher than that from the initial sample can be interpreted as a p-value generated by Monte Carlo simulation. This p-value is obtained by applying the function to the object.\n\n\nZeroTest(EPP_P.gof)\n\n Count    P_value\n    51 0.00990099\n\nThus, it is necessary to use a model able to provide a proportion of zeros higher than that expected under the Poisson assumption. This model could be the Zero-Inflated Poisson (ZIP) model.\n\n\nEPP_ZIP<-icc_counts(EPP,y=\"Social\",id=\"id\",met=\"Year\",type=\"con\",fam=\"zip\")\nICC(EPP_ZIP)\n\n           ICC    SE ICC 95% CI LL 95% CI UL\n[1,] 0.0477628 0.0362002  -0.02331 0.1183553\n\nVarComp(EPP_ZIP)\n\n       mu    BSVar     BMVar        Pi\n 4.487117 0.033328 0.0328427 0.2446678\n\nIn this case, the ICC is much lower than in the Poisson model (0.05, 95% CI: -0.02, 0.12) indicating a non-significant ICC. The ICC components are the same as those in the Poisson case (with different values) plus the proportion of excess of zeros (0.24). Next step is to check whether the model correctly fits the data.\n\n\nset.seed(100)\nEPP_ZIP.gof <- GOF_check(EPP_ZIP)\n\n\n\n\nplot(EPP_ZIP.gof)\n\n\n\n\n\nFigure 1: Goodness of fit for Sparrow fledglings paternity example. The Randomized Quantile Residuals (RQR) and counts of zeros of original data are compared to those from simulated data under the fitted model. The plots shown are RQR with envelopes, dispersion of RQR and count of zeros. Left column shows results for Poisson model while the plots for Zero Inflated Poisson (ZIP) model are on right column.\n\n\n\nFigure 1b shows the model to be appropriate because all the RQR are within the envelopes. Additionally, the dispersion and the proportion of zeros from the initial sample are within the values expected under the ZIP model (Figures 1d and 1f). This fact can be also verified by verifying that dispersion and excess of zeros tests are non significant.\n\n\nDispersionTest(EPP_ZIP.gof)\n\n        S   P_value\n 3.214152 0.5643564\n\nZeroTest(EPP_ZIP.gof)\n\n Count   P_value\n    51 0.4752475\n\nThus, the ZIP model fits the data appropriately. The next step is to check if the differences in means between years are significant and the concordance setting is therefore justified. With this aim let us apply the function to the ZIP model but in the repeatability setting.\n\n\nEPP_ZIP_0<-icc_counts(EPP,y=\"Social\",id=\"id\",fam=\"zip\")\n\n\nThe component in the object is an object of class that contains the generalized linear mixed model fit. The method applied to the model objects gives a comparison of deviances and a likelihood ratio test:\n\n\nanova(EPP_ZIP$model,EPP_ZIP_0$model)\n\nData: data\nModels:\nEPP_ZIP_0$model: y ~ (1 | id), zi=~1, disp=~1\nEPP_ZIP$model: y ~ met + (1 | id), zi=~1, disp=~1\n                Df    AIC    BIC  logLik deviance  Chisq Chi Df\nEPP_ZIP_0$model  3 847.63 857.42 -420.82   841.63              \nEPP_ZIP$model    5 836.35 852.67 -413.18   826.35 15.279      2\n                Pr(>Chisq)    \nEPP_ZIP_0$model               \nEPP_ZIP$model    0.0004811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe result of the comparison confirms a significant difference between the yearly means, and the convenience of the concordance setting.\nNext, let us apply the function to generate the Bland-Altman plot and the bar plot of the differences in EPP between years. The plots are shown in Figures 4a and 4b.\n\n\nEPP.BA<-plot_BA(EPP,y=\"Social\",id=\"id\",rm=\"Year\") # Bland-Altman plot\n\n\n\n\nplot_BA(EPP,y=\"Social\",id=\"id\",type=\"bars\") # Bar plot\n\n\nIt can be seen that the magnitude of the differences grows as the mean increases. This heteroscedastic pattern is expected in counts because of the relation between the within-subjects variance and mean. Furthermore, we can compute some descriptive statistics of the differences:\n\n\nsummary(EPP.BA$data$Diff)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-7.0000 -1.0000  1.0000  0.9737  3.0000 10.0000 \n\nquantile(EPP.BA$data$Diff,probs=c(0.05,0.95))\n\n 5% 95% \n -4   6 \n\nBriefly, the mean of the differences between years is 0.97 fledglings, and the median is 1 fledgling. Ninety percent of the differences are between -4 and 6 fledglings between years.\nCD34+ count cell example\nThe dataset includes data where a new method of flow cytometry for counting CD34+ cells is compared to the readings obtained by a standard approach (Fornas et al., 2000). Both methods (coded as 1 and 3 in the dataset) were applied to a sample of 20 subjects. The aim of the study is to assess the interchangeability between the methods so it is necessary to evaluate the degree of concordance. Hence, the ICC used here concurs with the concordance correlation coefficient estimated by variance components (Carrasco and Jover 2003).\nLet’s firstly consider the Poisson model. As we are facing a concordance analysis, in the function the name of the method’s variable () has to be provided along with the counts variable () and subject’s identifier (). Additionally, it is necessary to specify the concordance analysis in the type argument because the default is the repeatability analysis.\n\n\nAF_P <- icc_counts(AF, y = \"y\", id = \"id\", met = \"met\", type = \"con\")\nICC(AF_P)\n\n           ICC   SE ICC 95% CI LL 95% CI UL\n[1,] 0.8472696 0.021989 0.7982025 0.8851678\n\nVarComp(AF_P)\n\n      mu    BSVar     BMVar\n 761.809 1.234619 0.1199439\n\nThe function applied to the object shows that the ICC estimate is 0.85 (95% confidence interval: 0.80, 0.89). Moreover, the function gives the parameters involved in the ICC estimator. In this case are the overall mean (mu), the between-subjects variance (BSVar) and the between-methods variability (BMVar) (the two last in log-scale).\nNext, let’s check the validity of the model by applying the function to the object.\n\n\nset.seed(100)\nAF_P.gof <- GOF_check(AF_P)\n\n\nFigure 2a shows the plot of RQR with envelopes generated by simulation. Points on the plot stand for the RQR from the original sample. Most of points lie outside of the envelopes indicating the unsuitable fit of the model. Next plot (Figure 2b) shows the density of the RQR variances computed at the simulated samples. The RQR variance from the initial sample is 32.2 which is much larger than those from the simulations. The p-value to test overdispersion is obtained by applying the function to the . With regards the zero inflation, no zeros were found in data so it is unnecessary to check this issue.\n\n\nDispersionTest(AF_P.gof)\n\n        S    P_value\n 32.20049 0.00990099\n\nConsequently, it is necessary to use a model able to afford the overdispersion as the Negative Binomial distribution.\n\n\nAF_NB2 <- icc_counts(AF, y = \"y\", id = \"id\", met = \"met\", type = \"con\", fam = \"nbinom2\")\nICC(AF_NB2)\n\n          ICC    SE ICC 95% CI LL 95% CI UL\n[1,] 0.834794 0.0454062 0.7212048 0.9046669\n\nVarComp(AF_NB2)\n\n       mu    BSVar     BMVar         r\n 777.1946 1.188904 0.0809433 0.0488122\n\nIn this case, the ICC is quite similar to that from the Poisson model (0.83, 95% CI: 0.72, 0.90) but the confidence interval is wider. The ICC components are the same as those from the Poisson case (with different values) plus the Negative Binomial dispersion parameter (0.049). Concerning the fit of the model,\n\n\nset.seed(100)\nAF_NB2.gof <- GOF_check(AF_NB2)\n\n\n\n\nplot(AF_NB2.gof)\n\n\n\n\n\nFigure 2: Goodness of fit for CD34 cell count example. The Randomized Quantile Residuals (RQR) of original data are compared to those from simulated data under the fitted model. The plots shown are RQR with envelopes, and dispersion of RQR. First row shows results for Poisson model while the plots for Negative Binomial model are on second row.\n\n\n\nin Figure 2c all the RQR are within the envelopes indicating the appropriateness of the model. Additionally, the dispersion from the initial sample is within the expected values (Figure 2d). This result is also confirmed by running the dispersion test:\n\n\nDispersionTest(AF_NB2.gof)\n\n         S   P_value\n 0.8002765 0.4059406\n\nFigures 4c and 4d show the Bland-Altman plot and the Bar plot of the differences between method 1 and 2 that are generated by the following commands:\n\n\nAF.BA <- plot_BA(AF,y=\"y\",id=\"id\", rm=\"met\") # Bland-Altman plot\n\n\n\n\nplot_BA(AF,y=\"y\",id=\"id\", type=\"bars\") # Bar plot\n\n\nIt can be seen that for values of the mean below 700 the within-subjects differences are very close to 0. However, for larger values of the mean there is a trend in the differences in relation to the mean values.\nTick counts example\nIn this study, the repeatability of line transects survey method to estimate tick abundance was assessed (Kjellander et al. 2021) in the area of Grimso (Sweden). With this aim, sampling was performed by two parallel transects separated by 1m-2m where the total count of ticks was recorded. Here, the transects are the cluster variable and every pair of data from the same transect are considered as replicates. Data is stored in the package as the object.\nAs seen before the first model to consider is the Poisson model. As it is a repeatability analysis, in the function we just need to provide the name of the counts variable () and subject’s identifier ().\n\n\nG_P <- icc_counts(Grimso, y = \"Tot\", id = \"TransectID\")\nICC(G_P)\n\n           ICC    SE ICC 95% CI LL 95% CI UL\n[1,] 0.3494333 0.1369518 0.0589753 0.5853431\n\nVarComp(G_P)\n\n        mu    BSVar\n 0.2072297 1.278685\n\nThe function applied to the object shows that the ICC estimate is 0.35 (95% confidence interval: 0.06, 0.59). The function gives the parameters involved in the ICC estimator: the overall mean and the between-subjects variance (the latter in log-scale).\nLet’s apply the function to the object to check the validity of the model.\n\n\nset.seed(100)\nG_P.gof <- GOF_check(G_P)\n\n\n\n\nplot(G_P.gof)\n\n\nFigure 3a shows the plot of RQR with envelopes generated by simulation. All points on the plot lie within the envelopes indicating the fit of the model is correct. Additionally, Figure 3b shows the RQR variance from the initial sample (1.84) is compatible with the dispersion observed in the simulated samples. The overdispersion test is run by applying the function to the object.\n\n\nDispersionTest(G_P.gof)\n\n       S   P_value\n 1.83724 0.4158416\n\nThe p-value is 0.416 so the null hypothesis of no overdispersion is not rejected and no further models have to be fitted.\n\n\n\nFigure 3: Goodness of fit for Tick counts example. The Randomized Quantile Residuals (RQR) of original data are compared to those from simulated data under the fitted model. The plots shown are RQR with envelopes, and dispersion of RQR for Poisson model.\n\n\n\nThe Bland-Altman plot and the Bar plot of the within-subjects differences are shown in Figures 4e and 4f.\n\n\nG.BA <- plot_BA(Grimso,y=\"Tot\",id=\"TransectID\",rm=\"Round\") # Bland-Altman plot\n\n\n\n\nplot_BA(Grimso,y=\"Tot\",id=\"TransectID\", type=\"bars\") # Bar plot\n\n\nAs in the case of the sparrow fledgling paternity counts, we can observe a heteroscedastic pattern with the variability of the differences increasing with the mean. Most of the differences are 0 (75%) and 90% of the differences lie between -1 and 1.\n\n\nquantile(G.BA$data$Diff, probs=c(0.05,0.95))\n\n 5% 95% \n -1   1 \n\n\n\n\n\n\n\nFigure 4: Bland-Altman and Bar plots. The first column shows the Bland-Altman plots where difference between pairs of data from the same subject (Y-axis) is plotted against mean of data from the same subject (X-axis). The second column contains the Bar plots of the differences between pairs of data from the same subject. The plots for Sparrow fledglings paternity example are on the first row, the CD34+ count cell example plots are on second row, and plots for Tick counts example are on third row.\n\n\n\n5 Conclusion\nThe statistical assessment of agreement is an issue that has received a considerable attention in recent years. It is possible to find statistical software to carry out such analysis for qualitative or continuous data (see for example Revelle (2021);Carrasco et al. (2013);Feng (2020))\n. However, there is a lack of tools when dealing with discrete data. Here, the package have been introduced to assess the agreement with such type of data considering both repeatability and concordance settings. Furthermore, the package provides the methodology to assess the validity of the model fitted to data.\nIt is important to note that no factors or predictors other than subjects and methods have been considered in the linear predictor of the GLMM. When fitting a GLMM, the inclusion of further covariates allows controlling for confounding effects. In this case, the ICC computed after controlling for confounding effects is referred to as adjusted repeatability (Nakagawa and Schielzeth 2010). Including covariates in the linear predictor make sense when the aim is to estimate the magnitude of an effect (difference in means, odds ratio or ratio of means, for instance) adjusted by the covariates. When estimating the ICC in a linear model, the inclusion of covariates will lead to a change in the variance components estimates but they remain as common estimates for all subjects. However, when facing the models for count data, the addition of covariates in the linear predictor leads to different ICCs because the value of the marginal mean \\(\\mu\\) will be different for every level of the covariates. For this reason, in the case of count data, it is preferable to segregate the data to estimate a different ICC according to the covariates. In this way, both the variance components and the mean will be different.\nFurthermore, in the Normal model setting the ICC takes values from 0 to 1. A value of 0 means independence among the measures from the same cluster (no cluster effect) whilst a value of 1 implies perfect agreement, i.e. all data from the same subject are equal. However, it is not possible to reach a value of 1 in the counts setting. The reason for this is that some within-subject variability is unavoidable because of the relation between the variance and the mean in these models. Thus, it i s not possible to observe perfect agreement with count data but the interpretation remains the same: the proportion of the total variance accounted for between-subjects variability.\n6 Availability\nThe current stable version of the package requires R 4.0 and can be downloaded from CRAN . Furthermore, depends on the following R packages: glmmTMB (Brooks et al. 2017); ggplot2 (Wickham 2016); Deriv (Clausen and Sokol 2020); gridExtra (Auguie 2017); and dplyr (Wickham et al. 2020).\n7 Acknowledgments\nI would like to thank to Shinichi Nakagawa from UNSW Sydney (Australia) for kindly providing the Sparrow fledglings paternity data. I am also indebted to Pia Kjellander from Linköping University (Sweden) for sharing the Tick count data.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-034.zip\nCRAN packages used\nrptR, iccCounts, glmmTMB, ggplot2, Deriv, gridExtra, dplyr\nCRAN Task Views implied by cited packages\nDatabases, Epidemiology, ModelDeployment, NumericalMathematics, Spatial, TeachingStatistics\n\n\nB. Auguie. gridExtra: Miscellaneous functions for \"grid\" graphics. R package version 2.3. 2017. URL https://CRAN.R-project.org/package=gridExtra.\n\n\nJ. M. Bland and D. G. Altman. Comparing methods of measurement: Why plotting difference against standard method is misleading. The lancet, 346(8982): 1085–1087, 1995.\n\n\nB. M. Bolker, M. E. Brooks, C. J. Clark, S. W. Geange, J. R. Poulsen, M. H. H. Stevens and J.-S. S. White. Generalized linear mixed models: A practical guide for ecology and evolution. Trends in Ecology & Evolution, 24(3): 127–135, 2009.\n\n\nM. Brooks, K. Kristensen, K. van Benthem, A. Magnusson, C. Berg, A. Nielsen, H. Skaug, M. Maechler and B. Bolker. glmmTMB balances speed and flexibility among packages for zero-inflated generalized linear mixed modeling. The R Journal, 9: 299–314, 2017.\n\n\nJ. L. Carrasco. A generalized concordance correlation coefficient based on the variance components generalized linear mixed models for overdispersed count data. Biometrics, 66: 897–904, 2010.\n\n\nJ. L. Carrasco and L. Jover. Concordance correlation coefficient applied to discrete data. Statistics in Medicine, 24: 4021–4034, 2005.\n\n\nJ. L. Carrasco and L. Jover. Estimating the generalized concordance correlation coefficient through variance components. Biometrics, 59: 849–858, 2003.\n\n\nJ. L. Carrasco, B. Phillips, J. Puig-Martinez, T. King and V. Chinchilli. Estimation of the concordance correlation coefficient for repeated measures using SAS and R. Computer Methods and Programs in Biomedicine, 109: 293–304, 2013.\n\n\nP. Choudhary and H. Nagaraja. Measuring agreement: Models, methods, and applications. Hoboken: Wiley, 2017.\n\n\nA. Clausen and S. Sokol. Deriv: R-based symbolic differentiation. Deriv package version 4.1. 2020. URL https://CRAN.R-project.org/package=Deriv.\n\n\nH. DeVet, C. Terwee, L. Mokkink and D. Knol. Measurement in medicine. Cambridge: Cambridge University Press, 2011.\n\n\nP. Dunn and G. Smyth. Randomized quantile residuals. Journal Computational and Graphical Statistics, 5: 236–244, 1996.\n\n\nC. Feng, L. Li and A. Sadeghpour. A comparison of residual diagnosis tools for diagnosing regression models for count data. BMC Medical Research Methodology, 20: 175, 2020.\n\n\nD. Feng. agRee: Various methods for measuring agreement. 2020. URL https://CRAN.R-project.org/package=agRee. R package version 0.5.3.\n\n\nJ. Fleiss. The design and analysis of clinical experiments. New York: Wiley, 1986.\n\n\nW. Hardin and J. Hilbe. Generalized linear models and extensions. Stata Press, 2007.\n\n\nJ. V. Hoef. Who invented the delta method? The American Statistician, 66: 124–127, 2012.\n\n\nP. Kjellander, M. Aronsson, U. Bergvall, J. L. Carrasco, M. Christensson, P. Lindgren, M. Akesson and P. Kjellander. Validating a common tick survey method: Cloth-dragging and line transects. Experimental and Applied Acarolog, 83: 131–146, 2021.\n\n\nS. Nakagawa and H. Schielzeth. Repeatability for gaussian and non-gaussian data: A practical guide for biologists. Biological Reviews, 85: 935–956, 2010.\n\n\nW. Revelle. Psych: Procedures for psychological, psychometric, and personality research. Evanston, Illinois: Northwestern University, 2021. URL https://CRAN.R-project.org/package=psych. R package version 2.1.9.\n\n\nJ. Schroeder, T. Burke, M. Mannarelli, D. Dawson and S. Nakagawa. Maternal effects and heritability of annual productivity. Journal of Evolutionary Biology, 25: 149–156, 2012.\n\n\nM. W. Smith, J. Ma and R. S. Stafford. Bar charts enhance bland–altman plots when value ranges are limited. Journal of Clinical Epidemiology, 63(2): 180–184, 2010.\n\n\nM. Stoffel, S. Nakagawa and H. Schielzeth. rptR: Repeatability estimation and variance decomposition by generalized linear mixed-effects models. Methods in Ecology and Evolution, 8: 1639–1644, 2017.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. New York: Springer-Verlag, 2016.\n\n\nH. Wickham, R. François, L. Henry and K. Müller. Dplyr: A grammar of data manipulation. R package version 1.0.2. 2020. URL https://CRAN.R-project.org/package=dplyr.\n\n\nM. Wolak, J. Fairbairn and R. Paulsen. Guidelines for estimating repeatability. Methods in Ecology and Evolution, 3: 129–137, 2012.\n\n\n\n\n",
    "preview": "articles/RJ-2022-034/distill-preview.png",
    "last_modified": "2022-10-18T09:09:59+00:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 1728
  },
  {
    "path": "articles/RJ-2022-037/",
    "title": "PDFEstimator: An R Package for Density Estimation and Analysis",
    "description": "This article presents PDFEstimator, an R package for nonparametric probability density estimation and analysis, as both a practical enhancement and alternative to kernel-based estimators. PDFEstimator creates fast, highly accurate, data-driven probability density estimates for continuous random data through an intuitive interface. Excellent results are obtained for a diverse set of data distributions ranging from 10 to $10^6$ samples when invoked with default parameter definitions in the absence of user directives. Additionally, the package contains methods for assessing the quality of any estimate, including robust plotting functions for detailed visualization and trouble-shooting. Usage of PDFEstimator is illustrated through a variety of examples, including comparisons to several kernel density methods.",
    "author": [
      {
        "name": "Jenny Farmer",
        "url": {}
      },
      {
        "name": "Donald Jacobs",
        "url": {}
      }
    ],
    "date": "2022-10-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-037.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-038/",
    "title": "reclin2: a Toolkit for Record Linkage and Deduplication",
    "description": "The goal of record linkage and deduplication is to detect which records belong to the same object in data sets where the identifiers of the objects contain errors and missing values. The main design considerations of reclin2 are: modularity/flexibility, speed and the ability to handle large data sets. The first points makes it easy for users to extend the package with custom process steps. This flexibility is obtained by using simple data structures and by following as close as possible common interfaces in R. For large problems it is possible to distribute the work over multiple worker nodes. A benchmark comparison to other record linkage packages for R, shows that for this specific benchmark, the fastLink package performs best. However, this package only performs one specific type of record linkage model. The performance of reclin2 is not far behind the of fastLink while allowing for much greater flexibility.",
    "author": [
      {
        "name": "D. Jan van der Laan",
        "url": {}
      }
    ],
    "date": "2022-10-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-038.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-039/",
    "title": "The Concordance Test, an Alternative to Kruskal-Wallis Based on the Kendall-$\\tau$ Distance: An R Package",
    "description": "The Kendall rank correlation coefficient, based on the Kendall-$\\tau$ distance, is used to measure the ordinal association between two measurements. In this paper, we introduce a new coefficient also based on the Kendall-$\\tau$ distance, the Concordance coefficient, and a test to measure whether different samples come from the same distribution. This work also presents a new R package, ConcordanceTest, with the implementation of the proposed coefficient. We illustrate the use of the Concordance coefficient to measure the ordinal association between quantity and quality measures when two or more samples are considered. In this sense, the Concordance coefficient can be seen as a generalization of the Kendall rank correlation coefficient and an alternative to the non-parametric mean rank-based methods for comparing two or more samples. A comparison of the proposed Concordance coefficient and the classical Kruskal-Wallis statistic is presented through a comparison of the exact distributions of both statistics.",
    "author": [
      {
        "name": "Javier Alcaraz",
        "url": {}
      },
      {
        "name": "Laura Anton-Sanchez",
        "url": {}
      },
      {
        "name": "Juan Francisco Monge",
        "url": {}
      }
    ],
    "date": "2022-10-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-039.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-040/",
    "title": "R-miss-tastic: a unified platform for missing values methods and workflows",
    "description": "Missing values are unavoidable when working with data. Their occurrence is exacerbated as more data from different sources become available. However, most statistical models and visualization methods require complete data, and improper handling of missing data results in information loss or biased analyses. Since the seminal work of Rubin (1976), a burgeoning literature on missing values has arisen, with heterogeneous aims and motivations. This led to the development of various methods, formalizations, and tools. For practitioners, however, it remains a challenge to decide which method is most appropriate for their problem, in part because this topic is not systematically covered in statistics or data science curricula. To help address this challenge, we have launched the `R-miss-tastic` platform, which aims to provide an overview of standard missing values problems, methods, and relevant implementations of methodologies. Beyond gathering and organizing a large majority of the material on missing data (bibliography, courses, tutorials, implementations), `R-miss-tastic` covers the development of standardized analysis workﬂows. Indeed, we have developed several pipelines in R and Python to allow for hands-on illustration of and recommendations on missing values handling in various statistical tasks such as matrix completion, estimation, and prediction, while ensuring reproducibility of the analyses. Finally, the platform is dedicated to users who analyze incomplete data, researchers who want to compare their methods and search for an up-to-date bibliography, and teachers who are looking for didactic materials (notebooks, recordings, lecture notes).",
    "author": [
      {
        "name": "Imke Mayer",
        "url": {}
      },
      {
        "name": "Aude Sportisse",
        "url": {}
      },
      {
        "name": "Julie Josse",
        "url": {}
      },
      {
        "name": "Nicholas Tierney",
        "url": {}
      },
      {
        "name": "Nathalie Vialaneix",
        "url": {}
      }
    ],
    "date": "2022-10-10",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-029/",
    "title": "Refreg: An R Package for Estimating Conditional Reference Regions",
    "description": "Multivariate reference regions (MVR) represent the extension of the reference interval concept to the multivariate setting. A reference interval is defined by two threshold points between which a high percentage of healthy subjects' results, usually 95%, are contained. Analogously, an MVR characterizes the values of several diagnostic tests most frequently found among non-diseased subjects by defining a convex hull containing 95% of the results. MVRs have great applicability when working with diseases that are diagnosed via more than one continuous test, e.g., diabetes or hypothyroidism. The present work introduces refreg, an R package for estimating conditional MVRs. The reference region is non-parametrically estimated using a multivariate kernel density estimator, and its shape allowed to change under the influence of covariates. The effects of covariates on the multivariate variable means, and on their variance-covariance matrix, are estimated by flexible additive predictors. Continuous covariate non-linear effects can be estimated by penalized spline smoothers. The package allows the user to propose, for instance, an age-specific diagnostic rule based on the joint distribution of two non-Gaussian, continuous test results. The usefulness of the refreg package in clinical practice is illustrated with a real case in diabetes research, with an age-specific reference region proposed for the joint interpretation of two glycemia markers (fasting plasma glucose and glycated hemoglobin). To show that the refreg package can also be used in other, and indeed very different fields, an example is provided for the joint prediction of two atmospheric pollutants (SO$_2$, and NO$_x$). Additionally, the text discusses how, conceptually, this method could be extended to more than two dimensions.",
    "author": [
      {
        "name": "Óscar Lado-Baleato",
        "url": {}
      },
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      },
      {
        "name": "Carmen Cadarso-Suárez",
        "url": {}
      },
      {
        "name": "Francisco Gude",
        "url": {}
      }
    ],
    "date": "2022-09-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-029.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:59+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-022/",
    "title": "Power and Sample Size for Longitudinal Models in R -- The longpower Package and Shiny App",
    "description": "Longitudinal studies are ubiquitous in medical and clinical research. Sample size computations are critical to ensure that these studies are sufficiently powered to provide reliable and valid inferences. There are several methodologies for calculating sample sizes for longitudinal studies that depend on many considerations including the study design features, outcome type and distribution, and proposed analytical methods. We briefly review the literature and describe sample size formulas for continuous longitudinal data. We then apply the methods using example studies comparing treatment versus control groups in randomized trials assessing treatment effect on clinical outcomes. We also introduce a Shiny app that we developed to assist researchers with obtaining required sample sizes for longitudinal studies by allowing users to enter required pilot estimates. For Alzheimer's studies, the app can estimate required pilot parameters using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Illustrative examples are used to demonstrate how the package and app can be used to generate sample size and power curves. The package and app are designed to help researchers easily assess the operating characteristics of study designs for Alzheimer's clinical trials and other research studies with longitudinal continuous outcomes. Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu).",
    "author": [
      {
        "name": "Samuel Iddi",
        "url": {}
      },
      {
        "name": "Michael C Donohue",
        "url": {}
      }
    ],
    "date": "2022-07-04",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-022.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-019/",
    "title": "fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models",
    "description": "Machine learning decision systems are becoming omnipresent in our lives. From dating apps to rating loan seekers, algorithms affect both our well-being and future. Typically, however, these systems are not infallible. Moreover, complex predictive models are eager to learn social biases present in historical data that may increase discrimination. If we want to create models responsibly, we need tools for in-depth validation of models also from potential discrimination. This article introduces an R package fairmodels that helps to validate fairness and eliminate bias in binary classification models quickly and flexibly. The fairmodels package offers a model-agnostic approach to bias detection, visualization, and mitigation. The implemented functions and fairness metrics enable model fairness validation from different perspectives. In addition, the package includes a series of methods for bias mitigation that aim to diminish the discrimination in the model.  The package is designed to examine a single model and facilitate comparisons between multiple models.",
    "author": [
      {
        "name": "Jakub Wiśniewski",
        "url": {}
      },
      {
        "name": "Przemysław Biecek",
        "url": "https://pbiecek.github.io/"
      }
    ],
    "date": "2022-06-27",
    "categories": [],
    "contents": "\nIntroduction\nResponsible machine learning and, in particular, fairness is gaining attention within the machine learning community. This is because predictive algorithms are becoming more and more decisive and influential in our lives. This impact could be less or more significant in areas ranging from user feeds on social platforms, displayed ads, and recommendations at an online store to loan decisions, social scoring, and facial recognition systems used by police and authorities. Sometimes it leads to automated systems that learn some undesired bias preserved in data for some historical reason. Whether seeking a job (Lahoti et al. 2019) or having one’s data processed by court systems (Angwin et al. 2016), sensitive attributes such as sex, race, religion, ethnicity, etc., might play a significant role in the decision. Even if such variables are not directly included in the model, they are often captured by proxy variables such as zip code (a proxy for the race and wealth), purchased products (a proxy for gender and age), eye colour (a proxy for ethnicity). As one would expect, they can give an unfair advantage to a privileged group. Discrimination takes the form of more favorable predictions or higher accuracy for a privileged group. For example, some popular commercial gender classifiers were found to perform the worst on darker females (Buolamwini and Gebru 2018). From now on, such unfair and harmful decisions towards people with specific sensitive attributes will be called biased.\nThe list of protected attributes may depend on the region and domain for which the model is built. For example, the European Union law is summarized in the Handbook on European non-discrimination law European Union Agency for Fundamental Rights and Council of Europe (2018), which lists the following protected attributes that cannot be the basis for inferior treatment: sex, gender identity, sexual orientation, disability, age, race, ethnicity, nationality or national origin, religion or belief, social origin, birth, and property, language, political or other opinions. This list, though long, does not include all potentially relevant items, e.g. in the USA, a protected attribute is also pregnancy, the status of a war veteran, or genetic information.\nWhile there are historical and economic reasons for this to happen, such decisions are unacceptable in society, where nobody should have an unfair advantage.\nThe problem is not simple, especially when the only criterion set for the system is performance. We observe a trade-off between accuracy and fairness in some cases where lower discrimination leads to lower performance (Kamiran and Calders 2011). Sometimes labels, which are considered ground truth, might also be biased (Wick et al. 2019), and when controlling for that bias, the performance and fairness might improve simultaneously. However fairness is not a concept that a single number can summarize, so most of the time, when we want to improve fairness from one perspective, it becomes worse in another (Barocas et al. 2019).\nThe bias in machine learning systems has potentially many different sources. Mehrabi et al. (2019) categorized bias into its types like historical bias, where unfairness is already embedded into the data reflecting the world, observer bias, sampling bias, ranking and social biases, and many more. That shows how many dangers are potentially hidden in the data itself. Whether one would like to act on it or not, it is essential to detect bias and make well-informed decisions whose consequences could potentially harm many groups of people. Repercussions of such systems can be unpredictable. As argued by Barocas et al. (2019), machine learning systems can even aggravate the disparities between groups, which is called by the authors’ feedback loops. Sometimes the risk of potential harm resulting from the usage of such systems is high. This was noticed, for example, by the Council of Europe that wrote the set of guidelines where it states that the usage of facial recognition for the sake of determining a person’s sex, age, origin, or even emotions should be mostly prohibited (Council of Europe 2021).\nNot every difference in treatment is discrimination. Cirillo et al. (2020) presents examples of desirable and undesirable biases based on the medical domain. For example, in the case of cardiovascular diseases, documented medical knowledge indicates that different treatments are more effective for different genders. So different treatment regimens according to medical knowledge are examples of desirable bias. Later in this paper, we present tools to identify differences between groups defined by some protected attribute but note that this does not automatically mean that there is discrimination.\nWe would also like to point out that focusing on the machine learning model may not be enough in some cases, and sometimes the design of the data acquisition and/or annotation cause the model to be biased (Barocas et al. 2019).\nRelated work\nAssembling predictive models is getting easier nowadays. Packages like h2o (H2O.ai 2017) provide AutoML frameworks where non-experts can train quickly accurate models without deep domain knowledge. Model validation should also be that simple. Yet this is not the case. There are still very few tools to support the fairness diagnostics of the model.\nTwo main kinds of fairness are a concern to multiple stakeholders. These are group and individual fairness. The first one concerns groups of people with the same protected attributes (gender, race, etc.). It focuses on measuring if these groups are treated similarly by the model. The second one is focused on the individual. It is most intuitively defined as treating similar individuals similarly (Dwork et al. 2012). Both concepts are sometimes considered to conflict with each other, but they don’t need to be if we factor in certain assumptions, such as whether the disparities are due to personal choices or unjust structures (Binns 2020).\nSeveral frameworks have emerged for Python to verify various fairness criteria, the most popular are aif360 (Bellamy et al. 2018), fairlearn (Bird et al. 2020), or aequitas (Saleiro et al. 2018). They have various features for detecting, visualization, and mitigating bias in machine learning models.\nFor the R language, until recently, the only available tool was the fairness (Kozodoi and V. Varga 2021) package which compares various fairness metrics for specified subgroups. The fairness package is very helpful, but it lacks some features. For example, it does not allow comparing the machine learning models and aggregating fairness metrics to facilitate the visualization. Still, most of all, it does not give a quick verdict on whether a model is fair or not. Package fairadapt aims at removing bias from machine learning models by implementing pre-processing procedure described in Plečko and Meinshausen (2019). Our package tries to combine the detection and mitigation processes. It encourages the user to experiment with the bias, try different mitigation methods and compare results. The package fairmodels not only allows for that comparison between models and multiple exposed groups of people, but it gives direct feedback if the model is fair or not (more on that in the next section). Our package also equips the user with a so-called fairness_object, an object aggregating possibly many models, information about data, and fairness metrics. fairness_object can later be transformed into many other objects that can facilitate the visualization of metrics and models from different perspectives. If a model does not meet fairness criteria, various pre-processing and post-processing bias mitigation algorithms are implemented and ready to use. It aims to be a complete tool for dealing with discriminatory models in a group fairness setting.\nIn particular, in the following sections, we show how to use this package to address four key questions: How to measure bias? How to detect bias? How to visualize bias? and How to mitigate bias?\nIt is important to remember that fairness is not a binary concept that can be unambiguously defined, and there is no silver bullet that will make any model fair. The presented tools allow for fairness exploratory analysis, thanks to which we will be able to detect differences in the behavior of the model for different protected groups. But such analysis will not guarantee that all possible fairness problems have been detected. Also, fairness analysis is only one of a wide range of techniques for Explanatory Model Analysis (Biecek and Burzykowski 2021).\nLike other explanatory tools, it should be used with caution and awareness.\nMeasuring and detecting bias\nIn model fairness analysis, a distinction is often made between group fairness and individual fairness analysis. The former is defined by the equality of certain statistics determined on protected subgroups, and we focus on this approach in this section. We write more about the latter later in this paper.\nFairness metrics\nMachine learning models, just like human-based decisions, can be biased against observations related to people with certain sensitive attributes, which are also called protected groups. This is because they consist of subgroups - people who share the same sensitive attribute, like gender, race, or other features.\nTo address this problem, we need first to introduce fairness criteria. Following Barocas et al. (2019), we will present these criteria based on the following notation.\nLet \\(A \\in \\{a,b, ...\\}\\) mean protected group and values \\(A \\neq a\\) denote membership to unprivileged subgroups while \\(A = a\\) membership to privileged subgroup. To simplify the notation, we will treat this as a binary variable (so \\(A = b\\) will denote membership to unprivileged subgroup), but all results hold if \\(A\\) has a larger number of groups.\nLet \\(Y \\in \\{0,1\\}\\) be a binary label (binary target = binary classification) where \\(1\\) is preferred, favorable outcome.\nLet \\(R \\in [0,1]\\) be a probabilistic response of the model, and \\(\\hat{Y} \\in \\{0,1\\}\\) is the binarised model response, so \\(\\hat{Y} = 1\\) when \\(R \\geq 0.5\\), otherwise \\(\\hat{Y} = 0\\).\nFigure 1 summarizes possible situations for the subgroup \\(A=a\\). We can draw up the same table for each of the subgroups.\n\n\n\nFigure 1: Summary of possible model outcomes for subpopulation \\(A = a\\). We assume that outcome \\(Y = 1\\) is favourable.\n\n\n\nAccording to Barocas et al. (2019) most discrimination criteria can be derived as tests that validate the following probabilistic definitions:\nIndependence, i.e. \\(R \\perp A\\),\nSeparation, i.e. \\(R \\perp A \\mid Y\\),\nSufficiency, i.e. \\(Y \\perp A \\mid R\\).\nThose criteria and their relaxations might be expressed via different metrics based on a confusion matrix for a certain subgroup. To check if those fairness criteria are addressed, we propose checking five metrics among privileged group (a) and unprivileged group (b):\nStatistical parity: \\(P(\\hat{Y} = 1 | A = a) = P(\\hat{Y} = 1 | A = b)\\). Statistical parity (STP) ensures that fractions of assigned positive labels are the same in subgroups. It is equivalent of Independence (Dwork et al. 2012). In other words, the values in the last column of Figure 1 are the same for each subgroup.\nEqual opportunity: \\(P(\\hat{Y} = 1 | A = a, Y = 1) = P(\\hat{Y} = 1 | A = b, Y = 1)\\). Checks if classifier has equal True Positive Rate (TPR) for each subgroup. In other words, the column normalized values in the second column of Figure 1 are the same for each subgroup. It is a relaxation of Separation (Hardt et al. 2016).\nPredictive parity: \\(P(Y = 1 | A = a, \\hat{Y} = 1) = P(Y = 1 | A = b, \\hat{Y} = 1)\\). Measures if a model has equal Positive Predictive Value (PPV) for each subgroup. In other words, the row normalized values in the second row of Figure 1 are the same for each subgroup. It is relaxation of Sufficiency (Chouldechova 2016).\nPredictive equality: \\(P(\\hat{Y} = 1 | A = a, Y = 0) = P(\\hat{Y} = 1 | A = b, Y = 0)\\). Warrants that classifiers have equal False Positive Rate (FPR) for each subgroup. In other words, the column normalized values in the third column of Figure 1 are the same for each subgroup. It is relaxation of Separation (Corbett-Davies et al. 2017).\n(Overall) Accuracy equality: \\(P(\\hat{Y} = Y | A = a) = P(\\hat{Y} = Y | A = b)\\). Makes sure that models have the same Accuracy (ACC) for each subgroup. (Berk et al. 2017)\nThe reader should note that if the classifier passes Equal opportunity and Predictive equality, it also passes Equalized Odds (Hardt et al. 2016), which is equivalent to Separation criteria.\nLet us illustrate the intuition behind Independence, Separation, and Sufficiency criteria using the well-known example of the COMPAS model for estimating recidivism risk.\nFulfilling the Independence criterion means that the rate of sentenced prisoners should be equal in each subpopulation. It can be said that such an approach is fair from society’s perspective.\nFulfilling the Separation criterion means that the fraction of innocents/guilty sentenced should be equal in subgroups. Such an approach is fair from the prisoner’s perspective. The reasoning is the following: “If I am innocent, I should have the same chance of acquittal regardless of sub-population”. This was the expectation presented by the ProPublica Foundation in their study.\nMeeting the Sufficiency criterion means that there should be an equal fraction of innocents among the convicted, similarly, for the non-convicted. This approach is fair from the judge’s perspective. The reasoning is the following: “If I convicted someone, he should have the same chance of being innocent regardless of the sub-population”. This approach is presented by the company developing the COMPAS model, Northpointe.\nUnfortunately, as we have already written, it is not possible to meet all these criteria at the same time.\nWhile defining the metrics above, we assumed only two subgroups. This was done to facilitate notation, but there might be more unprivileged subgroups. A perfectly fair model would pass all criteria for each subgroup (Barocas et al. 2019).\nNot all fairness metrics are equally important in all cases. The metrics above aim to give a more holistic view into the fairness of the machine learning model. Practitioners informed in the domain may consider only those metrics that are relevant and beneficial from their point of view. For example, in Kozodoi et al. (2021) in the fair credit scoring use case, the authors concluded that the separation is the most suitable non-discrimination criteria. More general instructions can also be found in European Union Agency for Fundamental Rights (2018), along with examples of protected attributes. Sometimes, however, non-technical solutions to fairness problems might be beneficial. Note that group fairness metrics will discover not all types of unfairness, and the end-user should decide whether a model is acceptable in terms of bias or not.\nHowever tempting it is to think that all the criteria described above can be met at the same time, unfortunately, this is not possible. Barocas et al. (2019) shows that, apart from a few hypothetical situations, no two of {Independence, Separation, Sufficiency} can be fulfilled simultaneously. So we are left balancing between the degree of imbalance of the different criteria or deciding to control only one criterion.\nAcceptable amount of bias\nIt would be hard for any classifier to maintain the same relations between subgroups. That is why some margins around the perfect agreement are needed. To address this issue, we accepted the four-fifths rule (Code of Federal Regulations 1978) as the benchmark for discrimination rate, which states that “A selection rate for any race, sex, or ethnic group which is less than four-fifths (\\(\\frac{4}{5}\\)) (or eighty percent) of the rate for the group with the highest rate will generally be regarded by the Federal enforcement agencies as evidence of adverse impact[…].” The selection rate is originally represented by statistical parity, but we adopted this rule to define acceptable rates between subgroups for all metrics. There are a few caveats to the preceding citation concerning the size of the sample and the boundary itself. Nevertheless, the four-fifths rule is an excellent guideline to adhere to. In the implementation, this boundary is represented by \\(\\varepsilon\\), and it is adjustable by the user, but the default value will be 0.8.\nThis rule is often used, but users should check if the fairness criteria should be set differently in each case.\nLet \\(\\varepsilon > 0\\) be the acceptable amount of a bias. In this article, we would say that the model is not discriminatory for a particular metric if the ratio between every unprivileged \\(b, c, ...\\) and privileged subgroup \\(a\\) is within \\((\\varepsilon, \\frac{1}{\\varepsilon})\\). The common choice for the epsilon is 0.8, which corresponds to the four-fifths rule. For example, for the metric Statistical Parity (\\(STP\\)), a model would be \\(\\varepsilon\\)-non-discriminatory for privileged subgroup \\(a\\) if it satisfies.\n\\[\\begin{equation}\n\\forall_{b \\in A \\setminus \\{a\\}} \\;\\;\n   \\varepsilon < STP_{ratio} = \\frac{STP_b}{STP_a} < \\frac{1}{\\varepsilon}.\n  \\tag{1}\n\\end{equation}\\]\nEvaluating fairness\nThe main function in the fairmodels package is fairness_check. It returns fairness_object, which can be visualized or processed by other functions. This will be further explained in the “Structure” section. When calling fairness_check for the first time, the following three arguments are mandatory:\nexplainer - an object that combines model and data that gives a unified interface for predictions. It is a wrapper over a model created with the DALEX (Biecek 2018) package.\nprotected - a factor, vector containing sensitive attributes (protected group). It does not need to be binary. Instead, each level denotes a distinct subgroup. The most common examples are gender, race, nationality, etc.\nprivileged - a character/factor denoting a level in the protected vector which is suspected to be the most privileged one.\nExample\nIn the following example, we are using German Credit Data dataset (Dua and Graff 2017). In the dataset, there is information about people like age, sex, purpose, credit amount, etc. For each person, there is a risk assessed with taking credit, either good or bad. Therefore, it will be a target variable. We will train the model on the whole dataset and then measure fairness metrics to facilitate the notation (as opposed to training and testing on different subsets, which is also possible and advisable).\nFirst, we create a model. Let’s start with logistic regression.\n\n\nlibrary(\"fairmodels\")\ndata(\"german\")\n\nlm_model <- glm(Risk~., data = german, family = binomial(link = \"logit\"))\n\n\n\nThen, create a wrapper that unifies the model interface.\n\n\nlibrary(\"DALEX\")\n\ny_numeric <- as.numeric(german$Risk) -1\nexplainer_lm <- DALEX::explain(lm_model, data = german[,-1], y = y_numeric)\n\n\n\nNow we are ready to calculate and plot the fairness checks. Resulting plot is presented in Figure 2.\n\n\nfobject <- fairness_check(explainer_lm,\n                protected = german$Sex, privileged = \"male\",\n                verbose = FALSE)\n\n\n\n\n\nplot(fobject)\n\n\n\n\nFigure 2: The Fairness Check plot summarises the ratio of fairness measures between unprivileged and privileged subgroups. The light green areas correspond to values within \\((\\varepsilon, \\frac{1}{\\varepsilon})\\) and signify an acceptable difference in fairness metrics. They are bounded by red rectangles indicating values that do not meet the 4/5 rule. Fairness metrics names are given along the formulas used to calculate the score in some subgroups to facilitate interpretation. For example, the ratio here means that after metric scores were calculated, the values for unprivileged groups (female) were divided by values for the privileged subgroup (male). In this example, except for the predictive equality ratio, the other measures are \\(\\varepsilon\\)-non-discriminatory.\n\n\n\nFor a quick assessment, if a model passes fairness criteria, the object created with fairness_check() might be summarized with the print() function. Total loss is the sum of all fairness metrics. See equation (2) for more details.\n\n\nprint(fobject, colorize = FALSE)\n\n\n\nFairness check for models: lm \n\nlm passes 4/5 metrics\nTotal loss :  0.6153324 \n\nIn this example, fairness criteria are satisfied in all but one metric. The logistic regression model has a lower false-positive rate (FP/(FP+TN))) in the unprivileged group than in the privileged group. It exceeds the acceptable limit set by \\(\\varepsilon\\). Thus it does not satisfy the Predictive Equality ratio criteria.\nMore detailed visualizations are available, like Metric scores plot. It might be helpful to understand the intuition behind the Fairness check plot presented above. See an example in Figure 3. This plot might be a good first point for understanding the Fairness check plot. In fact, checks can be directly derived from the Metric scores plot. To do this, we need to divide the score denoted by the dot with the score denoted by the vertical line. This way, we obtain a value indicated by the height of the barplot. The orientation of the barplot depends on whether the value is bigger or lower than 1. Intuitively the longer the horizontal line in the figure below (the one connecting the dot with the vertical line) is, the longer the bar will be in Fairness check plot. If the scores of privileged and unprivileged subgroups are the same, then the bar will start from 1 and point to 1, so it will have a height equal to 0.\n\n\nplot(metric_scores(fobject))\n\n\n\n\nFigure 3: The Metric Scores plot summarises raw fairness metrics scores for subgroups. The dots stand for unprivileged subgroups (female) while vertical lines stans for the privileged subgroup (male). The horizontal lines act as a visual aid for measuring the difference between the scores of the metrics between the privileged and unprivileged subgroups.\n\n\n\nIt is rare that a model perfectly meets all the fairness criteria. Therefore, a handy feature is the ability to compare several models on the same scale. We add two more explainers to the fairness assessment in the example below. Now fairness_object (in code: fobject) wraps three models together with different labels and cutoffs for subgroups. The fairness_object can be later used as a basis for another fairness_object. In detail, while running fairness_check() for the first time, explainer/explainers have to be provided along with three arguments described at the start of this section. However, as shown below, when providing explainers with a fairness_object, those arguments are not necessary as they are already a part of the previously created fairness_object.\nFirst, let us create two more models based on the German Credit Data. The first will be a logistic regression model that uses fewer columns and has access to the Sex feature. The second is random forest from ranger (Wright and Ziegler 2017). It will be trained on the whole dataset.\n\n\ndiscriminative_lm_model <- glm(Risk~.,\n         data   = german[c(\"Risk\", \"Sex\",\"Age\",\n                \"Checking.account\", \"Credit.amount\")],\n         family = binomial(link = \"logit\"))\n\nlibrary(\"ranger\")\nrf_model <- ranger::ranger(Risk ~.,\n         data = german, probability = TRUE,\n         max.depth = 4, seed = 123)\n\n\n\nThese models differ in the way how the predict function works. To unify operations on these models, we need to create DALEX explainer objects. The label argument specifies how these models are named on plots.\n\n\nexplainer_dlm <- DALEX::explain(discriminative_lm_model,\n        data = german[c(\"Sex\", \"Age\", \"Checking.account\", \"Credit.amount\")],\n        y = y_numeric,\n        label = \"discriminative_lm\") \n\nexplainer_rf <- DALEX::explain(rf_model, \n        data = german[,-1], y = y_numeric)\n\n\n\nNow we are ready to assess fairness. The resulting plot is presented in Figure 4.\n\n\nfobject <- fairness_check(explainer_rf, explainer_dlm, fobject)\nplot(fobject)\n\n\n\n\nFigure 4: The Fairness Check plot for multiple models. It helps to compare models based on five selected fairness measures.\n\n\n\nWhen plotted, new bars appear on the fairness check plot. Those are new metric scores for added models. This information can be summarized in a numerical way with the print() function.\n\n\nprint(fobject, colorize = FALSE)\n\n\n\nFairness check for models: ranger, discriminative_lm, lm \n\nranger passes 5/5 metrics\nTotal loss :  0.1699186 \n\ndiscriminative_lm passes 3/5 metrics\nTotal loss :  0.7294678 \n\nlm passes 4/5 metrics\nTotal loss :  0.6153324 \n\nPackage architecture\nThe fairmodels package provides a unified interface for predictive models independently of their internal structure. Using a model agnostic approach with DALEX explainers facilitates this process (Biecek 2018). There is a unified way for each explainer to check if explained model lives up to user fairness standards. Checking fairness with fairmodels is straightforward and can be done with the three-step pipeline.\nclassification model   |>   explain()   |>   fairness_check() \nThe output of such a pipeline is an object of class fairness_object, a unified structure to wrap model explainer or multiple model explainers and other fairness_objects in a single container. Aggregation of fairness measures is done based on groups defined by model labels. This is why model explainers (even those wrapped by fairness_objects) must have different labels. Moreover, some visualizations for model comparison assume that all models are created from the same data. Of course, each model can use different variables or different feature transformations, but the order and number of rows shall stay the same. To facilitate aggregation of models fairmodels allows creating fairness_objects in other ways:\nexplainers |> fairness_check() - possibly many explainers can be passed to fairness_check(),\nfairness_objects |> fairness_check() - explainers stored in fairness_objects passed to fairness_check() will be aggregated into one fairness_object,\nexplainer & fairness_objects |> fairness_check() - explainers passed directly and explainers from fairness_objects will be aggregated into one fairness_object.\nWhen using the last two pipelines, protected vectors and privileged parameters are assumed to be the same, so passing them to fairness_check() is unnecessary.\nTo create a fairness_object, at least one explainer needs to be passed to fairness_check() function, which returns the said object. fairness_object metrics for each subgroup are calculated from the separate confusion matrices.\nThe fairness_object has numerous fields. Some of them are:\nparity_loss_metric_data - data.frame containing parity loss for each metric and classifier,\ngroups_data - list of metric scores for each metric and model,\ngroup_confusion_matrices - list of values in confusion matrices for each model and metric,\nexplainers - list of DALEX explainers. When explainers and/or fairness_object are added, then explainers and/or explainers extracted from fairness_object are added to that list,\nlabel - character vector of labels for each explainer.\n... - other fields.\nThe fairness_object methods are used to create numerous objects that help to visualize bias. In the next sections, we list more detailed functions for deeper exploration of bias. Detailed relations between objects created with fairmodels are depicted in Figure 5.\nThe general overview of the workflow is presented in Figure 6.\n\n\n\nFigure 5: Class diagram for objects created by functions from the fairmodels package. Each rectangle corresponds to one class, the name of this class is in the header of the rectangle. Each of these classes is a list containing a certain list of objects. The top slot lists the names and types of each object the list. The bottom slot contains a list of functions that can be performed on objects of the specified class. If two classes are connected by a line ending in a diamond it means that one class contains objects of the other class. If two rectangles are connected by a dashed line, it means that on the basis of one object, an object of another class can be produced. In this case, more detailed fairness statistics can be produced from the central object of the fairness check class. See the full resolution at https://bit.ly/3HNbNvo\n\n\n\n\n\n\nFigure 6: Flowchart for the fairness assessment with the fairmodels package. The arrows describe typical sequences of actions when exploring the fairness of the models. For ease of use, the names of the functions that can be used in a given step are indicated. Note that this procedure is intended to look at the model from multiple perspectives in order to track down potential problems in the model. Merely satisfying the fairness criteria does not automatically mean that the model is free of any errors\n\n\n\nVisualizing bias\nIn fairmodels there are 12 metrics based on confusion matrices for each subgroup, see the following table for the complete list. Some of them were already introduced before.\n\n\nFairness metrics implemented in the fairmodels\npackage.\n\n\nMetric\n\n\nFormula\n\n\nName\n\n\nFairness criteria\n\n\nTPR\n\n\n\\(\\frac{TP}{TP + FN}\\)\n\n\nTrue positive rate\n\n\nEqual opportunity (Hardt et al. 2016)\n\n\nTNR\n\n\n\\(\\frac{TN}{TN + FP}\\)\n\n\nTrue negative rate\n\n\n\n\nPPV\n\n\n\\(\\frac{TP}{TP + FP}\\)\n\n\nPositive predictive value\n\n\nPredictive parity (Chouldechova 2016)\n\n\nNPV\n\n\n\\(\\frac{TN}{TN + FN}\\)\n\n\nNegative predictive value\n\n\n\n\nFNR\n\n\n\\(\\frac{FN}{FN + TP}\\)\n\n\nFalse negative rate\n\n\n\n\nFPR\n\n\n\\(\\frac{FP}{FP + TN}\\)\n\n\nFalse positive rate\n\n\nPredictive equality (Corbett-Davies et al.\n2017)\n\n\nFDR\n\n\n\\(\\frac{FP}{FP + TP}\\)\n\n\nFalse discovery rate\n\n\n\n\nFOR\n\n\n\\(\\frac{FN}{FN + TN}\\)\n\n\nFalse omission rate\n\n\n\n\nTS\n\n\n\\(\\frac{TP}{TP + FN + FP}\\)\n\n\nThreat score\n\n\n\n\nF1\n\n\n\\(\\frac{2 \\cdot PPV * TPR}{PPV + TPR}\\)\n\n\nF1 score\n\n\n\n\nSTP\n\n\n\\(\\frac{TP + FP}{TP + FP + TN + FN}\\)\n\n\nPositive rate\n\n\nStatistical parity\n\n\nACC\n\n\n\\(\\frac{TP + TN}{TP + TN + FP + FN}\\)\n\n\nAccuracy\n\n\nOverall accuracy equality\n\n\n\n\n\nNot all metrics are needed to determine if the discrimination exists, but they are helpful to acquire a fuller picture. To facilitate the visualization over many subgroups, we introduce a function that maps metric scores among subgroups to a single value. This function, which we call parity_loss, has an attractive property. Due to the usage of the absolute value of the natural logarithm, it will return the same value whether the ratio is inverted or not.\nSo, for example, when we would like to know the parity loss of Statistical Parity between unprivileged (b) and privileged (a) subgroups, we mean value like this:\n\\[\\begin{equation}\nSTP_{\\textit{parity loss}} = \\Big | \\ln \\Big( \\frac{STP_b}{STP_a} \\Big)\\Big|.\n\\end{equation}\\]\nThis notation is very helpful because it allows to accumulate \\(STP_{\\textit{parity loss}}\\) overall unprivileged subgroups, so not only in the binary case.\n\\[\\begin{equation}\nSTP_{\\textit{parity loss}} = \\sum_{i \\in \\{a, b, ...\\}} \\Big|\\ln \\Big(\\frac{STP_i}{STP_a} \\Big)\\Big|.  \n  \\tag{2}\n\\end{equation}\\]\nThe parity_loss relates strictly to ratios. The classifier is more fair if parity_loss is low. This property is helpful in visualizations.\nThere are several modifying functions that operate on fairness_object. Their usage will return other objects. The relations between them is depicted on the class diagram (Figure 5). The objects can then be plotted with a generic plot() function. Additionally, a special plotting function works immediately on fairness_object, which is plot_density. The user can directly specify which metrics shall be visible in the plot in some functions. The detailed technical introduction for all these functions is presented in fairmodels.\nPlots visualizing different aspects of parity_loss can be created with one of the following pipelines:\nfairness_object |> modifying_function(...) |> plot()\nThis pipe is preferred and allows setting parameters in both modifying functions and certain plot functions, which is not the case with the next pipeline.\nfairness_object |> plot_fairmodels(type = modifying_function, ...)\nAdditional parameters are passed to the modifying functions and not to the plot function.\nUsing the pipelines, different plots can be obtained by superseding the modifying_function with function names.\nFour examples of additional graphical functions available in the fairmodels can be seen in Figure 7. This package implements a total of 8 different diagnostic plots, each describing a different fairness perspective. To see different aspects of fairness and bias, the user can choose the model with the smallest bias, find out the similarity between metrics and models, compare models in both fairness and performance, and see how cutoff manipulation might change the parity_loss. Find more information about each of them in the documentation.\n\n\nfp1  <- plot(ceteris_paribus_cutoff(fobject, \"male\", cumulated=TRUE))\nfp2  <- plot(fairness_heatmap(fobject))\nfp3  <- plot(stack_metrics(fobject))\nfp4  <- plot(plot_density(fobject))\n\n\n\n\n\nlibrary(\"patchwork\")\nfp1 + fp2 + fp3 + fp4 + \n  plot_layout(ncol = 2)\n\n\n\n\nFigure 7: Four examples of additional graphical functions are available in the fairmodels package that facilitates model and bias exploration. The Ceteris Paribus Cuttoff plot helps select the cutoff values for each model to maximize a particular measure of fairness. In this case, the suggested cutoff point for both linear models is similar. However, the ranger model does not have calibrated probabilities and thus requires a different cutoff. The Heatmap plot is very helpful when comparing large numbers of models. It shows profiles of selected fairness measures for each of the models under consideration. In this case, the fairness profiles for both linear models are similar. The Stacked Metric plot helps you compare models by summing five different fairness measures. The different layers of this plot allow you to compare individual measures, but if you don’t know which one to focus on, it is useful to look at the sum of the measures. In this case, the ranger model has the highest fairness values. Finally, the Density plot helps to compare the score distributions of the models between the advantaged and disadvantaged groups. In this case, we find that for females the distributions of the scores are lower in all models, with the largest difference for the lm model.\n\n\n\nBias mitigation\nWhat can be done if the model does not meet the fairness criteria? Machine learning practitioners might use other algorithms or variables to construct unbiased models, but this does not guarantee passing the fairness_check(). An alternative is to use bias mitigation techniques that adjust the data or model to meet fairness conditions.\nThere are essentially three types of such methods. The first is data pre-processing. There are many ways to “correct” the data when there are unwanted correlations between variables or sample sizes among subgroups in data. The second one is in-processing, which is, for example, optimizing classifiers not only to reduce classification error but also to minimize a fairness metric. Last but not least is post-processing which modifies model output so that predictions and miss-predictions among subgroups are more alike.\nThe fairmodels package offers five functions for bias mitigation, three for pre-processing, and two for post-processing. Most of these approaches are also implemented in (Bellamy et al. 2018). However, in fairmodels there are separate implementations of them in R. There are a lot of useful mitigation techniques that are not in fairmodels like those in Hardt et al. (2016) and numerous in-processing algorithms.\nData pre-processing\nDisparate impact remover\nIn fairmodels geometric repair, an algorithm originally introduced by Feldman et al. (2015), works on ordinal, numeric features. Depending on the \\(\\lambda \\in [0,1]\\) parameter, this method will transform the distribution of a given feature. The idea is simple. Given feature distribution in different subgroups, the algorithm finds optimal distribution (according to earth mover’s distance) and transforms distribution for each subgroup to match the optimal one. For example, if age is an important feature and its distribution is different in two subgroups, and we want to change that, then the geometric repair will map each individual’s age to a new distribution (different age). It will be preserving the order - the ranks (in our case, seniority) of observations are preserved. Parameter \\(\\lambda\\) is responsible for the repair degree, so for full repair, lambda should be set to 1. The method does not focus on a particular metric but rather tries to level out them by transforming potentially harmful feature distributions.\nReweighting\nReweighting is a rather straightforward approach. This method was implemented according to Kamiran and Calders (2011). It computes weights by dividing the theoretical probability of assigning favorable labels for a subgroup by real (observed) probability (based on the data). Theoretic probability for a subgroup is computed by multiplying the probability of assigning a favorable label (for all populations) by picking observation from a certain subgroup. It focuses on mitigating statistical parity.\nResampling\nResampling is based on weights calculated in reweighting. Each weight for a subgroup is multiplied by the size of the subgroup. Then, whether the subgroup is deprived or not (if weight is higher than one, the subgroup is considered deprived), observations are duplicated from either one that were assigned a favorable label or not. There are two types of resampling- uniform and preferential. The uniform is making algorithm pick or omit observations randomly without considering its probabilistic score. Preferential uses another probabilistic classifier, potentially different from the main model for final predictions. In Kamiran and Calders (2011) it is called ranker - it predicts the probabilities for the observations to decide which observations are close to the cutoff border (usually 0.5). Based on the probabilistic output of the ranker, the observations are sorted, and the ones with the highest/lowest ranks are either left out or duplicated depending on the case—more on that on Kamiran and Calders (2011). The fairmodels implementation, instead of training the ranker as in the aforementioned paper, uses a vector of previously calculated probabilities provided by the user. With this, it shifts the decision and responsibility of choosing a ranker to the user. It focuses on mitigating statistical parity.\nModel post-processing\nReject Option based Classification Pivot\nThe roc_pivot method is implemented based on Kamiran et al. (2012) in the fairmodels package. Let \\(\\theta \\in (0,1)\\) be the value that determines the radius of the so-called critical region, which is an area around the cutoff. The user specifies the \\(\\theta\\), and it should describe how big the critical region should be. For example if \\(\\theta = 0.1\\) and cutoff is 0.6, then the critical region will be (0.5, 0.7). Let’s assume that we are predicting a favorable outcome. If the assigned probability of observation is in the described region, then the probabilities are pivoting on the other side of the cutoff with a certain assumption. If an observation in a critical region is considered to be the privileged and it is on the right side of the cutoff, then its probabilities are pivoting from the right side of the cutoff to the left. So if an observation is in the critical region and it is considered unprivileged, then if it is on the left side of the cutoff, it will pivot to the right side. Pivoting here means changing the side of the cutoff so that the distance from the cutoff stays unchanged. It does not intend to mitigate a single metric but rather changes predictions in the critical region (the region with low certainty). By pivoting the predictions, it might lower more metrics.\nCutoff manipulation\nThe fairmodels package supports setting cutoff for each subgroup. Users may pick parity_loss metrics of their choice and find the minimal parity_loss. It is part of ceteris_paribus_cutoff() function. Based on picked metrics, the sum of parity loss is calculated for each cutoff of the chosen subgroup. Then the minimal value is found—this way, optimal values might be found for metrics of interest. The minimum is marked with a dashed vertical line (see Figure 7). This approach however might be to some extent concerning. Some might argue that setting different cutoffs for different subgroups is unfair and is punishing privileged subgroups for something they have no control of. Especially in the individual fairness field, it would be concerning if two similar people with different sensitive attributes would have two different thresholds and potentially two different outcomes. This is a valid point, and this method should be used with knowledge of all its drawbacks. The cutoff manipulation method targets metrics chosen by the user.\nAll pre-processing methods can be used with two pipelines, whereas post-processing can be used in one specific way.\nPre-processing pipelines\ndata/explainer |> method\nReturns either weights, indexes, or changed data depending on the method used.\ndata/explainer |> pre_process_data(data, protected, y, type = ...)\nAlways returns data.frame. In case of weights data has additional column called _weights_.\n\nPost-processing pipelines\nfairness_object |> ceteris_paribus_cutoff(subgroup, ...) |> print/plot\nThis is the pipeline for creating ceteris paribus cutoff print and plot.\nexplainer |> roc_pivot(protected, privileged, ...)\nThe pipeline will return explainer with y_hat field changed.\n\nThe user should be aware that debiasing one metric might enhance bias in another. It is a so-called fairness-fairness trade-off. There is also a fairness-performance trade-off where debiasing one metric leads to worse performance. Another thing to remember is, as found in Agrawal et al. (2020), metrics might not generalize well to out-of-distribution examples, so it is advised also to check the fairness metrics on a separate test set.\nExample\nNow we will show an example usage of one pre-processing and one post-processing method. As before, the German Credit Data will be used along with the previously created lm_model. So firstly, we create a new dataset using pre_process_data and then we use it to train the logistic regression classifier.\n\n\nresampled_german   <- german |> pre_process_data(protected = german$Sex,\n                y_numeric, type = 'resample_uniform')\n\nlm_model_resample  <- glm(Risk~.,\n                data   = resampled_german,\n                family = binomial(link = \"logit\"))\n\nexplainer_lm_resample <- DALEX::explain(lm_model_resample,\n                data = german[,-1], y = y_numeric, verbose = FALSE)\n\n\n\nThen we make other explainers. We use previously created explainer_lm with the post-processing function roc_pivot. We set parameter theta = 0.05 for a rather narrow area of a pivot.\n\n\nnew_explainer <- explainer_lm |> roc_pivot(protected = german$Sex,\n                privileged = \"male\", theta = 0.05)\n\n\n\nIn the end, we create fairness_object with explainers obtained with the code above and one created in the first example to see the difference.\n\n\nfobject <- fairness_check(explainer_lm_resample, new_explainer, explainer_lm,\n                protected = german$Sex, privileged = \"male\",\n                label = c(\"resample\", \"roc\", \"base\"),\n                verbose = FALSE)\n\nfobject |> plot()\n\n\n\n\nFigure 8: Graphical summary of a base model (blue bars) and model after applying two bias mitigation techniques (red and green bars). By comparing adjacent rectangles one can read how the respective technique affected the corresponding fairness measure\n\n\n\nThe result of the code above is presented in Figure 8. The mitigation methods successfully eliminated bias in all of the metrics. Both models are better than the original base. This is not always the case - sometimes, eliminating bias in one metric may increase bias in another metric. For example, let’s consider a perfectly accurate model, but some subgroups receive few positive predictions (bias in Statistical parity). In that case, mitigating the bias in Statistical parity would decrease the Accuracy equality ratio.\nSummary and future work\nThis paper showed that checking for bias in machine learning models can be done conveniently and flexibly. The package fairmodels described above is a self-sufficient tool for bias detection, visualization, and mitigation in classification machine learning models. We presented theory, package architecture, suggested usage, and examples along with plots. Along the way, we introduced the core concepts and assumptions that come along the bias detection and plot interpretation. The package is still improved and enhanced, which can be seen by adding the announced regression module based on Steinberg et al. (2020). We did not cover it in this article because it is still an experimental tool. Another tool for in-processing classification closely related to fairmodels has also been added and can be found on https://github.com/ModelOriented/FairPAN.\nThe source code of the package, vignettes, examples, and documentation can be found at https://modeloriented.github.io/fairmodels/. The stable version is available on CRAN. The code and the development version can be found on GitHub https://github.com/ModelOriented/fairmodels. This is also a place to report bugs or requests (through GitHub issues).\nIn the future, we plan to enhance the spectrum of bias visualization plots and introduce regression and individual fairness methods. The potential way to explore would be an in-processing bias mitigation - training models that minimize cost function and adhere to certain fairness criteria. This field is heavily developed in Python and lacks appropriate attention in R.\nAcknowledgements\nWork on this package was financially supported by the NCN Sonata Bis-9 grant 2019/34/E/ST6/00052.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-019.zip\nCRAN packages used\nfairmodels, h2o, fairness, fairadapt, DALEX, ranger\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MachineLearning, ModelDeployment, Survival, TeachingStatistics\n\n\nA. Agrawal, F. Pfisterer, B. Bischl, J. Chen, S. Sood, S. Shah, F. Buet-Golfouse, B. A. Mateen and S. Vollmer. Debiasing classifiers: Is reality at variance with expectation? Electronic, 2020.\n\n\nJ. Angwin, J. Larson, S. Mattu and and Lauren Kirchner. Machine bias: There’s software used across the country to predict future criminals. And it’s biased against blacks. ProPublica, 2016. URL https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n\n\nS. Barocas, M. Hardt and A. Narayanan. Fairness and machine learning. fairmlbook.org, 2019. http://www.fairmlbook.org.\n\n\nR. K. E. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta, A. Mojsilovic, et al. AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. 2018. URL https://arxiv.org/abs/1810.01943.\n\n\nR. Berk, H. Heidari, S. Jabbari, M. Kearns and A. Roth. Fairness in criminal justice risk assessments: The state of the art. Sociological Methods & Research, 2017. URL https://doi.org/10.1177/0049124118782533.\n\n\nP. Biecek. DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84): 1–5, 2018. URL http://jmlr.org/papers/v19/18-416.html.\n\n\nP. Biecek and T. Burzykowski. Explanatory Model Analysis. Chapman; Hall/CRC, New York, 2021. URL https://pbiecek.github.io/ema/.\n\n\nR. Binns. On the apparent conflict between individual and group fairness. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages. 514–524 2020. New York, NY, USA: Association for Computing Machinery. ISBN 9781450369367. URL https://doi.org/10.1145/3351095.3372864.\n\n\nS. Bird, M. Dudík, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H. Wallach and K. Walker. Fairlearn: A toolkit for assessing and improving fairness in AI. MSR-TR-2020-32. Microsoft. 2020. URL https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/.\n\n\nJ. Buolamwini and T. Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Proceedings of the 1st conference on fairness, accountability and transparency, Eds S. A. Friedler and C. Wilson pages. 77–91 2018. New York, NY, USA. URL http://proceedings.mlr.press/v81/buolamwini18a.html.\n\n\nA. Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 5: 2016. URL https://doi.org/10.1089/big.2016.0047.\n\n\nD. Cirillo, S. Catuara-Solarz, C. Morey, E. Guney, L. Subirats, S. Mellino, A. Gigante, A. Valencia, M. J. Rementeria, A. S. Chadha, et al. Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare. npj Digital Medicine, 3(1): 81, 2020. URL https://doi.org/10.1038/s41746-020-0288-5 [online; last accessed October 24, 2021].\n\n\nCode of Federal Regulations. SECTION 4D, UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978). 1978. URL https://www.govinfo.gov/content/pkg/CFR-2014-title29-vol4/xml/CFR-2014-title29-vol4-part1607.xml.\n\n\nS. Corbett-Davies, E. Pierson, A. Feller, S. Goel and A. Huq. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages. 797–806 2017. New York, NY, USA: Association for Computing Machinery. URL https://doi.org/10.1145/3097983.3098095.\n\n\nCouncil of Europe. Guidelines on facial recognition. 2021. URL https://www.coe.int/en/web/portal/-/facial-recognition-strict-regulation-is-needed-to-prevent-human-rights-violations-.\n\n\nD. Dua and C. Graff. UCI machine learning repository. 2017. URL https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data).\n\n\nC. Dwork, M. Hardt, T. Pitassi, O. Reingold and R. Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages. 214–226 2012. New York, NY, USA: Association for Computing Machinery. URL https://doi.org/10.1145/2090236.2090255.\n\n\nEuropean Union Agency for Fundamental Rights and Council of Europe. Handbook on european non-discrimination law. Luxembourg: Publications Office of the European Union, 2018. https://fra.europa.eu/en/publication/2018/handbook-european-non-discrimination-law-2018-edition.\n\n\nEuropean Union Agency for Fundamental Rights. Handbook on european non-discrimination law. 2018. DOI https://doi.org/10.2811/792676.\n\n\nM. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger and S. Venkatasubramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages. 259–268 2015. New York, NY, USA: Association for Computing Machinery. URL https://doi.org/10.1145/2783258.2783311.\n\n\nH2O.ai. H2O AutoML. 2017. URL http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html. H2O version 3.30.0.1.\n\n\nM. Hardt, E. Price, E. Price and N. Srebro. Equality of opportunity in supervised learning. In Advances in neural information processing systems 29, Eds D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon and R. Garnett pages. 3315–3323 2016. Curran Associates, Inc. URL http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf.\n\n\nF. Kamiran and T. Calders. Data pre-processing techniques for classification without discrimination. Knowledge and Information Systems, 33: 2011. URL https://doi.org/10.1007/s10115-011-0463-8.\n\n\nF. Kamiran, A. Karim and X. Zhang. Decision theory for discrimination-aware classification. In 2012 IEEE 12th international conference on data mining, pages. 924–929 2012. URL https://doi.org/10.1109/ICDM.2012.45.\n\n\nN. Kozodoi, J. Jacob and S. Lessmann. Fairness in credit scoring: Assessment, implementation and profit implications. European Journal of Operational Research, 2021. URL https://doi.org/10.1016/j.ejor.2021.06.023.\n\n\nN. Kozodoi and T. V. Varga. Fairness: Algorithmic fairness metrics. 2021. URL https://CRAN.R-project.org/package=fairness. R package version 1.2.1.\n\n\nP. Lahoti, K. P. Gummadi and G. Weikum. [iFair: Learning individually fair data representations for algorithmic decision making]. In 2019 IEEE 35th international conference on data engineering (ICDE), pages. 1334–1345 2019. URL https://doi.org/10.1109/ICDE.2019.00121.\n\n\nN. Mehrabi, F. Morstatter, N. Saxena, K. Lerman and A. Galstyan. A survey on bias and fairness in machine learning. 2019. URL https://arxiv.org/abs/1908.09635.\n\n\nD. Plečko and N. Meinshausen. Fair data adaptation with quantile preservation. 2019. URL https://arxiv.org/abs/1911.06685.\n\n\nP. Saleiro, B. Kuester, A. Stevens, A. Anisfeld, L. Hinkson, J. London and R. Ghani. Aequitas: A bias and fairness audit toolkit. 2018. URL https://arxiv.org/abs/1811.05577.\n\n\nD. C. Steinberg, A. Reid and S. T. O’Callaghan. Fairness measures for regression via probabilistic classification. ArXiv, abs/2001.06089: 2020.\n\n\nM. Wick, S. Panda and J.-B. Tristan. Unlocking fairness: A trade-off revisited. In Advances in neural information processing systems, Eds H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox and R. Garnett pages. 8783–8792 2019. Curran Associates, Inc. URL https://proceedings.neurips.cc/paper/2019/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf.\n\n\nM. N. Wright and A. Ziegler. ranger: A fast implementation of random forests for high dimensional data in C++ and R. Journal of Statistical Software, 77(1): 1–17, 2017. DOI 10.18637/jss.v077.i01.\n\n\n\n\n",
    "preview": "articles/RJ-2022-019/table1.png",
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {},
    "preview_width": 2696,
    "preview_height": 506
  },
  {
    "path": "articles/RJ-2022-001/",
    "title": "blindrecalc - An R Package for Blinded Sample Size Recalculation",
    "description": "Besides the type 1 and type 2 error rate and the clinically relevant effect size, the sample size of a clinical trial depends on so-called nuisance parameters for which the concrete values are usually unknown when a clinical trial is planned. When the uncertainty about the magnitude of these parameters is high, an internal pilot study design with a blinded sample size recalculation can be used to achieve the target power even when the initially assumed value for the nuisance parameter is wrong. In this paper, we present the R-package blindrecalc that helps with planning a clinical trial with such a design by computing the operating characteristics and the distribution of the total sample size under different true values of the nuisance parameter. We implemented methods for continuous and binary outcomes in the superiority and the non-inferiority setting.",
    "author": [
      {
        "name": "Lukas Baumann",
        "url": {}
      },
      {
        "name": "Maximilian Pilz",
        "url": {}
      },
      {
        "name": "Meinhard Kieser",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-001.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-002/",
    "title": "tvReg: Time-varying Coefficients in Multi-Equation Regression in R",
    "description": "This article explains the usage of R package [tvReg](https://CRAN.R-project.org/package=tvReg), publicly available for download from the Comprehensive R Archive Network, via its application to economic and finance problems. The six basic functions in this package cover the kernel estimation of semiparametric panel data, seemingly unrelated equations, vector autoregressive, impulse response, and linear regression models whose coefficients may vary with time or any random variable. Moreover, this package provides methods for the graphical display of results, forecast, prediction, extraction of the residuals and fitted values, bandwidth selection and nonparametric estimation of the time-varying variance-covariance matrix of the error term. Applications to risk management, portfolio management, asset management and monetary policy are used as examples of these functions usage.",
    "author": [
      {
        "name": "Isabel Casas",
        "url": {}
      },
      {
        "name": "Rubén Fernández-Casal",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-002.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-003/",
    "title": "RKHSMetaMod: An R Package to Estimate the Hoeffding Decomposition of a Complex Model by Solving RKHS Ridge Group Sparse Optimization Problem",
    "description": "In this paper, we propose an R package, called [RKHSMetaMod](https://CRAN.R-project.org/package=RKHSMetaMod), that implements a procedure for estimating a meta-model of a complex model. The meta-model approximates the Hoeffding decomposition of the complex model and allows us to perform sensitivity analysis on it. It belongs to a reproducing kernel Hilbert space that is constructed as a direct sum of Hilbert spaces. The estimator of the meta-model is the solution of a penalized empirical least-squares minimization with the sum of the Hilbert norm and the empirical $L^2$-norm. This procedure, called RKHS ridge group sparse, allows both to select and estimate the terms in the Hoeffding decomposition, and therefore, to select and estimate the Sobol indices that are non-zero. The [RKHSMetaMod](https://CRAN.R-project.org/package=RKHSMetaMod) package provides an interface from the R statistical computing environment to the C++ libraries Eigen and GSL. In order to speed up the execution time and optimize the storage memory, except for a function that is written in R, all of the functions of this package are written using the efficient C++ libraries through [RcppEigen](https://CRAN.R-project.org/package=RcppEigen) and [RcppGSL](https://CRAN.R-project.org/package=RcppGSL) packages. These functions are then interfaced in the R environment in order to propose a user-friendly package.",
    "author": [
      {
        "name": "Halaleh Kamari",
        "url": {}
      },
      {
        "name": "Sylvie Huet",
        "url": {}
      },
      {
        "name": "Marie-Luce Taupin",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-003.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-004/",
    "title": "Measuring the Extent and Patterns of Urban Shrinkage for Small Towns Using R",
    "description": "Urban shrinking is a phenomenon as common as urban expansion nowadays and it affects urban settlements of all sizes, especially from developed and industrialized countries in Europe, America and Asia. The paper aims to assess the patterns of shrinkage for small and medium sized towns in Oltenia region (Romania), considering demographic, economic and social indicators with a methodological approach which considers the use of different functions and applications of R packages. Thirteen selected indicators are analysed to perform the multivariate analysis on Principal Component Analysis using the prcomp() function and the [ggplot2](https://CRAN.R-project.org/package=ggplot2) package to visualize the patterns of urban shrinkage. Two composite indicators were additionally created to measure the extent of urban shrinkage: CSI (Composite Shrinking Index) and RDC (Regional Demographic Change) for two-time intervals. Based on the CSI, three major categories of shrinking were observed: persistent shrinkage, mild shrinking or slow evolution toward shrinking, where the vast majority of towns are found (including mining towns, where there still is a delayed restructuring of state-owned enterprises, and towns characterised by the agrarization of local economies), and stagnant/stabilized shrinkage.",
    "author": [
      {
        "name": "Cristiana Vîlcea",
        "url": {}
      },
      {
        "name": "Liliana Popescu",
        "url": {}
      },
      {
        "name": "Alin Clincea",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-004.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-005/",
    "title": "cpsurvsim: An R Package for Simulating Data from Change-Point Hazard Distributions",
    "description": "Change-point hazard models have several practical applications, including modeling processes such as cancer mortality rates and disease progression. While the inverse cumulative distribution function (CDF) method is commonly used for simulating data, we demonstrate the shortcomings of this approach when simulating data from change-point hazard distributions with more than a scale parameter. We propose an alternative method of simulating this data that takes advantage of the memoryless property of survival data and introduce the R package cpsurvsim which implements both simulation methods. The functions of cpsurvsim are discussed, demonstrated, and compared.",
    "author": [
      {
        "name": "Camille J. Hochheimer, PhD",
        "url": {}
      },
      {
        "name": "Roy T. Sabo, PhD",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-005.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-006/",
    "title": "A Computational Analysis of the Dynamics of R Style Based on 108 Million Lines of Code from All CRAN Packages in the Past 21 Years",
    "description": "The flexibility of R and the diversity of the R community leads to a large number of programming styles applied in R packages. We have analyzed 108 million lines of R code from CRAN and quantified the evolution in popularity of 12 style-elements from 1998 to 2019. We attribute 3 main factors that drive changes in programming style: the effect of style-guides, the effect of introducing new features, and the effect of editors. We observe in the data that a consensus in programming style is forming, such as using lower snake case for function names (e.g. softplus_func) and \\<- rather than = for assignment.",
    "author": [
      {
        "name": "Chia-Yi Yen",
        "url": {}
      },
      {
        "name": "Mia Huai-Wen Chang",
        "url": {}
      },
      {
        "name": "Chung-hong Chan",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-006.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-007/",
    "title": "rmonad: pipelines you can compute on",
    "description": "The [rmonad](https://CRAN.R-project.org/package=rmonad) package presents a monadic pipeline toolset for chaining functions into stateful, branching pipelines. As functions in the pipeline are run, their results are merged into a graph of all past operations. The resulting structure allows downstream computation on node documentation, intermediate data, performance stats, and any raised messages, warnings or errors, as well as the final results. [rmonad](https://CRAN.R-project.org/package=rmonad) is a novel approach to designing reproducible, well-documented, and maintainable workflows in R.",
    "author": [
      {
        "name": "Zebulun Arendsee",
        "url": {}
      },
      {
        "name": "Jennifer Chang",
        "url": {}
      },
      {
        "name": "Eve Wurtele",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-007.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-008/",
    "title": "A Software Tool For Sparse Estimation Of A General Class Of High-dimensional GLMs",
    "description": "Generalized linear models are the workhorse of many inferential problems. Also in the modern era with high-dimensional settings, such models have been proven to be effective exploratory tools. Most attention has been paid to Gaussian, binomial and Poisson settings, which have efficient computational implementations and where either the dispersion parameter is largely irrelevant or absent. However, general GLMs have dispersion parameters $\\phi$ that affect the value of the log-likelihood. This in turn, affects the value of various information criteria such as AIC and BIC, and has a considerable impact on the computation and selection of the optimal model. The R-package [dglars](https://CRAN.R-project.org/package=dglars) is one of the standard packages to perform high-dimensional analyses for GLMs. Being based on fundamental likelihood considerations, rather than arbitrary penalization, it naturally extends to the general GLM setting. In this paper, we present an improved predictor-corrector (IPC) algorithm for computing the differential geometric least angle regression (dgLARS) solution curve, proposed in [@Augug13] and [@pazira]. We describe the implementation of a stable estimator of the dispersion parameter proposed in [@pazira] for high-dimensional exponential dispersion models. A simulation study is conducted to test the performance of the proposed methods and algorithms. We illustrate the methods using an example. The described improvements have been implemented in a new version of the R-package [dglars](https://CRAN.R-project.org/package=dglars).",
    "author": [
      {
        "name": "Hassan Pazira",
        "url": {}
      },
      {
        "name": "Luigi Augugliaro",
        "url": {}
      },
      {
        "name": "Ernst C. Wit",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-008.zip\n\n\nL. Augugliaro, A. M. Mineo and E. C. Wit. Differential geometric least angle regression: A differential geometric approach to sparse generalized linear models. Journal of the Royal Statistical Society: Series B, 75(3): 471–498, 2013.\n\n\nH. Pazira, L. Augugliaro and E. C. Wit. Extended differential geometric LARS for high-dimensional GLMs with general dispersion parameter. Statistics and Computing, 28(4): 753–774, 2018. URL http://dx.doi.org/10.1007/s11222-017-9761-7.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-009/",
    "title": "bayesanova: An R package for Bayesian Inference in the Analysis of Variance via Markov Chain Monte Carlo in Gaussian Mixture Models",
    "description": "This paper introduces the R package [bayesanova](https://CRAN.R-project.org/package=bayesanova), which performs Bayesian inference in the analysis of variance (ANOVA). Traditional ANOVA based on null hypothesis significance testing (NHST) is prone to overestimating effects and stating effects if none are present. Bayesian ANOVAs developed so far are based on Bayes factors (BF), which also enforce a hypothesis testing stance. Instead, the Bayesian ANOVA implemented in bayesanova focusses on effect size estimation and is based on a Gaussian mixture with known allocations, for which full posterior inference for the component parameters is implemented via Markov-Chain-Monte-Carlo (MCMC). Inference for the difference in means, standard deviations and effect sizes between each of the groups is obtained automatically. Estimation of the parameters instead of hypothesis testing is embraced via the region of practical equivalence (ROPE), and helper functions provide checks of the model assumptions and visualization of the results.",
    "author": [
      {
        "name": "Riko Kelter",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-009.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-010/",
    "title": "Revisiting Historical Bar Graphics on Epidemics in the Era of R ggplot2",
    "description": "This study is motivated by an article published in a local history magazine on \"Pandemics in the History\". That article was also motivated by a government report involving several statistical graphics which were drawn by hand in 1938 and used to summarize official statistics on epidemics occurred between the years 1923 and 1937. Due to the aesthetic information design available on these historical graphs, in this study, we would like to investigate how graphical elements of the graphs such as titles, axis lines, axis tick marks, tick mark labels, colors, and data values are presented on these graphics and how to reproduce these historical graphics via well-known data visualization package [ggplot2](https://CRAN.R-project.org/package=ggplot2) in our era.",
    "author": [
      {
        "name": "Sami Aldag",
        "url": {}
      },
      {
        "name": "Dogukan Topcuoglu",
        "url": {}
      },
      {
        "name": "Gul Inan",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-010.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-011/",
    "title": "PSweight: An R Package for Propensity Score Weighting Analysis",
    "description": "Propensity score weighting is an important tool for comparative effectiveness research. Besides the inverse probability of treatment weights (IPW), recent development has introduced a general class of balancing weights, corresponding to alternative target populations and estimands. In particular, the overlap weights (OW) lead to optimal covariate balance and estimation efficiency, and a target population of scientific and policy interest. We develop the R package [PSweight](https://CRAN.R-project.org/package=PSweight) to provide a comprehensive design and analysis platform for causal inference based on propensity score weighting. PSweight supports (i) a variety of balancing weights, (ii) binary and multiple treatments, (iii) simple and augmented weighting estimators, (iv) nuisance-adjusted sandwich variances, and (v) ratio estimands. PSweight also provides diagnostic tables and graphs for covariate balance assessment. We demonstrate the functionality of the package using a data example from the National Child Development Survey (NCDS), where we evaluate the causal effect of educational attainment on income.",
    "author": [
      {
        "name": "Tianhui Zhou",
        "url": {}
      },
      {
        "name": "Guangyu Tong",
        "url": {}
      },
      {
        "name": "Fan Li",
        "url": {}
      },
      {
        "name": "Laine E. Thomas",
        "url": {}
      },
      {
        "name": "Fan Li",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-011.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-012/",
    "title": "RFpredInterval: An R Package for Prediction Intervals with Random Forests and Boosted Forests",
    "description": "Like many predictive models, random forests provide point predictions for new observations. Besides the point prediction, it is important to quantify the uncertainty in the prediction. Prediction intervals provide information about the reliability of the point predictions. We have developed a comprehensive R package, [RFpredInterval](https://CRAN.R-project.org/package=RFpredInterval), that integrates 16 methods to build prediction intervals with random forests and boosted forests. The set of methods implemented in the package includes a new method to build prediction intervals with boosted forests (PIBF) and 15 method variations to produce prediction intervals with random forests, as proposed by [@roy_prediction_2020]. We perform an extensive simulation study and apply real data analyses to compare the performance of the proposed method to ten existing methods for building prediction intervals with random forests. The results show that the proposed method is very competitive and, globally, outperforms competing methods.",
    "author": [
      {
        "name": "Cansu Alakus",
        "url": {}
      },
      {
        "name": "Denis Larocque",
        "url": {}
      },
      {
        "name": "Aurélie Labbe",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-012.zip\n\n\nM.-H. Roy and D. Larocque. Prediction intervals with random forests. Statistical Methods in Medical Research, 29(1): 205–229, 2020. URL https://doi.org/10.1177/0962280219829885 [online; last accessed May 4, 2021].\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-013/",
    "title": "etrm: Energy Trading and Risk Management in R",
    "description": "This paper introduces [etrm](https://CRAN.R-project.org/package=etrm), an R package with tools for trading and financial risk management in energy markets. Contracts for electric power and natural gas differ from most other commodities due to the fact that physical delivery takes place over a time interval, and not at a specific point in time. There is typically strong seasonality, limited storage and transmission capacity and strong correlation between price and required volume. Such characteristics need to be taken into account when pricing contracts and managing financial risk related to energy procurement. Tools for these task are usually bundled into proprietary Energy Trading Risk Management (ETRM) systems delivered by specialized IT vendors. The [etrm](https://CRAN.R-project.org/package=etrm) package offers a transparent solution for building a forward price curve for energy commodities which is consistent with methods widely used in the industry. The user's fundamental market view may be combined with contract price quotes to form a forward curve that replicate current market prices, as described in @ollmar2003analysis and @benth2007extracting. [etrm](https://CRAN.R-project.org/package=etrm) also provides implementations of five portfolio insurance trading strategies for energy price risk management. The forward market curve and the energy price hedging strategies are core elements in an ETRM system, which to the best of the author's knowledge has not been previously available in the R ecosystem.",
    "author": [
      {
        "name": "Anders D. Sleire",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-013.zip\n\n\nF. E. Benth, S. Koekkebakker and F. Ollmar. Extracting and applying smooth forward curves from average-based commodity contracts with seasonal variation. The Journal of Derivatives, 15(1): 52–66, 2007. URL https://doi.org/10.3905/jod.2007.694791 .\n\n\nF. Ollmar. An analysis of derivative prices in the Nordic power market. 2003. URL http://hdl.handle.net/11250/164248.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-014/",
    "title": "fcaR, Formal Concept Analysis with R",
    "description": "Formal concept analysis (FCA) is a solid mathematical framework to manage information based on logic and lattice theory. It defines two explicit representations of the knowledge present in a dataset as concepts and implications. This paper describes an R package called [fcaR](https://CRAN.R-project.org/package=fcaR) that implements FCA's core notions and techniques. Additionally, it implements the extension of FCA to fuzzy datasets and a simplification logic to develop automated reasoning tools. This package is the first to implement FCA techniques in R. Therefore, emphasis has been put on defining classes and methods that could be reusable and extensible by the community. Furthermore, the package incorporates an interface with the [arules](https://CRAN.R-project.org/package=arules) package, probably the most used package regarding association rules, closely related to FCA. Finally, we show an application of the use of the package to design a recommender system based on logic for diagnosis in neurological pathologies.",
    "author": [
      {
        "name": "Pablo Cordero",
        "url": {}
      },
      {
        "name": "Manuel Enciso",
        "url": {}
      },
      {
        "name": "Domingo López-Rodríguez",
        "url": {}
      },
      {
        "name": "Ángel Mora",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-014.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-015/",
    "title": "FMM: An R Package for Modeling Rhythmic Patterns in Oscillatory Systems",
    "description": "This paper is dedicated to the R package FMM which implements a novel approach to describe rhythmic patterns in oscillatory signals. The frequency modulated Möbius (FMM) model is defined as a parametric signal plus a Gaussian noise, where the signal can be described as a single or a sum of waves. The FMM approach is flexible enough to describe a great variety of rhythmic patterns. The FMM package includes all required functions to fit and explore single and multi-wave FMM models, as well as a restricted version that allows equality constraints between parameters representing a priori knowledge about the shape to be included. Moreover, the FMM package can generate synthetic data and visualize the results of the fitting process. The potential of this methodology is illustrated with examples of such biological oscillations as the circadian rhythm in gene expression, the electrical activity of the heartbeat and the neuronal activity.",
    "author": [
      {
        "name": "Itziar Fernández",
        "url": {}
      },
      {
        "name": "Alejandro Rodríguez-Collado",
        "url": {}
      },
      {
        "name": "Yolanda Larriba",
        "url": {}
      },
      {
        "name": "Adrián Lamela",
        "url": {}
      },
      {
        "name": "Christian Canedo",
        "url": {}
      },
      {
        "name": "Cristina Rueda",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-015.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-016/",
    "title": "spherepc: An R Package for Dimension Reduction on a Sphere",
    "description": "Dimension reduction is a technique that can compress given data and reduce noise. Recently, a dimension reduction technique on spheres, called spherical principal curves (SPC), has been proposed. SPC fits a curve that passes through the middle of data with a stationary property on spheres. In addition, a study of local principal geodesics (LPG) is considered to identify the complex structure of data. Through the description and implementation of various examples, this paper introduces an R package [spherepc](https://CRAN.R-project.org/package=spherepc) for dimension reduction of data lying on a sphere, including existing methods, SPC and LPG.",
    "author": [
      {
        "name": "Jongmin Lee",
        "url": {}
      },
      {
        "name": "Jang-Hyun Kim",
        "url": {}
      },
      {
        "name": "Hee-Seok Oh",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-016.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-017/",
    "title": "The smoots Package in R for Semiparametric Modeling of Trend Stationary Time Series",
    "description": "This paper is an introduction to the new package in R called [smoots](https://CRAN.R-project.org/package=smoots) (smoothing time series), developed for data-driven local polynomial smoothing of trend-stationary time series. Functions for data-driven estimation of the first and second derivatives of the trend are also built-in. It is first applied to monthly changes of the global temperature. The quarterly US-GDP series shows that this package can also be well applied to a semiparametric multiplicative component model for non-negative time series via the log-transformation. Furthermore, we introduced a semiparametric Log-GARCH and a semiparametric Log-ACD model, which can be easily estimated by the smoots package. Of course, this package applies to suitable time series from any other research area. The smoots package also provides a useful tool for teaching time series analysis, because many practical time series follow an additive or a multiplicative component model.",
    "author": [
      {
        "name": "Yuanhua Feng",
        "url": {}
      },
      {
        "name": "Thomas Gries",
        "url": {}
      },
      {
        "name": "Sebastian Letmathe",
        "url": {}
      },
      {
        "name": "Dominik Schulz",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-017.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-018/",
    "title": "starvars: An R Package for Analysing Nonlinearities in Multivariate Time Series",
    "description": "Although linear autoregressive models are useful to practitioners in different fields, often a nonlinear specification would be more appropriate in time series analysis. In general, there are many alternative approaches to nonlinearity modelling, one consists in assuming multiple regimes. Among the possible specifications that account for regime changes in the multivariate framework, smooth transition models are the most general, since they nest both linear and threshold autoregressive models. This paper introduces the [starvars](https://CRAN.R-project.org/package=starvars) package which estimates and predicts the Vector Logistic Smooth Transition model in a very general setting which also includes predetermined variables. In comparison to the existing R packages, starvars offers the estimation of the Vector Smooth Transition model both by maximum likelihood and nonlinear least squares. The package allows also to test for nonlinearity in a multivariate setting and detect the presence of common breaks. Furthermore, the package computes multi-step-ahead forecasts. Finally, an illustration with financial time series is provided to show its usage.",
    "author": [
      {
        "name": "Andrea Bucci",
        "url": {}
      },
      {
        "name": "Giulio Palomba",
        "url": {}
      },
      {
        "name": "Eduardo Rossi",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-018.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-020/",
    "title": "Palmer Archipelago Penguins Data in the palmerpenguins R Package - An Alternative to Anderson's Irises",
    "description": "In 1935, Edgar Anderson collected size measurements for 150 flowers from three species of *Iris* on the Gaspé Peninsula in Quebec, Canada. Since then, Anderson's *Iris* observations have become a classic dataset in statistics, machine learning, and data science teaching materials. It is included in the base R datasets package as `iris`, making it easy for users to access without knowing much about it. However, the lack of data documentation, presence of non-intuitive variables (e.g. \"sepal width\"), and perfectly balanced groups with zero missing values make `iris` an inadequate and stale dataset for teaching and learning modern data science skills. Users would benefit from working with a more representative, real-world environmental dataset with a clear link to current scientific research. Importantly, Anderson’s *Iris* data appeared in a 1936 publication by R. A. Fisher in the *Annals of Eugenics* (which is often the first-listed citation for the dataset), inextricably linking `iris` to eugenics research. Thus, a modern alternative to `iris` is needed. In this paper, we introduce the palmerpenguins R package [@R-palmerpenguins], which includes body size measurements collected from 2007 - 2009 for three species of *Pygoscelis* penguins that breed on islands throughout the Palmer Archipelago, Antarctica. The `penguins` dataset in palmerpenguins provides an approachable, charismatic, and near drop-in replacement for `iris` with topical relevance for polar climate change and environmental impacts on marine predators. Since the release on CRAN in July 2020, the palmerpenguins package has been downloaded over 462,000 times, highlighting the demand and widespread adoption of this viable `iris` alternative.  We directly compare the `iris` and `penguins` datasets for selected analyses to demonstrate that R users, in particular teachers and learners currently using `iris`, can switch to the Palmer Archipelago penguins for many use cases including data wrangling, visualization, linear modeling, multivariate analysis (e.g., PCA), cluster analysis and classification (e.g., by k-means).",
    "author": [
      {
        "name": "Allison M. Horst",
        "url": {}
      },
      {
        "name": "Alison Presmanes Hill",
        "url": {}
      },
      {
        "name": "Kristen B. Gorman",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\nIntroduction\n\nIn 1935, American botanist Edgar Anderson measured petal and sepal structural dimensions (length and width) for 50 flowers from three Iris species: Iris setosa, Iris versicolor, and Iris virginica (Anderson 1935). The manageable but non-trivial size (5 variables and 150 total observations) and characteristics of Anderson’s Iris dataset, including linear relationships and multivariate normality, have made it amenable for introducing a wide range of statistical methods including data wrangling, visualization, linear modeling, multivariate analyses, and machine learning. The Iris dataset is built into a number of software packages including the auto-installed datasets package in R (as iris, R Core Team 2021), Python’s scikit-learn machine learning library (Pedregosa et al. 2011), and the SAS Sashelp library (SAS Institute, Cary NC), which has facilitated its widespread use. As a result, eighty-six years after the data were initially published, the Iris dataset remains ubiquitous in statistics, computational methods, software documentation, and data science courses and materials.\nThere are a number of reasons that modern data science practitioners and educators may want to move on from iris. First, the dataset lacks metadata (Anderson 1935), which does not reinforce best practices and limits meaningful interpretation and discussion of research methods, analyses, and outcomes. Of the five variables in iris, two (Sepal.Width and Sepal.Length) are not intuitive for most non-botanists. Even with explanation, the difference between petal and sepal dimensions is not obvious. Second, iris contains equal sample sizes for each of the three species (n = 50) with no missing values, which is cleaner than most real-world data that learners are likely to encounter. Third, the single factor (Species) in iris limits options for analyses. Finally, due to its publication in the Annals of Eugenics by statistician R.A. Fisher (Fisher 1936), iris is burdened by a history in eugenics research, which we are committed to addressing through the development of new data science education products as described below.\nGiven the growing need for fresh data science-ready datasets, we sought to identify an alternative dataset that could be made easily accessible for a broad audience. After evaluating the positive and negative features of iris in data science and statistics materials, we established the following criteria for a suitable alternative:\nAvailable by appropriate license like a Creative Commons 0 license (CC0 “no rights reserved”)\nFeature intuitive subjects and variables that are interesting and understandable to learners across disciplines\nComplete metadata and documentation\nManageable (but not trivial) in size\nMinimal data cleaning and pre-processing required for most analyses\nReal-world (not manufactured) modern data\nProvides similar opportunities for teaching and learning R, data science, and statistical skills\nCan easily replace iris for most use cases\nHere, we describe an alternative to iris that largely satisfies these criteria: a refreshing, approachable, and charismatic dataset containing real-world body size measurements for three Pygoscelis penguin species that breed throughout the Western Antarctic Peninsula region, made available through the United States Long-Term Ecological Research (US LTER) Network. By comparing data structure, size, and a range of analyses side-by-side for the two datasets, we demonstrate that the Palmer Archipelago penguin data are an ideal substitute for iris for many use cases in statistics and data science education.\n\n\n\nFigure 1: The palmerpenguins package hex sticker designed by Allison Horst\n\n\n\nData source\nBody size measurements (bill length and depth, flipper length - flippers are the modified “wings” of penguins used for maneuvering in water, and body mass), clutch (i.e., egg laying) observations (e.g., date of first egg laid, and clutch completion), and carbon (13C/12C, \\(\\delta\\)13C) and nitrogen (15N/14N, \\(\\delta\\)15N) stable isotope values of red blood cells for adult male and female Adélie (P. adeliae), chinstrap (P. antarcticus), and gentoo (P. papua) penguins on three islands (Biscoe, Dream, and Torgersen) within the Palmer Archipelago were collected from 2007 - 2009 by Dr. Kristen Gorman in collaboration with the Palmer Station LTER, part of the US LTER Network. For complete data collection methods and published analyses, see Gorman et al. (2014). Throughout this paper, penguins species are referred to as “Adélie”, “Chinstrap”, and “Gentoo”.\nThe data in the palmerpenguins R package are available for use by CC0 license (“No Rights Reserved”) in accordance with the Palmer Station LTER Data Policy and the LTER Data Access Policy, and were imported from the Environmental Data Initiative (EDI) Data Portal at the links below:\nAdélie penguin data (Palmer Station Antarctica LTER and Gorman 2020a): KNB-LTER Data Package 219.5\nGentoo penguin data (Palmer Station Antarctica LTER and Gorman 2020c): KNB-LTER Data Package 220.5\nChinstrap penguin data (Palmer Station Antarctica LTER and Gorman 2020b): KNB-LTER Data Package 221.6\nThe palmerpenguins R package\nR users can install the palmerpenguins package from CRAN:\ninstall.packages(\"palmerpenguins\")\nInformation, examples, and links to community-contributed materials are available on the palmerpenguins package website: allisonhorst.github.io/palmerpenguins/. See the Appendix for how Python and Julia users can access the same data.\nThe palmerpenguins R package contains two data objects: penguins_raw and penguins. The penguins_raw data consists of all raw data for 17 variables, recorded completely or in part for 344 individual penguins, accessed directly from EDI (penguins_raw properties are summarized in Appendix B). We generally recommend using the curated data in penguins, which is a subset of penguins_raw retaining all 344 observations, minimally updated (Appendix A) and reduced to the following eight variables:\nspecies: a factor denoting the penguin species (Adélie, Chinstrap, or Gentoo)\nisland: a factor denoting the Palmer Archipelago island in Antarctica where each penguin was observed (Biscoe Point, Dream Island, or Torgersen Island)\nbill_length_mm: a number denoting length of the dorsal ridge of a penguin bill (millimeters)\nbill_depth_mm: a number denoting the depth of a penguin bill (millimeters)\nflipper_length_mm: an integer denoting the length of a penguin flipper (millimeters)\nbody_mass_g: an integer denoting the weight of a penguin’s body (grams)\nsex: a factor denoting the sex of a penguin sex (male, female) based on molecular data\nyear: an integer denoting the year of study (2007, 2008, or 2009)\nThe same data exist as comma-separated value (CSV) files in the package (“penguins_raw.csv” and “penguins.csv”), and can be read in using the built-in path_to_file() function in palmerpenguins. For example,\nlibrary(palmerpenguins)\ndf <- read.csv(path_to_file(\"penguins.csv\"))\nwill read in “penguins.csv” as if from an external file, thus automatically parsing species, island, and sex variables as characters instead of factors. This option allows users opportunities to practice or demonstrate reading in data from a CSV, then updating variable class (e.g., characters to factors).\nComparing iris and penguins\nThe penguins data in palmerpenguins is useful and approachable for data science and statistics education, and is uniquely well-suited to replace the iris dataset. Comparisons presented are selected examples for common iris uses, and are not exhaustive.\n\n\n\n\n\nTable 1: Overview comparison of penguins and iris dataset features and characteristics.\n\n\nFeature\n\n\niris\n\n\npenguins\n\n\nYear(s) collected\n\n\n1935\n\n\n2007 - 2009\n\n\nDimensions (col x row)\n\n\n5 x 150\n\n\n8 x 344\n\n\nDocumentation\n\n\nminimal\n\n\ncomplete metadata\n\n\nVariable classes\n\n\ndouble (4), factor (1)\n\n\ndouble (2), int (3), factor (3)\n\n\nMissing values?\n\n\nno (n = 0; 0.0%)\n\n\nyes (n = 19; 0.7%)\n\n\n\n\n\nData structure and sample size\nBoth iris and penguins are in tidy format (Wickham 2014) with each column denoting a single variable and each row containing measurements for a single iris flower or penguin, respectively. The two datasets are comparable in size: dimensions (columns × rows) are 5 × 150 and 8 × 344 for iris and penguins, respectively, and sample sizes within species are similar (Tables 1 & 2).\nNotably, while sample sizes in iris across species are all the same, sample sizes in penguins differ across the three species. The inclusion of three factor variables in penguins (species, island, and sex), along with year, create additional opportunities for grouping, faceting, and analysis compared to the single factor (Species) in iris.\nUnlike iris, which contains only complete cases, the penguins dataset contains a small number of missing values (nmissing = 19, out of 2,752 total values). Missing values and unequal sample sizes are common in real-world data, and create added learning opportunity to the penguins dataset.\n\n\n\n\n\nTable 2: Grouped sample size for iris (by species; n = 150 total) and penguins (by species and sex; n = 344 total). Data in penguins can be further grouped by island and study year.\n\n\n\niris sample size (by species)\n\n\n\n\npenguins sample size (by species and sex)\n\n\n\nIris species\n\n\nSample size\n\n\nPenguin species\n\n\nFemale\n\n\nMale\n\n\nNA\n\n\nsetosa\n\n\n50\n\n\nAdélie\n\n\n73\n\n\n73\n\n\n6\n\n\nversicolor\n\n\n50\n\n\nChinstrap\n\n\n34\n\n\n34\n\n\n0\n\n\nvirginica\n\n\n50\n\n\nGentoo\n\n\n58\n\n\n61\n\n\n5\n\n\n\n\n\nContinuous quantitative variables\nDistributions, relationships between variables, and clustering can be visually explored between species for the four structural size measurements in penguins (flipper length, body mass, bill length and depth; Figure 2) and iris (sepal width and length, petal width and length; Figure 3).\n\n\n\n\nFigure 2: Distributions and correlations for numeric variables in the penguins data (flipper length (mm), body mass (g), bill length (mm) and bill depth (mm)) for the three observed species: Gentoo (green, triangles); Chinstrap (blue, circles); and Adélie (orange, squares). Significance indicated for bivariate correlations: *p < 0.05; **p < 0.01; ***p < 0.001.\n\n\n\n\n\n\n\nFigure 3: Distributions and correlations for numeric variables in iris (petal length (cm), petal width (cm), sepal length (cm) and sepal width (cm)) for the three included iris species: Iris setosa (light gray, circles); Iris versicolor (dark gray, triangles); and Iris virginica (black, squares). Significance indicated for bivariate correlations: *p < 0.05; **p < 0.01; ***p < 0.001.\n\n\n\nBoth penguins and iris offer numerous opportunities to explore linear relationships and correlations, within and across species (Figures 2 & 3). A bivariate scatterplot made with the iris dataset reveals a clear linear relationship between petal length and petal width. Using penguins (Figure 4), we can create a uniquely similar scatterplot with flipper length and body mass. The overall trend across all three species is approximately linear for both iris and penguins. Teachers may encourage students to explore how simple linear regression results and predictions differ when the species variable is omitted, compared to, for example, multiple linear regression with species included (Figure 4).\n\n\n\n\nFigure 4: Representative linear relationships for (A): penguin flipper length (mm) and body mass (g) for Adélie (orange circles), Chinstrap (blue triangles), and Gentoo (green squares) penguins; (B): iris petal length (cm) and width (cm) for Iris setosa (light gray circles), Iris versicolor (dark gray triangles) and Iris virginica (black squares). Within-species linear model is visualized for each penguin or iris species.\n\n\n\nNotably, distinctions between species are clearer for iris petals - particularly, the much smaller petals for Iris setosa - compared to penguins, in which Adélie and Chinstrap penguins are largely overlapping in body size (body mass and flipper length), and are both generally smaller than Gentoo penguins.\nSimpson’s Paradox is a data phenomenon in which a trend observed between variables is reversed when data are pooled, omitting a meaningful variable. While often taught and discussed in statistics courses, finding a real-world and approachable example of Simpson’s Paradox can be a challenge. Here, we show one (of several possible - see Figure 2) Simpson’s Paradox example in penguins: exploring bill dimensions with and without species included (Figure 5). When penguin species is omitted (Figure 5A), bill length and depth appear negatively correlated overall. The trend is reversed when species is included, revealing an obviously positive correlation between bill length and bill depth within species (Figure 5B).\n\n\n\n\nFigure 5: Trends for penguin bill dimensions (bill length and bill depth, millimeters) if the species variable is excluded (A) or included (B), illustrating Simpson’s Paradox. Note: linear regression for bill dimensions without including species in (A) is ill-advised; the linear trendline is only included to visualize trend reversal for Simpson’s Paradox when compared to (B).\n\n\n\nPrincipal component analysis\nPrincipal component analysis (PCA) is a dimensional reduction method commonly used to explore patterns in multivariate data. The iris dataset frequently appears in PCA tutorials due to multivariate normality and clear interpretation of variable loadings and clustering.\nA comparison of PCA with the four variables of structural size measurements in penguins and iris (both normalized prior to PCA) reveals highly similar results (Figure 6). For both datasets, one species is distinct (Gentoo penguins, and setosa irises) while the other two species (Chinstrap/Adélie and versicolor/virginica) appear somewhat overlapping in the first two principal components (Figure 6 A,B). Screeplots reveal that the variance explained by each principal component (PC) is very similar across the two datasets, particularly for PC1 and PC2: for penguins, 88.15% of total variance is captured by the first two PCs, compared to 95.81% for iris, with a similarly large percentage of variance captured by PC1 and PC2 in each (Figure 6 C,D).\n\n\n\n\nFigure 6: Principal component analysis biplots and screeplots for structural size measurements in penguins (A,C) and iris (B,D), revealing similarities in multivariate patterns, variable loadings, and variance explained by each component. For penguins, variables are flipper length (mm), body mass (g), bill length (mm) and bill depth (mm); groups are visualized by species (Adélie = orange circles, Chinstrap = blue triangles, Gentoo = green squares). For iris, variables are petal length (cm), petal width (cm), sepal length (cm) and sepal width (cm); groups are visualized by species (Iris setosa = light gray circles, Iris versicolor = dark gray triangles, Iris virginica = black squares). Values above screeplot columns (C,D) indicate percent of total variance explained by each of the four principal components.\n\n\n\nK-means clustering\nUnsupervised clustering by k-means is a common and popular entryway to machine learning and classification, and again, the iris dataset is frequently used in introductory examples. The penguins data provides similar opportunities for introducing k-means clustering. For simplicity, we compare k-means clustering using only two variables for each dataset: for iris, petal width and petal length, and for penguins, bill length and bill depth. All variables are scaled prior to k-means. Three clusters (k = 3) are specified for each, since there are three species of irises (Iris setosa, Iris versicolor, and Iris virginica) and penguins (Adélie, Chinstrap and Gentoo).\nK-means clustering with penguin bill dimensions and iris petal dimensions yields largely distinct clusters, each dominated by one species (Figure 7). For iris petal dimensions, k-means yields a perfectly separated cluster (Cluster 3) containing all 50 Iris setosa observations and zero misclassified Iris virginica or Iris versicolor (Table 3). While clustering is not perfectly distinct for any penguin species, each species is largely contained within a single cluster, with little overlap from the other two species. For example, considering Adélie penguins (orange observations in Figure 7A): 147 (out of 151) Adélie penguins are assigned to Cluster 3, zero are assigned to Cluster 1, and 4 are assigned to the Chinstrap-dominated Cluster 2 (Table 3). Only 5 (of 68) Chinstrap penguins and 1 (of 123) Gentoo penguins are assigned to the Adélie-dominated Cluster 3 (Table 3).\n\n\n\n\nFigure 7: K-means clustering outcomes for penguin bill dimensions (A) and iris petal dimensions (B). Numbers indicate the cluster to which an observation was assigned, revealing a high degree of separation between species for both penguins and iris. Penguin species (Adélie = orange, Chinstrap = blue, Gentoo = green) and iris species (setosa = light gray, versicolor = medium gray, virginica = dark gray), along with bill dimensions and cluster number, are included in the tooltip when hovering.\n\n\n\n\n\nTable 3: K-means cluster assignments by species based on penguin bill length (mm) and depth (mm), and iris petal length (cm) and width (cm).\n\n\n\nPenguins cluster assignments\n\n\n\n\nIris cluster assignments\n\n\n\nCluster\n\n\nAdélie\n\n\nChinstrap\n\n\nGentoo\n\n\nCluster\n\n\nsetosa\n\n\nversicolor\n\n\nvirginica\n\n\n1\n\n\n0\n\n\n9\n\n\n116\n\n\n1\n\n\n0\n\n\n2\n\n\n46\n\n\n2\n\n\n4\n\n\n54\n\n\n6\n\n\n2\n\n\n0\n\n\n48\n\n\n4\n\n\n3\n\n\n147\n\n\n5\n\n\n1\n\n\n3\n\n\n50\n\n\n0\n\n\n0\n\n\nConclusion\nHere, we have shown that structural size measurements for Palmer Archipelago Pygoscelis penguins, available as penguins in the palmerpenguins R package, offer a near drop-in replacement for iris in a number of common use cases for data science and statistics education including exploratory data visualization, linear correlation and regression, PCA, and clustering by k-means. In addition, teaching and learning opportunities in penguins are increased due to a greater number of variables, missing values, unequal sample sizes, and Simpson’s Paradox examples. Importantly, the penguins dataset encompasses real-world information derived from several charismatic marine predator species with regional breeding populations notably responding to environmental change occurring throughout the Western Antarctic Peninsula region of the Southern Ocean (see Bestelmeyer et al. (2011), Gorman et al. (2014), Gorman et al. (2017), Gorman et al. (2021)). Thus, the penguins dataset can facilitate discussions more broadly on biodiversity responses to global change - a contemporary and critical topic in ecology, evolution, and the environmental sciences.\nPenguins data processing\nData in the penguins object have been minimally updated from penguins_raw as follows:\nAll variable names are converted to lower snake case (e.g. from Flipper Length (mm) to flipper_length_mm)\nEntries in species are truncated to only include the common name (e.g. “Gentoo”, instead of “gentoo penguin (Pygoscelis papua)”)\nRecorded sex for penguin N36A1, originally recorded as “.”, is updated to NA\nculmen_length_mm and culmen_depth_mm variable names are updated to bill_length_mm and bill_depth_mm, respectively\nClass for categorical variables (species, island, sex) is updated to factor\nVariable year was pulled from clutch observations\nSummary of the penguins_raw dataset\n\n\nFeature\n\n\npenguins_raw\n\n\nYear(s) collected\n\n\n2007 - 2009\n\n\nDimensions (col x row)\n\n\n17 x 344\n\n\nDocumentation\n\n\ncomplete metadata\n\n\nVariable classes\n\n\ncharacter (9), Date (1), numeric (7)\n\n\nMissing values?\n\n\nyes (n = 336; 5.7%)\n\n\npalmerpenguins for other programming languages\nPython: Python users can load the palmerpenguins datasets into their Python environment using the following code to install and access data in the palmerpenguins Python package:\npip install palmerpenguins\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()\nJulia: Julia users can access the penguins data in the PalmerPenguins.jl package. Example code to import the penguins data through PalmerPenguins.jl (more information on PalmerPenguins.jl from David Widmann can be found here):\njulia> using PalmerPenguins\njulia> table = PalmerPenguins.load()\nTensorFlow: TensorFlow users can access the penguins data in TensorFlow Datasets. Information and examples for penguins data in TensorFlow can be found here.\nAcknowledgements\nAll analyses were performed in the R language environment using version 4.1.2 (R Core Team 2021). Complete code for this paper is shared in the Supplemental Material. We acknowledge the following R packages used in analyses, with gratitude to developers and contributors:\nGGally (Schloerke et al. 2021): for pairs plots\nggiraph (Gohel and Skintzos 2022): for interactive ggplot2 graphics\nggplot2 (Wickham et al. 2021): for data visualizations\nkableExtra (Zhu 2021): for finalized tables\npaletteer (Hvitfeldt 2021): for the Okabe Ito color palette, provided by the colorblindr package\npatchwork (Pedersen 2020): for compound figures\nplotly (Sievert et al. 2021): for interactive graphics\nrecipes (Kuhn and Wickham 2021) and broom (Robinson et al. 2022): for modeling\nshadowtext (Yu 2022): to add a background color to text labels\ntidyverse (Wickham et al. 2019): for data import and cleaning\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-020.zip\nCRAN packages used\ndatasets, palmerpenguins, GGally, ggiraph, ggplot2, kableExtra, paletteer, colorblindr, patchwork, plotly, recipes, broom, shadowtext, tidyverse\nCRAN Task Views implied by cited packages\nSpatial, TeachingStatistics, WebTechnologies\n\n\nE. Anderson. The irises of the Gaspé Peninsula. Bulletin of the American Iris Society, 59: 2–5, 1935.\n\n\nB. T. Bestelmeyer, A. M. Ellison, W. R. Fraser, K. B. Gorman, S. J. Holbrook, C. M. Laney, M. D. Ohman, D. P. C. Peters, F. C. Pillsbury, A. Rassweiler, et al. Analysis of abrupt transitions in ecological systems. Ecosphere, 2(12): art129, 2011. URL http://doi.wiley.com/10.1890/ES11-00216.1 [online; last accessed March 27, 2021].\n\n\nR. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2): 179–188, 1936. URL http://doi.wiley.com/10.1111/j.1469-1809.1936.tb02137.x [online; last accessed July 1, 2020].\n\n\nD. Gohel and P. Skintzos. Ggiraph: Make ’ggplot2’ graphics interactive. 2022. URL https://CRAN.R-project.org/package=ggiraph. R package version 0.8.2.\n\n\nK. B. Gorman, K. E. Ruck, T. D. Williams and W. R. Fraser. Advancing the Sea Ice Hypothesis: Trophic Interactions Among Breeding pygoscelis Penguins With Divergent Population Trends Throughout the Western Antarctic Peninsula. Frontiers in Marine Science, 8: 526092, 2021. URL https://www.frontiersin.org/articles/10.3389/fmars.2021.526092/full [online; last accessed September 25, 2021].\n\n\nK. B. Gorman, S. L. Talbot, S. A. Sonsthagen, G. K. Sage, M. C. Gravely, W. R. Fraser and T. D. Williams. Population genetic structure and gene flow of Adélie penguins (Pygoscelis adeliae) breeding throughout the western Antarctic Peninsula. Antarctic Science, 29(6): 499–510, 2017. URL https://www.cambridge.org/core/product/identifier/S0954102017000293/type/journal_article [online; last accessed March 27, 2021].\n\n\nK. B. Gorman, T. D. Williams and W. R. Fraser. Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus pygoscelis). PLoS ONE, 9(3): e90081, 2014. URL https://dx.plos.org/10.1371/journal.pone.0090081 [online; last accessed July 1, 2020].\n\n\nA. Horst, A. Hill and K. Gorman. Palmerpenguins: Palmer archipelago (antarctica) penguin data. 2020. URL https://CRAN.R-project.org/package=palmerpenguins. R package version 0.1.0.\n\n\nE. Hvitfeldt. Paletteer: Comprehensive collection of color palettes. 2021. URL https://github.com/EmilHvitfeldt/paletteer. R package version 1.3.0.\n\n\nM. Kuhn and H. Wickham. Recipes: Preprocessing and feature engineering steps for modeling. 2021. URL https://CRAN.R-project.org/package=recipes. R package version 0.1.17.\n\n\nPalmer Station Antarctica LTER and K. B. Gorman. Structural size measurements and isotopic signatures of foraging among adult male and female Adélie penguins (Pygoscelis adeliae) nesting along the Palmer Archipelago near Palmer Station, 2007-2009. 2020a. URL https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-pal.219.5 [online; last accessed July 1, 2020].\n\n\nPalmer Station Antarctica LTER and K. B. Gorman. Structural size measurements and isotopic signatures of foraging among adult male and female Chinstrap penguin (Pygoscelis antarctica) nesting along the Palmer Archipelago near Palmer Station, 2007-2009. 2020b. URL https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-pal.221.6 [online; last accessed July 1, 2020].\n\n\nPalmer Station Antarctica LTER and K. B. Gorman. Structural size measurements and isotopic signatures of foraging among adult male and female Gentoo penguin (Pygoscelis papua) nesting along the Palmer Archipelago near Palmer Station, 2007-2009. 2020c. URL https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-pal.220.5 [online; last accessed July 1, 2020].\n\n\nT. L. Pedersen. Patchwork: The composer of plots. 2020. URL https://CRAN.R-project.org/package=patchwork. R package version 1.1.1.\n\n\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12: 2825–2830, 2011.\n\n\nR Core Team. R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing, 2021. URL https://www.R-project.org/.\n\n\nD. Robinson, A. Hayes and S. Couch. Broom: Convert statistical objects into tidy tibbles. 2022. URL https://CRAN.R-project.org/package=broom. R package version 0.7.11.\n\n\nB. Schloerke, D. Cook, J. Larmarange, F. Briatte, M. Marbach, E. Thoen, A. Elberg and J. Crowley. GGally: Extension to ggplot2. 2021. URL https://CRAN.R-project.org/package=GGally. R package version 2.1.2.\n\n\nC. Sievert, C. Parmer, T. Hocking, S. Chamberlain, K. Ram, M. Corvellec and P. Despouy. Plotly: Create interactive web graphics via plotly.js. 2021. URL https://CRAN.R-project.org/package=plotly. R package version 4.10.0.\n\n\nH. Wickham. Tidy Data. Journal of Statistical Software, 59(10): 2014. URL http://www.jstatsoft.org/v59/i10/ [online; last accessed July 1, 2020].\n\n\nH. Wickham, M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. François, G. Grolemund, A. Hayes, L. Henry, J. Hester, et al. Welcome to the tidyverse. Journal of Open Source Software, 4(43): 1686, 2019. DOI 10.21105/joss.01686.\n\n\nH. Wickham, W. Chang, L. Henry, T. L. Pedersen, K. Takahashi, C. Wilke, K. Woo, H. Yutani and D. Dunnington. ggplot2: Create elegant data visualisations using the grammar of graphics. 2021. URL https://CRAN.R-project.org/package=ggplot2. R package version 3.3.5.\n\n\nG. Yu. Shadowtext: Shadow text grob and layer. 2022. URL https://github.com/GuangchuangYu/shadowtext/. R package version 0.1.1.\n\n\nH. Zhu. kableExtra: Construct complex table with kable and pipe syntax. 2021. URL https://CRAN.R-project.org/package=kableExtra. R package version 1.3.4.\n\n\n\n\n",
    "preview": "https://allisonhorst.github.io/palmerpenguins/logo.png",
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-021/",
    "title": "Advancing Reproducible Research by Publishing R Markdown Notebooks as Interactive Sandboxes Using the learnr Package",
    "description": "Various R packages and best practices have played a pivotal role to promote the Findability, Accessibility, Interoperability, and Reuse (FAIR) principles of open science. For example, (1) well-documented R scripts and notebooks with rich narratives are deposited at a trusted data centre, (2) R Markdown interactive notebooks can be run on-demand as a web service, and (3) R Shiny web apps provide nice user interfaces to explore research outputs. However, notebooks require users to go through the entire analysis, while Shiny apps do not expose the underlying code and require extra work for UI design. We propose using the learnr package to expose certain code chunks in R Markdown so that users can readily experiment with them in guided, editable, isolated, executable, and resettable code sandboxes. Our approach does not replace the existing use of notebooks and Shiny apps, but it adds another level of abstraction between them to promote reproducible science.",
    "author": [
      {
        "name": "Chak Hau Michael Tso",
        "url": {}
      },
      {
        "name": "Michael Hollaway",
        "url": {}
      },
      {
        "name": "Rebecca Killick",
        "url": {}
      },
      {
        "name": "Peter Henrys",
        "url": {}
      },
      {
        "name": "Don Monteith",
        "url": {}
      },
      {
        "name": "John Watkins",
        "url": {}
      },
      {
        "name": "Gordon Blair",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\nIntroduction\nThere has been considerable recognition of the need to promote open and reproducible science in the past decade. The FAIR principles (Wilkinson et al. 2016; Stall et al. 2019) (https://www.go-fair.org/fair-principles/) of reproducible research are now known to most scientists. While significant advances has been made through the adoption of various best practices and policies (e.g. requirements from funders and publishers to archive data and source code, metadata standards), there remains considerable barriers to further advance open science and meet reproducible science needs. One of such issues the availability of various levels of abstraction of the same underlying analysis and code base to collaborate and engage with different stakeholders of diverse needs (Blair et al. 2019; Hollaway et al. 2020). For complex analysis or analysis that utilize a more advanced computing environment, it is essential to provide the capability to allow users to interact with the analysis at a higher level.\nExisting approach to reproducible research focuses on either documenting an entire analysis or allows user-friendly interaction. Within the R ecosystem, R scripts and notebooks allow researchers to work together and to view the entire workflow, while R Shiny apps (Chang et al. 2019) allows rapid showcase of methods and research outcomes to users with less experience. R Shiny has been widely adopted to share research output and engage stakeholders since its conception in 2013. A recent review (Kasprzak et al. 2021) shows that bioinformatics is the subject with the most Shiny apps published in journals while earth and environmental science ranks second. Shiny apps are especially helpful to create reproducible analysis (e.g. examples in Hollaway et al. 2020) and explore different scenarios (e.g. Whateley et al. 2015; Mose et al. 2018). Finally, the interactivity of Shiny apps makes it an excellent tool for teaching (e.g. Williams and Williams 2017; Field 2020). However, not all users fit nicely into this dichotomy. Some users may only want to adopt a small fraction of an analysis for their work, while others may simply want to modify a few parts of the analysis in order to test alternative hypothesis. Current use of notebooks do not seem to support such diverse needs as notebook output (e.g. figures and tables) are not easily reproducible. This issue essentially applies to all coding languages.\nOne potential way to address the problem described above is to allow\nusers to experiment with the code in protected computing environment.\nThis is not limited to creating instances for users to re-run the entire\ncode. Rather, this can also be done by exposing specific parts of a\nnotebook as editable and executable code boxes, as seen in many\ninteractive tutorial web pages for various coding languages. Recently,\nwhile discussing next steps for fostering reproducible research in\nartificial intelligence, Carter et al. (2019) lists creating a protected\ncomputing environment (‘data enclave’ or ‘sandbox’) for reviewers to log\nin and explore as one of the solutions. In software engineering, a\nsandbox is a testing environment that isolates untested code changes and\noutright experimentation from the production environment or repository,\nin the context of software development including Web development and\nrevision control. Making a sandbox environment available for users to\ntest and explore various changes to the code that leads to research\noutputs is a great step to further open science. Current practice of\nopen science largely requires users to assemble the notebooks, scripts\nand data files provided in their own computing environment, which\nrequires significant amount of time and effort. A sandbox environment\ncan greatly reduce such barriers and if such sandboxes are available as\na web service, users can explore and interact with the code that\ngenerates the research outputs at the convenience of their own web browser\non demand.\nIn this paper, we describe a rapid approach to create and publish\n‘interactive sandboxes’ R Shiny apps from R Markdown documents using\nthe learnr package, with the aim to bridge the gap between\ntypical R Markdown notebook and typical Shiny apps in terms of levels of\nabstraction. While code and markdown documents gives full details of the\ncode, standard R Shiny apps has too much limitations on users to\ninteract with the code and users often cannot see the underlying code.\nOur approach allows users to interact with selected parts of the code in\nan isolated manner, by specifying certain code chunks in a R Markdown\ndocument as executable code boxes.\nThe learnr R package\nlearnr (Schloerke et al. 2020) is an R package developed by RStudio to\nrapidly create interactive tutorials. It follows the general (the file has .Rmd extensions,\nhttps://rmarkdown.rstudio.com/index.html) architecture and\nessentially creates a pre-rendered Shiny document similar to the way\nShiny user interface (UI) components can be added to any R Markdown documents.\nPre-rendered Shiny documents\n(https://rmarkdown.rstudio.com/authoring_shiny_prerendered.HTML)\nis a key enabling technology for the learnr package since it\nallows users to specify the execution context in each code chunk of a R\nMarkdown document that is used to render a R Shiny web app. Its use\ncircumvents the need of a full document render for each end user browser\nsession so that this type of R Shiny apps can load quickly. To create a\nlearnr tutorial in RStudio after learnr is\ninstalled, the user chooses a learnr R Markdown template from\na list after clicking the “create new R Markdown document” button. This\ntemplate is not different from other .Rmd files, except it requires\nadditional chunk arguments to control the sandbox appearances. The two\nmain features of the learnr package are the “exercise” and\n“quiz” options. The former allows users to directly type in code,\nexecute it, and see its results to test their knowledge while the latter\nallows other question types such as multiple choice. Both of these\noptions include auto-graders, hints, and instructor feedback options.\nAdditional overall options include setting time limits and an option to\nforbid users to skip sections. Like any Shiny apps, learnr\napps can be easily embedded to other web pages, as seen in Higgins (2021).\nAlthough the learnr package has existed for a few years now,\nit is relatively not well known to scientists as a potential use of R\nShiny Apps and it has mostly been used for simple tutorial apps designed\nfor R beginners. We propose a novel application of the learnr\npackage to advance reproducible research, which we outline in the next\nsection.\nApproach: Using learnr for reproducible research ‘sandboxes’\nlearnr allows users to create executable code boxes. Our\napproach is to publish R notebooks and serve parts of the notebooks as\ninteractive sandboxes to allow users to re-create certain elements of a\npublished notebook containing research outputs. We do not use the\nauto-graders or any quiz-like functionality of learnr while\nkeeping the sandboxes. Notebook authors can go through their notebook\nand select the code chunks that they would allow users to experiment,\nwhile the others are rendered as static code snippets.\nRecognizing learnr documents are themselves R Shiny web apps,\nour approach essentially allows the publication of notebooks in the form\nof web apps. However, unlike a typical R Shiny web app, users do not\nneed to prepare a separate UI (i.e. user interface) layout. Advanced\nusers can modify the site appearance by supplying custom design in .css\nfiles.\nHere, we first show the skeleton of a R Markdown (.Rmd) file for a\nlearnr document (Figure 1). Notice that it is very similar to\na typical .Rmd file where there is a mixture of narratives written in\nmarkdown and R code chunks, in addition to a YAML header. However,\nthere are a couple of important exceptions, namely the use of the\n“exercise” chunk option (i.e. editable and executable code boxes)\nand different output type in the YAML header.\nNext, we outline the steps an author needs to take to publish notebooks (i.e. R Markdown documents) as interactive sandboxes:\nAll research output is included in the form of a well-documented R Markdown document.\nOpen a new learnr R Markdown template. Copy the content of the original notebook.\nFor the code chunks that you would like to become sandboxes, add exercise=TRUE. Make sure it has a unique chunk name. It may look something like this:\n```{r fig2, warning=FALSE, exercise=TRUE, exercise.lines=30,fig.fullwidth=TRUE}\nBefore any interactive code chunks, call the first code chunk ‘setup’.\nThis will pre-load everything that will be used later.\nCheck whether you would like to link any of the interactive code\nsnippets (by default each of them are independent, and only depends on\nthe output of the ‘setup’ chunk) You may want to modify your code\nchunks accordingly.\nDone! Knit the notebook to view outputs as an interactive web page.\nPublish it just like a Shiny app.\nThe entire process took us a few hours of effort and can be incorporated\nto the proof-reading of an R Markdown document. However, we note that as\nin any preparation of research output or web development several\niterations are often needed and the time required increases accordingly\nas the complexity of the analysis increases.\n\n\n\nFigure 1: A comparison of minimal examples of a typical .Rmd document and a .Rmd document for an interactive sandbox app.\n\n\n\nIn our implementation in DataLabs\n(https://datalab.datalabs.ceh.ac.uk/), the environment and folder\nto create the research is made available to the Shiny app in a read-only\nfashion. Therefore, the authors do not have to worry about versions of\npackages of data or a different software setup. Using DataLabs\nstraightforward visual tools to publish R Shiny apps, we can publish an\nR Markdown notebook with interactive code snippets to reproduce certain\nparts of research readily in a few clicks.\nDeployment\nIn general, learnr tutorial apps can be published the same way\nas R Shiny web apps in Shiny servers, such as the ones provided by cloud\nservice providers or https://shinyapps.io. The learnr\npackage vignettes provide additional help on deployment.\nWe also describe our deployment of these apps in DataLabs, a UK NERC\nvirtual research environment that is being developed. DataLabs is a\ncollaborative virtual research environment (Hollaway et al. 2020) (https://datalab.datalabs.ceh.ac.uk/) for environmental scientist\nto work together where data, software, and methods are all centrally\nlocated in projects. DataLabs provide a space for scientists from\ndifferent domains (data science, statisticians, environmental science\nand computer science) to work together and draw on each other’s\nexpertise. It includes an easy-to-use user interface where users can\npublish R Shiny apps with a few clicks, and this applies to these\nnotebooks with interactive code chunks as well. Importantly, when\nprovisioning a instance of R Shiny, this is deployed in a Docker\ncontainer with read-only access to the project data store being used for\nanalysis. This allows an unprecedented level of transparency as parts of\nthe analysis are readily exposed for users to experiment from the exact\nenvironments, datasets (can be large and includes many files), and\nversions of software that created the analysis. The use of Docker\ndeployed onto a Kubernetes infrastructure allows strict limits to be\nplaced on what visitors can do through the use of resource constraints\nand tools such as RAppArmor (Ooms 2013). While access to\nproject files is read-only, some author discretion is still advised to\nensure that visitors should not be able to view or list any private code\nor data. We also note that future releases of learnr will\ncontain external exercise evaluators, so that the code sandboxes can be\nexecuted by an independent engine (such as Google Cloud) and give the\nbenefit of not having to rely on RAppArmor.\nExample: GB rainfall paper\nTo demonstrate our concept, we have turned an R Markdown notebook for one of our recent papers (Tso et al. 2022) into a learnr site\n(https://cptecn-sandboxdemo.datalabs.ceh.ac.uk/) using the\nprocedures described in the previous sections. The paper investigates\nthe effect of weather and rainfall types on rainfall chemistry in the\nUK. As can be seen in Figure 2, the code chunks to generate certain\nparts of the paper is exposed. But unlike a static notebook site, the\ncode chunk is not only available for copy and paste but allows users to\nmodify and run on-demand. This makes it very straightforward for user to\nexperiment with various changes of the original analysis, thereby\npromoting transparency and trust.\nSince learnr apps are R Markdown documents, Shiny UI elements\ncan be easily added. We repeat one of the examples by replacing the\ninteractive code box by a simple selector, with minimal modification of\nthe code itself. This approach to publish Shiny apps requires\nsignificantly less work than typical R Shiny web apps since no UI design\nis needed and researchers can rapidly turn an R Markdown document to an R\nShiny web app. For some cases, the use of certain datasets may require a\nlicense, as in this example. A pop-up box is shown when the site is\nloaded and visitors are required to check the boxes to acknowledge the\nuse of the appropriate data licenses (an alternative is to require\nusers to register and load a token file) before they can proceed.\n\n\n\nFigure 2: A screenshot of the GB rainfall interactive notebook site. The main feature is the code box. When the site loads, the code that generates the published version of the figure is in the box and the published version of the figure is below it. Users can make edits and re-run the code in the code box and the figure will update accordingly. Users can use the “Start Over” button to see the published version of the code at any point without refreshing the entire site.\n\n\n\nEvaluation\nThe main strength of our approach is that it fills nicely the gap of\nexisting approaches in terms of levels of abstraction. While code and\nmarkdown documents gives full details of the code, standard R Shiny apps\nhas too much limitations on users to interact with the code (Figure 3)\nand users often cannot see the underlying code. Recently, it has become\npopular to publish ‘live’ Jupyter notebooks on Binder and Google Colab.\nWhile this is a great contribution to open science, users are still\nrequired to run and go through the entire notebook step-by-step and\nit can be easy to break it if users change something in between. Our\napproach allows users to interact with portions of the code in a guided\nand isolated manner, without the need to understand all the other parts\nof a notebook or the fear to break it (Table 1). We emphasize that R\nscripts/notebooks and R Shiny apps work well for their intended uses,\nbut our approach adds an additional level of accessibility to users.\nThe openness and ease-to-access our approach provides can benefit many\ndifferent stakeholders (Table 2). Researchers can more rapidly reproduce\nof the analysis of their choice without studying the entire\nnotebook or installing software or downloading all the data. They can\nquickly test alternative hypothesis and stimulate scientific\ndiscussions. For funders, encouraging the use of this approach means\nless time is needed for future projects to pick up results from previous\nwork. And since this is based on learnr which is originally\ndesigned as a tutorial tool, this approach will no doubt speed up the\nprocess to train other users to use similar methods. Overall, it\npromotes open science and make a better value of public funds.\nAn obvious limitation of our approach is that it does not work well for\nideal conditions where other R file formats are designed for. For\ninstance, R scripts and R notebooks are much better suited for more\ncomplex analysis for users to adopt to their own problems. Meanwhile, R\nShiny web apps provides a much richer user experience and is most suited\nwhen the exposed code is generally not useful to stakeholders.\nNevertheless, as discussed above, our approach is designed for users to\nreproduce of an analysis. The user should evaluate these\noptions carefully, paying special attention to the needs of intended\nusers.\nServing notebooks as a web service will inevitably face provenance\nissues. It is surely beneficial if the author’s institution can host\nthese interactive notebooks for a few years after its publication (and\nthat of its related publications). In the future, publishers and data\ncentres may consider providing services to provide longer term\nprovenance of serving these interactive notebooks online. As for any web\napps, funding for the computation servers can be a potential issue. This\nwork uses DataLabs computation time which is part of the UK research\nfunding that develops it. However, a more rigorous funding model may be\nneeded in the future to ensure provenance of these notebooks.\nOur approach focuses on improving reproducibility by exposing parts of R\nscript for users to run them live on an R Shiny web app, leveraging the\noption to render R Markdown documents as R Shiny web apps and the\nlearnr package. It focuses on the R scripts and R Markdown\ndocuments. Users, however, may want to improve reproducibility from the\nopposite direction, namely to allow outputs from an R Shiny web app to\nbe reproducible outside of the Shiny context. For such a requirement, we\nrecommend the use of the shinymeta package,\nwhich allows users to capture the underlying code of selected output\nelements and allows users to download it as well as the underlying data\nto re-create the output in their own R instance. The shinymeta\napproach can be more involved and requires more effort than\nlearnr so we think it is more suitable for users that are\nfocusing their effort on the R Shiny app (particularly the UI). In\nsummary, these two approaches complements each other and we recommend\nusers to consider them to improve reproducibility of their work.\n\n\n\nFigure 3: The various levels of abstraction of various types of R documents. Our approach fills nicely the gap between R Markdown or Jupyter notebooks and Shiny apps.\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Advantages of the proposed approach to various stakeholders\n\n\nAdvantages\n\n\nAuthors\n\n\nVery little extra work required in additional to writing R markdown document.\n\n\nNo experience to generate web interfaces required.\n\n\nMuch greater impact in research output.\n\n\nOther researchers (those wanting to try or compare the method)\n\n\nA much more enriched experience to try methods and data and to test alternative hypothesis and scenarios.\n\n\nNo need to download data and scripts/notebooks and install packages to try a method.\n\n\nMore efficient to learn the new method.\n\n\nOther researchers (those curious about the results)\n\n\nTry running different scenarios quickly than the published ones without the hassle of full knowledge of the code, downloading the code and data, and setting up the software environment.\n\n\nQuickly reset to the published version of code snippet.\n\n\nNo need to worry about breaking the code.\n\n\nData Centres\n\n\nA new avenue to demonstrate impact to funders if end users try methods or datasets hosted by them in sandboxes.\n\n\nFunders\n\n\nBetter value of investment if even small parts of a research is readily reproducible.\n\n\nTime saving to fund related work that builds on research documented this way.\n\n\nWider research community and general public\n\n\nPromotes trust and confidence in research through transparency.\n\n\nSummary and outlook\nWe have proposed and demonstrated a rapid approach to publish R Markdown\nnotebooks as interactive sandboxes to allow users to experiment with\nchanges with various elements of a research output. It provides an\nadditional level of abstraction for users to interact with research\noutputs and the codes that generates down. Since it can be linked to the\nenvironment and data that generated the published output and has\nindependent document object identifiers (DOI), it is a suitable\ncandidate to preserve research workflow while exposing parts of it to\nallow rapid experimentation by users. Our work is a demonstration on how\nwe may publish a notebook from virtual research environments such as\nDataLabs, with data, packages, and workflow pre-loaded in a coding\nenvironment, accompanied by rich narratives. While this paper outlines\nthe approach using R, the same approach can benefit other coding\nlanguages such as Python. In fact, this can already be achieved as\nlearnr can run Python chunks (as well as other execution\nengines knitr supports such as SAS and mySQL) as long as the\nusers generate and host the document using R. This paper contributes to\nthe vision towards publishing interactive notebooks as standalone\nresearch outputs and the advancement of open science practices.\nData availability and acknowledgements\nThe GB rainfall example notebook is accessible via this URL\n(https://cptecn-sandboxdemo.datalabs.ceh.ac.uk/) and the R\nMarkdown file is deposited in the NERC Environmental Information Data\nCentre (EIDC) (Tso 2022). The DataLab code stack is available at\nhttps://github.com/NERC-CEH/datalab. We thank the DataLabs\ndevelopers team (especially Iain Walmsley, UKCEH) for the assistance to\ndeploy interactive R Markdown documents on DataLabs. This work is\nsupported by NERC Grant NE/T006102/1, Methodologically Enhanced Virtual\nLabs for Early Warning of Significant or Catastrophic Change in\nEcosystems: Changepoints for a Changing Planet, funded under the\nConstructing a Digital Environment Strategic Priority Fund. Additional\nsupport is provided by the UK Status, Change and Projections of the\nEnvironment (UK-SCAPE) programme started in 2018 and is funded by the\nNatural Environment Research Council (NERC) as National Capability\n(award number NE/R016429/1). The initial development work of DataLabs\nwas supported by a NERC Capital bid as part of the Environmental Data\nServices (EDS).\n\n\n\n\n\n\n\n\nTable 2: Advantages of the proposed approach over existing approaches\n\n\nPotential Issues\n\n\nHow our approach can help?\n\n\nR script\n\n\nLimited narrative. Needs to run all the scripts.\n\n\nMuch richer narrative and interactive experience.\n\n\nStatic notebooks\n\n\nNeeds to download the code, data and package to try it out.\n\n\nCan instantly try out the code in a controlled manner, using the published data/packages/software environment.\n\n\nWeb apps (e.g. Shiny)\n\n\nWhile web apps helpful to some stakeholders, it can be too high-level to some.\n\n\nUsers can interact with the code within the code snippet sandboxes themselves.\n\n\nLots of extra work to create web interface.\n\n\nThe published version of the code is shown to users.\n\n\nDoes not expose the code to generate results.\n\n\nUsers can run the code snippets live.\n\n\nBinder/Google Colab\n\n\nUsers change the entire notebook.\n\n\nA much more enriched and guided experience.\n\n\nUsers need to run all cells about the section they are interested in.\n\n\nUsers can choose to only run the sandboxes they are interested in.\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-021.zip\nCRAN packages used\nRAppArmor, knitr\nCRAN Task Views implied by cited packages\nReproducibleResearch\n\n\nG. S. Blair, P. Henrys, A. Leeson, J. Watkins, E. Eastoe, S. Jarvis and P. J. Young. Data Science of the Natural Environment: A Research Roadmap. Frontiers in Environmental Science, 7: 2019. URL https://www.frontiersin.org/article/10.3389/fenvs.2019.00121/full.\n\n\nR. E. Carter, Z. I. Attia, F. Lopez-Jimenez and P. A. Friedman. Pragmatic considerations for fostering reproducible research in artificial intelligence. npj Digital Medicine, 2(1): 2019. URL https://doi.org/10.1038/s41746-019-0120-2.\n\n\nW. Chang, J. Cheng, J. Allaire, Y. Xie and J. McPherson. Shiny: Web application framework for r. 2019. URL https://CRAN.R-project.org/package=shiny. R package version 1.4.0.\n\n\nA. Field. adventr: Interactive R Tutorials to Accompany Field (2016), \"An Adventure in Statistics\". 2020. URL https://cran.r-project.org/web/packages/adventr/index.html.\n\n\nP. Higgins. Reproducible medical research with r. 2021. URL https://bookdown.org/pdr_higgins/rmrwr/.\n\n\nM. J. Hollaway, G. Dean, G. S. Blair, M. Brown, P. A. Henrys and J. Watkins. Tackling the challenges of 21st-century open science and beyond: A data science lab approach. Patterns, 1(7): 100103, 2020. URL https://doi.org/10.1016/j.patter.2020.100103.\n\n\nP. Kasprzak, L. Mitchell, O. Kravchuk and A. Timmins. Six years of shiny in research - collaborative development of web tools in R. R Journal, (3): 155–162, 2021. URL https://journal.r-project.org/archive/2021/RJ-2021-009/RJ-2021-009.pdf.\n\n\nV. N. Mose, D. Western and P. Tyrrell. Application of open source tools for biodiversity conservation and natural resource management in east africa. Ecological Informatics, 47: 35–44, 2018. URL https://doi.org/10.1016/j.ecoinf.2017.09.006.\n\n\nJ. Ooms. The RAppArmor package: Enforcing security policies in R using dynamic sandboxing on linux. Journal of Statistical Software, 55(7): 1–34, 2013. URL http://www.jstatsoft.org/v55/i07/.\n\n\nB. Schloerke, J. Allaire and B. Borges. Learnr: Interactive tutorials for r. 2020. URL https://CRAN.R-project.org/package=learnr. R package version 0.10.1.\n\n\nS. Stall, L. Yarmey, J. Cutcher-Gershenfeld, B. Hanson, K. Lehnert, B. Nosek, M. Parsons, E. Robinson and L. Wyborn. Make scientific data FAIR. 2019. URL https://doi.org/10.1038/d41586-019-01720-7.\n\n\nC.-H. M. Tso. R shiny sandbox app demonstrator for advancing reproducible research. 2022. URL https://doi.org/10.5285/df57b002-2a42-4a7d-854f-870dd867618c.\n\n\nC.-H. M. Tso, D. Monteith, T. Scott, H. Watson, B. Dodd, M. G. Pereira, P. Henrys, M. Hollaway, S. Rennie, A. Lowther, et al. The evolving role of weather types on rainfall chemistry under large reductions in pollutant emissions. Environmental Pollution, 299: 118905, 2022. URL https://doi.org/10.1016/j.envpol.2022.118905.\n\n\nS. Whateley, J. D. Walker and C. Brown. A web-based screening model for climate risk to water supply systems in the northeastern united states. Environmental Modelling & Software, 73: 64–75, 2015. URL https://doi.org/10.1016/j.envsoft.2015.08.001.\n\n\nM. D. Wilkinson, M. Dumontier, Ij. J. Aalbersberg, G. Appleton, M. Axton, A. Baak, N. Blomberg, J. W. Boiten, L. B. da Silva Santos, P. E. Bourne, et al. Comment: The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 2016. URL https://doi.org/10.1038/sdata.2016.18.\n\n\nI. J. Williams and K. K. Williams. Using an R shiny to enhance the learning experience of confidence intervals. Teaching Statistics, 40(1): 24–28, 2017. URL https://doi.org/10.1111/test.12145.\n\n\n\n\n",
    "preview": "articles/RJ-2022-021/distill-preview.png",
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {},
    "preview_width": 405,
    "preview_height": 442
  },
  {
    "path": "articles/RJ-2021-106/",
    "title": "RPESE: Risk and Performance Estimators Standard Errors with Serially Dependent Data",
    "description": "The R package RPESE (Risk and Performance Estimators Standard Errors) implements a new method for computing accurate standard errors of risk and performance estimators when returns are serially dependent. The new method makes use of the representation of a risk or performance estimator as a summation of a time series of influence-function (IF) transformed returns, and computes estimator standard errors using a sophisticated method of estimating the spectral density at frequency zero of the time series of IF-transformed returns. Two additional packages used by RPESE are introduced, namely RPEIF which computes and provides graphical displays of the IF of risk and performance estimators, and RPEGLMEN which implements a regularized Gamma generalized linear model polynomial fit to the periodogram of the time series of the IF-transformed returns. A Monte Carlo study shows that the new method provides more accurate estimates of standard errors for risk and performance estimators compared to well-known alternative methods in the presence of serial correlation.",
    "author": [
      {
        "name": "Anthony-Alexander Christidis",
        "url": {}
      },
      {
        "name": "R. Douglas Martin",
        "url": {}
      }
    ],
    "date": "2022-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRPESE, RPEIF, RPEGLMEN, PerformanceAnalytics, RobStatTM, nse, sandwich\nCRAN Task Views implied by cited packages\nEconometrics, Finance, Robust, SocialSciences\n\n\n",
    "preview": "articles/RJ-2021-106/preview.png",
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {},
    "preview_width": 2331,
    "preview_height": 1287
  },
  {
    "path": "articles/RJ-2021-097/",
    "title": "BayesSenMC: an R package for Bayesian Sensitivity Analysis of Misclassification",
    "description": "In case–control studies, the odds ratio is commonly used to summarize the association be tween a binary exposure and a dichotomous outcome. However, exposure misclassification frequently appears in case–control studies due to inaccurate data reporting, which can produce bias in measures of association. In this article, we implement a Bayesian sensitivity analysis of misclassification to provide a full posterior inference on the corrected odds ratio under both non-differential and differen tial misclassification. We present an R (R Core Team, 2018) package BayesSenMC, which provides user-friendly functions for its implementation. The usage is illustrated by a real data analysis on the association between bipolar disorder and rheumatoid arthritis.",
    "author": [
      {
        "name": "Jinhui Yang",
        "url": {}
      },
      {
        "name": "Lifeng Lin",
        "url": {}
      },
      {
        "name": "Haitao Chu",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nBayesSenMC, episensr, lme4, rstan, ggplot2\nCRAN Task Views implied by cited packages\nBayesian, Econometrics, Environmetrics, OfficialStatistics, Phylogenetics, Psychometrics, SocialSciences, SpatioTemporal, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-097/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 754,
    "preview_height": 521
  },
  {
    "path": "articles/RJ-2021-098/",
    "title": "EMSS: New EM-type algorithms for the Heckman selection model in R",
    "description": "When investigators observe non-random samples from populations, sample selectivity problems may occur. The Heckman selection model is widely used to deal with selectivity problems. Based on the EM algorithm, Zhao et al. (2020) developed three algorithms, namely, ECM, ECM(NR), and ECME(NR), which also have the EM algorithm’s main advantages: stability and ease of imple mentation. This paper provides the implementation of these three new EM-type algorithms in the package EMSS and illustrates the usage of the package on several simulated and real data examples. The comparison between the maximum likelihood estimation method (MLE) and three new EM-type algorithms in robustness issues is further discussed.",
    "author": [
      {
        "name": "Kexuan Yang",
        "url": {}
      },
      {
        "name": "Sang Kyu Lee",
        "url": {}
      },
      {
        "name": "Jun Zhao",
        "url": {}
      },
      {
        "name": "Hyoung-Moon Kim",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsampleSelection, mvtnorm\nCRAN Task Views implied by cited packages\nDistributions, Econometrics, Finance, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-099/",
    "title": "Analysis of Corneal Data in R with the rPACI Package",
    "description": "In ophthalmology, the early detection of keratoconus is still a crucial problem. Placido disk corneal topographers are essential in clinical practice, and many indices for diagnosing corneal irregularities exist. The main goal of this work is to present the R package rPACI, providing several functions to handle and analyze corneal data. This package implements primary indices of corneal irregularity (based on geometrical properties) and compound indices built from the primary ones, either using a generalized linear model or as a Bayesian classifier using a hybrid Bayesian network and performing approximate inference. rPACI aims to make the analysis of corneal data accessible for practitioners and researchers in the field. Moreover, a shiny app was developed to use rPACI in any web browser in a truly user-friendly graphical interface without installing R or writing any R code. It is openly deployed at https://admaldonado.shinyapps.io/rPACI/.",
    "author": [
      {
        "name": "Darío Ramos-López",
        "url": {}
      },
      {
        "name": "Ana D. Maldonado",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrPACI, shiny, bnlearn\nCRAN Task Views implied by cited packages\nBayesian, GraphicalModels, HighPerformanceComputing, TeachingStatistics, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-099/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 980,
    "preview_height": 500
  },
  {
    "path": "articles/RJ-2021-100/",
    "title": "DRHotNet: An R package for detecting differential risk hotspots on a linear network",
    "description": "One of the most common applications of spatial data analysis is detecting zones, at a certain scale, where a point-referenced event under study is especially concentrated. The detection of such zones, which are usually referred to as hotspots, is essential in certain fields such as criminology, epidemiology, or traffic safety. Traditionally, hotspot detection procedures have been developed over areal units of analysis. Although working at this spatial scale can be suitable enough for many research or practical purposes, detecting hotspots at a more accurate level (for instance, at the road segment level) may be more convenient sometimes. Furthermore, it is typical that hotspot detection procedures are entirely focused on the determination of zones where an event is (overall) highly concentrated. It is less common, by far, that such procedures focus on detecting zones where a specific type of event is overrepresented in comparison with the other types observed, which have been denoted as differential risk hotspots. The R package DRHotNet provides several functionalities to facilitate the detection of differential risk hotspots within a linear network. In this paper, DRHotNet is depicted, and its usage in the R console is shown through a detailed analysis of a crime dataset.",
    "author": [
      {
        "name": "Álvaro Briz-Redón",
        "url": {}
      },
      {
        "name": "Francisco Martínez-Ruiz",
        "url": {}
      },
      {
        "name": "Francisco Montes",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspdep, DCluster, spatstat.linnet, spatstat, DRHotNet, sp, sf, sfnetworks, maptools, spatstat.geom, SpNetPrep, rgeos, spatstat.data, tigris, raster, crimedata, lubridate\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, ReproducibleResearch, Survival, TimeSeries\n\n\n",
    "preview": "articles/RJ-2021-100/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 504
  },
  {
    "path": "articles/RJ-2021-101/",
    "title": "Automatic Time Series Forecasting with Ata Method in R: ATAforecasting Package",
    "description": "Ata method is a new univariate time series forecasting method that provides innovative solutions to issues faced during the initialization and optimization stages of existing methods. The Ata method’s forecasting performance is superior to existing methods in terms of easy implementation and accurate forecasting. It can be applied to non-seasonal or deseasonalized time series, where",
    "author": [
      {
        "name": "Ali Sabri Taylan",
        "url": {}
      },
      {
        "name": "Güçkan Yapar",
        "url": {}
      },
      {
        "name": "Hanife Taylan Selamlar",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nATAforecasting, forecast, stats, stlplus, stR, tsibbledata, ucra, tseries, seasonal, Rcpp, RcppArmadillo, urca, uroot, xts, timeSeries, TSA, Mcomp, fable.ata, fabletools, fable\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance, MissingData, Environmetrics, NumericalMathematics, HighPerformanceComputing, OfficialStatistics, SpatioTemporal\n\n\n",
    "preview": "articles/RJ-2021-101/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 768
  },
  {
    "path": "articles/RJ-2021-102/",
    "title": "spNetwork: A Package for Network Kernel Density Estimation",
    "description": "This paper introduces the new package spNetwork that provides functions to perform Network Kernel Density Estimate analysis (NKDE). This method is an extension of the classical Kernel Density Estimate (KDE), a non parametric approach to estimate the intensity of a spatial process. More specifically, it adapts the KDE for cases when the study area is a network, constraining the location of events (such as accidents on roads, leaks in pipes, fish in rivers, etc.). We present and discuss in this paper the three main versions of NKDE: simple, discontinuous, and continuous that are implemented in spNetwork. We illustrate how to apply the three methods and map their results using a sample from a real dataset representing bike accidents in a central neighborhood of Montreal. We also describe the optimization techniques used to reduce calculation time and investigate their impacts when applying the three NKDE to a city-wide dataset.",
    "author": [
      {
        "name": "Jeremy Gelb",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspNetwork, spatstat, rgdal, sp, rgeos, maptools, igraph, Rcpp, future, SpNetwork, SearchTrees, future.apply, RcppArmadillo\nCRAN Task Views implied by cited packages\nSpatial, HighPerformanceComputing, NumericalMathematics, SpatioTemporal, GraphicalModels, Optimization, Survival\n\n\n",
    "preview": "articles/RJ-2021-102/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 815,
    "preview_height": 443
  },
  {
    "path": "articles/RJ-2021-103/",
    "title": "bssm: Bayesian Inference of Non-linear and Non-Gaussian State Space Models in R",
    "description": "We present an R package bssm for Bayesian non-linear/non-Gaussian state space modeling. Unlike the existing packages, bssm allows for easy-to-use approximate inference based on Gaussian approximations such as the Laplace approximation and the extended Kalman filter. The package also accommodates discretely observed latent diffusion processes. The inference is based on fully automatic, adaptive Markov chain Monte Carlo (MCMC) on the hyperparameters, with optional importance sampling post-correction to eliminate any approximation bias. The package also implements a direct pseudo-marginal MCMC and a delayed acceptance pseudo-marginal MCMC using intermediate approximations. The package offers an easy-to-use interface to define models with linear-Gaussian state dynamics with non-Gaussian observation models and has an Rcpp interface for specifying custom non-linear and diffusion models.",
    "author": [
      {
        "name": "Jouni Helske",
        "url": {}
      },
      {
        "name": "Matti Vihola",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbssm, Rcpp, pomp, rbi, nimbleSMC, rstan, ramcmc, RcppArmadillo, KFAS, sde, coda, ggplot2, dplyr\nCRAN Task Views implied by cited packages\nTimeSeries, Bayesian, DifferentialEquations, NumericalMathematics, Databases, Finance, GraphicalModels, HighPerformanceComputing, ModelDeployment, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-103/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 468,
    "preview_height": 324
  },
  {
    "path": "articles/RJ-2021-104/",
    "title": "Generalized Linear Randomized Response Modeling using GLMMRR",
    "description": "Randomized response (RR) designs are used to collect response data about sensitive behaviors (e.g., criminal behavior, sexual desires). The modeling of RR data is more complex since it requires a description of the RR process. For the class of generalized linear mixed models (GLMMs), the RR process can be represented by an adjusted link function, which relates the expected RR to the linear predictor for most common RR designs. The package GLMMRR includes modified link functions for four different cumulative distributions (i.e., logistic, cumulative normal, Gumbel, Cauchy) for GLMs and GLMMs, where the package lme4 facilitates ML and REML estimation. The mixed modeling framework in GLMMRR can be used to jointly analyze data collected under different designs (e.g., dual questioning, multilevel, mixed mode, repeated measurements designs, multiple-group designs). Model-fit tests, tools for residual analyses, and plot functions to give support to a profound RR data analysis are added to the well-known features of the GLM and GLMM software (package lme4). Data of Höglinger and Jann (2018) and Höglinger, Jann, and Diekmann (2014) are used to illustrate the methodology and software.",
    "author": [
      {
        "name": "Jean-Paul Fox",
        "url": {}
      },
      {
        "name": "Konrad Klotzke",
        "url": {}
      },
      {
        "name": "Duco Veen",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrr, RRreg, GLMMRR, stats, lme4\nCRAN Task Views implied by cited packages\nOfficialStatistics, Psychometrics, Econometrics, Environmetrics, SocialSciences, SpatioTemporal\n\n\n",
    "preview": "articles/RJ-2021-104/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 376,
    "preview_height": 264
  },
  {
    "path": "articles/RJ-2021-105/",
    "title": "Visual Diagnostics for Constrained Optimisation with Application to Guided Tours",
    "description": "A guided tour helps to visualise high-dimensional data by showing low-dimensional projections along a projection pursuit optimisation path. Projection pursuit is a generalisation of principal component analysis in the sense that different indexes are used to define the interestingness of the projected data. While much work has been done in developing new indexes in the literature, less has been done on understanding the optimisation. Index functions can be noisy, might have multiple local maxima as well as an optimal maximum, and are constrained to generate orthonormal projection frames, which complicates the optimization. In addition, projection pursuit is primarily used for exploratory data analysis, and finding the local maxima is also useful. The guided tour is especially useful for exploration because it conducts geodesic interpolation connecting steps in the optimisation and shows how the projected data changes as a maxima is approached. This work provides new visual diagnostics for examining a choice of optimisation procedure based on the provision of a new data object which collects information throughout the optimisation. It has helped to diagnose and fix several problems with projection pursuit guided tour. This work might be useful more broadly for diagnosing optimisers and comparing their performance. The diagnostics are implemented in the R package [ferrn](https://github.com/huizezhang-sherry/ferrn).",
    "author": [
      {
        "name": "H. Sherry Zhang",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      },
      {
        "name": "Ursula Laa",
        "url": {}
      },
      {
        "name": "Nicolas Langrené",
        "url": {}
      },
      {
        "name": "Patricia Menéndez",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nIntroduction\nVisualisation is widely used in exploratory data analysis (Tukey 1977; Unwin 2015; Healy 2018; Wilke 2019). Presenting information in graphics often unveils insights that would otherwise not be discovered and provides a more comprehensive understanding of the problem at hand. Task-specific tools such as Li et al. (2020) show how visualisation can be used to understand, for instance, the behaviour of the optimisation for the example of neural network classification models. However, no general visualisation tool is available for diagnosing optimisation procedures. The work presented in this paper brings visualization tools into optimisation problems with the aim to better understand the performance of optimisers in practice.\nThe focus of this paper is on the optimisation problem arising in the projection pursuit guided tour (Buja et al. 2005), an exploratory data analysis technique used for detecting interesting structures in high-dimensional data through a set of lower-dimensional projections (Cook et al. 2008). The goal of the optimisation is to identify the projection, represented by the projection matrix, that gives the most interesting low-dimensional view. A view is said to be interesting if it can show some structures of the data that depart from normality, such as bimodality, clustering, or outliers.\nThe optimization challenges encountered in the projection pursuit guided tour problem are common to those of optimization in general. Examples include the existence of multiple optima (local and global), the trade-off between computational burden and proximity to the optima, or dealing with noisy objective functions that might be non-smooth and non-differentiable (Jones et al. 1998). The visualization tools, optimization methods, and conceptual framework presented in this paper can therefore be applied to other optimization problems.\nThe remainder of the paper is organised as follows. The next section provides an overview of optimisation methods, specifically random search and line search methods. A review of the projection pursuit guided tour, an overview of the optimisation problem and, outlines of three existing algorithms follows. The third section presents the new visual diagnostics, including the design of a data structure to capture information during the optimisation, from which several diagnostic plots are created. An illustration of how the diagnostic plots can be used to examine the performance of different optimisers and guide improvements to existing algorithms is shown using simulated data. Finally, an explanation of the implementation in the R package, ferrn (Zhang et al. 2021), is provided.\nOptimisation methods\nThe type of optimisation problem considered in this paper is constrained optimization (Bertsekas 2014), assuming it is not possible to find a solution to the problem in the way of a closed-form. That is, the problem consists in finding the minimum or maximum of a function \\(f \\in L^p\\) in the constrained space \\(\\mathcal{A}\\), where \\(L^p\\) defines the vector space of function \\(f\\), whose \\(p\\)th power is integrable.\nGradient-based methods are commonly used to optimise an objective function, with the most notable one being the gradient ascent (descent) method. Although these methods are popular, they rely on the availability of the objective function derivatives. As will be shown in the next section, the independent variables in our optimisation problem are the entries of a projection matrix, and the computational time required to perform differentiation on a matrix could impede the rendering of tour animation. In addition, some objective functions rely on the empirical distribution of the data, which makes it in general not possible to get the gradient. Hence, gradient-based methods are not the focus of this paper, and consideration will be given to derivative-free methods.\nDerivative-free methods (Conn et al. 2009; Rios and Sahinidis 2013), which do not rely on the knowledge of the gradient, are more generally applicable. Derivative-free methods have been developed over the years, where the emphasis is on finding, in most cases, a near-optimal solution. Here we consider three derivative-free methods, two of which are random search methods: creeping random search and simulated annealing, and the other one is pseudo-derivative search.\nRandom search methods (Romeijn 2009; Zabinsky 2013; Andradóttir 2015) have a random sampling component as part of their algorithms and have been shown to have the ability to optimise non-convex and non-smooth functions. The initial random search algorithm, pure random search (Brooks 1958), draws candidate points from the entire space without using any information of the current position and updates the current position when an improvement on the objective function is made. As the dimension of the space becomes larger, sufficient sampling from the entire space would require a long time for convergence to occur, despite a guaranteed global convergence (Spall 2005). Various algorithms have thus been developed to improve pure random search by either concentrating on a narrower sampling space or using a different updating mechanism. Creeping random search (White 1971) is such a variation, where a candidate point is generated within a neighbourhood of the current point. This makes creeping random search faster to compute but global convergence is no longer guaranteed. On the other hand, simulated annealing (Kirkpatrick et al. 1983; Bertsimas and Tsitsiklis 1993), introduces a different updating mechanism. Rather than only updating the current point when an improvement is made, it uses a Metropolis acceptance criterion, where worse candidates still have a chance to be accepted. The convergence of simulated annealing algorithms has been widely researched (Mitra et al. 1986; Granville et al. 1994) and the global optimum can be attained under mild regularity conditions.\nThe pseudo-derivative search uses a common search scheme in optimisation: line search. In line search methods, users are required to provide an initial estimate \\(x_{1}\\) and, at each iteration, a search direction \\(S_k\\) and a step size \\(\\alpha_k\\) are generated. Then one moves on to the next point following \\(x_{k+1} = x_k + \\alpha_kS_k\\) and the process is repeated until the desired convergence is reached. In derivative-free methods, local information of the objective function is used to determine the search direction. The choice of step size also needs consideration, as inadequate step sizes might prevent the optimisation method from converging to an optimum. An ideal step size can be chosen by finding the value of \\(\\alpha_k \\in \\mathbb{R}\\) that maximises \\(f(x_k + \\alpha_kS_k)\\) with respect to \\(\\alpha_k\\) at each iteration.\nProjection pursuit guided tour\nA projection pursuit guided tour combines two different methods (projection pursuit and guided tour) to explore interesting features in a high-dimensional space. Projection pursuit, coined by Friedman and Tukey (1974), detects interesting structures (e.g., clustering, outliers, and skewness) in multivariate data via low-dimensional projections. Guided tour (Cook et al. 1995) is one variation of a broader class of data visualisation methods, tour (Buja et al. 2005), which displays high-dimensional data through a series of animated projections.\nLet \\(\\mathbf{X}_{n \\times p}\\) be the data matrix with \\(n\\) observations in \\(p\\) dimensions. A \\(d\\)-dimensional projection is a linear transformation from \\(\\mathbb{R}^p\\) into \\(\\mathbb{R}^d\\) defined as \\(\\mathbf{Y} = \\mathbf{X} \\cdot \\mathbf{A}\\), where \\(\\mathbf{Y}_{n \\times d}\\) is the projected data and \\(\\mathbf{A}_{p\\times d}\\) is the projection matrix. We define \\(f: \\mathbb{R}^{n \\times d} \\mapsto \\mathbb{R}\\) to be an index function that maps the projected data \\(\\mathbf{Y}\\) onto a scalar value. This is commonly known as the projection pursuit index function, or just index function, and is used to measure the “interestingness” of a given projection. An interesting projection shows structures that are non-normal since theoretical proofs from Diaconis and Freedman (1984) have shown that projections tend to be normal as \\(n\\) and \\(p\\) approach infinity under certain conditions. There have been many index functions proposed in the literature, here are a few examples: early indexes that can be categorised as measuring the \\(L^2\\) distance between the projection and a normal distribution: Legendre index (Friedman and Tukey 1974); Hermite index (Hall 1989); natural Hermite index (Cook et al. 1993); chi-square index (Posse 1995) for detecting spiral structure; LDA index (Lee et al. 2005) and PDA (Lee and Cook 2010) index for supervised classification; kurtosis index (Loperfido 2020) and skewness index (Loperfido 2018) for detecting outliers in financial time series; and most recently, scagnostic indexes (Laa and Cook 2020) for summarising structures in scatterplot matrices based on eight scagnostic measures (Wilkinson et al. 2005; Wilkinson and Wills 2008).\nAs a general visualisation method, tour produces animations of high-dimensional data via rotations of low-dimensional planes. There are different versions depending on how the high-dimensional space is investigated: grand tour (Cook et al. 2008) selects the planes randomly to provide a general overview; manual tour (Cook and Buja 1997) gradually phases in and out one variable to understand the contribution of that variable in the projection. Guided tour, the main interest of this paper, chooses the planes with the aid of projection pursuit to gradually reveal the most interesting projection. Given a random start, projection pursuit iteratively finds bases with higher index values, and the guided tour constructs a geodesic interpolation between these planes to form a tour path. Figure 1 shows a sketch of the tour path where the blue squares represent planes (targets) selected by the projection pursuit optimisation, and the white squares represent planes in the geodesic interpolation between targets. Mathematical details of the geodesic interpolation can be found in Buja et al. (2005). (Note that the term frame used in Buja’s paper refers to a particular set of orthonormal vectors defining a plane. This is also conventionally referred to as a basis, which is used in this paper and the associated software.) The aforementioned tour method has been implemented in the R package tourr (Wickham et al. 2011).\n\n\n\nFigure 1: An illustration for demonstrating the frames in a tour path. Each square (frame) represents the projected data with a corresponding basis. Blue frames are returned by the projection pursuit optimisation and white frames are constructed between two blue frames by geodesic interpolation.\n\n\n\nOptimisation in the tour\nIn projection pursuit, the optimisation aims at finding the global and local maxima that give interesting projections according to an index function. That is, it starts with a given randomly selected basis \\(\\mathbf{A}_1\\) and aims at finding an optimal final projection basis \\(\\mathbf{A}_T\\) that satisfies the following optimisation problem:\n\\[\\begin{align}\n\\arg \\max_{\\mathbf{A} \\in \\mathcal{A}} f(\\mathbf{X} \\cdot \\mathbf{A})  ~~~ s.t. ~~~ \\mathbf{A}^{\\prime} \\mathbf{A} = I_d \\,,\n\\end{align}\\]\nwhere \\(f\\) and \\(\\mathbf{X}\\) are defined as in the previous section, \\(\\mathcal{A}\\) is the set of all \\(p\\)-dimensional projection bases, \\(I_d\\) is the \\(d\\)-dimensional identity matrix, and the constraint ensures the projection bases, \\(\\mathbf{A}\\), to be orthonormal. It is worth noticing the following: 1) The optimisation is constrained, and the orthonormality constraint imposes a geometrical structure on the bases space: it forms a Stiefel manifold. 2) There may be index functions for which the objective function might not be differentiable. 3) While finding the global optimum is the goal of the optimisation problem, interesting projections may also appear in the local optimum. 4) The optimisation should be fast to compute since the tour animation is viewed by the users during the optimisation.\nExisting algorithms\nThree optimisers have been implemented in the tourr (Wickham et al. 2011) package: creeping random search (CRS), simulated annealing (SA), and pseudo-derivative (PD). Creeping random search (CRS) is a random search optimiser that samples a candidate basis \\(\\mathbf{A}_{l}\\) in the neighbourhood of the current basis \\(\\mathbf{A}_{\\text{cur}}\\) by \\(\\mathbf{A}_{l} = (1- \\alpha)\\mathbf{A}_{\\text{cur}} + \\alpha \\mathbf{A}_{\\text{rand}}\\) where \\(\\alpha \\in [0,1]\\) controls the radius of the sampling neighbourhood and \\(\\mathbf{A}_{\\text{rand}}\\) is generated randomly. \\(\\mathbf{A}_{l}\\) is then orthonormalised to fulfil the basis constraint. If \\(\\mathbf{A}_{l}\\) has an index value higher than the current basis \\(\\mathbf{A}_{\\text{cur}}\\), the optimiser outputs \\(\\mathbf{A}_{l}\\) for a guided tour to construct an interpolation path. The neighbourhood parameter \\(\\alpha\\) is adjusted by a cooling parameter: \\(\\alpha_{j+1} = \\alpha_j * \\text{cooling}\\) before the next iteration starts. The optimiser terminates when the maximum number of iteration \\(l_{\\max}\\) is reached before a better basis can be found. The algorithm of CRS can be found in the appendix. Posse (1995) has proposed a slightly different cooling scheme by introducing a halving parameter \\(c\\). In his proposal, \\(\\alpha\\) is only adjusted if the last iteration takes more than \\(c\\) times to find a better basis.\nSimulated annealing (SA) uses the same sampling process as CRS but allows a probabilistic acceptance of a basis with lower index value than the current one. Given an initial value of \\(T_0 \\in \\mathbb{R^{+}}\\), the “temperature” at iteration \\(l\\) is defined as \\(T(l) = \\frac{T_0}{\\log(l + 1)}\\). When a candidate basis fails to have an index value larger than the current basis, SA gives it a second chance to be accepted with probability \\[P= \\min\\left\\{\\exp\\left[-\\frac{\\mid I_{\\text{cur}} - I_{l} \\mid}{T(l)}\\right],1\\right\\} \\,,\\] where \\(I_{(\\cdot)} \\in \\mathbb{R}\\) denotes the index value of a given basis. This implementation allows the optimiser to make a move and explore the basis space even if the candidate basis does not have a higher index value. Hence it enables the optimiser to jump out of a local optimum. The second algorithm in the appendix highlights how SA differs from CRS in the inner loop.\nPseudo-derivative (PD) search uses a different strategy than CRS and SA. Rather than randomly sample the basis space, PD first computes a search direction by evaluating bases close to the current basis. The step size is then chosen along the corresponding geodesic by another optimisation over a 90 degree angle from \\(-\\pi/4\\) to \\(\\pi/4\\). The resulting candidate basis \\(\\mathbf{A}_{**}\\) is returned for the current iteration if it has a higher index value than the current one. The third algorithm in the appendix summarises the inner loop of the PD.\nVisual diagnostics\nA data structure for diagnosing optimisers in projection pursuit guided tour is first defined. With this data structure, four types of diagnostic plots are presented.\nData structure for diagnostics\nThree main pieces of information are recorded during the projection pursuit optimisation: 1) projection bases \\(\\mathbf{A}\\), 2) index values \\(I\\), and 3) state \\(S\\). For CRS and SA, possible states include random_search, new_basis, and interpolation. Pseudo-derivative (PD) has a wider variety of states, including new_basis, direction_search, best_direction_search, best_line_search, and interpolation. Multiple iterators index the information collected at different levels: \\(t\\) is a unique identifier prescribing the natural ordering of each observation; \\(j\\) and \\(l\\) are the counter of the outer and inner loop, respectively. Other parameters of interest recorded, \\(V\\), include method that tags the name of the optimiser, and alpha that indicates the sampling neighbourhood size for searching observations. A matrix notation describing the data structure is:\n\n\\(t\\)\n\\(\\mathbf{A}\\)\n\\(I\\)\n\\(S\\)\n\\(j\\)\n\\(l\\)\n\\(V_{1}\\)\n\\(V_{2}\\)\nDescription\n\\(1\\)\n\\(\\mathbf{A}_1\\)\n\\(I_1\\)\n\\(S_1\\)\n1\n1\n\\(V_{11}\\)\n\\(V_{12}\\)\nstart basis\n\\(2\\)\n\\(\\mathbf{A}_2\\)\n\\(I_2\\)\n\\(S_2\\)\n2\n1\n\\(V_{21}\\)\n\\(V_{22}\\)\nsearch\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n2\n\\(l_2\\)\n…\n…\nsearch (accepted)\n…\n…\n…\n…\n2\n1\n…\n…\ninterpolation\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n2\n\\(k_2\\)\n…\n…\ninterpolation\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\\(J\\)\n1\n…\n…\nsearch\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\\(T\\)\n\\(\\mathbf{A}_T\\)\n\\(I_T\\)\n\\(S_T\\)\n\\(J\\)\n\\(l_J\\)\n\\(V_{T1}\\)\n\\(V_{T2}\\)\nsearch (final)\n…\n…\n…\n…\n\\(J\\)\n1\n…\n…\ninterpolation\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\\(J\\)\n\\(k_J\\)\n…\n…\ninterpolation\n…\n…\n…\n…\n\\(J+1\\)\n1\n…\n…\nsearch (last round)\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\\(T^{\\prime}\\)\n\\(\\mathbf{A}_{T^{\\prime}}\\)\n\\(I_{T^{\\prime}}\\)\n\\(S_{T^{\\prime}}\\)\n\\(J+1\\)\n\\(l_{J+1}\\)\n\\(V_{{T}^{\\prime}1}\\)\n\\(V_{{T}^{\\prime}2}\\)\nsearch (last round)\nwhere \\(T^{\\prime} = T + k_{J}+ l_{J+1}\\). Note that there is no output in iteration \\(J + 1\\) since the optimiser does not find a better basis in the last iteration and terminates. The final basis found is \\(A_T\\) with index value \\(I_T\\).\nThe data structure constructed above meets the tidy data principle (Wickham 2014) that requires each observation to form a row and each variable to form a column. With tidy data structure, data wrangling and visualisation can be significantly simplified by well-developed packages such as dplyr (Wickham et al. 2020) and ggplot2 (Wickham 2016).\nDiagnostic 1: Checking how hard the optimiser is working\nA starting point of diagnosing an optimiser is to understand how many searches it has conducted, i.e., we want to summarise how the index is increasing over iterations and how many basis need to be sampled at each iteration. This is achieved using the function explore_trace_search(): a boxplot shows the distribution of index values for each try, where the accepted basis (corresponding to the highest index value) is always shown as a point. When there are only few tries at a given iteration, showing the data points directly is preferred over the boxplot and this is controlled via the cutoff argument. Additional annotations are added to facilitate better reading of the plot, and these include 1) the number of points searched in each iteration can be added as text label at the bottom of each iteration; 2) the anchor bases to interpolate are connected and highlighted in a larger size; 3) the colour of the last iteration is in greyscale to indicate no better basis found in this iteration.\nFigure 2 shows an example of the search plot for CRS (left) and SA (right). Both optimisers quickly find better bases in the first few iterations and then take longer to find one in the later iterations. The anchor bases, the ones found with the highest index value in each iteration, always have an increased index value in the optimiser CRS while this is not the case for SA. This feature gives CRS an advantage in this simple example to quickly find the optimum.\n\n\n\nFigure 2: A comparison of the searches by two optimisers: CRS (left) and SA (right) on a 2D projection problem of a six-variable dataset, using the holes index. Both optimisers reach the final basis with a similar index value, while it takes SA longer to find the final basis. In the earlier iterations, optimisers quickly find a better basis to proceed, while in the later iterations, most sampled bases fail to make an improvement on the index value, and a boxplot is used to summarise the distribution of the index values. There is no better basis found in the last iteration, 9 (left) and 15 (right), before reaching the maximum number of try and hence it is coloured grey. The colour scale is from the customised botanical palette in the ferrn package.\n\n\n\nDiagnostic 2: Examining the optimisation progress\nAnother interesting feature to examine is the changes in the index value between interpolating bases since the projection on these bases is shown in the tour animation. Trace plots are created by plotting the index value against time. Figure 3 presents the trace plot of the same optimisations as Figure 2, and one can observe that the trace is smooth in both cases. It may seem bizarre at first sight that the interpolation sometimes passes bases with higher index values before it decreases to a lower target. This happens because, on the one hand, the probabilistic acceptance in SA implies that some worse bases will be accepted by the optimiser. In addition, the guided tour interpolates between the current and target basis to provide a smooth transition between projections, and sometimes a higher index value will be observed along the interpolation path. This indicates that a non-monotonic interpolation cannot be avoided, even for CRS. Later, in Section A problem of non-monotonicity, there will be a discussion on improving the non-monotonic interpolation for CRS.\n\n\n\nFigure 3: An inspection of the index values as the optimisation progress for two optimisers: CRS (left) and SA (right). The holes index is optimised for a 2D projection problem on the six-variable dataset . Lines indicate the interpolation, and dots indicate new target bases generated by the optimisers. Interpolation in both optimisation is smooth, while SA is observed to first pass by some bases with higher index values before reaching the target bases in time 76-130.\n\n\n\nDiagnostic 3a: Understanding the optimiser’s coverage of the search space\nApart from checking the search and progression of an optimiser, looking at where the bases are positioned in the basis space is also of interest. Given the orthonormality constraint, the space of projection bases \\(\\mathbf{A}_{p \\times d}\\) is a Stiefel manifold. For one-dimensional projections, this forms a \\(p\\)-dimensional sphere. A dimensionality reduction method, e.g., principal component analysis, is applied to first project all the bases onto a 2D space. In a projection pursuit guided tour optimisation, there are various types of bases involved: 1) The starting basis; 2) The search bases that the optimiser evaluated to produce the anchor bases; 3) The anchor bases that have the highest index value in each iteration; 4) The interpolating bases on the interpolation path; and finally, 5) the end basis. The importance of these bases differs but the most important ones are the starting, interpolating, and end bases. Sometimes, two optimisers can start with the same basis but finish with bases of opposite signs. This happens because the projection is invariant to the orientation of the basis, and so is the index value. However, this creates difficulties for comparing the optimisers since the end bases will be symmetric to the origin. A sign flipping step is conducted to flip the signs of all the bases in one routine if different optimisations finish at opposite places.\nSeveral annotations have been made to help understand this plot. As mentioned previously, the original basis space is a high-dimensional sphere, and random bases on the sphere can be generated via the geozoo (Schloerke 2016) package. We use PCA to project and visualize the parameters/ bases in 2D. The centre of the 2D view is the first two PCs of the data matrix. It theoretically should be a circle but may have some irregular edges due to finite sampling. Thus the edge is smoothed by using a radius estimated as the largest distance from the centre to any basis. In the simulation, the theoretical best basis is known and can be labelled to compare how close to this that the optimisers stopped. Various aesthetics, i.e., size, alpha (transparency), and colour, are applicable to emphasize critical elements and adjust for the presentation. For example, anchor points and search points are less important, and hence a smaller size and alpha are used. Alpha can also be applied on the interpolation paths to show start to finish from transparent to opaque.\nFigure 4 shows the PCA plot of CRS and PD for a 1D projection problem. Both optimisers find the optimum, but PD gets closer. With the PCA plot, one can visually appreciate the nature of these two optimisers: PD first evaluates points in a small neighbourhood for a promising direction, while CRS evaluates points randomly in the search space to search for the next target. There are dashed lines annotated for CRS, and it describes the interruption of the interpolation, which will be discussed in detail in Section A problem of non-monotonicity.\n\n\n\nFigure 4: Search paths of CRS (green) and PD (brown) in the PCA-reduced basis space for 1D projection problem on the five-variable dataset, using holes index. The basis space, a 5D unit sphere, is projected onto a 2D circle by PCA. The black star represents the theoretical best basis the optimisers are aiming to find. All the bases in PD have been flipped for easier comparison of the final bases, and a grey dashed line has been annotated to indicate the symmetry of the two start bases.\n\n\n\nDiagnostic 3b: Animating the diagnostic plots\nAnimation is another type of display to show how the search progresses from start to finish in the space. Figure 5 shows the animated version (six frames from the animation if viewed in pdf) of the PCA plot in Figure 4. An additional piece of information one can learn from this animation is that CRS finds its end basis quicker than PD since CRS finishes its search in the 5th frame while PD is still making more progress.\n\n\n\n\n\n\nFigure 5: Animated version of Figure 4. With animation, the progression of the search paths from start to finish can better identified and CRS (green) finishes the optimisation quicker than PD (brown).\n\n\n\nDiagnostic 4a: The tour looking at itself\nAs mentioned previously, the original \\(p \\times d\\) dimension space can be simulated via randomly generated bases in the geozoo (Schloerke 2016) package. While the PCA plot projects the bases from the direction that maximises the variance, the tour plot displays the original high-dimensional space from various directions using animation. Figure 6 shows some frames from the tour plot of the same two optimisations in its original space.\n\n\n\n\n\n\nFigure 6: Animating the two paths from Figure 4 and 5 in the full basis space. The full basis space in this example is a 5D unit sphere, on which points (grey) are randomly generated via the CRAN package geozoo.\n\n\n\nDiagnostic 4b: Forming a torus\nWhile the previous few examples have looked at the space of 1D basis in a unit sphere, this section visualises the space of 2D basis. Recall that the columns in a 2D basis are orthogonal to each other, so the space of \\(p \\times 2\\) bases is a torus in the \\(p\\)-D space (Buja and Asimov 1986). For \\(p = 3\\) one would see a classical 3D torus shape as shown by the grey points in Figure 7. The two circles of the torus can be observed to be perpendicular to each other and this can be linked back to the orthogonality condition. Two paths from CRS and PD are plotted on top of the torus and coloured in green and brown, respectively, to match the previous plots. The final basis found by PD and CRS are shown in a larger shape and printed below, respectively:\n\n             [,1]        [,2]\n[1,]  0.001196285  0.03273881\n[2,] -0.242432715  0.96965761\n[3,] -0.970167484 -0.24226493\n\n\n            [,1]         [,2]\n[1,]  0.05707994 -0.007220138\n[2,] -0.40196202 -0.915510160\n[3,] -0.91387549  0.402230054\n\nBoth optimisers have found the third variable in the first direction and the second variable in the second direction. Note, however, the different orientation of the basis, following from the different sign in the second column. One would expect to see this in the torus plot as the final bases match each other when projected onto one torus circle (due to the same sign in the first column) and are symmetric when projected onto the other (due to the different sign in the second column). In Figure 7, this can be seen most clearly in frame 5 where the two circles are rotated into a line from our view.\n\n\n\n\n\n\nFigure 7: Animation of rotating the 2D basis space along with two search paths optimised by PD (brown) and CRS (green). The projection problem is a 2D projection with three variables using the holes index. The grey points are randomly generated 2D projection bases in the space and it can be observed that these points form a torus.\n\n\n\nDiagnosing an optimiser\nIn this section, several examples will be presented to show how the diagnostic plots discover something unexpected in projection pursuit optimisation, and guide the implementation of new features.\nSimulation setup\nRandom variables with different distributions have been simulated as follows:\n\\[\\begin{align}\nx_1 \\overset{d}{=} x_8 \\overset{d}{=} x_9 \\overset{d}{=} x_{10}& \\sim \\mathcal{N}(0, 1) \\\\\nx_2 &\\sim 0.5 \\mathcal{N}(-3, 1) + 0.5 \\mathcal{N}(3, 1)\\\\\n\\Pr(x_3) &=\n\\begin{cases}\n0.5 & \\text{if $x_3 = -1$ or $1$}\\\\\n0 & \\text{otherwise}\n\\end{cases}\\\\\nx_4 &\\sim 0.25 \\mathcal{N}(-3, 1) + 0.75 \\mathcal{N}(3, 1)\\\\\nx_5 &\\sim \\frac{1}{3} \\mathcal{N}(-5, 1) + \\frac{1}{3} \\mathcal{N}(0, 1) + \\frac{1}{3} \\mathcal{N}(5, 1)\\\\\nx_6 &\\sim 0.45 \\mathcal{N}(-5, 1) + 0.1 \\mathcal{N}(0, 1) + 0.45 \\mathcal{N}(5, 1)\\\\\nx_7 &\\sim 0.5 \\mathcal{N}(-5, 1) + 0.5 \\mathcal{N}(5, 1)\n\\end{align}\\]\nVariables x1, x8 to x10 are normal noise with zero mean, and unit variance and x2 to x7 are normal mixtures with varied weights and locations. All the variables have been scaled to have overall unit variance before projection pursuit. The holes index (Cook et al. 2008), used for detecting bimodality of the variables, is used throughout the examples unless otherwise specified.\nA problem of non-monotonicity\nAn example of non-monotonic interpolation has been given in Figure 3: a path that passes bases with a higher index value than the target one. For SA, a non-monotonic interpolation is justified since target bases do not necessarily have a higher index value than the current one, while this is not the case for CRS. The original trace plot for a 2D projection problem, optimised by CRS, is shown on the left panel of Figure 8, and one can observe that the non-monotonic interpolation has undermined the optimiser to realise its full potential. Hence, an interruption is implemented to stop at the best basis found in the interpolation. The right panel of Figure 8 shows the trace plot after implementing the interruption, and while the first two interpolations are identical, the basis at time 61 has a higher index value than the target in the third interpolation. Rather than starting the next iteration from the target basis on time 65, CRS starts the next iteration at time 61 on the right panel and reaches a better final basis.\n\n\n\nFigure 8: Comparison of the interpolation before and after implementing the interruption for the 2D projection problem on data using holes index, optimised by CRS. On the left panel, the basis with a higher index value is found during the interpolation but not used. On the right panel, the interruption stops the interpolation at the basis with the highest index value for each iteration and results in a final basis with a higher index value, as shown on the right panel.\n\n\n\nClose but not close enough\nOnce the final basis has been found by an optimiser, one may want to push further in the close neighbourhood to see if an even better basis can be found. A polish search takes the final basis of an optimiser as the start of a new guided tour to search for local improvements. The polish algorithm is similar to the CRS but with three distinctions: 1) a hundred rather than one candidate bases are generated each time in the inner loop; 2) the neighbourhood size is reduced in the inner loop, rather than in the outer loop; and 3) three more termination conditions have been added to ensure the new basis generated is distinguishable from the current one in terms of the distance in the space, the relative change in the index value, and neighbourhood size:\nthe distance between the basis found and the current needs to be larger than 1e-3;\nthe relative change of the index value needs to be larger than 1e-5; and\nthe alpha parameter needs to be larger than 0.01.\nFigure 9 presents the projected data and trace plot of a 2D projection, optimised by CRS and followed by the polish step. The top row shows the initial projection, the final projection after CRS, and the final projection after polish, respectively. The end basis found by CRS reveals the four clusters in the data, but the edges of each cluster are not clean-cut. Polish works with this end basis and further pushes the index value to produce clearer edges of the cluster, especially along the vertical axis.\n\n\n\nFigure 9: Comparison of the projected data before and after using polishing for a 2D projection problem on data using holes index. The top row shows the initial projected data and the final views after CRS and polish search, and the second row traces the index value. The clustering structure in the data is detected by CRS (top middle panel), but the polish step improves the index value and produces clearer boundaries of the clusters (top right panel), especially along the vertical axis. Note that the parameter is set to 400 in this experiment for CRS to do its best.\n\n\n\nSeeing the signal in the noise\nThe holes index function used for all the examples before this section produces a smooth interpolation, while this is not the case for all the indexes. An example of a noisy index function for 1D projections compares the projected data, \\(\\mathbf{Y}_{n \\times 1}\\), to a randomly generated normal distribution, \\(\\mathcal{N}_{n \\times 1}\\), using the Kolmogorov test. Let \\(F_{.}(n)\\) be the empirical cumulative distribution function (ECDF) with two possible subscripts, \\(Y\\) and \\(\\mathcal{N}\\), representing the projected and randomly generated data, and \\(n\\) denoting the number of observations, the Kolmogorov index \\(I^{nk}(n)\\), is defined as:\n\\[I^{K}(n) = \\max \\left[F_{Y}(n) - F_{\\mathcal{N}}(n)\\right].\\] With a non-smooth index function, two research questions are raised:\nwhether any optimiser fails to optimise this non-smooth index; and\nwhether the optimisers can find the global optimum despite the presence of a local optimum.\nFigure 10 presents the trace and PCA plots of all three optimisers, and as expected, none of the interpolated paths are smooth. There is barely any improvement made by PD, indicating its failure in optimising non-smooth index functions. While CRS and SA have managed to get close to the index value of the theoretical best, the trace plot shows that it takes SA much longer to find the final basis. This long interpolation path is partially due to the fluctuation in the early iterations, where SA tends to generously accept inferior bases before concentrating near the optimum. The PCA plot shows the interpolation path and search points, excluding the last termination iteration. Pseudo-Derivative (PD) quickly gets stuck near the starting position. Comparing the amount of random search done by CRS and SA, it is surprising that SA does not carry as many samples as CRS. Combining the insights from both the trace and PCA plot, one can learn the two different search strategies by CRS and SA: CRS frequently samples in the space and only make a move when an improvement is guaranteed to be made, while SA first broadly accepts bases in the space and then starts the extensive sampling in a narrowed subspace. The specific decision of which optimiser to use will depend on the index curve in the basis space, but if the basis space is non-smooth, accepting inferior bases at first, as SA has done here, can lead to a more efficient search in terms of the overall number of points evaluated.\n\n\n\nFigure 10: Comparison of the three optimisers in optimising \\(I^{nk}(n)\\) index for a 1D projection problem on a five-variable dataset, . Both CRS and SA succeed in the optimisation, PD fails to optimise this non-smooth index. Further, SA takes much longer than CRS to finish the optimisation, but finishes off closer to the theoretical best.\n\n\n\nThe next experiment compares the performance of CRS and SA when a local maximum exists. Two search neighbourhood sizes, 0.5 and 0.7, are compared to understand how a large search neighbourhood would affect the final basis and the length of the search. Figure 11 shows 80 paths simulated using 20 seeds in the PCA plot, faceted by the optimiser and search size. With CRS and a search size of 0.5, despite being the simplest and fastest, the optimiser fails in three instances where the final basis lands neither near the local nor the global optimum. With a larger search size of 0.7, more seeds have found the global maximum. Comparing CRS and SA for a search size of 0.5, SA does not seem to improve the final basis found, despite having longer interpolation paths. However, the denser paths near the local maximum are an indicator that SA is working hard to examine if there is any other optimum in the basis space, but the relatively small search size has diminished its ability to reach the global maximum. With a larger search size, almost all the seeds (16 out of 20) have found the global maximum, and some final bases are much closer to the theoretical best, as compared to the three other cases. This indicates that SA, with a reasonable large search window, is able to overcome the local optimum and optimise close towards the global optimum.\n\n\n\nFigure 11: Comparing 20 search paths in the PCA-projected basis space faceted by two optimisers: CRS and SA, and two search sizes: 0.5 and 0.7. The optimisation is on the 1D projection index, \\(I^{nk}(n)\\), for data, where a local optimum, annotated by the cross (x), is presented in this experiment, along with the global optimum (*).\n\n\n\nReconciling the orientation\nOne interesting situation observed in the previous examples is that, for some simulations, as shown on the left panel of Figure 12, the target basis is generated on the other half of the basis space, and the interpolator seems to draw a straight line to interpolate. Bases with opposite signs do not affect the projection and index value, but we would prefer the target to have the same orientation as the current basis. The orientation of two bases can be computationally checked by calculating the determinant – a negative value suggests the two bases have a different orientation. For 1D bases, this can be corrected by flipping the sign on one basis. For higher dimensions, it can be a bit more difficult because the orthonormality of the basis needs to be also maintained when an individual vector is flipped. Here, an orientation check is carried out once a new target basis is generated, and the sign in the target basis will be flipped if a negative determinant is obtained. The interpolation after implementing the orientation check is shown on the right panel of Figure 12, where the unsatisfactory interpolation no longer exists.\n\n\n\nFigure 12: Comparison of the interpolation in the PCA-projected basis space before and after reconciling the orientation of the target basis. Optimisation is on the 1D projection index, \\(I^{nk}(n)\\), for boa6 data using CRS with seed 2463. The dots represent the target basis in each iteration, and the path shows the interpolation. On the left panel, one target basis is generated with an opposite orientation to the current basis (hence appear on the other side of the basis space), and the interpolator crosses the origin to perform the interpolation. The right panel shows the same interpolation after implementing an orientation check, and the undesirable interpolation disappears.\n\n\n\nImplementation\nThis project contributes to the software development in two packages: a data collection object is implemented in tourr (Wickham et al. 2011), while the visual diagnostics of the optimisers is implemented in ferrn (Zhang et al. 2021). The functions in the ferrn (Zhang et al. 2021) package are listed below:\nMain plotting functions:\nexplore_trace_search() produces summary plots in Figure 2.\nexplore_trace_interp() produces trace plots for the interpolation points in Figure 3.\nexplore_space_pca() produces the PCA plot of projection bases on the reduced space. Figure 4 includes the additional details of anchor and search bases, which can be turned on by the argument details = TRUE. The animated version in Figure 5 is produced with argument animate = TRUE.\nexplore_space_tour() produces animated tour view on the full space of the projection bases in Figure 6.\n\nget_*() extracts and manipulates certain components from the existing data object.\nget_anchor() extracts target observations.\nget_basis_matrix() flattens all the bases into a matrix.\nget_best() extracts the observation with the highest index value in the data object.\nget_dir_search() extracts directional search observations for PD search.\nget_interp() extracts interpolated observations.\nget_interp_last() extracts the ending interpolated observations in each iteration.\nget_interrupt() extracts the ending interpolated observations and the target observations if the interpolation is .interrupted\nget_search() extracts search observations.\nget_search_count() extracts the count of search observations.\nget_space_param() produces the coordinates of the centre and radius of the basis space.\nget_start() extracts the starting observation.\nget_theo() extracts the theoretical best observations, if given.\n\nbind_*() incorporates additional information outside the tour optimisation into the data object.\nbind_theoretical() binds the theoretical best observation in simulated experiment.\nbind_random() binds randomly generated bases in the projection bases space to the data object.\nbind_random_matrix() binds randomly generated bases and outputs in a matrix format.\n\nadd_*() provides wrapper functions to create ggprotos for different components for the PCA plot\nadd_anchor() for plotting anchor bases.\nadd_anno() for annotating the symmetry of start bases.\nadd_dir_search() for plotting the directional search bases with magnified distance.\nadd_end() for plotting end bases.\nadd_interp() for plotting the interpolation path.\nadd_interp_last() for plotting the last interpolation bases for comparing with target bases when interruption is used.\nadd_interrupt() for linking the last interpolation bases with target ones when interruption is used.\nadd_search() for plotting search bases.\nadd_space() for plotting the circular space.\nadd_start() for plotting start bases.\nadd_theo() for plotting theoretical best bases, if applicable.\n\nUtilities\ntheme_fern() and format_label() for better display of the grid lines and axis formatting.\nclean_method() to clean up the name of the optimisers.\nbotanical_palettes() is a collection of colour palettes from Australian native plants. Quantitative palettes include daisy, banksia, and cherry, and sequential palettes contain fern and acacia.\nbotanical_pal() as the colour interpolator.\nscale_color_*() and scale_fill_*() for scaling the colour and fill of the plot.\n\nConclusion\nThis paper has provided several visual diagnostics that can be used for understanding a complex optimisation procedure and are implemented in the ferrn package. The methods were illustrated using the optimisers available for projection pursuit guided tour. Here the constraint is the orthonormality condition of the projection bases, which corresponds to optimisation over spheres and torii. The approach described broadly applies to other constrained optimisers. Although the manifold in \\(p\\)-space might be different the diagnostic techniques are the same. A researcher would begin by saving the path of the optimiser in a form required to input into the ferrn package, as described in this paper. One might generally make more samples from the constrained space upon which to assess and compare the optimisation paths. These high-dimensional data objects can then be viewed using the tour.\nThe progressive optimisation of a target function and its coverage of the search space can be viewed in both reduced 2D space and the full space. These visualisations can lead to insights for evaluating and comparing the performance of multiple optimisers operating on the same task. They can provide a better understanding of existing methods or motivate the development of new approaches. For example, we have compared how three optimisers perform when maximising a non-smooth index function and have illustrated how the pseudo-derivative search fails in this setting. The observations from our experiments have also been translated into improved optimisation methods for the guided tour, e.g., we introduced the option to interrupt the search if a better basis is found along the path.\nThis work might be considered an effort to bring transparency into algorithms. Although little attention is paid by algorithm developers to providing ways to output information during intermediate steps, this is an important component for enabling others to understand and diagnose the performance. Algorithms are an essential component of artificial intelligence that is used to make daily life easier. Interpretability of algorithms is important to guard against aspects like bias and inappropriate use. The data object underlying the visual diagnostics here is an example of what might be useful in algorithm development generally.\nAcknowledgements\nThis article is created using knitr (Xie 2015) and rmarkdown (Xie et al. 2018) in R. The source code for reproducing this paper can be found at: https://github.com/huizezhang-sherry/paper-ferrn.\n\nCRAN packages used\nferrn, tourr, dplyr, ggplot2, geozoo, knitr, rmarkdown\nCRAN Task Views implied by cited packages\nReproducibleResearch, Databases, ModelDeployment, Multivariate, Phylogenetics, TeachingStatistics\n\n\nS. Andradóttir. A review of random search methods. In Handbook of simulation optimization, pages. 277–292 2015. Springer. URL https://doi.org/10.1007/978-1-4939-1384-8.\n\n\nD. P. Bertsekas. Constrained optimization and lagrange multiplier methods. Academic press, 2014. URL https://doi.org/10.1016/C2013-0-10366-2.\n\n\nD. Bertsimas and J. Tsitsiklis. Simulated annealing. Statistical Science, 8(1): 10–15, 1993. URL https://doi.org/10.1214/ss/1177011077.\n\n\nS. H. Brooks. A discussion of random methods for seeking maxima. Operations Research, 6(2): 244–251, 1958. URL https://doi.org/10.1287/opre.6.2.244.\n\n\nA. Buja and D. Asimov. Grand tour methods: An outline. In Proceedings of the seventeenth symposium on the interface of computer sciences and statistics, pages. 63–67 1986. USA: Elsevier North-Holland, Inc. ISBN 9780444700186. URL https://dl.acm.org/doi/10.5555/26036.26046.\n\n\nA. Buja, D. Cook, D. Asimov and C. Hurley. Computational methods for high-dimensional rotations in data visualization. Handbook of Statistics, 24: 391–413, 2005. URL https://doi.org/10.1016/S0169-7161(04)24014-7.\n\n\nA. R. Conn, K. Scheinberg and L. N. Vicente. Introduction to derivative-free optimization. SIAM, 2009. URL https://doi.org/10.1137/1.9780898718768.\n\n\nD. Cook and A. Buja. Manual controls for high-dimensional data projections. Journal of Computational and Graphical Statistics, 6(4): 464–480, 1997. URL https://doi.org/10.2307/1390747.\n\n\nD. Cook, A. Buja and J. Cabrera. Projection pursuit indexes based on orthonormal function expansions. Journal of Computational and Graphical Statistics, 2(3): 225–250, 1993. URL https://doi.org/10.2307/1390644.\n\n\nD. Cook, A. Buja, J. Cabrera and C. Hurley. Grand tour and projection pursuit. Journal of Computational and Graphical Statistics, 4(3): 155–172, 1995. URL https://doi.org/10.1080/10618600.1995.10474674.\n\n\nD. Cook, A. Buja, E.-K. Lee and H. Wickham. Grand tours, projection pursuit guided tours, and manual controls. In Handbook of data visualization, pages. 295–314 2008. Springer. URL https://doi.org/10.1007/978-3-540-33037-0_13.\n\n\nP. Diaconis and D. Freedman. Asymptotics of graphical projection pursuit. The Annals of Statistics, 793–815, 1984. URL https://doi.org/10.1214/aos/1176346703.\n\n\nJ. H. Friedman and J. W. Tukey. A projection pursuit algorithm for exploratory data analysis. IEEE Transactions on Computers, 100(9): 881–890, 1974. URL https://doi.org/10.1109/T-C.1974.224051.\n\n\nV. Granville, M. Krivánek and J.-P. Rasson. Simulated annealing: A proof of convergence. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(6): 652–656, 1994. URL https://doi.org/10.1109/34.295910.\n\n\nP. Hall. On polynomial-based projection indices for exploratory projection pursuit. The Annals of Statistics, 17(2): 589–605, 1989. URL https://doi.org/10.1214/aos/1176347127.\n\n\nK. Healy. Data visualization: A practical introduction. Princeton University Press, 2018. URL https://socviz.co/.\n\n\nD. R. Jones, M. Schonlau and W. J. Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4): 455–492, 1998. URL https://doi.org/10.1023/A:1008306431147.\n\n\nS. Kirkpatrick, C. D. Gelatt and M. P. Vecchi. Optimization by simulated annealing. Science, 220(4598): 671–680, 1983. URL https://doi.org/10.1126/science.220.4598.671.\n\n\nU. Laa and D. Cook. Using tours to visually investigate properties of new projection pursuit indexes with application to problems in physics. Computational Statistics, 1–35, 2020. URL https://doi.org/10.1007/s00180-020-00954-8.\n\n\nE. Lee, D. Cook, S. Klinke and T. Lumley. Projection pursuit for exploratory supervised classification. Journal of Computational and Graphical Statistics, 14(4): 831–846, 2005. URL https://doi.org/10.1198/106186005X77702.\n\n\nE.-K. Lee and D. Cook. A projection pursuit index for large p small n data. Statistics and Computing, 20(3): 381–392, 2010. URL https://doi.org/10.1007/s11222-009-9131-1.\n\n\nM. Li, Z. Zhao and C. Scheidegger. Visualizing neural networks with the grand tour. Distill, 2020. URL https://doi.org/10.23915/distill.00025.\n\n\nN. Loperfido. Kurtosis-based projection pursuit for outlier detection in financial time series. The European Journal of Finance, 26(2-3): 142–164, 2020. URL https://doi.org/10.1080/1351847X.2019.1647864.\n\n\nN. Loperfido. Skewness-based projection pursuit: A computational approach. Computational Statistics and Data Analysis, 120(C): 42–57, 2018. URL https://doi.org/10.1016/j.csda.2017.11.001.\n\n\nD. Mitra, F. Romeo and A. Sangiovanni-Vincentelli. Convergence and finite-time behavior of simulated annealing. Advances in Applied Probability, 18(3): 747–771, 1986. URL https://doi.org/10.1109/CDC.1985.268600.\n\n\nC. Posse. Projection pursuit exploratory data analysis. Computational Statistics & Data Analysis, 20(6): 669–687, 1995. URL https://doi.org/10.1016/0167-9473(95)00002-8.\n\n\nL. M. Rios and N. V. Sahinidis. Derivative-free optimization: A review of algorithms and comparison of software implementations. Journal of Global Optimization, 56(3): 1247–1293, 2013. URL https://doi.org/10.1007/s10898-012-9951-y.\n\n\nH. E. Romeijn. Random search methods. In Encyclopedia of optimization, pages. 3245–3251 2009. Boston, MA: Springer US. URL https://doi.org/10.1007/978-0-387-74759-0_556.\n\n\nB. Schloerke. Geozoo: Zoo of geometric objects. 2016. URL https://CRAN.R-project.org/package=geozoo. R package version 0.5.1.\n\n\nJ. C. Spall. Introduction to stochastic search and optimization: Estimation, simulation, and control. John Wiley & Sons, 2005. URL https://www.jhuapl.edu/ISSO/.\n\n\nJ. W. Tukey. Exploratory data analysis. Reading, MA, 1977.\n\n\nA. Unwin. Graphical data analysis with r. CRC Press, 2015. URL https://doi.org/10.1201/9781315370088.\n\n\nR. C. White. A survey of random methods for parameter optimization. Simulation, 17(5): 197–205, 1971. URL https://doi.org/10.1177/003754977101700504.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016. URL https://doi.org/10.1007/978-0-387-98141-3.\n\n\nH. Wickham. Tidy data. Journal of Statistical Software, 59(10): 1–23, 2014. URL https://doi.org/10.18637/jss.v059.i10.\n\n\nH. Wickham, D. Cook, H. Hofmann and A. Buja. tourr: An R package for exploring multivariate data with projections. Journal of Statistical Software, 40(2): 1–18, 2011. URL http://doi.org/10.18637/jss.v040.i02.\n\n\nH. Wickham, R. François, L. Henry and K. Müller. Dplyr: A grammar of data manipulation. 2020. URL https://CRAN.R-project.org/package=dplyr. R package version 1.0.2.\n\n\nC. O. Wilke. Fundamentals of data visualization: A primer on making informative and compelling figures. O’Reilly Media, 2019. URL https://clauswilke.com/dataviz/.\n\n\nL. Wilkinson, A. Anand and R. Grossman. Graph-theoretic scagnostics. In IEEE symposium on information visualization, 2005. INFOVIS 2005., pages. 157–164 2005.\n\n\nL. Wilkinson and G. Wills. Scagnostics distributions. Journal of Computational and Graphical Statistics, 17(2): 473–491, 2008.\n\n\nY. Xie. Dynamic documents with R and knitr. 2nd ed Boca Raton, Florida: Chapman; Hall/CRC, 2015. URL https://yihui.name/knitr/. ISBN 978-1498716963.\n\n\nY. Xie, J. J. Allaire and G. Grolemund. R markdown: The definitive guide. Boca Raton, Florida: Chapman; Hall/CRC, 2018. URL https://bookdown.org/yihui/rmarkdown. ISBN 978-1138359338.\n\n\nZ. B. Zabinsky. Stochastic adaptive search for global optimization. Springer Science & Business Media, 2013. URL https://doi.org/10.1007/978-1-4419-9182-9.\n\n\nH. S. Zhang, D. Cook, U. Laa, N. Langrené and P. Menéndez. Ferrn: Facilitate exploration of touRR optimisatioN. 2021. URL https://github.com/huizezhang-sherry/ferrn/. R package version 0.0.1.\n\n\n\n\n",
    "preview": "articles/RJ-2021-105/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 720,
    "preview_height": 360
  },
  {
    "path": "articles/RJ-2021-107/",
    "title": "RobustBF: An R Package for Robust Solution to the Behrens-Fisher Problem",
    "description": "Welch’s two-sample t-test based on least squares (LS) estimators is generally used to test the equality of two normal means when the variances are not equal. However, this test loses its power when the underlying distribution is not normal. In this paper, two different tests are proposed to test the equality of two long-tailed symmetric (LTS) means under heterogeneous variances. Adaptive modified maximum likelihood (AMML) estimators are used in developing the proposed tests since they are highly efficient under LTS distribution. An R package called RobustBF is given to show the implementation of these tests. Simulated Type I error rates and powers of the proposed tests are also given and compared with Welch’s t-test based on LS estimators via an extensive Monte Carlo simulation study.",
    "author": [
      {
        "name": "Gamze Güven",
        "url": {}
      },
      {
        "name": "Şükrü Acıtaş",
        "url": {}
      },
      {
        "name": "Hatice Şamkar",
        "url": {}
      },
      {
        "name": "Birdal Şenoğlu",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRobustBF, asht, WRS2\nCRAN Task Views implied by cited packages\nRobust\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-108/",
    "title": "Software Engineering and R Programming: A Call for Research",
    "description": "Although R programming has been a part of research since its origins in the 1990s, few studies address scientific software development from a Software Engineering (SE) perspective. The past few years have seen unparalleled growth in the R community, and it is time to push the boundaries of SE research and R programming forwards. This paper discusses relevant studies that close this gap Additionally, it proposes a set of good practices derived from those findings aiming to act as a call-to-arms for both the R and RSE (Research SE) community to explore specific, interdisciplinary paths of research.",
    "author": [
      {
        "name": "Melina Vidoni",
        "url": {}
      }
    ],
    "date": "2021-12-14",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngenthat, roxygen2, pkgdown, covr, testthat, tidyverse\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-109/",
    "title": "We Need Trustworthy R Packages",
    "description": "There is a need for rigorous software engineering in R packages, and there is a need for new research to bridge scientific computing with more traditional computing. Automated tools, interdisciplinary graduate courses, code reviews, and a welcoming developer community will continue to democratize best practices. Democratized software engineering will improve the quality, correctness, and integrity of scientific software, and by extension, the disciplines that rely on it.",
    "author": [
      {
        "name": "William Michael Landau",
        "url": {}
      }
    ],
    "date": "2021-12-14",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntestthat, covr, R6\nCRAN Task Views implied by cited packages\nDatabases\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-110/",
    "title": "The R Developer Community Does Have a Strong Software Engineering Culture",
    "description": "There is a strong software engineering culture in the R developer community. We recommend creating, updating and vetting packages as well as keeping up with community standards. We invite contributions to the rOpenSci project, where participants can gain experience that will shape their work and that of their peers.",
    "author": [
      {
        "name": "Maëlle Salmon",
        "url": "https://masalmon.eu"
      },
      {
        "name": "Karthik Ram",
        "url": "https://ram.berkeley.edu/"
      }
    ],
    "date": "2021-12-14",
    "categories": [],
    "contents": "\nIntroduction\nThe R programming language was originally created for statisticians, by statisticians, but evolved over time to attract a “massive pool of talent that was previously untapped” (Hadley Wickham in Thieme (2018)).\nDespite the fact that most R users are academic researchers and business data analysts without a background in software engineering, we are witnessing a rapid rise in software engineering within the community.\nIn this comment we spotlight recent progress in tooling, dissemination and support, including specific efforts led by the rOpenSci project.\nWe hope that readers will take advantage of and participate in the tools and practices we describe.\nThe modern R package developer toolbox: user-friendlier, more comprehensive\nThe basic infrastructure for creating, building, installing, and checking packages has been in place since the early days of the R language. During this time (1998-2011), the barriers to entry were very high and access to support and Q&A for beginners were extremely limited. With the introduction of the devtools (Wickham et al. 2021b) package in 2011, the process of creating and updating packages became substantially easier. Documentation also became simpler to maintain. The roxygen2 (Wickham et al. 2021a) package allowed developers to keep documentation in sync with changes in code, similar to the doxygen approach that was embraced in more mature languages. Combined with the rise in popularity of StackOverflow and the growth of rstats blogs, the number of packages on the Comprehensive R Archive Network (CRAN) skyrocketed from 400 new packages in 2010 to 1000 new packages by 2014. As of this writing, there are nearly 19k packages on CRAN.\nFor novices without substantial software engineer experience, the early testing frameworks were also difficult to use. With the release of testthat (Wickham 2011), testing also became smoother.\nThere are now several actively maintained testing frameworks such as tinytest (van der Loo 2020); as well as testthat-compatible specialized tooling for testing database interactions (dittodb (Keane and Vargas 2020)), web resources (vcr (Chamberlain 2021)), httptest (Richardson 2021), and webfakes (Csárdi 2021) which enables the use of an embedded C/C++ web server for testing HTTP clients like httr2 (Wickham 2021)).\nThe testthat package has recently been improved with snapshot tests that make it possible to test plot outputs.\nThe rOpenSci project has released autotest (Padgham 2021), a package that supports automatic mutation testing.\nBeyond checking for compliance with R CMD CHECK, several other packages such as goodpractice (Csárdi and Frick 2018), riskmetric (R Validation Hub et al. 2021), rOpenSci’s pkgcheck (Padgham and Salmon 2021) check packages against a large list of actionable, community recommended best practices for software development. Collectively these tools allow domain researchers to release software packages that meet high standards for software engineering.\nThe development and testing ecosystem of R is rich and has sometimes borrowed successful implementations from other languages (e.g. the vcr R package is a port, i.e. translation to R, of the vcr Ruby gem; testthat snapshot tests were inspired by JS Jest1).\nEmergence of a welcoming community\nAs underlined in Thieme (2018), community is the strong suit of the R language. Many organizations and venues offer dedicated support for package developers.\nExamples include Q&A on the r-package-devel mailing list2, and the package development category of the RStudio community forum3, and the rstats section of StackOverflow4. Traditionally, R package developers have been mostly male and white. Although the status quo remains similar, efforts from groups such as R-Ladies5 meetups, Minorities in R (Scott and Smalls-Perkins 2020), and the package development modules offered by Forwards for underrepresented groups6 have made considerable inroads towards improving diversity. These efforts have\nworked hard to put the spotlight on developers beyond the “usual suspects”.\nrOpenSci community and software review\nThe rOpenSci organization (Boettiger et al. 2015) is an attractive venue for developers & supporters of scientific R software. One of our most successful and continuing initiatives is our Software Peer Review system (Ram et al. 2019), a combination of academic peer-review and code review from industry.\nAbout 150 packages have been reviewed by volunteers to date, creating better packages as well as a growing knowledgebase in our development guide (rOpenSci et al. 2021) while also building a living community of practice.\nOur model has been the fundamental inspiration for projects such as the Journal of Open Source Software (Smith et al. 2018), and PyOpenSci [Wasser and Holdgraf (2019)](Trizna et al. 2021).\nWe are continuously improving our system and reducing cognitive overload on editors and reviewers by automating repetitive tasks. Most recently we have expanded our offerings to peer review of packages that implement statistical methods (Statistical Software Peer Review) (Padgham et al. 2021).\nBeside software review, rOpenSci community is a safe, welcoming and informative place for package developers, with Q&A happening on our public forum and semi-open Slack workspace. (Butland and LaZerte 2020)\nCreation and dissemination of resources for R programmers\nThe aforementioned tools, venues and organizations benefit from and support crucial dissemination efforts.\nPublishing technical know-how is crucial for progress of the R community. R news has been circulating on Twitter7, R Weekly8 and R-Bloggers9.\nSome sources have been more specifically aimed at R package developers of various experience and interests.\nWhile “Writing R Extensions” 10 is the official & exhaustive reference on writing R packages, it is a reference rather than a learning resource: many R package developers, if not learning by example, get introduced to R package development via introductory blog posts or tutorials, and the R packages book by Hadley Wickham and Jenny Bryan [Wickham (2015)](Wickham and Bryan) that accompany the devtools suite of packages is freely available online and strives to improving the R package development experience.\nThe rOpenSci guide “rOpenSci Packages: Development, Maintenance, and Peer Review” (rOpenSci et al. 2021) contains our community-contributed guidance on how to develop packages and review them.\nIt features opinionated requirements such as the use of roxygen2 (Wickham et al. 2021a) for package documentation; criteria helping make an informed decision on gray area topics such as limiting dependencies; advice on widely accepted and emerging tools.\nAs it is a living document also used as reference for editorial decisions, we maintain a changelog11, and summarize each release in a blog post12.\nrOpenSci also hosts a book on a specialized topic, HTTP testing in R13, that presents both principles for testing packages that interact with web resources, as well as relevant packages.\nBeside these examples of long-form documentation, knowledge around R software engineering is shared through blogs and talks.\nIn the R blogging world, the rOpenSci blog posts14, technical notes15 and a section of our monthly newsletter16 feature some topics relevant to package developers, as do some of the posts on the Tidyverse blog17.\nThe blog of the R-hub project18 contains information on package development topics, in particular about common problems such as sharing data via R packages or understanding CRAN checks.\nExpert programmers have been sharing their R specific wisdom as well as software engineering lessons learned from other languages (e.g. Jenny Bryan’s useR! Keynote address “code feels, code smells”19).\nConclusion\nIn summary, we observe that there is already a strong software engineering culture in the R developer community. By surfacing the rich suite of resources to new developers we can but only hope the future will bring success to all aforementioned initiatives.\nWe recommend creating, updating and vetting packages with the tools we mentioned as well as keeping up with community standards with the venues we mentioned in the previous section.\nWe invite contributions to the rOpenSci project, where participants can gain experience that will shape their work and that of their peers.\nThanks to these efforts, we hope the R community will continue to be a thriving place of application for software engineering, by diverse practitioners from many different paths.\n\nCRAN packages used\ndevtools, roxygen2, testthat, tinytest, dittodb, vcr, httptest, webfakes, httr2, autotest, goodpractice, riskmetric, pkgcheck\nCRAN Task Views implied by cited packages\nWebTechnologies, Databases\n\n\nC. Boettiger, S. Chamberlain, E. Hart and K. Ram. Building software, building community: Lessons from the rOpenSci project. Journal of Open Research Software, 3(1): e8, 2015. DOI 10.5334/jors.bu.\n\n\nS. Butland and S. LaZerte. rOpenSci community contributing guide. Zenodo, 2020. URL https://contributing.ropensci.org/.\n\n\nS. Chamberlain. Vcr: Record ’HTTP’ calls to disk. 2021. URL https://CRAN.R-project.org/package=vcr. R package version 1.0.2.\n\n\nG. Csárdi. Webfakes: Fake web apps for HTTP testing. 2021. https://webfakes.r-lib.org/, https://github.com/r-lib/webfakes.\n\n\nG. Csárdi and H. Frick. Goodpractice: Advice on r package building. 2018. URL https://CRAN.R-project.org/package=goodpractice. R package version 1.0.2.\n\n\nJ. Keane and M. Vargas. Dittodb: A test environment for database requests. 2020. URL https://CRAN.R-project.org/package=dittodb. R package version 0.1.3.\n\n\nM. Padgham. Autotest: Automatic package testing. 2021. https://docs.ropensci.org/autotest/, https://github.com/ropensci-review-tools/autotest.\n\n\nM. Padgham and M. Salmon. Pkgcheck: rOpenSci package checks. 2021. https://docs.ropensci.org/pkgcheck/, https://github.com/ropensci-review-tools/pkgcheck.\n\n\nM. Padgham, M. Salmon, N. Ross, J. Nowosad, R. FitzJohn, yilong zhang, C. Sax, F. Rodriguez-Sanchez, F. Briatte and L. Collado-Torres. ropensci/statistical-software-review-book: Official first standards versions. Zenodo, 2021. URL https://doi.org/10.5281/zenodo.5556756.\n\n\nR Validation Hub, D. Kelkhoff, M. Gotti, E. Miller, K. K, Y. Zhang, E. Milliman and J. Manitz. Riskmetric: Risk metrics to evaluating r packages. 2021. https://pharmar.github.io/riskmetric/, https://github.com/pharmaR/riskmetric.\n\n\nK. Ram, C. Boettiger, S. Chamberlain, N. Ross, M. Salmon and S. Butland. A community of practice around peer review for long-term research software sustainability. Computing in Science Engineering, 21(2): 59–65, 2019. DOI 10.1109/MCSE.2018.2882753.\n\n\nN. Richardson. Httptest: A test environment for HTTP requests. 2021. https://enpiar.com/r/httptest/, https://github.com/nealrichardson/httptest.\n\n\nrOpenSci, B. Anderson, S. Chamberlain, L. DeCicco, J. Gustavsen, A. Krystalli, M. Lepore, L. Mullen, K. Ram, N. Ross, et al. rOpenSci Packages: Development, Maintenance, and Peer Review. Zenodo, 2021. URL https://doi.org/10.5281/zenodo.4554776.\n\n\nD. Scott and D. Smalls-Perkins. Introducing MiR: A community for underrepresented minority users of r. Medium, 2020. URL https://medium.com/@doritolay/introducing-mir-a-community-for-underrepresented-users-of-r-7560def7d861.\n\n\nA. M. Smith, K. E. Niemeyer, D. S. Katz, L. A. Barba, G. Githinji, M. Gymrek, K. D. Huff, C. R. Madan, A. C. Mayes, K. M. Moerman, et al. Journal of open source software (JOSS): Design and first-year review. PeerJ Computer Science, 4: e147, 2018. URL https://doi.org/10.7717/peerj-cs.147.\n\n\nN. Thieme. R generation. Significance, 15(4): 14–19, 2018. URL https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2018.01169.x.\n\n\nM. Trizna, L. A. Wasser and D. Nicholson. pyOpenSci: Open and reproducible research, powered by python. Biodiversity Information Science and Standards, 5: e75688, 2021. URL https://doi.org/10.3897/biss.5.75688.\n\n\nM. van der Loo. A method for deriving information from running r code. The R Journal, Accepted for publication, 2020. URL https://arxiv.org/abs/2002.07472.\n\n\nL. A. Wasser and C. Holdgraf. pyOpenSci Promoting Open Source Python Software To Support Open Reproducible Science. In AGU fall meeting abstracts, pages. NS21A–13 2019.\n\n\nH. Wickham. httr2: Perform HTTP requests and process the responses. 2021. URL https://CRAN.R-project.org/package=httr2. R package version 0.1.1.\n\n\nH. Wickham. R packages. O’Reilly Media, 2015.\n\n\nH. Wickham. Testthat: Get started with testing. The R Journal, 3: 5–10, 2011. URL https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.\n\n\nH. Wickham and J. Bryan. R packages.URL https://r-pkgs.org/.\n\n\nH. Wickham, P. Danenberg, G. Csárdi and M. Eugster. roxygen2: In-line documentation for r. 2021a. URL https://CRAN.R-project.org/package=roxygen2. R package version 7.1.2.\n\n\nH. Wickham, J. Hester and W. Chang. Devtools: Tools to make developing r packages easier. 2021b. URL https://CRAN.R-project.org/package=devtools. R package version 2.4.2.\n\n\nhttps://www.tidyverse.org/blog/2020/10/testthat-3-0-0/#snapshot-testing↩︎\nhttps://stat.ethz.ch/mailman/listinfo/r-package-devel↩︎\nhttps://community.rstudio.com/c/package-development/11↩︎\nhttps://stackoverflow.com/questions/tagged/r?tab=Newest↩︎\nhttp://rladies.org/↩︎\nhttps://buzzrbeeline.blog/2021/02/09/r-forwards-package-development-modules-for-women-and-other-underrepresented-groups/↩︎\nhttps://www.t4rstats.com/↩︎\nhttps://rweekly.org/↩︎\nhttps://www.r-bloggers.com/↩︎\nhttps://cran.r-project.org/doc/manuals/R-exts.html↩︎\nhttps://devguide.ropensci.org/booknews.html↩︎\nhttps://ropensci.org/tags/dev-guide/↩︎\nhttps://books.ropensci.org/http-testing/↩︎\nhttps://ropensci.org/blog/↩︎\nhttps://ropensci.org/technotes/↩︎\nhttps://ropensci.org/news/↩︎\nhttps://www.tidyverse.org/categories/programming/↩︎\nhttps://blog.r-hub.io/post/↩︎\nhttps://github.com/jennybc/code-smells-and-feels↩︎\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-111/",
    "title": "The R Quest: from Users to Developers",
    "description": "R is not a programming language, and this produces the inherent dichotomy between analytics and software engineering. With the emergence of data science, the opportunity exists to bridge this gap, especially through teaching practices.",
    "author": [
      {
        "name": "Simon Urbanek",
        "url": {}
      }
    ],
    "date": "2021-12-14",
    "categories": [],
    "contents": "\nGenesis: How did we get here?\nThe article “Software Engineering and R Programming: A Call to Action” summarizes the dichotomy between analytics and software engineering in the R ecosystem, provides examples where this leads to problems and proposes what we as R users can do to bridge the gap.\nData Analytic Language\nThe fundamental basis of the dichotomy is inherent in the evolution of S and R: they are not programming languages, but they ended up being mistaken for such. S was designed to be a data analytic language: to turn ideas into software quickly and faithfully, often used in “non-programming” style (Chambers 1998). Its original goal was to enable the statisticians to apply code which was written in programming languages (at the time mostly FORTRAN) to analyze data quickly and interactively - for some suitable definition of “interactive” at the time (Becker 1994). The success of S and then R can be traced to the ability to perform data analysis by applying existing tools to data in creative ways. A data analysis is a quest - at every step we learn more about the data which informs our decision about next steps. Whether it is an exploratory data analysis leveraging graphics or computing statistics or fitting models - the final goal is typically not known ahead of time, it is obtained by an iterative process of applying tools that we as analysts think may lead us further (Tukey 1977). It is important to note that this is exactly the opposite of software engineering where there is a well-defined goal: a specification or desired outcome, which simply needs to be expressed in a way understandable to the computer.\nFreedom for All\nThe second important design aspect rooted in the creativity required is the freedom the language provides. Given that the language can be computed upon means that a given expression may have different meaning depending on how the called function decides to treat it and such deviations are not entirely uncommon, typically referred to as non-standard evaluation. Probably the best example is the sub-language defined by the package (Dowle and Srinivasan 2021) featuring the := operator which is parsed, but not even used by the R language.\nAnalogously, there is no specific, prescribed object system, but rather one is free to implement any idea desirable, as witnessed by the fact that there are more than a handful of object system definitions available in R and contributed packages. This freedom is what makes R great for experimentation with new ideas or concepts, but very hard to treat as a programming language.\nWe have a language that is built on the idea of applying tools and which allows freedom to express new ideas so the last important step is how to define new tools. R add-on packages (R Core Team 2021) are the vehicle by which new tools can be defined and distributed to R users. Note that true to design goals, packages are not limited to R code but rather can also include code written in programming languages such as C, C++ or Fortran. That in turn makes it possible to write packages that expand the scope of tools to other languages such as Java with RJava (Urbanek 2021) or Python with reticulate (Ushey et al. 2022) simply by creating an R package which defines the interface.\nSharing Packages\nBut this is also where we are entering the realm of software engineering. Now we are in the business of defining the tools as opposed to just using the tools. It also means that the tools have to worry about programming interfaces, defining behavior and all those pesky things we as statisticians don’t want to worry about. Although we originally started as R users, the moment we want to share any re-usable piece of code with others we are becoming developers. Since no developer would mistake R for a programming language, it is analysts with background in various fields which use statistics one way or another that are more likely to use R. However, as we become more comfortable with R, we start using it as a programming language, not just analytic language, often because it is simply more convenient than having to learn a programming language. This explains the empirical evidence (Pinto et al. 2018) of R package authors not being trained software engineers, but often scientists from other fields and any consequences thereof.\nHowever, as R packages started to emerge, it became clear that a loosely coupled structure is not enough and have to introduce software engineering concepts such as documentation and testing. R includes tools for automated checking for packages to be able to provide at least some basic guarantees. Packages provide examples which are supposed to be illustrative, but soon were used to perform limited testing. R itself is using the same package structure and it was clear early that a test suite is important and so was introduced. Consequently, the same facilities were available to packages, but only very few were using it. There are, however, no built-in tools for creating test suites. In core R those are hand-curated by experienced developers, but that does not scale to package space.\nOver 18,000 packages are now present in the Comprehensive R Archive Network (CRAN), a repository which has arguably played major role in the success of R (Hornik and Leisch 2002). This is not only a valuable resource for users, but today this rich collection of contributed R code in being used as an automated test-suite for R. This is no coincidence, the importance of software engineering concepts has been identified by the CRAN team long time ago and the tools in R have been enhanced for that purpose (Hornik 2016). CRAN has been an invaluable asset for the development of R based on examples and limited tests alone. It allows us the R Core Team to test changes in R against code that was written by ingenious people that do not necessarily follow documentation, but instead write code that seems to work - possibly in ways not intended in the first place. Consequently, improving the quality and coverage of tests in packages has not only positive impact on the individual package, but on the quality of the entire CRAN ecosystem and R itself.\nCRAN performs reverse-dependency checks where packages are not allowed to break dependent package which is an important software engineering concept. One can see CRAN as performing continuous integration and continuous testing if we consider all submitted packages as one big project. This is not universally liked among package authors, though. Some find it too tedious to be responsible for software in the way a software engineer would be - a concern which is also highlighted by the article.\nSteal and Borrow\nOne perhaps surprising finding of the article was the analysis of code fragment re-use (Claes et al. 2015). A quite recent example how dangerous such practice is was a piece of badly written JavaScript code from Stack Overflow (StackOverflow) which was copied so often that it made it into the popular Unity game engine, effectively forcing browsers to lie about macOS versions (Chromium Bugs) just to not break millions of released products. R code fragments are less likely to have such world-wide impact, but can be equally frustrating. The historically relatively high cost of loading other packages was an incentive to simply copy fragments instead, but the performance impact has been diminishing with advancements in the R implementation. Still, I believe the exact reasons for fragment re-use deserve further examination and may reveal other, more benign motives.\nEvery Project Needs a Conductor\nAnother good example of introducing software engineering principles into the R world successfully is the Bioconductor project (Gentleman et al. 2004). The authors realized early that the project is too big for it to allow organic growth and have strongly encouraged the use of the S4 class system to build a class hierarchy specific to the tasks common to the Bioconductor packages. This enabled optimizations of implementation as a core part of the system as opposed to individual approaches in each package. Bioconductor was also encouraging unit tests and has maintained a build and reporting system similar to that of CRAN, in the early days even pioneering functionality that was later added to core R.\nThe Gospel of Data Science\nI believe the Call to Action is a very timely contribution. Many R users start as statisticians or data analysts in some domain since that is the main strength of R. Consequently, a lot of R code is never publicly visible. Code written for data analyses is not software development and is not published as software. So any global statistics about R code have to be taken with that in mind. When considering R packages we are talking only about a fraction of the code written in R. However, building new tools is an important part of the R ecosystem and it has to be made clear that it is different from data analysis and thus requires different skills and tools.\nThe main realization here is that at some point an R user may become an R developer, crossing the line from analysis into software engineering. And we are often unprepared for that, in part because of our diverse background. When I asked my junior colleagues at the Labs what they find most challenging yet valuable, the top item was learning software engineering skills on the job. We were lucky to have both the authors of S as well as the authors of Unix on the same floor, so we were able to bridge the gap, but generally our schools don’t prepare for that. That’s why I believe we must teach statistical computing together with software engineering skills such as re-usability and testing concepts. The current popularity of data science which bridges both worlds is a good excuse to make it actually happen in practice.\n\nCRAN packages used\ndata.table, RJava, reticulate\nCRAN Task Views implied by cited packages\nFinance, HighPerformanceComputing, ModelDeployment, NumericalMathematics, TimeSeries\n\n\nR. A. Becker. A brief history of S. AT&T Bell Laboratories. 1994.\n\n\nJ. M. Chambers. Programming with data: A guide to the s language. 1st ed Berlin, Heidelberg: Springer-Verlag, 1998.\n\n\nChromium Bugs. Nearly all Unity WebGL games fail to run in Chrome on macOS 11 because of userAgent.,URL https://bugs.chromium.org/p/chromium/issues/detail?id=1171998.\n\n\nM. Claes, T. Mens, N. Tabout and P. Grosjean. An empirical study of identical function clones in CRAN. In 2015 IEEE 9th International Workshop on Software Clones (IWSC), pages. 19–25 2015. DOI 10.1109/IWSC.2015.7069885.\n\n\nM. Dowle and A. Srinivasan. Data.table: Extension of ‘data.frame‘. 2021. URL https://CRAN.R-project.org/package=data.table. R package version 1.14.2.\n\n\nR. C. Gentleman, V. J. Carey, D. M. Bates, B. Bolstad, M. Dettling, S. Dudoit, B. Ellis, L. Gautier, Y. Ge, J. Gentry, et al. Bioconductor: Open software development for computational biology and bioinformatics. Genome Biology, 5(10): R80, 2004. URL https://doi.org/10.1186/gb-2004-5-10-r80.\n\n\nK. Hornik. Are there too many R packages? Austrian Journal of Statistics, 41(1): 59–66, 2016. DOI 10.17713/ajs.v41i1.188.\n\n\nK. Hornik and F. Leisch. Vienna and R: Love, marriage and the future. Festschrift 50 Jahre Österreichische Statistische Gesellschaft, 61–70, 2002.\n\n\nG. Pinto, I. Wiese and L. F. Dias. How do scientists develop scientific software? An external replication. In 2018 IEEE 25th international conference on software analysis, evolution and reengineering (SANER), pages. 582–591 2018. DOI 10.1109/SANER.2018.8330263.\n\n\nR Core Team. Writing R extensions. Vienna, Austria: R Foundation for Statistical Computing, 2021. URL https://cran.r-project.org/doc/manuals/r-release/R-exts.html.\n\n\nStackOverflow. How to find the operating system details using JavaScript.,URL https://stackoverflow.com/questions/9514179/how-to-find-the-operating-system-details-using-javascript.\n\n\nJ. W. Tukey. Exploratory data analysis. Addison-Wesley Publishing Company, 1977.\n\n\nS. Urbanek. rJava: Low-level R to Java interface. 2021. URL https://CRAN.R-project.org/package=rJava. R package version 1.0-6.\n\n\nK. Ushey, J. Allaire and Y. Tang. Reticulate: Interface to Python. 2022. URL https://CRAN.R-project.org/package=reticulate. R package version 1.23.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-112/",
    "title": "Rejoinder: Software Engineering and R Programming",
    "description": "It is a pleasure to take part in such fruitful discussion about the relationship between Software Engineering and R programming, and what could be gain by allowing each to look more closely at the other. Several discussants make valuable arguments that ought to be further discussed.",
    "author": [
      {
        "name": "Melina Vidoni",
        "url": {}
      }
    ],
    "date": "2021-12-14",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:58+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-096/",
    "title": "msae: An R Package of Multivariate Fay-Herriot Models for Small Area Estimation",
    "description": "The paper introduces an R Package of multivariate Fay-Herriot models for small area estimation named msae. This package implements four types of Fay-Herriot models, including univariate Fay-Herriot model (model 0), multivariate Fay-Herriot model (model 1), autoregressive multivariate Fay-Herriot model (model 2), and heteroskedastic autoregressive multivariate Fay-Herriot model (model 3). It also contains some datasets generated based on multivariate Fay-Herriot models. We describe and implement functions through various practical examples. Multivariate Fay-Herriot models produce a more efficient parameter estimation than direct estimation and univariate model.",
    "author": [
      {
        "name": "Novia Permatasari",
        "url": {}
      },
      {
        "name": "Azka Ubaidillah",
        "url": {}
      }
    ],
    "date": "2021-11-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsae, rsae, nlme, hbsae, JoSAE, BayesSAE, mme, saery, msae\nCRAN Task Views implied by cited packages\nOfficialStatistics, Bayesian, ChemPhys, Econometrics, Environmetrics, Finance, Psychometrics, SocialSciences, Spatial, SpatioTemporal\n\n\n",
    "preview": "articles/RJ-2021-096/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 1153,
    "preview_height": 379
  },
  {
    "path": "articles/RJ-2021-087/",
    "title": "NGSSEML: Non-Gaussian State Space with Exact Marginal Likelihood",
    "description": "The number of packages/software for Gaussian State Space models has increased over recent decades. However, there are very few codes available for non-Gaussian State Space (NGSS) models due to analytical intractability that prevents exact calculations. One of the few tractable exceptions is the family of NGSS with exact marginal likelihood, named NGSSEML. In this work, we present the wide range of data formats and distributions handled by NGSSEML and a package in the R language to perform classical and Bayesian inference for them. Special functions for filtering, forecasting, and smoothing procedures and the exact calculation of the marginal likelihood function are provided. The methods implemented in the package are illustrated for count and volatility time series and some reliability/survival models, showing that the codes are easy to handle. Therefore, the NGSSEML family emerges as a simple and interesting option/alternative for modeling non-Gaussian time-varying structures commonly encountered in time series and reliability/survival studies. Keywords: Bayesian, classical inference, reliability, smoothing, time series, software R",
    "author": [
      {
        "name": "Thiago R. Santos",
        "url": {}
      },
      {
        "name": "Glaura C. Franco",
        "url": {}
      },
      {
        "name": "Dani Gamerman",
        "url": {}
      }
    ],
    "date": "2021-10-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nStructTS, dlm, dlmodeler, SSsimple, MARSS, sspir, pomp, KFAS, bssm, dynamichazard, NGSSEML, coda\nCRAN Task Views implied by cited packages\nTimeSeries, Bayesian, DifferentialEquations, Finance, GraphicalModels\n\n\n",
    "preview": "articles/RJ-2021-087/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 498,
    "preview_height": 497
  },
  {
    "path": "articles/RJ-2021-088/",
    "title": "PAsso: an R Package for Assessing Partial Association between Ordinal Variables",
    "description": "Partial association, the dependency between variables after adjusting for a set of covariates, is an important statistical notion for scientific research. However, if the variables of interest are ordered categorical data, the development of statistical methods and software for assessing their partial association is limited. Following the framework established by Liu et al. (2021), we develop an R package PAsso for assessing Partial Associations between ordinal variables. The package provides various functions that allow users to perform a wide spectrum of assessments, including quantification, visualization, and hypothesis testing. In this paper, we discuss the implementation of PAsso in detail and demonstrate its utility through an analysis of the 2016 American National Election Study.",
    "author": [
      {
        "name": "Shaobo Li",
        "url": {}
      },
      {
        "name": "Xiaorui Zhu",
        "url": {}
      },
      {
        "name": "Yuejie Chen",
        "url": {}
      },
      {
        "name": "Dungang Liu",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPAsso, sure, MASS, stats, pcaPP, copBasic, rms, ordinal, VGAM, GGally, ggplot2, plotly\nCRAN Task Views implied by cited packages\nEconometrics, Psychometrics, Distributions, Multivariate, SocialSciences, Environmetrics, Robust, Survival, TeachingStatistics, ChemPhys, ExtremeValue, NumericalMathematics, Phylogenetics, ReproducibleResearch, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-088/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 1193,
    "preview_height": 576
  },
  {
    "path": "articles/RJ-2021-089/",
    "title": "Robust and Efficient Optimization Using a Marquardt-Levenberg Algorithm with R Package marqLevAlg",
    "description": "Implementations in R of classical general-purpose algorithms for local optimization generally have two major limitations which cause difficulties in applications to complex problems: too loose convergence criteria and too long calculation time. By relying on a Marquardt-Levenberg algorithm (MLA), a Newton-like method particularly robust for solving local optimization problems, we provide with marqLevAlg package an efficient and general-purpose local optimizer which (i) prevents con vergence to saddle points by using a stringent convergence criterion based on the relative distance to minimum/maximum in addition to the stability of the parameters and of the objective function; and (ii) reduces the computation time in complex settings by allowing parallel calculations at each iteration. We demonstrate through a variety of cases from the literature that our implementation reli ably and consistently reaches the optimum (even when other optimizers fail) and also largely reduces computational time in complex settings through the example of maximum likelihood estimation of different sophisticated statistical models.",
    "author": [
      {
        "name": "Viviane Philipps",
        "url": {}
      },
      {
        "name": "Boris P. Hejblum",
        "url": {}
      },
      {
        "name": "Mélanie Prague",
        "url": {}
      },
      {
        "name": "Daniel Commenges",
        "url": {}
      },
      {
        "name": "Cécile Proust-Lima",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbase, optimx, minpack.lm, nlmrt, marqLevAlg, doParallel, foreach, JM, lcmm, optimParallel, optim, roptim, DEoptim, GA, rgenoud, hydroPSO\nCRAN Task Views implied by cited packages\nOptimization, HighPerformanceComputing, ChemPhys, Cluster, Hydrology, MachineLearning, Survival\n\n\n",
    "preview": "articles/RJ-2021-089/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 772,
    "preview_height": 366
  },
  {
    "path": "articles/RJ-2021-090/",
    "title": "An R package for Non-Normal Multivariate Distributions: Simulation and Probability Calculations from Multivariate Lomax (Pareto Type II) and Other Related Distributions",
    "description": "Convenient and easy-to-use programs are readily available in R to simulate data from and probability calculations for several common multivariate distributions such as normal and t. However, functions for doing so from other less common multivariate distributions, especially those which are asymmetric, are not as readily available, either in R or otherwise. We introduce the R package NonNorMvtDist to generate random numbers from multivariate Lomax distribution, which constitutes a very flexible family of skewed multivariate distributions. Further, by applying certain useful properties of multivariate Lomax distribution, multivariate cases of generalized Lomax, Mardia’s Pareto of Type I, Logistic, Burr, Cook-Johnson’s uniform, F, and inverted beta can be also considered, and random numbers from these distributions can be generated. Methods for the probability and the equicoordinate quantile calculations for all these distributions are then provided. This work substantially enriches the existing R toolbox for nonnormal or nonsymmetric multivariate probability distributions.",
    "author": [
      {
        "name": "Zhixin Lun",
        "url": {}
      },
      {
        "name": "Ravindra Khattree",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nNonNorMvtDist, VGAM, stats, cubature, plot3D, ggplot2\nCRAN Task Views implied by cited packages\nDistributions, Econometrics, Environmetrics, ExtremeValue, Multivariate, NumericalMathematics, Phylogenetics, Psychometrics, SocialSciences, Survival, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-090/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 460
  },
  {
    "path": "articles/RJ-2021-091/",
    "title": "cat.dt: An R package for fast construction of accurate Computerized Adaptive Tests using Decision Trees",
    "description": "This article introduces the cat.dt package for the creation of Computerized Adaptive Tests (CATs). Unlike existing packages, the cat.dt package represents the CAT in a Decision Tree (DT) structure. This allows building the test before its administration, ensuring that the creation time of the test is independent of the number of participants. Moreover, to accelerate the construction of the tree, the package controls its growth by joining nodes with similar estimations or distributions of the ability level and uses techniques such as message passing and pre-calculations. The constructed tree, as well as the estimation procedure, can be visualized using the graphical tools included in the package. An experiment designed to evaluate its performance shows that the cat.dt package drastically reduces computational time in the creation of CATs without compromising accuracy.",
    "author": [
      {
        "name": "Javier Rodríguez-Cuadrado",
        "url": {}
      },
      {
        "name": "Juan C. Laria",
        "url": {}
      },
      {
        "name": "David Delgado-Gómez",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncatR, mirtCAT, catIrt, Matrix, Rglpk, ggplot2\nCRAN Task Views implied by cited packages\nPsychometrics, Econometrics, Multivariate, NumericalMathematics, Optimization, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-091/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 442,
    "preview_height": 272
  },
  {
    "path": "articles/RJ-2021-092/",
    "title": "SIQR: An R Package for Single-index Quantile Regression",
    "description": "We develop an R package SIQR that implements the single-index quantile regression (SIQR) models via an efficient iterative local linear approach in Wu et al. (2010). Single-index quantile regression models are important tools in semiparametric regression to provide a comprehensive view of the conditional distributions of a response variable. It is especially useful when the data is heterogeneous or heavy-tailed. The package provides functions that allow users to fit SIQR models, predict, provide standard errors of the single-index coefficients via bootstrap, and visualize the estimated univariate function. We apply the R package SIQR to a well-known Boston Housing data.",
    "author": [
      {
        "name": "Tianhai Zu",
        "url": {}
      },
      {
        "name": "Yan Yu",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nquantreg, KernSmooth\nCRAN Task Views implied by cited packages\nEconometrics, Environmetrics, Multivariate, Optimization, ReproducibleResearch, Robust, SocialSciences, Survival\n\n\n",
    "preview": "articles/RJ-2021-092/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 468,
    "preview_height": 324
  },
  {
    "path": "articles/RJ-2021-093/",
    "title": "mgee2: An R package for marginal analysis of longitudinal ordinal data with misclassified responses and covariates",
    "description": "Marginal methods have been widely used for analyzing longitudinal ordinal data due to their simplicity in model assumptions, robustness in inference results, and easiness in the implementation. However, they are often inapplicable in the presence of measurement errors in the variables. Under the setup of longitudinal studies with ordinal responses and covariates subject to misclassification, Chen et al. (2014) developed marginal methods for misclassification adjustments using the second-order estimating equations and proposed a two-stage estimation approach when the validation subsample is available. Parameter estimation is conducted through the Newton-Raphson algorithm, and the asymptotic distribution of the estimators is established. While the methods of Chen et al. (2014) can successfully correct the misclassification effects, its implementation is not accessible to general users due to the lack of a software package. In this paper, we develop an R package, mgee2, to implement the marginal methods proposed by Chen et al. (2014). To evaluate the performance and illustrate the features of the package, we conduct numerical studies.",
    "author": [
      {
        "name": "Yuliang Xu",
        "url": {}
      },
      {
        "name": "Shuo Shuo Liu",
        "url": {}
      },
      {
        "name": "Grace Y. Yi",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmgee2, SAMBA, misclassGLM, augSIMEX, kml, kml3d, gee, wgeesel, swgee, MASS, Matrix, ggplot2, mgee2k, mgee2v, ordGEE2\nCRAN Task Views implied by cited packages\nCluster, Distributions, Econometrics, Environmetrics, MissingData, NumericalMathematics, Psychometrics, Robust, Spatial, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-093/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 9600,
    "preview_height": 6000
  },
  {
    "path": "articles/RJ-2021-094/",
    "title": "PASSED: Calculate Power and Sample Size for Two Sample Tests",
    "description": "Power and sample size estimation are critical aspects of study design to demonstrate minimized risk for subjects and justify the allocation of time, money, and other resources. Researchers often work with response variables that take the form of various distributions. Here, we present an R package, PASSED, that allows flexibility with seven common distributions and multiple options to accommodate sample size or power analysis. The relevant statistical theory, calculations, and examples for each distribution using PASSED are discussed in this paper.",
    "author": [
      {
        "name": "Jinpu Li",
        "url": {}
      },
      {
        "name": "Ryan P. Knigge",
        "url": {}
      },
      {
        "name": "Kaiyi Chen",
        "url": {}
      },
      {
        "name": "Emily V. Leary",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPASSED, samplesize, TrialSize, simglm, stats, pwr, MESS, pwr2ppl, WebPower, MKmisc\nCRAN Task Views implied by cited packages\nClinicalTrials\n\n\n",
    "preview": "articles/RJ-2021-094/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 762,
    "preview_height": 394
  },
  {
    "path": "articles/RJ-2021-095/",
    "title": "openSkies - Integration of Aviation Data into the R Ecosystem",
    "description": "Aviation data has become increasingly more accessible to the public thanks to the adoption of technologies such as Automatic Dependent Surveillance-Broadcast (ADS-B) and Mode S, which provide aircraft information over publicly accessible radio channels. Furthermore, the OpenSky Network provides multiple public resources to access such air traffic data from a large network of ADS-B receivers. Here, we present openSkies, the first R package for processing public air traffic data. The package provides an interface to the OpenSky Network resources, standardized data structures to represent the different entities involved in air traffic data, and functionalities to analyze and visualize such data. Furthermore, the portability of the implemented data structures makes openSkies easily reusable by other packages, therefore laying the foundation of aviation data engineering in R.",
    "author": [
      {
        "name": "Rafael Ayala",
        "url": {}
      },
      {
        "name": "Daniel Ayala",
        "url": {}
      },
      {
        "name": "Lara Sellés Vidal",
        "url": {}
      },
      {
        "name": "David Ruiz",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nopenSkies, ggmap\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-095/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 477,
    "preview_height": 229
  },
  {
    "path": "articles/RJ-2021-079/",
    "title": "lg: An R package for Local Gaussian Approximations",
    "description": "The package lg for the R programming language provides implementations of recent methodological advances on applications of the local Gaussian correlation. This includes the estimation of the local Gaussian correlation itself, multivariate density estimation, conditional density estimation, various tests for independence and conditional independence, as well as a graphical module for creating dependence maps. This paper describes the lg package, its principles, and its practical use.",
    "author": [
      {
        "name": "Håkon Otneim",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlg, localgauss, magrittr, dplyr, ggplot2, mvtnorm\nCRAN Task Views implied by cited packages\nDatabases, Distributions, Finance, ModelDeployment, Multivariate, Phylogenetics, TeachingStatistics, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-079/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 360,
    "preview_height": 288
  },
  {
    "path": "articles/RJ-2021-080/",
    "title": "Multiple Imputation and Synthetic Data Generation with NPBayesImputeCat",
    "description": "In many contexts, missing data and disclosure control are ubiquitous and challenging issues. In particular, at statistical agencies, the respondent-level data they collect from surveys and censuses can suffer from high rates of missingness. Furthermore, agencies are obliged to protect respondents’ privacy when publishing the collected data for public use. The NPBayesImputeCat R package, introduced in this paper, provides routines to i) create multiple imputations for missing data and ii) create synthetic data for statistical disclosure control, for multivariate categorical data, with or without structural zeros. We describe the Dirichlet process mixture of products of the multinomial distributions model used in the package and illustrate various uses of the package using data samples from the American Community Survey (ACS). We also compare results of the missing data imputation to the mice R package and those of the synthetic data generation to the synthpop R package.",
    "author": [
      {
        "name": "Jingchen Hu",
        "url": {}
      },
      {
        "name": "Olanrewaju Akande",
        "url": {}
      },
      {
        "name": "Quanli Wang",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nNPBayesImputeCat, mice, synthpop, bayesplot, tidyverse\nCRAN Task Views implied by cited packages\nMissingData, OfficialStatistics, Multivariate, SocialSciences\n\n\n",
    "preview": "articles/RJ-2021-080/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 432,
    "preview_height": 288
  },
  {
    "path": "articles/RJ-2021-081/",
    "title": "A GUIded tour of Bayesian regression",
    "description": "This paper presents a Graphical User Interface (GUI) to carry out a Bayesian regression analysis in a very friendly environment without any programming skills (drag and drop). This paper is designed for teaching and applied purposes at an introductory level. Our GUI is based on an interactive web application using shiny and libraries from R. We carry out some applications to highlight the potential of our GUI for applied researchers and practitioners. In addition, the Help option in the main tap panel has an extended version of this paper, where we present the basic theory underlying all regression models that we developed in our GUI and more applications associated with each model.",
    "author": [
      {
        "name": "Andrés Ramírez–Hassan",
        "url": {}
      },
      {
        "name": "Mateo Graciano-Londoño",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nshinystan\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": "articles/RJ-2021-081/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 2351,
    "preview_height": 1017
  },
  {
    "path": "articles/RJ-2021-082/",
    "title": "miRecSurv Package: Prentice-Williams-Peterson Models with Multiple Imputation of Unknown Number of Previous Episodes",
    "description": "Left censoring can occur with relative frequency when analyzing recurrent events in epi demiological studies, especially observational ones. Concretely, the inclusion of individuals that were already at risk before the effective initiation in a cohort study may cause the unawareness of prior episodes that have already been experienced, and this will easily lead to biased and inefficient estimates. The miRecSurv package is based on the use of models with specific baseline hazard, with multiple imputation of the number of prior episodes when unknown by means of the COMPoisson distribution, a very flexible count distribution that can handle over, sub, and equidispersion, with a stratified model depending on whether the individual had or had not previously been at risk, and the use of a frailty term. The usage of the package is illustrated by means of a real data example based on an occupational cohort study and a simulation study.",
    "author": [
      {
        "name": "David Moriña",
        "url": {}
      },
      {
        "name": "Gilma Hernández-Herrera",
        "url": {}
      },
      {
        "name": "Albert Navarro",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmiRecSurv, compoisson, survsim\nCRAN Task Views implied by cited packages\nSurvival\n\n\n",
    "preview": "articles/RJ-2021-082/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 1073,
    "preview_height": 589
  },
  {
    "path": "articles/RJ-2021-083/",
    "title": "bcmixed: A Package for Median Inference on Longitudinal Data with the Box–Cox Transformation",
    "description": "This article illustrates the use of the bcmixed package and focuses on the two main functions: bcmarg and bcmmrm. The bcmarg function provides inference results for a marginal model of a mixed ef fect model using the Box–Cox transformation. The bcmmrm function provides model median inferences based on the mixed effect models for repeated measures analysis using the Box–Cox transformation for longitudinal randomized clinical trials. Using the bcmmrm function, analysis results with high power and high interpretability for treatment effects can be obtained for longitudinal randomized clinical trials with skewed outcomes. Further, the bcmixed package provides summarizing and visualization tools, which would be helpful to write clinical trial reports.",
    "author": [
      {
        "name": "Kazushi Maruo",
        "url": {}
      },
      {
        "name": "Ryota Ishii",
        "url": {}
      },
      {
        "name": "Yusuke Yamaguchi",
        "url": {}
      },
      {
        "name": "Masahiko Gosho",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbcmixed, nlme, glme, lme4, CLME, PLmixed, MCMCglmm, ggplot2, MissMech\nCRAN Task Views implied by cited packages\nPsychometrics, SocialSciences, Econometrics, Environmetrics, OfficialStatistics, Phylogenetics, SpatioTemporal, Bayesian, ChemPhys, Finance, Spatial, Survival, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-083/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 576,
    "preview_height": 360
  },
  {
    "path": "articles/RJ-2021-085/",
    "title": "spfilteR: An R package for Semiparametric Spatial Filtering with Eigenvectors in (Generalized) Linear Models",
    "description": "Eigenvector-based Spatial filtering constitutes a highly flexible semiparametric approach to account for spatial autocorrelation in a regression framework. It combines judiciously selected eigenvectors from a transformed connectivity matrix to construct a synthetic spatial filter and remove spatial patterns from model residuals. This article introduces the spfilteR package that provides several useful and flexible tools to estimate spatially filtered linear and generalized linear models in R. While the package features functions to identify relevant eigenvectors based on different selection criteria in an unsupervised fashion, it also helps users to perform supervised spatial filtering and to select eigenvectors based on alternative user-defined criteria. Besides a brief discussion of the eigenvector-based spatial filtering approach, this article presents the main functions of the package and illustrates their usage. Comparison to alternative implementations in other R packages highlights the added value of the spfilteR package.",
    "author": [
      {
        "name": "Sebastian Juhl",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspfilteR, spatialreg, spmoran, adespatial, vegan\nCRAN Task Views implied by cited packages\nSpatial, Econometrics, Environmetrics, Multivariate, Phylogenetics, Psychometrics\n\n\n",
    "preview": "articles/RJ-2021-085/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 300,
    "preview_height": 300
  },
  {
    "path": "articles/RJ-2021-086/",
    "title": "The vote Package: Single Transferable Vote and Other Electoral Systems in R",
    "description": "We describe the vote package in R, which implements the plurality (or first-past-the-post), two-round runoff, score, approval, and Single Transferable Vote (STV) electoral systems, as well as methods for selecting the Condorcet winner and loser. We emphasize the STV system, which we have found to work well in practice for multi-winner elections with small electorates, such as committee and council elections, and the selection of multiple job candidates. For single-winner elections, STV is also called Instant Runoff Voting (IRV), Ranked Choice Voting (RCV), or the alternative vote (AV) system. The package also implements the STV system with equal preferences, for the first time in a software package, to our knowledge. It also implements a new variant of STV, in which a minimum number of candidates from a specified group are required to be elected. We illustrate the package with several real examples.",
    "author": [
      {
        "name": "Adrian E. Raftery",
        "url": {}
      },
      {
        "name": "Hana Ševčíková",
        "url": {}
      },
      {
        "name": "Bernard W. Silverman",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nvote, votesys, rcv, STV, HighestMedianRules, electoral, esaps\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": "articles/RJ-2021-086/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 504
  },
  {
    "path": "articles/RJ-2021-069/",
    "title": "Estimating Social Influence Effects in Networks Using A Latent Space Adjusted Approach in R ",
    "description": "Social influence effects have been extensively studied in various empirical network research. However, many challenges remain in estimating social influence effects in networks, as influence effects are often entangled with other factors, such as homophily in the selection process and the common social-environmental factors that individuals are embedded in. Methods currently available either do not solve these problems or require stringent assumptions. Recent works by Xu (2018) and others have shown that a latent space adjusted approach based on the latent space model has the potential to disentangle the influence effects from other processes, and the simulation evidence has shown that this approach outperforms other state-of-the-art approaches in terms of recovering the true social influence effect when there is an unobserved trait co-determining influence and selection. In this paper, I will further illustrate how the latent space adjusted approach can account for bias in the estimation of social influence effects and how this approach can be easily implemented in R.",
    "author": [
      {
        "name": "Ran Xu",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlavaan, plm, latentnet, statnet, RSiena\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, Cluster, HighPerformanceComputing, MissingData, OfficialStatistics, Psychometrics, SpatioTemporal\n\n\n",
    "preview": "articles/RJ-2021-069/preview.png",
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {},
    "preview_width": 1113,
    "preview_height": 522
  },
  {
    "path": "articles/RJ-2021-070/",
    "title": "survidm: An R package for Inference and Prediction in an Illness-Death Model",
    "description": "Multi-state models are a useful way of describing a process in which an individual moves through a number of finite states in continuous time. The illness-death model plays a central role in the theory and practice of these models, describing the dynamics of healthy subjects who may move to an intermediate \"diseased\" state before entering into a terminal absorbing state. In these models, one important goal is the modeling of transition rates which is usually done by studying the relationship between covariates and disease evolution. However, biomedical researchers are also interested in reporting other interpretable results in a simple and summarized manner. These include estimates of predictive probabilities, such as the transition probabilities, occupation probabilities, cumulative incidence functions, and the sojourn time distributions. The development of survidm package has been motivated by recent contribution that provides answers to all these topics. An illustration of the software usage is included using real data.",
    "author": [
      {
        "name": "Gustavo Soutinho",
        "url": {}
      },
      {
        "name": "Marta Sestelo",
        "url": {}
      },
      {
        "name": "Luís Meira-Machado",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsurvidm, p3state.msm, TPmsm, etm, mstate, TP.idm, cmprsk, timereg, msSurv, msm, ggplot2, plotly, survival, KernSmooth\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Distributions, Econometrics, Multivariate, Phylogenetics, SocialSciences, TeachingStatistics, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-070/preview.png",
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {},
    "preview_width": 456,
    "preview_height": 317
  },
  {
    "path": "articles/RJ-2021-071/",
    "title": "dad: an R Package for Visualisation, Classification and Discrimination of Multivariate Groups Modelled by their Densities",
    "description": "Multidimensional scaling (MDS), hierarchical cluster analysis (HCA), and discriminant analysis (DA) are classical techniques which deal with data made of n individuals and p variables. When the individuals are divided into T groups, the R package dad associates with each group a multivariate probability density function and then carries out these techniques on the densities, which are estimated by the data under consideration. These techniques are based on distance measures between densities: chi-square, Hellinger, Jeffreys, Jensen-Shannon, and L p for discrete densities, Hellinger , Jeffreys, L2 , and 2-Wasserstein for Gaussian densities, and L2 for numeric non-Gaussian densities estimated by the Gaussian kernel method. Practical methods help the user to give meaning to the outputs in the context of MDS and HCA and to look for an optimal prediction in the context of DA based on the one-leave-out misclassification ratio. Some functions for data management or basic statistics calculations on groups are annexed.",
    "author": [
      {
        "name": "Rachid Boumaza",
        "url": {}
      },
      {
        "name": "Pierre Santagostini",
        "url": {}
      },
      {
        "name": "Smail Yousfi",
        "url": {}
      },
      {
        "name": "Sabine Demotes-Mainard",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nstats, MASS, ade4, FactoMineR, cluster, dad, fda, fda.usc, fdadensity, compositions, Compositional, robCompositions\nCRAN Task Views implied by cited packages\nMultivariate, Distributions, Environmetrics, FunctionalData, Psychometrics, Robust, ChemPhys, Cluster, Econometrics, MissingData, NumericalMathematics, SocialSciences, Spatial, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-071/preview.png",
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 504
  },
  {
    "path": "articles/RJ-2021-072/",
    "title": "diproperm: An R Package for the DiProPerm Test",
    "description": "High-dimensional low sample size (HDLSS) data sets frequently emerge in many biomedical applications. The direction-projection-permutation (DiProPerm) test is a two-sample hypothesis test for comparing two high-dimensional distributions. The DiProPerm test is exact, i.e., the type I error is guaranteed to be controlled at the nominal level for any sample size, and thus is applicable in the HDLSS setting. This paper discusses the key components of the DiProPerm test, introduces the diproperm R package, and demonstrates the package on a real-world data set.",
    "author": [
      {
        "name": "Andrew G. Allmon",
        "url": {}
      },
      {
        "name": "J.S. Marron",
        "url": {}
      },
      {
        "name": "Michael G. Hudgens",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndiproperm, DWDLargeR, Matrix\nCRAN Task Views implied by cited packages\nEconometrics, Multivariate, NumericalMathematics\n\n\n",
    "preview": "articles/RJ-2021-072/preview.png",
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {},
    "preview_width": 1956,
    "preview_height": 1437
  },
  {
    "path": "articles/RJ-2021-073/",
    "title": "MatchThem:: Matching and Weighting after Multiple Imputation",
    "description": "Balancing the distributions of the confounders across the exposure levels in an observational study through matching or weighting is an accepted method to control for confounding due to these variables when estimating the association between an exposure and outcome and reducing the degree of dependence on certain modeling assumptions. Despite the increasing popularity in practice, these procedures cannot be immediately applied to datasets with missing values. Multiple imputation of the missing data is a popular approach to account for missing values while preserving the number of units in the dataset and accounting for the uncertainty in the missing values. However, to the best of our knowledge, there is no comprehensive matching and weighting software that can be easily implemented with multiply imputed datasets. In this paper, we review this problem and suggest a framework to map out the matching and weighting of multiply imputed datasets to 5 actions as well as the best practices to assess balance in these datasets after matching and weighting. We also illustrate these approaches using a companion package for R, MatchThem.",
    "author": [
      {
        "name": "Farhad Pishgar",
        "url": {}
      },
      {
        "name": "Noah Greifer",
        "url": {}
      },
      {
        "name": "Clémence Leyrat",
        "url": {}
      },
      {
        "name": "Elizabeth Stuart",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMatchThem, MatchIt, WeightIt, cobalt, mice, Amelia, survey\nCRAN Task Views implied by cited packages\nOfficialStatistics, SocialSciences, MissingData, Multivariate, Survival\n\n\n",
    "preview": "articles/RJ-2021-073/preview.png",
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 540
  },
  {
    "path": "articles/RJ-2021-074/",
    "title": "MAINT.Data: Modelling and Analysing Interval Data in R",
    "description": "We present the CRAN R package MAINT.Data for the modelling and analysis of multivariate interval data, i.e., where units are described by variables whose values are intervals of IR, representing intrinsic variability. Parametric inference methodologies based on probabilistic models for interval variables have been developed, where each interval is represented by its midpoint and log-range, for",
    "author": [
      {
        "name": "A. Pedro Duarte Silva",
        "url": {}
      },
      {
        "name": "Paula Brito",
        "url": {}
      },
      {
        "name": "Peter Filzmoser",
        "url": {}
      },
      {
        "name": "José G. Dias",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsurvreg, crch, MAINT.Data, symbolicDA, RSDA, iRegression, GPCSIV, sn, Rcpp, RcppArmadillo, rrcov, mclust, nycflights13, tidyverse\nCRAN Task Views implied by cited packages\nCluster, Distributions, Econometrics, Environmetrics, HighPerformanceComputing, NumericalMathematics, Robust\n\n\n",
    "preview": "articles/RJ-2021-074/preview.png",
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {},
    "preview_width": 720,
    "preview_height": 540
  },
  {
    "path": "articles/RJ-2021-075/",
    "title": "tramME: Mixed-Effects Transformation Models Using Template Model Builder",
    "description": "Linear transformation models constitute a general family of parametric regression models for discrete and continuous responses. To accommodate correlated responses, the model is extended by incorporating mixed effects. This article presents the R package tramME, which builds on existing implementations of transformation models (mlt and tram packages) as well as Laplace approximation and automatic differentiation (using the TMB package), to calculate estimates and perform likelihood inference in mixed-effects transformation models. The resulting framework can be readily applied to a wide range of regression problems with grouped data structures.",
    "author": [
      {
        "name": "Bálint Tamási",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nnlme, lme4, tramME, mlt, tram, TMB, glmmTMB, survival, boxcoxmix, ordinalCont, coxme, parfm, frailtypack, ordinal\nCRAN Task Views implied by cited packages\nSurvival, Econometrics, Psychometrics, SocialSciences, Environmetrics, OfficialStatistics, SpatioTemporal, ChemPhys, ClinicalTrials, Finance, Spatial\n\n\n",
    "preview": "articles/RJ-2021-075/preview.png",
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 360
  },
  {
    "path": "articles/RJ-2021-076/",
    "title": "CompModels: A Suite of Computer Model Test Functions for Bayesian Optimization",
    "description": "The CompModels package for R provides a suite of computer model test functions that can be used for computer model prediction/emulation, uncertainty quantification, and calibration. Moreover, the CompModels package is especially well suited for the sequential optimization of computer models. The package is a mix of real-world physics problems, known mathematical functions, and black-box functions that have been converted into computer models with the goal of Bayesian (i.e., sequential) optimization in mind. Likewise, the package contains computer models that represent either the constrained or unconstrained optimization case, each with varying levels of difficulty. In this paper, we illustrate the use of the package with both real-world examples and black-box functions by solving constrained optimization problems via Bayesian optimization. Ultimately, the package is shown to provide users with a source of computer model test functions that are reproducible, shareable, and that can be used for benchmarking of novel optimization methods.",
    "author": [
      {
        "name": "Tony Pourmohamad",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCompModels, laGP\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": "articles/RJ-2021-076/preview.png",
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 504
  },
  {
    "path": "articles/RJ-2021-077/",
    "title": "volesti: Volume Approximation and Sampling for Convex Polytopes in R",
    "description": "Sampling from high-dimensional distributions and volume approximation of convex bodies are fundamental operations that appear in optimization, finance, engineering, artificial intelligence, and machine learning. In this paper, we present volesti, an R package that provides efficient, scalable algorithms for volume estimation, uniform, and Gaussian sampling from convex polytopes. volesti scales to hundreds of dimensions, handles efficiently three different types of polyhedra and pro vides non existing sampling routines to R. We demonstrate the power of volesti by solving several challenging problems using the R language.",
    "author": [
      {
        "name": "Apostolos Chalkis",
        "url": {}
      },
      {
        "name": "Vissarion Fisikopoulos",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nvolesti, tmg, multinomineq, lineqGPR, restrictedMVN, tmvmixnorm, hitandrun, limSolve, HybridMC, rhmc, mcmc, MHadaptive, geometry, Rcpp, Rfast, coda, SimplicialCubature, cubature, stats, methods, BH, RcppEigen, testthat, ggplot2, plotly, rgl\nCRAN Task Views implied by cited packages\nNumericalMathematics, Bayesian, Multivariate, Distributions, GraphicalModels, HighPerformanceComputing, Optimization, Phylogenetics, SpatioTemporal, TeachingStatistics, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-077/preview.png",
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {},
    "preview_width": 595,
    "preview_height": 842
  },
  {
    "path": "articles/RJ-2021-078/",
    "title": "Elliptical Symmetry Tests in R",
    "description": "The assumption of elliptical symmetry has an important role in many theoretical develop ments and applications. Hence, it is of primary importance to be able to test whether that assumption actually holds true or not. Various tests have been proposed in the literature for this problem. To the best of our knowledge, none of them has been implemented in R. This article describes the R package ellipticalsymmetry which implements several well-known tests for elliptical symmetry together with some recent tests. We demonstrate the testing procedures with a real data example.",
    "author": [
      {
        "name": "Slad̄ana Babić",
        "url": {}
      },
      {
        "name": "Christophe Ley",
        "url": {}
      },
      {
        "name": "Marko Palangetić",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nellipticalsymmetry\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:57+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-065/",
    "title": "The bdpar Package: Big Data Pipelining Architecture for R",
    "description": "In the last years, big data has become a useful paradigm for taking advantage of multiple sources to find relevant knowledge in real domains (such as the design of personalized marketing campaigns or helping to palliate the effects of several fatal diseases). Big data programming tools and methods have evolved over time from a MapReduce to a pipeline-based archetype. Concretely the use of pipelining schemes has become the most reliable way of processing and analyzing large amounts of data. To this end, this work introduces bdpar, a new highly customizable pipeline-based framework (using the OOP paradigm provided by [R6](https://CRAN.R-project.org/package=R6) package) able to execute multiple preprocessing tasks over heterogeneous data sources. Moreover, to increase the flexibility and performance, bdpar provides helpful features such as (i) the definition of a novel object-based pipe operator ( %\\>\\|%), (ii) the ability to easily design and deploy new (and customized) input data parsers, tasks, and pipelines, (iii) only-once execution which avoids the execution of previously processed information (instances), guaranteeing that only new both input data and pipelines are executed, (iv) the capability to perform serial or parallel operations according to the user needs, (v) the inclusion of a debugging mechanism which allows users to check the status of each instance (and find possible errors) throughout the process.",
    "author": [
      {
        "name": "Miguel Ferreiro-Díaz",
        "url": {}
      },
      {
        "name": "Tomás R. Cotos-Yáñez",
        "url": {}
      },
      {
        "name": "José R. Méndez",
        "url": {}
      },
      {
        "name": "David Ruano-Ordás",
        "url": {}
      }
    ],
    "date": "2021-07-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-065.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-064/",
    "title": "g2f as a Novel Tool to Find and Fill Gaps in Metabolic Networks",
    "description": "During the building of a genome-scale metabolic model, there are several dead-end metabo lites and substrates which cannot be imported, produced, nor used by any reaction incorporated in the network. The presence of these dead-end metabolites can block out the net flux of the objective function when it is evaluated through Flux Balance Analysis (FBA), and when it is not blocked, bias in the biological conclusions increase. In this aspect, the refinement to restore the connectivity of the network can be carried out manually or using computational algorithms. The g2f package was designed as a tool to find the gaps from dead-end metabolites and fill them from the stoichiometric reactions of a reference, filtering candidate reactions using a weighting function. Additionally, this algorithm allows downloading all the sets of gene-associated stoichiometric reactions for a specific organism from the KEGG database. Our package is compatible with both 4.0.0 and 3.6.0 R versions.",
    "author": [
      {
        "name": "Daniel Osorio",
        "url": {}
      },
      {
        "name": "Kelly Botero",
        "url": {}
      },
      {
        "name": "Andrés Pinzón Velasco",
        "url": {}
      },
      {
        "name": "Nicolás Mendoza-Mejía",
        "url": {}
      },
      {
        "name": "Felipe Rojas-            Rodríguez",
        "url": {}
      },
      {
        "name": "George Barreto",
        "url": {}
      },
      {
        "name": "Janneth González",
        "url": {}
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ng2f, sybil\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-066/",
    "title": "ROCnReg: An R Package for Receiver Operating Characteristic Curve Inference With and Without Covariates",
    "description": "This paper introduces the package ROCnReg that allows estimating the pooled ROC curve, the covariate-specific ROC curve, and the covariate-adjusted ROC curve by different methods, both from (semi) parametric and nonparametric perspectives and within Bayesian and frequentist paradigms. From the estimated ROC curve (pooled, covariate-specific, or covariate-adjusted), several summary measures of discriminatory accuracy, such as the (partial) area under the ROC curve and the Youden index, can be obtained. The package also provides functions to obtain ROC-based optimal threshold values using several criteria, namely, the Youden index criterion and the criterion that sets a target value for the false positive fraction. For the Bayesian methods, we provide tools for assessing model fit via posterior predictive checks, while the model choice can be carried out via several information criteria. Numerical and graphical outputs are provided for all methods. This is the only package implementing Bayesian procedures for ROC curves.",
    "author": [
      {
        "name": "María Xosé Rodríguez-Álvarez",
        "url": {}
      },
      {
        "name": "Vanda Inácio",
        "url": {}
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-066.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-067/",
    "title": "A New Versatile Discrete Distribution",
    "description": "This paper introduces a new flexible distribution for discrete data. Approximate moment estimators of the parameters of the distribution, to be used as starting values for numerical opti mization procedures, are discussed. “Exact” moment estimation, effected via a numerical procedure, and maximum likelihood estimation, are considered. The quality of the results produced by these estimators is assessed via simulation experiments. Several examples are given of fitting instances of the new distribution to real and simulated data. It is noted that the new distribution is a member of the exponential family. Expressions for the gradient and Hessian of the log-likelihood of the new distribution are derived. The former facilitates the numerical maximization of the likelihood with optim(); the latter provides means of calculating or estimating the covariance matrix of of the parame ter estimates. A discrepancy between estimates of the covariance matrix obtained by inverting the Hessian and those obtained by Monte Carlo methods is discussed.",
    "author": [
      {
        "name": "Rolf Turner",
        "url": {}
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhmm.discnp, spcadjust, rmutil\nCRAN Task Views implied by cited packages\nDistributions\n\n\n",
    "preview": "articles/RJ-2021-067/preview.png",
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 432
  },
  {
    "path": "articles/RJ-2021-068/",
    "title": "BayesSPsurv: An R Package to Estimate Bayesian (Spatial) Split-Population Survival Models",
    "description": "Survival data often include a fraction of units that are susceptible to an event of interest as well as a fraction of \"immune\" units. In many applications, spatial clustering in unobserved risk factors across nearby units can also affect their survival rates and odds of becoming immune. To address these methodological challenges, this article introduces our [BayesSPsurv](https://CRAN.R-project.org/package=BayesSPsurv) R-package, which fits parametric Bayesian Spatial split-population survival (cure) models that can account for spatial autocorrelation in both subpopulations of the user's time-to-event data. Spatial autocorrelation is modeled with spatially weighted frailties, which are estimated using a conditionally autoregressive prior. The user can also fit parametric cure models with or without nonspatial i.i.d. frailties, and each model can incorporate time-varying covariates. BayesSPsurv also includes various functions to conduct pre-estimation spatial autocorrelation tests, visualize results, and assess model performance, all of which are illustrated using data on post-civil war peace survival.",
    "author": [
      {
        "name": "Brandon Bolte",
        "url": {}
      },
      {
        "name": "Nicolás Schmidt",
        "url": {}
      },
      {
        "name": "Sergio Béjar",
        "url": {}
      },
      {
        "name": "Nguyen Huynh",
        "url": {}
      },
      {
        "name": "Bumba Mukherjee",
        "url": {}
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-056/",
    "title": "A Method for Deriving Information from Running R Code",
    "description": "It is often useful to tap information from a running R script. Obvious use cases include monitoring the consumption of resources (time, memory) and logging. Perhaps less obvious cases include tracking changes in R objects or collecting the output of unit tests. In this paper, we demonstrate an approach that abstracts the collection and processing of such secondary information from the running R script. Our approach is based on a combination of three elements. The first element is to build a customized way to evaluate code. The second is labeled *local masking* and it involves temporarily masking a user-facing function so an alternative version of it is called. The third element we label *local side effect*. This refers to the fact that the masking function exports information to the secondary information flow without altering a global state. The result is a method for building systems in pure R that lets users create and control secondary flows of information with minimal impact on their workflow and no global side effects.",
    "author": [
      {
        "name": "Mark P.J. van der Loo",
        "url": {}
      }
    ],
    "date": "2021-07-13",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-056.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-053/",
    "title": "Reproducible Summary Tables with the gtsummary Package",
    "description": "The gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R. A critical part of the work of statisticians, data scientists, and analysts is summarizing data sets and regression models in R and publishing or sharing polished summary tables. The gtsummary package was created to streamline these everyday analysis tasks by allowing users to easily create reproducible summaries of data sets, regression models, survey data, and survival data with a simple interface and very little code. The package follows a tidy framework, making it easy to integrate with standard data workflows, and offers many table customization features through function arguments, helper functions, and custom themes.",
    "author": [
      {
        "name": "Daniel D. Sjoberg",
        "url": {}
      },
      {
        "name": "Karissa Whiting",
        "url": {}
      },
      {
        "name": "Michael Curry",
        "url": {}
      },
      {
        "name": "Jessica A. Lavery",
        "url": {}
      },
      {
        "name": "Joseph Larmarange",
        "url": {}
      }
    ],
    "date": "2021-06-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-053.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-057/",
    "title": "garchx: Flexible and Robust GARCH-X Modeling",
    "description": "The garchx package provides a user-friendly, fast, flexible, and robust framework for the estimation and inference of GARCH($p,q,r$)-X models, where $p$ is the ARCH order, $q$ is the GARCH order, $r$ is the asymmetry or leverage order, and 'X' indicates that covariates can be included. Quasi Maximum Likelihood (QML) methods ensure estimates are consistent and standard errors valid, even when the standardized innovations are non-normal or dependent, or both. Zero-coefficient restrictions by omission enable parsimonious specifications, and functions to facilitate the non-standard inference associated with zero-restrictions in the null-hypothesis are provided. Finally, in the formal comparisons of precision and speed, the garchx package performs well relative to other prominent GARCH-packages on CRAN.",
    "author": [
      {
        "name": "Genaro Sucarrat",
        "url": {}
      }
    ],
    "date": "2021-06-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-057.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-060/",
    "title": "gofCopula: Goodness-of-Fit Tests for Copulae",
    "description": "The last decades show an increased interest in modeling various types of data through copulae. Different copula models have been developed, which lead to the challenge of finding the best fitting model for a particular dataset. From the other side, a strand of literature developed a list of different Goodness-of-Fit (GoF) tests with different powers under different conditions. The usual practice is the selection of the best copula via the $p$-value of the GoF test. Although this method is not purely correct due to the fact that non-rejection does not imply acception, this strategy is favored by practitioners. Unfortunately, different GoF tests often provide contradicting outputs. The proposed R-package brings under one umbrella 13 most used copulae - plus their rotated variants - together with 16 GoF tests and a hybrid one. The package offers flexible margin modeling, automatized parallelization, parameter estimation, as well as a user-friendly interface, and pleasant visualizations of the results. To illustrate the functionality of the package, two exemplary applications are provided.",
    "author": [
      {
        "name": "Ostap Okhrin",
        "url": {}
      },
      {
        "name": "Simon Trimborn",
        "url": {}
      },
      {
        "name": "Martin Waltz",
        "url": {}
      }
    ],
    "date": "2021-06-22",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-059/",
    "title": "The HBV.IANIGLA Hydrological Model",
    "description": "Over the past 40 years, the HBV (Hydrologiska Byråns Vattenbalansavdelning) hydrological model has been one of the most used worldwide due to its robustness, simplicity, and reliable results. Despite these advantages, the available versions impose some limitations for research studies in mountain watersheds dominated by ice-snow melt runoff (i.e., no glacier module, a limited number of elevation bands, among other constraints). Here we present HBV.IANIGLA, a tool for hydroclimatic studies in regions with steep topography and/or cryospheric processes which provides a modular and extended implementation of the HBV model as an R package. To our knowledge, this is the first modular version of the original HBV model. This feature can be very useful for teaching hydrological modeling, as it offers the possibility to build a customized, open-source model that can be adjusted to different requirements of students and users.",
    "author": [
      {
        "name": "Ezequiel Toum",
        "url": {}
      },
      {
        "name": "Mariano H. Masiokas",
        "url": {}
      },
      {
        "name": "Ricardo Villalba",
        "url": {}
      },
      {
        "name": "Pierre Pitte",
        "url": {}
      },
      {
        "name": "Lucas Ruiz",
        "url": {}
      }
    ],
    "date": "2021-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-059.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-061/",
    "title": "penPHcure: Variable Selection in Proportional Hazards Cure Model with Time-Varying Covariates",
    "description": "We describe the [penPHcure](https://CRAN.R-project.org/package=penPHcure) R package, which implements the semiparametric proportional-hazards (PH) cure model of @Sy_Taylor_2000 extended to time-varying covariates and the variable selection technique based on its SCAD-penalized likelihood proposed by @Beretta_Heuchenne_2019. In survival analysis, cure models are a useful tool when a fraction of the population is likely to be immune from the event of interest. They can separate the effects of certain factors on the probability of being susceptible and on the time until the occurrence of the event. Moreover, the penPHcure package allows the user to simulate data from a PH cure model, where the event-times are generated on a continuous scale from a piecewise exponential distribution conditional on time-varying covariates, with a method similar to @Hendry_2014. We present the results of a simulation study to assess the finite sample performance of the methodology and illustrate the functionalities of the penPHcure package using criminal recidivism data.",
    "author": [
      {
        "name": "Alessandro Beretta",
        "url": {}
      },
      {
        "name": "Cédric Heuchenne",
        "url": {}
      }
    ],
    "date": "2021-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-061.zip\n\n\nA. Beretta and C. Heuchenne. Variable selection in proportional hazards cure model with time-varying covariates, application to US bank failures. Journal of Applied Statistics, 46(9): 1529–1549, 2019. URL https://doi.org/10.1080/02664763.2018.1554627.\n\n\nD. J. Hendry. Data generation for the cox proportional hazards model with time-dependent covariates: A method for medical researchers. Statistics in medicine, 33: 436–454, 2014. URL https://doi.org/10.1002/sim.5945.\n\n\nJ. P. Sy and J. M. G. Taylor. Estimation in a cox proportional hazards cure model. Biometrics, 56(1): 227–236, 2000. URL https://doi.org/10.1111/j.0006-341X.2000.00227.x.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-062/",
    "title": "Unidimensional and Multidimensional Methods for Recurrence Quantification Analysis with crqa",
    "description": "Recurrence quantification analysis is a widely used method for characterizing patterns in time series. This article presents a comprehensive survey for conducting a wide range of recurrence-based analyses to quantify the dynamical structure of single and multivariate time series and capture coupling properties underlying leader-follower relationships. The basics of recurrence quantification analysis (RQA) and all its variants are formally introduced step-by-step from the simplest auto-recurrence to the most advanced multivariate case. Importantly, we show how such RQA methods can be deployed under a single computational framework in R using a substantially renewed version of our crqa 2.0 package. This package includes implementations of several recent advances in recurrence-based analysis, among them applications to multivariate data and improved entropy calculations for categorical data. We show concrete applications of our package to example data, together with a detailed description of its functions and some guidelines on their usage.",
    "author": [
      {
        "name": "Moreno I. Coco",
        "url": {}
      },
      {
        "name": "Dan Mønster",
        "url": {}
      },
      {
        "name": "Giuseppe Leonardi",
        "url": {}
      },
      {
        "name": "Rick Dale",
        "url": {}
      },
      {
        "name": "Sebastian Wallot",
        "url": {}
      }
    ],
    "date": "2021-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-062.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-063/",
    "title": "stratamatch: Prognostic Score Stratification Using a Pilot Design",
    "description": "Optimal propensity score matching has emerged as one of the most ubiquitous approaches for causal inference studies on observational data. However, outstanding critiques of the statistical properties of propensity score matching have cast doubt on the statistical efficiency of this technique, and the poor scalability of optimal matching to large data sets makes this approach inconvenient if not infeasible for sample sizes that are increasingly commonplace in modern observational data. The [stratamatch](https://CRAN.R-project.org/package=stratamatch) package provides implementation support and diagnostics for 'stratified matching designs,' an approach that addresses both of these issues with optimal propensity score matching for large-sample observational studies. First, stratifying the data enables more computationally efficient matching of large data sets. Second, stratamatch implements a 'pilot design' approach in order to stratify by a prognostic score, which may increase the precision of the effect estimate and increase power in sensitivity analyses of unmeasured confounding.",
    "author": [
      {
        "name": "Rachael C. Aikens",
        "url": {}
      },
      {
        "name": "Joseph Rigdon",
        "url": {}
      },
      {
        "name": "Justin Lee",
        "url": {}
      },
      {
        "name": "Michael Baiocchi",
        "url": {}
      },
      {
        "name": "Andrew B. Goldstone",
        "url": {}
      },
      {
        "name": "Peter Chiu",
        "url": {}
      },
      {
        "name": "Y. Joseph Woo",
        "url": {}
      },
      {
        "name": "Jonathan H. Chen",
        "url": {}
      }
    ],
    "date": "2021-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-063.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-055/",
    "title": "distr6: R6 Object-Oriented Probability Distributions Interface in R",
    "description": "distr6 is an object-oriented (OO) probability distributions interface leveraging the extensibility and scalability of R6 and the speed and efficiency of Rcpp. Over 50 probability distributions are currently implemented in the package with 'core' methods, including density, distribution, and generating functions, and more 'exotic' ones, including hazards and distribution function anti-derivatives. In addition to simple distributions, distr6 supports compositions such as truncation, mixtures, and product distributions. This paper presents the core functionality of the package and demonstrates examples for key use-cases. In addition, this paper provides a critical review of the object-oriented programming paradigms in R and describes some novel implementations for design patterns and core object-oriented features introduced by the package for supporting distr6 components.",
    "author": [
      {
        "name": "Raphael Sonabend",
        "url": {}
      },
      {
        "name": "Franz J. Király",
        "url": {}
      }
    ],
    "date": "2021-06-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-055.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-044/",
    "title": "OneStep : Le Cam's One-step Estimation Procedure",
    "description": "The OneStep package proposes principally an eponymic function that numerically computes Le Cam's one-step estimator, which is asymptotically efficient and can be computed faster than the maximum likelihood estimator for large datasets. Monte Carlo simulations are carried out for several examples (discrete and continuous probability distributions) in order to exhibit the performance of Le Cam's one-step estimation procedure in terms of efficiency and computational cost on observation samples of finite size.",
    "author": [
      {
        "name": "Alexandre Brouste",
        "url": {}
      },
      {
        "name": "Christophe Dutang",
        "url": {}
      },
      {
        "name": "Darel Noutsa Mieniedou",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-044.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-045/",
    "title": "The R Package smicd: Statistical Methods for Interval-Censored Data",
    "description": "The package allows the use of two new statistical methods for the analysis of interval-censored data: 1) direct estimation/prediction of statistical indicators and 2) linear (mixed) regression analysis. Direct estimation of statistical indicators, for instance, poverty and inequality indicators, is facilitated by a non parametric kernel density algorithm. The algorithm is able to account for weights in the estimation of statistical indicators. The standard errors of the statistical indicators are estimated with a non parametric bootstrap. Furthermore, the package offers statistical methods for the estimation of linear and linear mixed regression models with an interval-censored dependent variable, particularly random slope and random intercept models. Parameter estimates are obtained through a stochastic expectation-maximization algorithm. Standard errors are estimated using a non parametric bootstrap in the linear regression model and by a parametric bootstrap in the linear mixed regression model. To handle departures from the model assumptions, fixed (logarithmic) and data-driven (Box-Cox) transformations are incorporated into the algorithm.",
    "author": [
      {
        "name": "Paul Walter",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-046/",
    "title": "krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff's Alpha Coefficient",
    "description": "R package [krippendorffsalpha](https://CRAN.R-project.org/package=krippendorffsalpha) provides tools for measuring agreement using Krippendorff's $\\alpha$ coefficient, a well-known nonparametric measure of agreement (also called inter-rater reliability and various other names). This article first develops Krippendorff's $\\alpha$ in a natural way and situates $\\alpha$ among statistical procedures. Then, the usage of package [krippendorffsalpha](https://CRAN.R-project.org/package=krippendorffsalpha) is illustrated via analyses of two datasets, the latter of which was collected during an imaging study of hip cartilage. The package permits users to apply the $\\alpha$ methodology using built-in distance functions for the nominal, ordinal, interval, or ratio levels of measurement. User-defined distance functions are also supported. The fitting function can accommodate any number of units, any number of coders, and missingness. Bootstrap inference is supported, and the bootstrap computation can be carried out in parallel.",
    "author": [
      {
        "name": "John Hughes",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-046.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-047/",
    "title": "Working with CRSP/COMPUSTAT in R: Reproducible Empirical Asset Pricing",
    "description": "It is common to come across SAS or Stata manuals while working on academic empirical finance research. Nonetheless, given the popularity of open-source programming languages such as R, there are fewer resources in R covering popular databases such as CRSP and COMPUSTAT. The aim of this article is to bridge the gap and illustrate how to leverage R in working with both datasets. As an application, we illustrate how to form size-value portfolios with respect to [@fama1993common] and study the sensitivity of the results with respect to different inputs. Ultimately, the purpose of the article is to advocate reproducible finance research and contribute to the recent idea of \"Open Source Cross-Sectional Asset Pricing\", proposed by @chen2020open.",
    "author": [
      {
        "name": "Majeed Simaan",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-047.zip\n\n\nA. Y. Chen and T. Zimmermann. Open source cross-sectional asset pricing. Available at SSRN, 2020.\n\n\nE. F. Fama and K. R. French. Common risk factors in the returns on stocks and bonds. Journal of Finance, 1993.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-049/",
    "title": "Analyzing Dependence between Point Processes in Time Using IndTestPP",
    "description": "The need to analyze the dependence between two or more point processes in time appears in many modeling problems related to the occurrence of events, such as the occurrence of climate events at different spatial locations or synchrony detection in spike train analysis. The package IndTestPP provides a general framework for all the steps in this type of analysis, and one of its main features is the implementation of three families of tests to study independence given the intensities of the processes, which are not only useful to assess independence but also to identify factors causing dependence. The package also includes functions for generating different types of dependent point processes, and implements computational statistical inference tools using them. An application to characterize the dependence between the occurrence of extreme heat events in three Spanish locations using the package is shown.",
    "author": [
      {
        "name": "Ana C. Cebrián",
        "url": {}
      },
      {
        "name": "Jesús Asín",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-049.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-050/",
    "title": "Conversations in Time: Interactive Visualization to Explore Structured Temporal Data",
    "description": "Temporal data often has a hierarchical structure, defined by categorical variables describing different levels, such as political regions or sales products. The nesting of categorical variables produces a hierarchical structure. The tsibbletalk package is developed to allow a user to interactively explore temporal data, relative to the nested or crossed structures. It can help to discover differences between category levels, and uncover interesting periodic or aperiodic slices. The package implements a shared `tsibble` object that allows for linked brushing between coordinated views, and a shiny module that aids in wrapping timelines for seasonal patterns. The tools are demonstrated using two data examples: domestic tourism in Australia and pedestrian traffic in Melbourne.",
    "author": [
      {
        "name": "Earo Wang",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nIntroduction\nTemporal data typically arrives as a set of many observational units measured over time. Some variables may be categorical, containing a hierarchy in the collection process, that may be measurements taken in different geographic regions, or types of products sold by one company. Exploring these multiple features can be daunting. Ensemble graphics (Unwin and Valero-Mora 2018) bundle multiple views of a data set together into one composite figure. These provide an effective approach for exploring and digesting many different aspects of temporal data. Adding interactivity to the ensemble can greatly enhance the exploration process.\nThis paper describes new software, the tsibbletalk package, for exploring temporal data using linked views and time wrapping. We first provide some background to the approach based on setting up data structures and workflow, and give an overview of interactive systems in R. The section following introduces the tsibbletalk package. We explain the mechanism for constructing interactivity, to link between multiple hierarchical data objects and hence plots, and describe the set up for interactively slicing and dicing time to wrap a series on itself to investigate periodicities.\nBackground: tidy temporal data and workflow\nThe tsibble package (Wang et al. 2020) introduced a unified temporal data structure, referred to as a tsibble, to represent time series and longitudinal data in a tidy format (Wickham 2014). A tsibble extends the data.frame and tibble classes with the temporal contextual metadata: index and key. The index declares a data column that holds time-related indices. The key identifies a collection of related series or panels observed over the index-defined period, which can comprise multiple columns. An example of a tsibble can be found in the monthly Australian retail trade turnover data (aus_retail), available in the tsibbledata package (O’Hara-Wild et al. 2020c), shown below. The Month column holds year-months as the index. State and Industry are the identifiers for these 152 series, which form the key. Note that the column Series ID could be an alternative option for setting up the key, but State and Industry are more readable and informative. The index and key are “sticky” columns to a tsibble, forming critical pieces for fluent downstream temporal data analysis.\n\n#> # A tsibble: 64,532 x 5 [1M]\n#> # Key:       State, Industry [152]\n#>   State                        Industry       Serie…¹    Month Turno…²\n#>   <chr>                        <chr>          <chr>      <mth>   <dbl>\n#> 1 Australian Capital Territory Cafes, restau… A33498… 1982 Apr     4.4\n#> 2 Australian Capital Territory Cafes, restau… A33498… 1982 May     3.4\n#> 3 Australian Capital Territory Cafes, restau… A33498… 1982 Jun     3.6\n#> 4 Australian Capital Territory Cafes, restau… A33498… 1982 Jul     4  \n#> 5 Australian Capital Territory Cafes, restau… A33498… 1982 Aug     3.6\n#> # … with 64,527 more rows, and abbreviated variable names\n#> #   ¹​`Series ID`, ²​Turnover\n\nIn the spirit of tidy data from the tidyverse (Wickham et al. 2019), the tidyverts suite features tsibble as the foundational data structure, and helps to build a fluid and fluent pipeline for time series analysis. Besides tsibble, the feasts (O’Hara-Wild et al. 2020b) and fable (O’Hara-Wild et al. 2020a) packages fill the role of statistical analysis and forecasting in the tidyverts ecosystem. During all the steps of a time series analysis, the series of interest, denoted by the key variable, typically persist, through the trend modeling and also forecasting. We would typically want to examine the series across all of the keys.\nFigure 1 illustrates examining temporal data with many keys. The data has 152 series corresponding to different industries in retail data. The multiple series are displayed using an overlaid time series plot, along with a scatterplot of two variables (trend versus seasonal strength) from feature space, where each series is represented by a dot. The feature space is computed using the features() function from feasts, which summarises the original data for each series using various statistical features. This function along with other tidyverts functions is tsibble-aware, and outputs a table in a reduced form where each row corresponds to a series, which can be graphically displayed as in Figure ??.\n\n\n\nFigure 1: Plots for the data, with the series of strongest seasonal strength highlighted. (a) An overlaid time series plot. (b) A scatter plot drawn from their time series features, where each dot represents a time series from (a).\n\n\n\nFigure 1 has also been highlighted to focus on the one series with the strongest seasonality. To create this highlighting, one needs to first filter the interesting series from the features table, and join back to the original tsibble in order to examine its trend in relation to others. This procedure can soon grow cumbersome if many series are to be explored. It illustrates a need to query interesting series on the fly. Although these two plots are static, we can consider them as linked views because the common key variables link between the two data tables producing the two plots. This motivates the work in this package, described in this paper, to enable interactivity of tsibble and tsibble-derived objects for rapid exploratory data analysis.\nOverview of interactivity\nThere is a long history of interactive data visualization research and corresponding systems. Within R, the systems can be roughly divided into systems utilizing web technology and those that do not.\nR shiny (Chang et al. 2020) and htmlwidgets (Vaidyanathan et al. 2019) provide infrastructure connecting R with HTML elements and JavaScript that support the interactivity. The htmlwidgets package makes it possible to embed JavaScript libraries into R so that users are able to write only R code to generate web-based plots. Many JavaScript charting libraries have been ported to R as HTML widgets, including plotly (Sievert 2020), rbokeh (Hafen and Continuum Analytics, Inc. 2020), and leaflet (Cheng et al. 2019) for maps. Interactions between different widgets can be achieved with shiny or crosstalk (Cheng 2020). The crosstalk extends htmlwidgets with shared R6 instances to support linked brushing and filtering across widgets, without relying on shiny.\nSystems without the web technology include grDevices, loon (Waddell and Oldford 2020), based on Tcl/Tk, and cranvas (Xie et al. 2014) based on Qt. They offer a wide array of pre-defined interactions, such as selecting and zooming, to manipulate plots via mouse action, keyboard strokes, and menus. The cranvastime package (Cheng et al. 2016) is an add-on to cranvas, which provides specialized interactions for temporal data, such as wrapping and mirroring.\nThe techniques implemented in the work described in this paper utilize web technology, including crosstalk, plotly, and R shiny.\nUsing a shared temporal data object for interactivity\nThe tsibbletalk package introduces a shared tsibble instance built on a tsibble. This allows for seamless communication between different plots of temporal data. The as_shared_tsibble() function turns a tsibble into a shared instance, SharedTsibbleData, which is a subclass of SharedData from crosstalk. This is an R6 object driving data transmission across multiple views, due to its mutable and lightweight properties. The tsibbletalk package aims to streamline interactive exploration of temporal data, with the focus of temporal elements and structured linking.\nLinking between plots\nAs opposed to one-to-one linking, tsibbletalk defaults to categorical variable linking, where selecting one or more observations in one category will broadcast to all other observations in this category. That is, linking is by key variables: within the time series plot, click on any data point, and the whole line will be highlighted in response. The as_shared_tsibble() uses tsibble’s key variables to achieve these types of linking.\nThe approach can also accommodate temporal data of nesting and crossing structures. These time series are referred to as hierarchical and grouped time series in the literature (Hyndman and Athanasopoulos 2017). The aus_retail above is an example of grouped time series. Each series in the data corresponds to all possible combinations of the State and Industry variables, which means they are intrinsically crossed with each other. When one key variable is nested within another, such as regional areas within a state, this is considered to be a hierarchical structure.\nThe spec argument in as_shared_tsibble() provides a means to construct hybrid linking, that incorporates hierarchical and categorical linking. A symbolic formula can be passed to the spec argument, to define the crossing and/or nesting relationships among the key variables. Adopting Wilkinson and Rogers (1973)’s notation for factorial models, the spec follows the / and * operator conventions to declare nesting and crossing variables, respectively. The spec for the aus_retail data is therefore specified as State * Industry or Industry * State, which is the default for the presence of multiple key variables. If there is a hierarchy in the data, using / is required to indicate the parent-child relation, for a strictly one directional parent/child.\nTo illustrate nesting and crossing we use the tourism_monthly dataset (Tourism Research Australia 2020) packaged in tsibbletalk. It contains monthly domestic overnight trips across Australia. The key is comprised of three identifying variables: State, Region, and Purpose (of the trip), in particular State nesting of Region, crossed together with Purpose. This specification can be translated as follows:\n\n\nlibrary(tsibble)\nlibrary(tsibbletalk)\ntourism_shared <- tourism_monthly %>% \n  as_shared_tsibble(spec = (State / Region) * Purpose)\n\n\n\n\n\nFigure 2: Snapshot of exploring an ensemble of linked plots of the Australian tourism data, built on a object. It also illustrates persistent linked brushing to compare two groups.\n\n\n\nThere is a three-level hierarchy: the root node is implicitly Australia, geographically disaggregated to states, and lower-level tourism regions. A new handy function plotly_key_tree() has been implemented to help explore the hierarchy. It interprets hierarchies in the shared tsibble’s spec as a tree view, built with plotly. The following code line produces the linked tree diagram (left panel of Figure 2). The visual for the tree hierarchy detangles a group of related series and provides a bird’s eye view of the data organization.\n\n\np_l <- plotly_key_tree(tourism_shared, height = 1100, width = 800)\n\n\nThe tree plot provides the graphics skeleton, upon which the rest of the data plots can be attached. In this example, small multiples of line plots are placed at the top right of Figure 2 to explore the temporal trend across regions by the trip purpose. The shared tsibble data can be directly piped into ggplot2 code to create this.\n\n\nlibrary(ggplot2)\np_tr <- tourism_shared %>%\n  ggplot(aes(x = Month, y = Trips)) +\n  geom_line(aes(group = Region), alpha = .5, size = .4) +\n  facet_wrap(~ Purpose, scales = \"free_y\") +\n  scale_x_yearmonth(date_breaks = \"5 years\", date_labels = \"%Y\")\n\n\nThese line plots are heavily overplotted. To tease apart structure in the multiple time series, the features() function computes interesting characteristics, including the measures of trend and seasonality. These are displayed in the scatterplot at the bottom right, where one dot represents one series.\n\n\nlibrary(feasts)\ntourism_feat <- tourism_shared %>%\n  features(Trips, feat_stl)\np_br <- tourism_feat %>%\n  ggplot(aes(x = trend_strength, y = seasonal_strength_year)) +\n  geom_point(aes(group = Region), alpha = .8, size = 2)\n\n\nThere is one final step, to compose the three plots into an ensemble of coordinated views for exploration, shown in Figure 2. (This is the interactive realization of Figure 1). \n\n\nlibrary(plotly)\nsubplot(p_l,\n  subplot(\n    ggplotly(p_tr, tooltip = \"Region\", width = 1100),\n    ggplotly(p_br, tooltip = \"Region\", width = 1100),\n    nrows = 2),\n  widths = c(.4, .6)) %>%\n  highlight(dynamic = TRUE)\n\n\nSince all plots are created from one shared tsibble data source, they are self-linking views. Nodes, lines, and points are hoverable and clickable. Given the spec, clicking either one element in any plot highlights all points that match the Region category, that is, categorical linking. Figure 2 is a static view of an interactive exploration. The steps in getting to this point were:\nA branch of the tree corresponding to Western Australia was first selected. (The names of the regions are a little odd, which is a quirk of the data set, but all four areas, Australia’s South West, …., correspond to tourist destinations in Western Australia. Hovering over the node on the branch brings up the state name.) This generated the response in the line plots and the scatterplot that colored corresponding time series and points as blue.\nTo enable persistent selection, in oder to compare regions or states, “Shift” and click on the tree was done, after switching the color to red. This generated the response that points and time series corresponding to Sydney were highlighted in red.\nHovering over the points brings up the label for Sydney.\nDomestic tourism sees Sydney as one of the most popular destinations in the realm of business and friends visiting over the years. Despite the relatively weaker performance in Western Australia, Australia’s North West region sees a strongest upward trend in business, bypassing Sydney in some years.\nIn summary, shared tsibble data nicely bridges between the crosstalk and tidyverts ecosystems for temporal data using the common “key”. The as_shared_tsibble() provides a symbolic user interface for the effortless construction of a hybrid of hierarchical and categorical linking between plots. The plotly_key_tree() function, in turn, decodes the hierarchical specification to plot a tree for data overview and navigation, when accompanied by more detailed plots.\nSlicing and dicing time\nAn important aspect of temporal data is the time context. Time has a cyclical structure, that may correspond to seasonal patterns to be discovered. The index component of the (shared) tsibble data forms the basis for exploring seasonality. To investigate for periodic or aperiodic patterns, series should be wrapped on themselves, where the index is broken into temporal components like quarter or day. We shall explore this with pedestrian traffic in Melbourne, Australia.\n\n\n\nFigure 3: Snapshots wrapping after slicing the data at different intervals, (a) none, (b) daily and (c) weekly. This type of interaction is made possible with Shiny elements.\n\n\n\nThe city of Melbourne has sensors installed at various locations, to record hourly counts of pedestrians, in order to capture the daily rhythms of the downtown (City of Melbourne 2020). Figure 3 shows the first five months of 2020 foot traffic at four different locations, for three different time slices, daily, weekly and full five months. Plot ?? shows hourly counts from January to May on an absolute timeline, facetted by locations. The stage 3 COVID-19 lockdown, on March 16, is marked by a change of color. (The pre-lockdown period is colored with dark green and lockdown with orange.) We can see a significant decline in foot traffic at all four locations. QV Market is less affected probably because this is a major produce market, an essential service that continued to operate. Bourke St, a major shopping center, sees a gradual uptick in the last weeks of the period indicating that people were getting back into the shops.\nFigure ?? and ?? show the slicing and wrapping of the series into daily and weekly sections, respectively. Multiple seasonalities pop out. There tends to be a daily pattern, especially visible at the main train station, Southern Cross Station. There is also a weekday vs weekend pattern, also most visible at Southern Cross Station. These seasonal patterns are still present during the lockdown, but the magnitude is greatly reduced. Numbers are also down at the produce market and the shopping center. Birrarung Marr is the most affected. This is the location of special events, and it is clear that these have completely disappeared during the lockdown.\nThe wrapping procedure involves slicing the time index into seasonal periods of interest, and the result is diced time. For example, hourly pedestrian data can be decomposed into 24-hour blocks, which then overlays the counts for all respective days, as done in plot ??. For exploration, this slice position should be controlled interactively, so that many different slices can be examined rapidly. This can be achieved using shiny, with the functions provided in the tsibbletalk.\nThis shiny module, decoupled to tsibbleWrapUI() and tsibbleWrapServer(), presents a clean interface and forms a reusable component that could be embedded in any shiny application. In general, a shiny module provides a vehicle for modularising shiny applications, relevant for both users and developers. As with all shiny modules, the first argument in both functions in tsibbletalk requires a user-supplied id string that must be unique. The UI function tsibbleWrapUI() simply shows a slider that animates or controls the number of periods to be diced. The workhorse is the server function tsibbleWrapServer(), encapsulating the algorithm that transforms data and sends messages to update the plot accordingly. The plot argument expects a ggplot or plotly object, where one can plot data using either lines or other graphical elements (such as boxplots). As the function name suggests, a (shared) tsibble is needed to start the engine, so that the time index can be retrieved for dissection. The period option semantically takes a desired number of seasonal periods to be shifted, for example data shifted by “1 day”, “2 days”, or “1 week”, etc. In other words, the period defines the grind level. For date-times (represented by POSIXt), the granularity ranges from fine “day” to a much coarser “year”. The following code snippet generates Figure 3. The creation of the pedestrian20 data is available in supplementary R files.\n\n\n\n\n\nlibrary(shiny)\np_line <- pedestrian20 %>%\n  ggplot(aes(x = Date_Time, y = Count, colour = Lockdown)) +\n  geom_line(size = .3) +\n  facet_wrap(~ Sensor, scales = \"free_y\") +\n  labs(x = \"Date Time\") +\n  scale_colour_brewer(palette = \"Dark2\") +\n  theme(legend.position = \"none\")\n\nui <- fluidPage(\n  tsibbleWrapUI(\"dice\")\n)\nserver <- function(input, output, session) {\n  tsibbleWrapServer(\"dice\", ggplotly(p_line, height = 700), period = \"1 day\")\n}\nshinyApp(ui, server)\n\n\nFigure ?? corresponds to the initial state, with the slider incremented by 1-day units. The “play” button near the end of the slider can automatically animate the slicing and dicing process, walking the viewer through all 24 hours of the 152 days. Alternatively, users can drag the slider to examine selected slices.\nIn response to the slider input, the plot will be updated and loaded with newly transformed data. At its core, keeping the application as performant as possible is the top priority. Without completely redrawing the plot, the plotlyProxy() react method is invoked internally for talking to shiny. The underlying tsibble data is being called back and processed in R. Only transformed data gets fed back to the shiny server, for updating with resetting the x-axis ranges and breaks. The other plot configurations, such as marks, y-axes, and layouts, are cached and used as is.\nThe new shiny module exploits the temporal aspect for a tsibble object, available through the index attribute. It allows users to slide through relative periods to digest seasonal behaviors, with a nimble user experience.\nSummary\nAt the heart of the tsibbletalk package is a blending of the best bits from tsibble, crosstalk, plotly, and shiny.\nThe as_shared_tsibble() turns a tsibble object to a shared data class, with an option to express any nesting and crossing structures from the key attribute. If nesting is found in the data, the plotly_key_tree() creates an interactive hierarchical tree to help with the data overview. This sets the stage for hierarchical and categorical linking between multiple views from one shared tsibble.\nA new shiny module, tsibbleWrapUI() and tsibbleWrapServer(), provides a lens for looking at temporal aspects of a tsibble, in particular seasonal or cyclical variations. The slicing and dicing technique efficiently wrap time lines for user-defined plots. The plotlyProxy() react method makes it possible to send wrapped data to the server and amend the plot straight way.\nReferences\n\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-050.zip\nCRAN packages used\ntsibbletalk, tsibble, tsibbledata, tidyverse, feasts, fable, shiny, htmlwidgets, plotly, rbokeh, leaflet, crosstalk, grDevices, loon, ggplot2\nCRAN Task Views implied by cited packages\nMissingData, Spatial, TeachingStatistics, TimeSeries, WebTechnologies\n\n\nW. Chang, J. Cheng, J. Allaire, Y. Xie and J. McPherson. Shiny: Web application framework for r. 2020. URL https://CRAN.R-project.org/package=shiny. R package version 1.5.0.\n\n\nJ. Cheng. Crosstalk: Inter-widget interactivity for HTML widgets. 2020. URL https://CRAN.R-project.org/package=crosstalk. R package version 1.1.0.1.\n\n\nJ. Cheng, B. Karambelkar and Y. Xie. Leaflet: Create interactive web maps with the JavaScript ’leaflet’ library. 2019. URL https://CRAN.R-project.org/package=leaflet. R package version 2.0.3.\n\n\nX. Cheng, D. Cook and H. Hofmann. Enabling interactivity on displays of multivariate time series and longitudinal data. Journal of Computational and Graphical Statistics, 25(4): 1057–1076, 2016. URL https://www.tandfonline.com/doi/full/10.1080/10618600.2015.1105749.\n\n\nCity of Melbourne. Pedestrian volume in melbourne. City of Melbourne, Australia, 2020. URL http://www.pedestrian.melbourne.vic.gov.au.\n\n\nR. Hafen and Continuum Analytics, Inc. Rbokeh: R interface for bokeh. 2020. URL https://CRAN.R-project.org/package=rbokeh. R package version 0.5.1.\n\n\nR. J. Hyndman and G. Athanasopoulos. Forecasting: Principles and practice. Melbourne, Australia: OTexts, 2017. URL OTexts.org/fpp2.\n\n\nM. O’Hara-Wild, R. Hyndman and E. Wang. Fable: Forecasting models for tidy time series. 2020a. URL https://CRAN.R-project.org/package=fable. R package version 0.2.1.\n\n\nM. O’Hara-Wild, R. Hyndman and E. Wang. Feasts: Feature extraction and statistics for time series. 2020b. URL https://CRAN.R-project.org/package=feasts. R package version 0.1.5.\n\n\nM. O’Hara-Wild, R. Hyndman and E. Wang. Tsibbledata: Diverse datasets for ’tsibble’. 2020c. URL https://CRAN.R-project.org/package=tsibbledata. R package version 0.2.0.\n\n\nC. Sievert. Interactive web-based data visualization with r, plotly, and shiny. Chapman; Hall/CRC, 2020. URL https://plotly-r.com.\n\n\nTourism Research Australia. Australian domestic overnight trips. Tourism Research Australia, Australia, 2020. URL https://www.tra.gov.au.\n\n\nA. Unwin and P. Valero-Mora. Ensemble Graphics. Journal of Computational and Graphical Statistics, 27(1): 157–165, 2018. URL https://www.tandfonline.com/doi/full/10.1080/10618600.2017.1383264 [online; last accessed April 23, 2019].\n\n\nR. Vaidyanathan, Y. Xie, J. Allaire, J. Cheng and K. Russell. Htmlwidgets: HTML widgets for r. 2019. URL https://CRAN.R-project.org/package=htmlwidgets. R package version 1.5.1.\n\n\nA. Waddell and R. W. Oldford. Loon: Interactive statistical data visualization. 2020. URL https://CRAN.R-project.org/package=loon. R package version 1.3.1.\n\n\nE. Wang, D. Cook and R. J. Hyndman. A new tidy data structure to support exploration and modeling of temporal data. Journal of Computational and Graphical Statistics, 29(3): 466–478, 2020. DOI 10.1080/10618600.2019.1695624.\n\n\nH. Wickham. Tidy data. Journal of Statistical Software, 59(10): 1–23, 2014.\n\n\nH. Wickham, M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. François, G. Grolemund, A. Hayes, L. Henry, J. Hester, et al. Welcome to the tidyverse. Journal of Open Source Software, 4(43): 1686, 2019. URL https://doi.org/10.21105/joss.01686.\n\n\nG. N. Wilkinson and C. E. Rogers. Symbolic description of factorial models for analysis of variance. Journal of the Royal Statistical Society. Series C (Applied Statistics), 22(3): 392–399, 1973. URL http://www.jstor.org/stable/2346786.\n\n\nY. Xie, H. Hofmann and X. Cheng. Reactive programming for interactive graphics. Statistical Science, 29(2): 201–213, 2014. URL http://projecteuclid.org/euclid.ss/1408368571.\n\n\n\n\n",
    "preview": "articles/RJ-2021-050/figure/highlight-retail-1.png",
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1152
  },
  {
    "path": "articles/RJ-2021-051/",
    "title": "Automating Reproducible, Collaborative Clinical Trial Document Generation with the listdown Package",
    "description": "The conveyance of clinical trial explorations and analysis results from a statistician to a clinical investigator is a critical component of the drug development and clinical research cycle. Automating the process of generating documents for data descriptions, summaries, exploration, and analysis allows the statistician to provide a more comprehensive view of the information captured by a clinical trial, and efficient generation of these documents allows the statistican to focus more on the conceptual development of a trial or trial analysis and less on the implementation of the summaries and results on which decisions are made. This paper explores the use of the listdown package for automating reproducible documents in clinical trials that facilitate the collaboration between statisticians and clinicians as well as defining an analysis pipeline for document generation.",
    "author": [
      {
        "name": "Michael Kane",
        "url": {}
      },
      {
        "name": "Xun Jiang",
        "url": {}
      },
      {
        "name": "Simon Urbanek",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-051.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-052/",
    "title": "Towards a Grammar for Processing Clinical Trial Data",
    "description": "The goal of this paper is to help define a path toward a grammar for processing clinical trials by a) defining a format in which we would like to represent data from standardized clinical trial data b) describing a standard set of operations to transform clinical trial data into this format, and c) to identify a set of verbs and other functionality to facilitate data processing and encourage reproducibility in the processing of these data. It provides a background on standard clinical trial data and goes through a simple preprocessing example illustrating the value of the proposed approach through the use of the forceps package, which is currently being used for data of this kind.",
    "author": [
      {
        "name": "Michael J. Kane",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-052.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-054/",
    "title": "Regularized Transformation Models: The tramnet Package",
    "description": "The [tramnet](https://CRAN.R-project.org/package=tramnet) package implements regularized linear transformation models by combining the flexible class of transformation models from [tram](https://CRAN.R-project.org/package=tram) with constrained convex optimization implemented in [CVXR](https://CRAN.R-project.org/package=CVXR). Regularized transformation models unify many existing and novel regularized regression models under one theoretical and computational framework. Regularization strategies implemented for transformation models in tramnet include the Lasso, ridge regression, and the elastic net and follow the parameterization in [glmnet](https://CRAN.R-project.org/package=glmnet). Several functionalities for optimizing the hyperparameters, including model-based optimization based on the [mlrMBO](https://CRAN.R-project.org/package=mlrMBO) package, are implemented. A multitude of S3 methods is deployed for visualization, handling, and simulation purposes. This work aims at illustrating all facets of tramnet in realistic settings and comparing regularized transformation models with existing implementations of similar models.",
    "author": [
      {
        "name": "Lucas Kook, Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-054.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-026/",
    "title": "SEEDCCA: An Integrated R-Package for Canonical Correlation Analysis and Partial Least Squares",
    "description": "Canonical correlation analysis (CCA) has a long history as an explanatory statistical method in high-dimensional data analysis and has been successfully applied in many scientific fields such as chemometrics, pattern recognition, genomic sequence analysis, and so on. The so-called seedCCA is a newly developed R package that implements not only the standard and seeded CCA but also partial least squares. The package enables us to fit CCA to large-$p$ and small-$n$ data. The paper provides a complete guide. Also, the seeded CCA application results are compared with the regularized CCA in the existing R package. It is believed that the package, along with the paper, will contribute to high-dimensional data analysis in various science field practitioners and that the statistical methodologies in multivariate analysis become more fruitful.",
    "author": [
      {
        "name": "Bo-Young Kim, Researcher",
        "url": {}
      },
      {
        "name": "Yunju Im, Postdoctoral Associate",
        "url": {}
      },
      {
        "name": "Jae Keun Yoo, Professor",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-027/",
    "title": "npcure: An R Package for Nonparametric Inference in Mixture Cure Models",
    "description": "Mixture cure models have been widely used to analyze survival data with a cure fraction. They assume that a subgroup of the individuals under study will never experience the event (cured subjects). So, the goal is twofold: to study both the cure probability and the failure time of the uncured individuals through a proper survival function (latency). The R package npcure implements a completely nonparametric approach for estimating these functions in mixture cure models, considering right-censored survival times. Nonparametric estimators for the cure probability and the latency as functions of a covariate are provided. Bootstrap bandwidth selectors for the estimators are included. The package also implements a nonparametric covariate significance test for the cure probability, which can be applied with a continuous, discrete, or qualitative covariate.",
    "author": [
      {
        "name": "Ana López-Cheda",
        "url": {}
      },
      {
        "name": "M. Amalia Jácome",
        "url": {}
      },
      {
        "name": "Ignacio López-de-Ullibarri",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-027.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-028/",
    "title": "JMcmprsk: An R Package for Joint Modelling of Longitudinal and Survival Data with Competing Risks",
    "description": "In this paper, we describe an R package named **JMcmprsk**, for joint modelling of longitudinal and survival data with competing risks. The package in its current version implements two joint models of longitudinal and survival data proposed to handle competing risks survival data together with continuous and ordinal longitudinal outcomes respectively [@elashoff2008joint; @li2010joint]. The corresponding R implementations are further illustrated with real examples. The package also provides simulation functions to simulate datasets for joint modelling with continuous or ordinal outcomes under the competing risks scenario, which provide useful tools to validate and evaluate new joint modelling methods.",
    "author": [
      {
        "name": "Hong Wang",
        "url": {}
      },
      {
        "name": "Ning Li",
        "url": {}
      },
      {
        "name": "Shanpeng Li",
        "url": {}
      },
      {
        "name": "Gang Li\\*",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-028.zip\n\n\nR. M. Elashoff, G. Li and N. Li. A joint model for longitudinal measurements and survival data in the presence of multiple failure types. Biometrics, 64(3): 762–771, 2008.\n\n\nN. Li, R. M. Elashoff, G. Li and J. Saver. Joint modeling of longitudinal ordinal data and competing risks survival times and analysis of the NINDS rt-PA stroke trial. Statistics in medicine, 29(5): 546–557, 2010.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-029/",
    "title": "Wide-to-tall Data Reshaping Using Regular Expressions and the nc Package",
    "description": "Regular expressions are powerful tools for extracting tables from non-tabular text data. Capturing regular expressions that describe the information to extract from column names can be especially useful when reshaping a data table from wide (few rows with many regularly named columns) to tall (fewer columns with more rows). We present the R package nc (short for named capture), which provides functions for wide-to-tall data reshaping using regular expressions. We describe the main new ideas of nc, and provide detailed comparisons with related R packages (stats, utils, data.table, tidyr, tidyfast, tidyfst, reshape2, cdata).",
    "author": [
      {
        "name": "Toby Dylan Hocking",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-029.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-030/",
    "title": "Linear Regression with Stationary Errors: the R Package slm",
    "description": "This paper introduces the R package [slm](https://CRAN.R-project.org/package=slm), which stands for Stationary Linear Models. The package contains a set of statistical procedures for linear regression in the general context where the error process is strictly stationary with a short memory. We work in the setting of [@hannan1973central], who proved the asymptotic normality of the (normalized) least squares estimators (LSE) under very mild conditions on the error process. We propose different ways to estimate the asymptotic covariance matrix of the LSE and then to correct the type I error rates of the usual tests on the parameters (as well as confidence intervals). The procedures are evaluated through different sets of simulations.",
    "author": [
      {
        "name": "Emmanuel Caron",
        "url": {}
      },
      {
        "name": "Jérôme Dedecker",
        "url": {}
      },
      {
        "name": "Bertrand Michel",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-030.zip\n\n\nE. J. Hannan. Central limit theorems for time series regression. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete, 26(2): 157–170, 1973.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-031/",
    "title": "exPrior: An R Package for the Formulation of Ex-Situ Priors",
    "description": "The [exPrior](https://CRAN.R-project.org/package=exPrior) package implements a procedure for formulating informative priors of geostatistical properties for a target field site, called *ex-situ priors* and introduced in [@Cucchi2019]. The procedure uses a Bayesian hierarchical model to assimilate multiple types of data coming from multiple sites considered as similar to the target site. This prior summarizes the information contained in the data in the form of a probability density function that can be used to better inform further geostatistical investigations at the site. The formulation of the prior uses ex-situ data, where the data set can either be gathered by the user or come in the form of a structured database. The package is designed to be flexible in that regard. For illustration purposes and for easiness of use, the package is ready to be used with the worldwide hydrogeological parameter database (WWHYPDA) [@Comunian2009].",
    "author": [
      {
        "name": "Falk Heße",
        "url": {}
      },
      {
        "name": "Karina Cucchi",
        "url": {}
      },
      {
        "name": "Nura Kawa",
        "url": {}
      },
      {
        "name": "Yoram Rubin",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\n\n\nA. Comunian and P. Renard. Introducing wwhypda: A world-wide collaborative hydrogeological parameters database. Hydrogeology Journal, 17(2): 481–489, 2009. DOI 10.1007/s10040-008-0387-x.\n\n\nK. Cucchi, F. Heße, N. Kawa, C. Wang and Y. Rubin. Ex-situ priors: A Bayesian hierarchical framework for defining informative prior distributions in hydrogeology. Advances in Water Resources, 126: 65–78, 2019. DOI 10.1016/j.advwatres.2019.02.003.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-032/",
    "title": "clustcurv: An R Package for Determining Groups in Multiple Curves ",
    "description": "In many situations, it could be interesting to ascertain whether groups of curves can be performed, especially when confronted with a considerable number of curves. This paper introduces an R package, known as [clustcurv](https://CRAN.R-project.org/package=clustcurv), for determining clusters of curves with an automatic selection of their number. The package can be used for determining groups in multiple survival curves as well as for multiple regression curves. Moreover, it can be used with large numbers of curves. An illustration of the use of clustcurv is provided, using both real data examples and artificial data.",
    "author": [
      {
        "name": "Nora M. Villanueva",
        "url": {}
      },
      {
        "name": "Marta Sestelo",
        "url": {}
      },
      {
        "name": "Luís Meira-Machado",
        "url": {}
      },
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-032.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-033/",
    "title": "Benchmarking R packages for Calculation of Persistent Homology",
    "description": "Several persistent homology software libraries have been implemented in R. Specifically, the Dionysus, GUDHI, and Ripser libraries have been wrapped by the **TDA** and **TDAstats** CRAN packages. These software represent powerful analysis tools that are computationally expensive and, to our knowledge, have not been formally benchmarked. Here, we analyze runtime and memory growth for the 2 R packages and the 3 underlying libraries. We find that datasets with less than 3 dimensions can be evaluated with persistent homology fastest by the GUDHI library in the **TDA** package. For higher-dimensional datasets, the Ripser library in the TDAstats package is the fastest. Ripser and **TDAstats** are also the most memory-efficient tools to calculate persistent homology.",
    "author": [
      {
        "name": "Eashwar V. Somasundaram",
        "url": {}
      },
      {
        "name": "Shael E. Brown",
        "url": {}
      },
      {
        "name": "Adam Litzler",
        "url": {}
      },
      {
        "name": "Jacob G. Scott",
        "url": {}
      },
      {
        "name": "Raoul R. Wadhwa",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-033.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-034/",
    "title": "Statistical Quality Control with the qcr Package",
    "description": "The R package [qcr](https://CRAN.R-project.org/package=qcr) for Statistical Quality Control (SQC) is introduced and described. It includes a comprehensive set of univariate and multivariate SQC tools that completes and increases the SQC techniques available in R. Apart from integrating different R packages devoted to SQC ([qcc](https://CRAN.R-project.org/package=qcc), [MSQC](https://CRAN.R-project.org/package=MSQC)), [qcr](https://CRAN.R-project.org/package=qcr) provides nonparametric tools that are highly useful when Gaussian assumption is not met. This package computes standard univariate control charts for individual measurements, $\\bar{x}$, $S$, $R$, $p$, $np$, $c$, $u$, EWMA, and CUSUM. In addition, it includes functions to perform multivariate control charts such as Hotelling T$^2$, MEWMA and MCUSUM. As representative features, multivariate nonparametric alternatives based on data depth are implemented in this package: $r$, $Q$ and $S$ control charts. The [qcr](https://CRAN.R-project.org/package=qcr) library also estimates the most complete set of capability indices from first to the fourth generation, covering the nonparametric alternatives, and performing the corresponding capability analysis graphical outputs, including the process capability plots. Moreover, Phase I and II control charts for functional data are included.",
    "author": [
      {
        "name": "Miguel Flores",
        "url": {}
      },
      {
        "name": "Rubén Fernández-Casal",
        "url": {}
      },
      {
        "name": "Salvador Naya",
        "url": {}
      },
      {
        "name": "Javier Tarrío-Saavedra",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-034.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-035/",
    "title": "pdynmc: A Package for Estimating Linear Dynamic Panel Data Models Based on Nonlinear Moment Conditions",
    "description": "This paper introduces [pdynmc](https://CRAN.R-project.org/package=pdynmc), an R package that provides users sufficient flexibility and precise control over the estimation and inference in linear dynamic panel data models. The package primarily allows for the inclusion of nonlinear moment conditions and the use of iterated GMM; additionally, visualizations for data structure and estimation results are provided. The current implementation reflects recent developments in literature, uses sensible argument defaults, and aligns commercial and noncommercial estimation commands. Since the understanding of the model assumptions is vital for setting up plausible estimation routines, we provide a broad introduction of linear dynamic panel data models directed towards practitioners before concisely describing the functionality available in pdynmc regarding instrument type, covariate type, estimation methodology, and general configuration. We then demonstrate the functionality by revisiting the popular firm-level dataset of @AreBon1991.",
    "author": [
      {
        "name": "Markus Fritsch",
        "url": {}
      },
      {
        "name": "Andrew Adrian Yu Pua",
        "url": {}
      },
      {
        "name": "Joachim Schnurbus",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\n\n\nM. Arellano and S. Bond. Some Tests of Specification for Panel Data: Monte Carlo Evidence and an Application to Employment Equations. The Review of Economic Studies, 58(2): 277–297, 1991. URL https://doi.org/10.2307/2297968.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-036/",
    "title": "DChaos: An R Package for Chaotic Time Series Analysis",
    "description": "Chaos theory has been hailed as a revolution of thoughts and attracting ever-increasing attention of many scientists from diverse disciplines. Chaotic systems are non-linear deterministic dynamic systems which can behave like an erratic and apparently random motion. A relevant field inside chaos theory is the detection of chaotic behavior from empirical time-series data. One of the main features of chaos is the well-known initial-value sensitivity property. Methods and techniques related to testing the hypothesis of chaos try to quantify the initial-value sensitive property estimating the so-called Lyapunov exponents. This paper describes the main estimation methods of the Lyapunov exponent from time series data. At the same time, we present the DChaos library. R users may compute the delayed-coordinate embedding vector from time series data, estimates the best-fitted neural net model from the delayed-coordinate embedding vectors, calculates analytically the partial derivatives from the chosen neural nets model. They can also obtain the neural net estimator of the Lyapunov exponent from the partial derivatives computed previously by two different procedures and four ways of subsampling by blocks. To sum up, the DChaos package allows the R users to test robustly the hypothesis of chaos in order to know if the data-generating process behind time series behaves chaotically or not. The package's functionality is illustrated by examples.",
    "author": [
      {
        "name": "Julio E. Sandubete",
        "url": {}
      },
      {
        "name": "Lorenzo Escot",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-036.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-038/",
    "title": "IndexNumber: An R Package for Measuring the Evolution of Magnitudes",
    "description": "Index numbers are descriptive statistical measures useful in economic settings for comparing simple and complex magnitudes registered, usually in two time periods. Although this theory has a large history, it still plays an important role in modern today's societies where big amounts of economic data are available and need to be analyzed. After a detailed revision on classical index numbers in literature, this paper is focused on the description of the R package [IndexNumber](https://CRAN.R-project.org/package=IndexNumber) with strong capabilities for calculating them. Two of the four real data sets contained in this library are used for illustrating the determination of the index numbers in this work. Graphical tools are also implemented in order to show the time evolution of considered magnitudes simplifying the interpretation of the results.",
    "author": [
      {
        "name": "Alejandro Saavedra-Nieves",
        "url": {}
      },
      {
        "name": "Paula Saavedra-Nieves",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-038.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-040/",
    "title": "ROBustness In Network (robin): an R Package for Comparison and Validation of Communities ",
    "description": "In network analysis, many community detection algorithms have been developed. However, their implementation leaves unaddressed the question of the statistical validation of the results. Here, we present [robin](https://CRAN.R-project.org/package=robin) (ROBustness In Network), an R package to assess the robustness of the community structure of a network found by one or more methods to give indications about their reliability. The procedure initially detects if the community structure found by a set of algorithms is statistically significant and then compares two selected detection algorithms on the same graph to choose the one that better fits the network of interest. We demonstrate the use of our package on the American College Football benchmark dataset.",
    "author": [
      {
        "name": "Valeria Policastro (*package author and creator*)",
        "url": {}
      },
      {
        "name": "Dario Righelli",
        "url": {}
      },
      {
        "name": "Annamaria Carissimo",
        "url": {}
      },
      {
        "name": "Luisa Cutillo",
        "url": {}
      },
      {
        "name": "Italia De Feis",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-041/",
    "title": "Finding Optimal Normalizing Transformations via bestNormalize",
    "description": "The bestNormalize R package was designed to help users find a transformation that can effectively normalize a vector regardless of its actual distribution. Each of the many normalization techniques that have been developed has its own strengths and weaknesses, and deciding which to use until data are fully observed is difficult or impossible. This package facilitates choosing between a range of possible transformations and will automatically return the best one, i.e., the one that makes data look the *most* normal. To evaluate and compare the normalization efficacy across a suite of possible transformations, we developed a statistic based on a goodness of fit test divided by its degrees of freedom. Transformations can be seamlessly trained and applied to newly observed data and can be implemented in conjunction with caret and recipes for data preprocessing in machine learning workflows. Custom transformations and normalization statistics are supported.",
    "author": [
      {
        "name": "Ryan A. Peterson",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-041.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-042/",
    "title": "Package wsbackfit for Smooth Backfitting Estimation of Generalized Structured Models",
    "description": "A package is introduced that provides the weighted smooth backfitting estimator for a large family of popular semiparametric regression models. This family is known as *generalized structured models*, comprising, for example, generalized varying coefficient model, generalized additive models, mixtures, potentially including parametric parts. The kernel-based weighted smooth backfitting belongs to the statistically most efficient procedures for this model class. Its asymptotic properties are well-understood thanks to the large body of literature about this estimator. The introduced weights allow for the inclusion of sampling weights, trimming, and efficient estimation under heteroscedasticity. Further options facilitate easy handling of aggregated data, prediction, and the presentation of estimation results. Cross-validation methods are provided which can be used for model and bandwidth selection.[^1]",
    "author": [
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      },
      {
        "name": "María Xosé Rodríguez-Álvarez",
        "url": {}
      },
      {
        "name": "Stefan Sperlich",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-042.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-043/",
    "title": "RLumCarlo: Simulating Cold Light using Monte Carlo Methods",
    "description": "The luminescence phenomena of insulators and semiconductors (e.g., natural minerals such as quartz) have various application domains. For instance, Earth Sciences and archaeology exploit luminescence as a dating method. Herein, we present the R package [RLumCarlo](https://CRAN.R-project.org/package=RLumCarlo) implementing sets of luminescence models to be simulated with Monte Carlo (MC) methods. MC methods make a powerful ally to all kinds of simulation attempts involving stochastic processes. Luminescence production is such a stochastic process in the form of charge (electron-hole pairs) interaction within insulators and semiconductors. To simulate luminescence-signal curves, we distribute single and independent MC processes to virtual MC clusters. [RLumCarlo](https://CRAN.R-project.org/package=RLumCarlo) comes with a modularized design and consistent user interface: (1) C++ functions represent the modeling core and implement models for specific stimulations modes. (2) R functions give access to combinations of models and stimulation modes, start the simulation and render terminal and graphical feedback. The combination of MC clusters supports the simulation of complex luminescence phenomena.",
    "author": [
      {
        "name": "Sebastian Kreutzer",
        "url": {}
      },
      {
        "name": "Johannes Friedrich",
        "url": {}
      },
      {
        "name": "Vasilis Pagonis",
        "url": {}
      },
      {
        "name": "Christian Laag",
        "url": {}
      },
      {
        "name": "Ena Rajovic",
        "url": {}
      },
      {
        "name": "Christoph Schmidt",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-043.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:56+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-027/",
    "title": "spinifex: An R Package for Creating a Manual Tour of Low-dimensional Projections of Multivariate Data",
    "description": "Dynamic low-dimensional linear projections of multivariate data collectively known as tours provide an important tool for exploring multivariate data and models. The R package tourr provides functions for several types of tours: grand, guided, little, local and frozen. Each of these can be viewed dynamically, or saved into a data object for animation. This paper describes a new package, spinifex, which provides a manual tour of multivariate data where the projection coefficient of a single variable is controlled. The variable is rotated fully into the projection, or completely out of the projection. The resulting sequence of projections can be displayed as an animation, with functions from either the plotly or gganimate packages. By varying the coefficient of a single variable, it is possible to explore the sensitivity of structure in the projection to that variable. This is particularly useful when used with a projection pursuit guided tour to simplify and understand the solution. The use of the manual tour is applied particle physics data to illustrate the sensitivity of structure in a projection to specific variable contributions.",
    "author": [
      {
        "name": "Nicholas Spyrison",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      }
    ],
    "date": "2020-09-13",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-027.zip\nCRAN packages used\ntourr, spinifex, plotly, gganimate, ggplot2, shiny, knitr, rmarkdown\nCRAN Task Views implied by cited packages\nTeachingStatistics, ReproducibleResearch, WebTechnologies, Graphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-007/",
    "title": "The Rockerverse: Packages and Applications for Containerisation with R",
    "description": "The Rocker Project provides widely used Docker images for R across different application scenarios. This article surveys downstream projects that build upon the Rocker Project images and presents the current state of R packages for managing Docker images and controlling containers. These use cases cover diverse topics such as package development, reproducible research, collaborative work, cloud-based data processing, and production deployment of services. The variety of applications demonstrates the power of the Rocker Project specifically and containerisation in general. Across the diverse ways to use containers, we identified common themes: reproducible environments, scalability and efficiency, and portability across clouds. We conclude that the current growth and diversification of use cases is likely to continue its positive impact, but see the need for consolidating the Rockerverse ecosystem of packages, developing common practices for applications, and exploring alternative containerisation software.",
    "author": [
      {
        "name": "Daniel Nüst",
        "url": {}
      },
      {
        "name": "Dirk Eddelbuettel",
        "url": {}
      },
      {
        "name": "Dom Bennett",
        "url": {}
      },
      {
        "name": "Robrecht Cannoodt",
        "url": {}
      },
      {
        "name": "Dav Clark",
        "url": {}
      },
      {
        "name": "Gergely Daróczi",
        "url": {}
      },
      {
        "name": "Mark Edmondson",
        "url": {}
      },
      {
        "name": "Colin Fay",
        "url": {}
      },
      {
        "name": "Ellis Hughes",
        "url": {}
      },
      {
        "name": "Lars Kjeldgaard",
        "url": {}
      },
      {
        "name": "Sean Lopp",
        "url": {}
      },
      {
        "name": "Ben Marwick",
        "url": {}
      },
      {
        "name": "Heather Nolis",
        "url": {}
      },
      {
        "name": "Jacqueline Nolis",
        "url": {}
      },
      {
        "name": "Hong Ooi",
        "url": {}
      },
      {
        "name": "Karthik Ram",
        "url": {}
      },
      {
        "name": "Noam Ross",
        "url": {}
      },
      {
        "name": "Lori Shepherd",
        "url": {}
      },
      {
        "name": "Péter Sólymos",
        "url": {}
      },
      {
        "name": "Tyson Lee Swetnam",
        "url": {}
      },
      {
        "name": "Nitesh Turaga",
        "url": {}
      },
      {
        "name": "Charlotte Van Petegem",
        "url": {}
      },
      {
        "name": "Jason Williams",
        "url": {}
      },
      {
        "name": "Craig Willis",
        "url": {}
      },
      {
        "name": "Nan Xiao",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-007.zip\nCRAN packages used\nsys, stevedore, AzureContainers, googleCloudRunner, babelwhale, BiocManager, reticulate, Shiny, dockerfiler, dockr, liftr, checkpoint, renv, sf, rgdal, sanitizers, RSelenium, batchtools, googleComputeEngineR, future, plumber, drake, golem, analogsea, Rserve, svSocket, keras, DBI, dbplyr, dplyr, testthat, tinytest, ttdo, diffobj\nCRAN Task Views implied by cited packages\nModelDeployment, WebTechnologies, HighPerformanceComputing, ReproducibleResearch, Databases, NumericalMathematics, Spatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-008/",
    "title": "Linear Fractional Stable Motion with the rlfsm R Package",
    "description": "Linear fractional stable motion is a type of a stochastic integral driven by symmetric alpha-stable Lévy motion. The integral could be considered as a non-Gaussian analogue of the fractional Brownian motion. The present paper discusses R package rlfsm created for numerical procedures with the linear fractional stable motion. It is a set of tools for simulation of these processes as well as performing statistical inference and simulation studies on them. We introduce: tools that we developed to work with that type of motions as well as methods and ideas underlying them. Also we perform numerical experiments to show finite-sample behavior of certain estimators of the integral, and give an idea of how to envelope workflow related to the linear fractional stable motion in S4 classes and methods. Supplementary materials, including codes for numerical experiments, are available online. rlfsm could be found on CRAN and gitlab.",
    "author": [
      {
        "name": "Stepan Mazur",
        "url": {}
      },
      {
        "name": "Dmitry Otryakhin",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-008.zip\nCRAN packages used\nrlfsm, somebm, stabledist, stable, ggplot2\nCRAN Task Views implied by cited packages\nDistributions, Graphics, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-009/",
    "title": "ProjectManagement: an R Package for Managing Projects",
    "description": "Project management is an important body of knowledge and practices that comprises the planning, organisation and control of resources to achieve one or more pre-determined objectives. In this paper, we introduce ProjectManagement, a new R package that provides the necessary tools to manage projects in a broad sense, and illustrate its use by examples.",
    "author": [
      {
        "name": "Juan Carlos Gonçalves-Dosantos",
        "url": {}
      },
      {
        "name": "Ignacio García-Jurado",
        "url": {}
      },
      {
        "name": "Julián Costa",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-009.zip\nCRAN packages used\nPlotPrjNetworks, plan, ProjectManagement, triangle, plotly, igraph, kappalab, GameTheory, lpSolveAPI\nCRAN Task Views implied by cited packages\nOptimization, Distributions, gR, Graphics, Spatial, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-010/",
    "title": "gk: An R Package for the g-and-k and Generalised g-and-h Distributions",
    "description": "The g-and-k and (generalised) g-and-h distributions are flexible univariate distributions which can model highly skewed or heavy tailed data through only four parameters: location and scale, and two shape parameters influencing the skewness and kurtosis. These distributions have the unusual property that they are defined through their quantile function (inverse cumulative distribution function) and their density is unavailable in closed form, which makes parameter inference complicated. This paper presents the gk R package to work with these distributions. It provides the usual distribution functions and several algorithms for inference of independent identically distributed data, including the finite difference stochastic approximation method, which has not been used before for this problem.",
    "author": [
      {
        "name": "Dennis Prangle",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-010.zip\nCRAN packages used\ngk, microbenchmark, abc, EasyABC, Ecdat\nCRAN Task Views implied by cited packages\nBayesian, Distributions, Econometrics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-011/",
    "title": "Tools for Analyzing R Code the Tidy Way",
    "description": "With the current emphasis on reproducibility and replicability, there is an increasing need to examine how data analyses are conducted. In order to analyze the between researcher variability in data analysis choices as well as the aspects within the data analysis pipeline that contribute to the variability in results, we have created two R packages: matahari and tidycode. These packages build on methods created for natural language processing; rather than allowing for the processing of natural language, we focus on R code as the substrate of interest. The matahari package facilitates the logging of everything that is typed in the R console or in an R script in a tidy data frame. The tidycode package contains tools to allow for analyzing R calls in a tidy manner. We demonstrate the utility of these packages as well as walk through two examples.",
    "author": [
      {
        "name": "Lucy D’Agostino McGowan",
        "url": {}
      },
      {
        "name": "Sean Kross",
        "url": {}
      },
      {
        "name": "Jeffrey Leek",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-011.zip\nCRAN packages used\nmatahari, tidycode, tidyverse, tidytext, dplyr, purrr, wordcloud, data.table, gh\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, Databases, Finance, HighPerformanceComputing, ModelDeployment, TimeSeries, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-012/",
    "title": "rcosmo: R Package for Analysis of Spherical, HEALPix and Cosmological Data",
    "description": "The analysis of spatial observations on a sphere is important in areas such as geosciences, physics and embryo research, just to name a few. The purpose of the package rcosmo is to conduct efficient information processing, visualisation, manipulation and spatial statistical analysis of Cosmic Microwave Background (CMB) radiation and other spherical data. The package was developed for spherical data stored in the Hierarchical Equal Area isoLatitude Pixelation (Healpix) representation. rcosmo has more than 100 different functions. Most of them initially were developed for CMB, but also can be used for other spherical data as rcosmo contains tools for transforming spherical data in cartesian and geographic coordinates into the HEALPix representation. We give a general description of the package and illustrate some important functionalities and benchmarks.",
    "author": [
      {
        "name": "Daniel Fryer",
        "url": {}
      },
      {
        "name": "Ming Li",
        "url": {}
      },
      {
        "name": "Andriy Olenko",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-012.zip\nCRAN packages used\nsp, sphereplot, rgl, geosphere, SpherWave, SphericalCubature, RandomFields, geoR, Directional, gensphere, CircNNTSR, VecStatGraphs3D, sm, cosmoFns, CRAC, FITSio, spider, astro, rcosmo, microbenchmark\nCRAN Task Views implied by cited packages\nSpatial, ChemPhys, SpatioTemporal, Bayesian, Distributions, Graphics, Multivariate, NumericalMathematics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-013/",
    "title": "Variable Importance Plots—An Introduction to the vip Package",
    "description": "In the era of “big data”, it is becoming more of a challenge to not only build state-of-the-art by Brandon M. Greenwell, Bradley C. Boehmke Introduction to the vip Package Variable Importance Plots—An",
    "author": [
      {
        "name": "Brandon M. Greenwell",
        "url": {}
      },
      {
        "name": "Bradley C. Boehmke",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\niml, R6, foreach, ingredients, DALEX, mmpf, varImp, party, measures, vita, rfVarImpOOB, randomForestExplainer, tree.interpreter, pkgsearch, caret, mlr, ranger, vip, ggplot2, partykit, earth, nnet, vivo, pdp, microbenchmark, iBreakDown, fastshap, xgboost, ALEPlot, DT, mlr3, data.table, AmesHousing, SuperLearner, glmnet, kernlab, plyr, doParallel\nCRAN Task Views implied by cited packages\nMachineLearning, HighPerformanceComputing, Multivariate, Survival, Environmetrics, TeachingStatistics, Cluster, Econometrics, Finance, Graphics, ModelDeployment, NaturalLanguageProcessing, Optimization, Phylogenetics, ReproducibleResearch, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-014/",
    "title": "difNLR: Generalized Logistic Regression Models for DIF and DDF Detection",
    "description": "Differential item functioning (DIF) and differential distractor functioning (DDF) are impor tant topics in psychometrics, pointing to potential unfairness in items with respect to minorities or different social groups. Various methods have been proposed to detect these issues. The difNLR R package extends DIF methods currently provided in other packages by offering approaches based on generalized logistic regression models that account for possible guessing or inattention, and by pro viding methods to detect DIF and DDF among ordinal and nominal data. In the current paper, we describe implementation of the main functions of the difNLR package, from data generation, through the model fitting and hypothesis testing, to graphical representation of the results. Finally, we provide a real data example to bring the concepts together.",
    "author": [
      {
        "name": "Adéla Hladká",
        "url": {}
      },
      {
        "name": "Patrícia Martinková",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-014.zip\nCRAN packages used\ndifR, DIFlasso, DIFboost, GDINA, mirt, lordif, psychotree, difNLR, ShinyItemAnalysis, ggplot2, stats, VGAM, nnet\nCRAN Task Views implied by cited packages\nPsychometrics, Econometrics, SocialSciences, Distributions, Environmetrics, ExtremeValue, Graphics, MachineLearning, MissingData, Multivariate, Phylogenetics, Survival, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-015/",
    "title": "The R package NonProbEst for estimation in non-probability surveys",
    "description": "Different inference procedures are proposed in the literature to correct selection bias that might be introduced with non-random sampling mechanisms. The R package NonProbEst enables the estimation of parameters using some of these techniques to correct selection bias in non-probability surveys. The mean and the total of the target variable are estimated using Propensity Score Adjustment, calibration, statistical matching, model-based, model-assisted and model-calibratated techniques. Confidence intervals can also obtained for each method. Machine learning algorithms can be used for estimating the propensities or for predicting the unknown values of the target variable for the non-sampled units. Variance of a given estimator is performed by two different Leave-One-Out jackknife procedures. The functionality of the package is illustrated with example data sets.",
    "author": [
      {
        "name": "M. Rueda",
        "url": {}
      },
      {
        "name": "R. Ferri-García",
        "url": {}
      },
      {
        "name": "L. Castro",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-015.zip\nCRAN packages used\nNonProbEst, caret, sampling, survey\nCRAN Task Views implied by cited packages\nOfficialStatistics, HighPerformanceComputing, MachineLearning, Multivariate, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-016/",
    "title": "NlinTS: An R Package For Causality Detection in Time Series",
    "description": "The causality is an important concept that is widely studied in the literature, and has several applications, especially when modelling dependencies within complex data, such as multivariate time series. In this article, we present a theoretical description of methods from the NlinTS package, and we focus on causality measures. The package contains the classical Granger causality test. To handle non-linear time series, we propose an extension of this test using an artificial neural network. The package includes an implementation of the Transfer entropy, which is also considered as a non linear causality measure based on information theory. For discrete variables, we use the classical Shannon Transfer entropy, while for continuous variables, we adopt the k-nearest neighbors approach to estimate it.",
    "author": [
      {
        "name": "Youssef Hmamouche",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-016.zip\nCRAN packages used\nNlinTS, vars, lmtest, RTransferEntropy, timeSeries, Rcpp\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, Econometrics, HighPerformanceComputing, MissingData, NumericalMathematics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-017/",
    "title": "SimilaR: R Code Clone and Plagiarism Detection",
    "description": "Third-party software for assuring source code quality is becoming increasingly popular. Tools that evaluate the coverage of unit tests, perform static code analysis, or inspect run-time memory use are crucial in the software development life cycle. More sophisticated methods allow for performing meta-analyses of large software repositories, e.g., to discover abstract topics they relate to or common design patterns applied by their developers. They may be useful in gaining a better understanding of the component interdependencies, avoiding cloned code as well as detecting plagiarism in programming classes. A meaningful measure of similarity of computer programs often forms the basis of such tools. While there are a few noteworthy instruments for similarity assessment, none of them turns out particularly suitable for analysing R code chunks. Existing solutions rely on rather simple techniques and heuristics and fail to provide a user with the kind of sensitivity and specificity required for working with R scripts. In order to fill this gap, we propose a new algorithm based on a Program Dependence Graph, implemented in the SimilaR package. It can serve as a tool not only for improving R code quality but also for detecting plagiarism, even when it has been masked by applying some obfuscation techniques or imputing dead code. We demonstrate its accuracy and efficiency in a real-world case study.",
    "author": [
      {
        "name": "Maciej Bartoszuk",
        "url": {}
      },
      {
        "name": "Marek Gagolewski",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-017.zip\nCRAN packages used\nmagrittr, SimilaR, nortest, DescTools\nCRAN Task Views implied by cited packages\nMissingData, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-018/",
    "title": "SurvBoost: An R Package for High-Dimensional Variable Selection in the Stratified Proportional Hazards Model via Gradient Boosting",
    "description": "High-dimensional variable selection in the proportional hazards (PH) model has many successful applications in different areas. In practice, data may involve confounding variables that do not satisfy the PH assumption, in which case the stratified proportional hazards (SPH) model can be adopted to control the confounding effects by stratification without directly modeling the confounding effects. However, there is a lack of computationally efficient statistical software for high-dimensional variable selection in the SPH model. In this work an R package, SurvBoost, is developed to implement the gradient boosting algorithm for fitting the SPH model with high-dimensional covariate variables. Simulation studies demonstrate that in many scenarios SurvBoost can achieve better selection accuracy and reduce computational time substantially compared to the existing R package that implements boosting algorithms without stratification. The proposed R package is also illustrated by an analysis of gene expression data with survival outcome in The Cancer Genome Atlas study. In addition, a detailed hands-on tutorial for SurvBoost is provided.",
    "author": [
      {
        "name": "Emily Morris",
        "url": {}
      },
      {
        "name": "Kevin He",
        "url": {}
      },
      {
        "name": "Yanming Li",
        "url": {}
      },
      {
        "name": "Yi Li",
        "url": {}
      },
      {
        "name": "Jian Kang",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-018.zip\nCRAN packages used\nmboost, survival, Rcpp, RcppArmadillo, RcppParallel\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics, Survival, ClinicalTrials, Econometrics, MachineLearning, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-019/",
    "title": "Skew-t Expected Information Matrix Evaluation and Use for Standard Error Calculations",
    "description": "Skew-t distributions derived from skew-normal distributions, as developed by Azzalini and several co-workers, are popular because of their theoretical foundation and the availability of computational methods in the R package sn. One difficulty with this skew-t family is that the elements of the expected information matrix do not have closed form analytic formulas. Thus, we developed a numerical integration method of computing the expected information matrix in the R package skewtInfo. The accuracy of our expected information matrix calculation method was confirmed by comparing the result with that obtained using an observed information matrix for a very large sample size. A Monte Carlo study to evaluate the accuracy of the standard errors obtained with our expected information matrix calculation method, for the case of three realistic skew-t parameter vectors, indicates that use of the expected information matrix results in standard errors as accurate as, and sometimes a little more accurate than, use of an observed information matrix.",
    "author": [
      {
        "name": "R. Douglas Martin",
        "url": {}
      },
      {
        "name": "Chindhanai Uthaisaad",
        "url": {}
      },
      {
        "name": "Daniel Z. Xia",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-019.zip\nCRAN packages used\nsn\nCRAN Task Views implied by cited packages\nDistributions, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-020/",
    "title": " Individual-Level Modelling of Infectious Disease Data: EpiILM",
    "description": "In this article we introduce the R package EpiILM, which provides tools for simulation from, and inference for, discrete-time individual-level models of infectious disease transmission proposed by Deardon et al. (2010). The inference is set in a Bayesian framework and is carried out via Metropolis Hastings Markov chain Monte Carlo (MCMC). For its fast implementation, key functions are coded in Fortran. Both spatial and contact network models are implemented in the package and can be set in either susceptible-infected (SI) or susceptible-infected-removed (SIR) compartmental frameworks. Use of the package is demonstrated through examples involving both simulated and real data.",
    "author": [
      {
        "name": "Vineetha Warriyar K. V.",
        "url": {}
      },
      {
        "name": "Waleed Almutiry",
        "url": {}
      },
      {
        "name": "Rob Deardon",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-020.zip\nCRAN packages used\nR0, EpiEstim, EpiModel, epinet, surveillance, EpiILM, igraph, ergm, adaptMCMC, coda\nCRAN Task Views implied by cited packages\ngR, Bayesian, Environmetrics, Graphics, Optimization, SocialSciences, Spatial, SpatioTemporal, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-021/",
    "title": "tsmp: An R Package for Time Series with Matrix Profile",
    "description": "This article describes tsmp, an R package that implements the MP concept for TS. The tsmp package is a toolkit that allows all-pairs similarity joins, motif, discords and chains discovery, semantic segmentation, etc. Here we describe how the tsmp package may be used by showing some of the use-cases from the original articles and evaluate the algorithm speed in the R environment. This package can be downloaded at https://CRAN.R-project.org/package=tsmp.",
    "author": [
      {
        "name": "Francisco Bischoff",
        "url": {}
      },
      {
        "name": "Pedro Pereira Rodrigues",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-021.zip\nCRAN packages used\ntsmp\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-022/",
    "title": "npordtests: An R Package of Nonparametric Tests for Equality of Location Against Ordered Alternatives",
    "description": "Ordered alternatives are an important statistical problem in many situation such as increased risk of congenital malformation caused by excessive alcohol consumption during pregnancy life test experiments, drug-screening studies, dose-finding studies, the dose-response studies, age-related response. There are numerous other examples of this nature. In this paper, we present the npordtests package to test the equality of locations for ordered alternatives. The package includes the Jonckheere Terpstra, Beier and Buning’s Adaptive, Modified Jonckheere-Terpstra, Terpstra-Magel, Ferdhiana Terpstra-Magel, KTP, S and Gaur’s Gc tests. A simulation study is conducted to determine which test is the most appropriate test for which scenario and to suggest it to the researchers.",
    "author": [
      {
        "name": "Bulent Altunkaynak",
        "url": {}
      },
      {
        "name": "Hamza Gamgam",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-022.zip\nCRAN packages used\nclinfun, jtGWAS, fastJT, kSamples, StatCharrms, PMCMRplus, npordtests\nCRAN Task Views implied by cited packages\nClinicalTrials, Environmetrics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-023/",
    "title": "ari: The Automated R Instructor",
    "description": "We present the ari package for automatically generating technology-focused educational videos. The goal of the package is to create reproducible videos, with the ability to change and update video content seamlessly. We present several examples of generating videos including using R Markdown slide decks, PowerPoint slides, or simple images as source material. We also discuss how ari can help instructors reach new audiences through programmatically translating materials into other languages.",
    "author": [
      {
        "name": "Sean Kross",
        "url": {}
      },
      {
        "name": "Jeffrey T. Leek",
        "url": {}
      },
      {
        "name": "John Muschelli",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-023.zip\nCRAN packages used\nari, text2speech, aws.polly, tuneR, ariExtra, animation, aws.signature, rmarkdown, xaringan, webshot, rgoogleslides, readOffice, officer, pdftools, docxtractr, mscstts, googleLanguageR\nCRAN Task Views implied by cited packages\nReproducibleResearch, WebTechnologies, Graphics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-024/",
    "title": "CoxPhLb: An R Package for Analyzing Length Biased Data under Cox Model",
    "description": "Data subject to length-biased sampling are frequently encountered in various applications including prevalent cohort studies and are considered as a special case of left-truncated data under the stationarity assumption. Many semiparametric regression methods have been proposed for length biased data to model the association between covariates and the survival outcome of interest. In this paper, we present a brief review of the statistical methodologies established for the analysis of length-biased data under the Cox model, which is the most commonly adopted semiparametric model, and introduce an R package CoxPhLb that implements these methods. Specifically, the package includes features such as fitting the Cox model to explore covariate effects on survival times and checking the proportional hazards model assumptions and the stationarity assumption. We illustrate usage of the package with a simulated data example and a real dataset, the Channing House data, which are publicly available.",
    "author": [
      {
        "name": "Chi Hyun Lee",
        "url": {}
      },
      {
        "name": "Heng Zhou",
        "url": {}
      },
      {
        "name": "Jing Ning",
        "url": {}
      },
      {
        "name": "Diane D. Liu",
        "url": {}
      },
      {
        "name": "Yu Shen",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-024.zip\nCRAN packages used\nCoxPhLb, survival, KMsurv, coxphw\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-025/",
    "title": "CopulaCenR: Copula based Regression Models for Bivariate Censored Data in R",
    "description": "Bivariate time-to-event data frequently arise in research areas such as clinical trials and epidemiological studies, where the occurrence of two events are correlated. In many cases, the exact event times are unknown due to censoring. The copula model is a popular approach for modeling correlated bivariate censored data, in which the two marginal distributions and the between margin dependence are modeled separately. This article presents the R package CopulaCenR, which is designed for modeling and testing bivariate data under right or (general) interval censoring in a regression setting. It provides a variety of Archimedean copula functions including a flexible two-parameter copula and different types of regression models (parametric and semiparametric) for marginal distributions. In particular, it implements a semiparametric transformation model for the margins with proportional hazards and proportional odds models being its special cases. The numerical optimization is based on a novel two-step algorithm. For the regression parameters, three likelihood-based tests (Wald, generalized score and likelihood ratio tests) are also provided. We use two real data examples to illustrate the key functions in CopulaCenR.",
    "author": [
      {
        "name": "Tao Sun",
        "url": {}
      },
      {
        "name": "Ying Ding",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-025.zip\nCRAN packages used\nCopulaCenR, survival, parfm, frailtypack, coxme, phmm, copula, VineCopula, CopulaRegression, gcmr, gamCopula, Copula.surv, Sunclarco, GJRM\nCRAN Task Views implied by cited packages\nSurvival, Distributions, ClinicalTrials, Econometrics, ExtremeValue, Finance, Multivariate, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-026/",
    "title": "BayesMallows: An R Package for the Bayesian Mallows Model",
    "description": "BayesMallows is an R package for analyzing preference data in the form of rankings with the Mallows rank model, and its finite mixture extension, in a Bayesian framework. The model is grounded on the idea that the probability density of an observed ranking decreases exponentially with the distance to the location parameter. It is the first Bayesian implementation that allows wide choices of distances, and it works well with a large amount of items to be ranked. BayesMallows handles non-standard data: partial rankings and pairwise comparisons, even in cases including non-transitive preference patterns. The Bayesian paradigm allows coherent quantification of posterior uncertainties of estimates of any quantity of interest. These posteriors are fully available to the user, and the package comes with convienient tools for summarizing and visualizing the posterior distributions.",
    "author": [
      {
        "name": "Øystein Sørensen",
        "url": {}
      },
      {
        "name": "Marta Crispino",
        "url": {}
      },
      {
        "name": "Qinghua Liu",
        "url": {}
      },
      {
        "name": "Valeria Vitelli",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-026.zip\nCRAN packages used\nBayesMallows, PerMallows, pmr, rankdist, microbenchmark, dplyr, parallel, tidyr, label.switching\nCRAN Task Views implied by cited packages\nDatabases, ModelDeployment\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-028/",
    "title": "S, R, and Data Science",
    "description": "Data science is increasingly important and challenging. It requires computational tools and programming environments that handle big data and difficult computations, while supporting creative, high-quality analysis. The R language and related software play a major role in computing for data science. R is featured in most programs for training in the field. R packages provide tools for a wide range of purposes and users. The description of a new technique, particularly from research in statistics, is frequently accompanied by an R package, greatly increasing the usefulness of the description. The history of R makes clear its connection to data science. R was consciously designed to replicate in open-source software the contents of the S software. S in turn was written by data analysis researchers at Bell Labs as part of the computing environment for research in data analysis and collaborations to apply that research, rather than as a separate project to create a programming language. The features of S and the design decisions made for it need to be understood in this broader context of supporting effective data analysis (which would now be called data science). These characteristics were all transferred to R and remain central to its effectiveness. Thus, R can be viewed as based historically on a domain-specific language for the domain of data science. Note to R Journal readers: The following paper was published online in the History of Programming Languages (HOPL), Volume 4, in June 2020 (DOI 10.1145/3386334). The content seems likely to be of interest to many R Journal readers, and since HOPL is plausibly not typical reading for data scientists, the editors of the R Journal have kindly offered to republish the paper here. This is possible thanks also to the enlightened policy of the ACM, providing for open distribution through the chosen copyright declaration.",
    "author": [
      {
        "name": "John M. Chambers",
        "url": {}
      }
    ],
    "date": "2020-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-029/",
    "title": "Provenance of R’s Gradient Optimizers",
    "description": "Gradient optimization methods (function minimizers) are well-represented in both the base and package universe of R (R Core Team, 2019). However, some of the methods and the codes developed from them were published before standards for hardware and software were established, in particular the IEEE arithmetic (IEEE, 1985). There have been cases of unexpected behaviour or outright errors, and these are the focus of the histoRicalg project. A summary history of some of the tools in R for gradient optimization methods is presented to give perspective on such methods and the occasions where they could be used effectively.",
    "author": [
      {
        "name": "John C. Nash",
        "url": {}
      }
    ],
    "date": "2020-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-001/",
    "title": "Mapping Smoothed Spatial Effect Estimates from Individual-Level Data: MapGAM ",
    "description": "We introduce and illustrate the utility of MapGAM, a user-friendly R package that provides a unified framework for estimating, predicting and drawing inference on covariate-adjusted spatial effects using individual-level data. The package also facilitates visualization of spatial effects via automated mapping procedures. MapGAM estimates covariate-adjusted spatial associations with a univariate or survival outcome using generalized additive models that include a non-parametric bivariate smooth term of geolocation parameters. Estimation and mapping methods are implemented for continuous, discrete, and right-censored survival data. In the current manuscript, we summarize the methodology implemented in MapGAM and illustrate the package using two example simulated datasets: the first considering a case-control study design from the state of Massachusetts and the second considering right-censored survival data from California.",
    "author": [
      {
        "name": "Lu Bai",
        "url": {}
      },
      {
        "name": "Daniel L. Gillen",
        "url": {}
      },
      {
        "name": "Scott M. Bartell",
        "url": {}
      },
      {
        "name": "Verónica M. Vieira",
        "url": {}
      }
    ],
    "date": "2020-03-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-001.zip\nCRAN packages used\nMapGAM\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-002/",
    "title": "mudfold: An R Package for Nonparametric IRT Modelling of Unfolding Processes",
    "description": "Item response theory (IRT) models for unfolding processes use the responses of individuals to attitudinal tests or questionnaires in order to infer item and person parameters located on a latent continuum. Parametric models in this class use parametric functions to model the response process, which in practice can be restrictive. MUDFOLD (Multiple UniDimensional unFOLDing) can be used to obtain estimates of person and item ranks without imposing strict parametric assumptions on the item response functions (IRFs). This paper describes the implementation of the MUDFOLD method for binary preferential-choice data in the R package mudfold. The latter incorporates estimation, visualization, and simulation methods in order to provide R users with utilities for nonparametric analysis of attitudinal questionnaire data. After a brief introduction in IRT, we provide the method ological framework implemented in the package. A description of the available functions is followed by practical examples and suggestions on how this method can be used even outside the field of psychometrics.",
    "author": [
      {
        "name": "Spyros E. Balafas",
        "url": {}
      },
      {
        "name": "Wim P. Krijnen",
        "url": {}
      },
      {
        "name": "Wendy J. Post",
        "url": {}
      },
      {
        "name": "Ernst C. Wit",
        "url": {}
      }
    ],
    "date": "2020-03-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-002.zip\nCRAN packages used\nmudfold, GGUM, mirt, mokken, boot, mice, gtools, glmnet, mgcv, zoo, reshape2, ggplot2, smacof\nCRAN Task Views implied by cited packages\nPsychometrics, Econometrics, MissingData, SocialSciences, Environmetrics, Survival, TimeSeries, Bayesian, Finance, Graphics, MachineLearning, Multivariate, OfficialStatistics, Optimization, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-005/",
    "title": "lspartition: Partitioning-Based Least Squares Regression",
    "description": "Nonparametric partitioning-based least squares regression is an important tool in empirical work. Common examples include regressions based on splines, wavelets, and piecewise polynomials. This article discusses the main methodological and numerical features of the R software package lspartition, which implements results for partitioning-based least squares (series) regression estimation and inference from Cattaneo and Farrell (2013) and Cattaneo, Farrell, and Feng (2020). These results cover the multivariate regression function as well as its derivatives. First, the package provides data-driven methods to choose the number of partition knots optimally, according to integrated mean squared error, yielding optimal point estimation. Second, robust bias correction is implemented to combine this point estimator with valid inference. Third, the package provides estimates and inference for the unknown function both pointwise and uniformly in the conditioning variables. In particular, valid confidence bands are provided. Finally, an extension to two-sample analysis is developed, which can be used in treatment-control comparisons and related problems.",
    "author": [
      {
        "name": "Matias D. Cattaneo",
        "url": {}
      },
      {
        "name": "Max H. Farrell",
        "url": {}
      },
      {
        "name": "Yingjie Feng",
        "url": {}
      }
    ],
    "date": "2020-03-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-005.zip\nCRAN packages used\nggplot2\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-006/",
    "title": "SortedEffects: Sorted Causal Effects in R",
    "description": "Chernozhukov et al. (2018) proposed the sorted effect method for nonlinear regression models. This method consists of reporting percentiles of the partial effects, the sorted effects, in addition to the average effect commonly used to summarize the heterogeneity in the partial effects. They also propose to use the sorted effects to carry out classification analysis where the observational units are classified as most and least affected if their partial effect are above or below some tail sorted effects. The R package SortedEffects implements the estimation and inference methods therein and provides tools to visualize the results. This vignette serves as an introduction to the package and displays basic functionality of the functions within.",
    "author": [
      {
        "name": "Shuowen Chen",
        "url": {}
      },
      {
        "name": "Victor Chernozhukov",
        "url": {}
      },
      {
        "name": "Iván Fernández-Val",
        "url": {}
      },
      {
        "name": "Ye Luo",
        "url": {}
      }
    ],
    "date": "2020-03-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-006.zip\nCRAN packages used\nSortedEffects, SortedEffect, quantreg, margins, parallel, boot\nCRAN Task Views implied by cited packages\nEconometrics, Optimization, SocialSciences, Survival, Environmetrics, ReproducibleResearch, Robust, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-054/",
    "title": "The R Package trafo for Transforming Linear Regression Models",
    "description": "Researchers and data-analysts often use the linear regression model for descriptive, predictive, and inferential purposes. This model relies on a set of assumptions that, when not satisfied, yields biased results and noisy estimates. A common problem that can be solved in many ways – use of less restrictive methods (e.g. generalized linear regression models or non-parametric methods ), variance corrections or transformations of the response variable just to name a few. We focus on the latter option as it allows to keep using the simple and well-known linear regression model. The list of transformations proposed in the literature is long and varies according to the problem they aim to solve. Such diversity can leave analysts lost and confused. We provide a framework implemented as an R-package, trafo, to help select suitable transformations depending on the user requirements and data being analyzed. The package trafo contains a collection of selected transformations and estimation methods that complement and increase the breadth of methods that exist in R.",
    "author": [
      {
        "name": "Lily Medina",
        "url": {}
      },
      {
        "name": "Ann-Kristin Kreutzmann",
        "url": {}
      },
      {
        "name": "Natalia Rojas-Perilla",
        "url": {}
      },
      {
        "name": "Piedad Castro",
        "url": {}
      }
    ],
    "date": "2020-01-06",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-054.zip\nCRAN packages used\ntrafo, car, rcompanio, bestNormalize, caret, Johnson, jtrans, MASS, AID, Ecdat\nCRAN Task Views implied by cited packages\nEconometrics, Multivariate, SocialSciences, TeachingStatistics, Distributions, Environmetrics, Finance, HighPerformanceComputing, MachineLearning, NumericalMathematics, Psychometrics, Robust, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-055/",
    "title": "BondValuation: An R Package for Fixed Coupon Bond Analysis",
    "description": "The purpose of this paper is to introduce the R package BondValuation for the analysis of large datasets of fixed coupon bonds. The conceptual heterogeneity of fixed coupon bonds traded in the global markets imposes a high degree of complexity on their comparative analysis. Contrary to baseline fixed income theory, in practice, most bonds feature coupon period irregularities. In addition, there are a multitude of day count methods that determine the interest accrual, the cash flows and the discount factors used in bond valuation. Several R packages, e.g., fBonds, RQuantLib, and YieldCurve, provide tools for fixed income analysis. Nevertheless, none of them is capable of evaluating bonds featuring irregular first and/or final coupon periods, and neither provides adequate coverage of day count conventions currently used in the global bond markets. The R package BondValuation closes this gap using the generalized valuation methodology presented in Djatschenko (2019).",
    "author": [
      {
        "name": "Wadim Djatschenko",
        "url": {}
      }
    ],
    "date": "2020-01-06",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-055.zip\nCRAN packages used\nBondValuation, fBonds, RQuantLib, YieldCurve\nCRAN Task Views implied by cited packages\nFinance\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-057/",
    "title": "HCmodelSets: An R Package for Specifying Sets of Well-fitting Models in High Dimensions",
    "description": "In the context of regression with a large number of explanatory variables, Cox and Battey (2017) emphasize that if there are alternative reasonable explanations of the data that are statistically indistinguishable, one should aim to specify as many of these explanations as is feasible. The standard practice, by contrast, is to report a single effective model for prediction. This paper illustrates the R implementation of the new ideas in the package HCmodelSets, using simple reproducible examples and real data. Results of some simulation experiments are also reported.",
    "author": [
      {
        "name": "Henrique Hoeltgebaum",
        "url": {}
      },
      {
        "name": "Heather Battey",
        "url": {}
      }
    ],
    "date": "2020-01-06",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-057.zip\nCRAN packages used\nHCmodelSets, glmnet, survival\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, MachineLearning, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-053/",
    "title": "spGARCH: An R-Package for Spatial and Spatiotemporal ARCH and GARCH models",
    "description": "In this paper, a general overview on spatial and spatiotemporal ARCH models is provided. In particular, we distinguish between three different spatial ARCH-type models. In addition to the original definition of ?, we introduce an logarithmic spatial ARCH model in this paper. For this new model, maximum-likelihood estimators for the parameters are proposed. In addition, we consider a new complex-valued definition of the spatial ARCH process. Moreover, spatial GARCH models are briefly discussed. From a practical point of view, the use of the R-package spGARCH is demonstrated. To be precise, we show how the proposed spatial ARCH models can be simulated and summarize the variety of spatial models, which can be estimated by the estimation functions provided in the package. Eventually, we apply all procedures to a real-data example.",
    "author": [
      {
        "name": "Philipp Otto",
        "url": {}
      }
    ],
    "date": "2019-12-28",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspGARCH, spdep, gstat, Stem, rugarch, Rsolnp, Rcpp, RcppEigen\nCRAN Task Views implied by cited packages\nSpatial, NumericalMathematics, SpatioTemporal, Finance, HighPerformanceComputing, Optimization, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-044/",
    "title": "Fitting Tails by the Empirical Residual Coefficient of Variation: The ercv Package",
    "description": "This article is a self-contained introduction to the R package ercv and to the methodology on which it is based through the analysis of nine examples. The methodology is simple and trustworthy for the analysis of extreme values and relates the two main existing methodologies. The package contains R functions for visualizing, fitting and validating the distribution of tails. It also provides multiple threshold tests for a generalized Pareto distribution, together with an automatic threshold selection algorithm.",
    "author": [
      {
        "name": "Joan del Castillo",
        "url": {}
      },
      {
        "name": "Isabel Serra",
        "url": {}
      },
      {
        "name": "Maria Padilla",
        "url": {}
      },
      {
        "name": "David Moriña",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-044.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-045/",
    "title": "biclustermd: An R Package for Biclustering with Missing Values",
    "description": "Biclustering is a statistical learning technique that attempts to find homogeneous partitions of rows and columns of a data matrix. For example, movie ratings might be biclustered to group both raters and movies. biclust is a current R package allowing users to implement a variety of biclustering algorithms. However, its algorithms do not allow the data matrix to have missing values. We provide a new R package, biclustermd, which allows users to perform biclustering on numeric data even in the presence of missing values.",
    "author": [
      {
        "name": "John Reisner",
        "url": {}
      },
      {
        "name": "Hieu Pham",
        "url": {}
      },
      {
        "name": "Sigurdur Olafsson",
        "url": {}
      },
      {
        "name": "Stephen Vardeman",
        "url": {}
      },
      {
        "name": "Jing Li",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-045.zip\nCRAN packages used\nbiclust, superbiclust, s4vd, BiBitR, biclustermd, clues, nycflights13, tidyverse, ggplot2\nCRAN Task Views implied by cited packages\nGraphics, Cluster, Phylogenetics, TeachingStatistics\nBioconductor packages used\niBBiG, QUBIC\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-046/",
    "title": "PPCI: an R Package for Cluster Identification using Projection Pursuit",
    "description": "This paper presents the R package PPCI which implements three recently proposed projec tion pursuit methods for clustering. The methods are unified by the approach of defining an optimal hyperplane to separate clusters, and deriving a projection index whose optimiser is the vector normal to this separating hyperplane. Divisive hierarchical clustering algorithms that can detect clusters defined in different subspaces are readily obtained by recursively bi-partitioning the data through such hyperplanes. Projecting onto the vector normal to the optimal hyperplane enables visualisations of the data that can be used to validate the partition at each level of the cluster hierarchy. Clustering models can also be modified in an interactive manner to improve their solutions. Extensions to problems involving clusters which are not linearly separable, and to the problem of finding maximum hard margin hyperplanes for clustering are also discussed.",
    "author": [
      {
        "name": "David P. Hofmeyr",
        "url": {}
      },
      {
        "name": "Nicos G. Pavlidis",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-046.zip\nCRAN packages used\nProjectionBasedClustering, subspace\nCRAN Task Views implied by cited packages\nCluster\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-048/",
    "title": "Associative Classification in R: arc, arulesCBA, and rCBA",
    "description": "Several methods for creating classifiers based on rules discovered via association rule mining have been proposed in the literature. These classifiers are called associative classifiers and the best-known algorithm is Classification Based on Associations (CBA). Interestingly, only very few implementations are available and, until recently, no implementation was available for R. Now, three packages provide CBA. This paper introduces associative classification, the CBA algorithm, and how it can be used in R. A comparison of the three packages is provided to give the potential user an idea about the advantages of each of the implementations. We also show how the packages are related to the existing infrastructure for association rule mining already available in R.",
    "author": [
      {
        "name": "Michael Hahsler",
        "url": {}
      },
      {
        "name": "Ian Johnson",
        "url": {}
      },
      {
        "name": "Tomáš Kliegr",
        "url": {}
      },
      {
        "name": "Jaroslav Kuchař",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-048.zip\nCRAN packages used\narc, arulesCBA, rCBA, RWeka, arules, arulesViz, qcba, sbrl, RKEEL, discretization, Matrix, qCBA, matrix, rJAVA, mlbench, datasets\nCRAN Task Views implied by cited packages\nMachineLearning, ModelDeployment, Econometrics, Multivariate, NaturalLanguageProcessing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-050/",
    "title": "Comparing namedCapture with other R packages for regular expressions",
    "description": "Regular expressions are powerful tools for manipulating non-tabular textual data. For many tasks (visualization, machine learning, etc), tables of numbers must be extracted from such data before processing by other R functions. We present the R package namedCapture, which facilitates such tasks by providing a new user-friendly syntax for defining regular expressions in R code. We begin by describing the history of regular expressions and their usage in R. We then describe the new features of the namedCapture package, and provide detailed comparisons with related R packages (rex, stringr, stringi, tidyr, rematch2, re2r).",
    "author": [
      {
        "name": "Toby Dylan Hocking",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-050.zip\nCRAN packages used\nnamedCapture, rex, stringr, stringi, tidyr, rematch2, re2r, microbenchmark\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-051/",
    "title": "Resampling-Based Analysis of Multivariate Data and Repeated Measures Designs with the R Package MANOVA.RM",
    "description": "Nonparametric statistical inference methods for a modern and robust analysis of longitudinal and multivariate data in factorial experiments are essential for research. While existing approaches that rely on specific distributional assumptions of the data (multivariate normality and/or equal covariance matrices) are implemented in statistical software packages, there is a need for user-friendly software that can be used for the analysis of data that do not fulfill the aforementioned assumptions and provide accurate p value and confidence interval estimates. Therefore, newly developed nonpara metric statistical methods based on bootstrapand permutation-approaches, which neither assume multivariate normality nor specific covariance matrices, have been implemented in the freely available R package MANOVA.RM. The package is equipped with a graphical user interface for plausible applications in academia and other educational purpose. Several motivating examples illustrate the application of the methods.",
    "author": [
      {
        "name": "Sarah Friedrich",
        "url": {}
      },
      {
        "name": "Frank Konietschke",
        "url": {}
      },
      {
        "name": "Markus Pauly",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-051.zip\nCRAN packages used\nstats, npmv, nparLD, GFD, SCGLR, car, flip, ffmanova, MANOVA.RM, RGtk2, plotrix, HSAUR, ellipse, multcomp\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, SocialSciences, ClinicalTrials, Econometrics, Finance, Survival, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-052/",
    "title": "lpirfs: An R Package to Estimate Impulse Response Functions by Local Projections",
    "description": "Impulse response analysis is a cornerstone in applied (macro-)econometrics. Estimating impulse response functions using local projections (LPs) has become an appealing alternative to the traditional structural vector autoregressive (SVAR) approach. Despite its growing popularity and applications, however, no R package yet exists that makes this method available. In this paper, I introduce lpirfs, a fast and flexible R package that provides a broad framework to compute and visualize impulse response functions using LPs for a variety of data sets.",
    "author": [
      {
        "name": "Philipp Adämmer",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-052.zip\nCRAN packages used\nvars, lpirfs, Rcpp, plm\nCRAN Task Views implied by cited packages\nEconometrics, Finance, HighPerformanceComputing, NumericalMathematics, SpatioTemporal, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-003/",
    "title": "mistr: A Computational Framework for Mixture and Composite Distributions",
    "description": "Finite mixtures and composite distributions allow to model the probabilistic representation of data with more generality than simple distributions and are useful to consider in a wide range of applications. The R package mistr provides an extensible computational framework for creating, transforming, and evaluating these models, together with multiple methods for their visualization and description. In this paper we present the main computational framework of the package and illustrate its application. In addition, we provide and show functions for data modeling using two specific composite distributions as well as a numerical example where a composite distribution is estimated to describe the log-returns of selected stocks.",
    "author": [
      {
        "name": "Lukas Sablica",
        "url": {}
      },
      {
        "name": "Kurt Hornik",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-003.zip\nCRAN packages used\nmistr, distr, CompLognormal, evmix, OpVar, ReIns, gendist, ggplot2, actuar, bbmle\nCRAN Task Views implied by cited packages\nDistributions, ExtremeValue, Finance, Graphics, Phylogenetics, Robust, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-042/",
    "title": "coxed: An R Package for Computing Duration-Based Quantities from the Cox Proportional Hazards Model",
    "description": "The Cox proportional hazards model is one of the most frequently used estimators in duration (survival) analysis. Because it is estimated using only the observed durations’ rank ordering, typical quantities of interest used to communicate results of the Cox model come from the hazard function (e.g., hazard ratios or percentage changes in the hazard rate). These quantities are substantively vague and difficult for many audiences of research to understand. We introduce a suite of methods in the R package coxed to address these problems. The package allows researchers to calculate duration-based quantities from Cox model results, such as the expected duration (or survival time) given covariate values and marginal changes in duration for a specified change in a covariate. These duration-based quantities often match better with researchers’ substantive interests and are easily understood by most readers. We describe the methods and illustrate use of the package.",
    "author": [
      {
        "name": "Jonathan Kropko",
        "url": {}
      },
      {
        "name": "Jeffrey J. Harden",
        "url": {}
      }
    ],
    "date": "2019-12-26",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-042.zip\nCRAN packages used\ncoxed\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-043/",
    "title": "The IDSpatialStats R Package: Quantifying Spatial Dependence of Infectious Disease Spread",
    "description": "Spatial statistics for infectious diseases are important because the spatial and temporal scale over which transmission operates determine the dynamics of disease spread. Many methods for quantifying the distribution and clustering of spatial point patterns have been developed (e.g. K function and pair correlation function) and are routinely applied to infectious disease case occurrence data. However, these methods do not explicitly account for overlapping chains of transmission and require knowledge of the underlying population distribution, which can be limiting when analyzing epidemic case occurrence data. Therefore, we developed two novel spatial statistics that account for these effects to estimate: 1) the mean of the spatial transmission kernel, and 2) the τ-statistic, a measure of global clustering based on pathogen subtype. We briefly introduce these statistics and show how to implement them using the IDSpatialStats R package.",
    "author": [
      {
        "name": "John R. Giles",
        "url": {}
      },
      {
        "name": "Henrik Salje",
        "url": {}
      },
      {
        "name": "Justin Lessler",
        "url": {}
      }
    ],
    "date": "2019-12-26",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-043.zip\nCRAN packages used\nlgcp, ppmlasso, spdep, ads, spatstat, splancs, IDSpatialStats, DCluster, SGCS, sparr\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-037/",
    "title": "Fixed Point Acceleration in R",
    "description": "A fixed point problem is one where we seek a vector, X, for a function, f, such that f(X) = X. The solution of many such problems can be accelerated by using a fixed point acceleration algorithm. With the release of the FixedPoint package there is now a number of algorithms available in R that can be used for accelerating the finding of a fixed point of a function. These algorithms include Newton acceleration, Aitken acceleration and Anderson acceleration as well as epsilon extrapolation methods and minimal polynomial methods. This paper demonstrates the use of fixed point accelerators in solving numerical mathematics problems using the algorithms of the FixedPoint package as well as the squarem method of the SQUAREM package.",
    "author": [
      {
        "name": "Stuart Baumann",
        "url": {}
      },
      {
        "name": "Margaryta Klymak",
        "url": {}
      }
    ],
    "date": "2019-08-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-037.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-038/",
    "title": "SemiCompRisks: An R Package for the Analysis of Independent and Cluster-correlated Semi-competing Risks Data",
    "description": "Semi-competing risks refer to the setting where primary scientific interest lies in estimation and inference with respect to a non-terminal event, the occurrence of which is subject to a terminal event. In this paper, we present the R package SemiCompRisks that provides functions to perform the analysis of independent/clustered semi-competing risks data under the illness-death multi-state model. The package allows the user to choose the specification for model components from a range of options giving users substantial flexibility, including: accelerated failure time or proportional hazards regression models; parametric or non-parametric specifications for baseline survival functions; parametric or non-parametric specifications for random effects distributions when the data are cluster correlated; and, a Markov or semi-Markov specification for terminal event following non-terminal event. While estimation is mainly performed within the Bayesian paradigm, the package also provides the maximum likelihood estimation for select parametric models. The package also includes functions for univariate survival analysis as complementary analysis tools.",
    "author": [
      {
        "name": "Danilo Alvares",
        "url": {}
      },
      {
        "name": "Sebastien Haneuse",
        "url": {}
      },
      {
        "name": "Catherine Lee",
        "url": {}
      },
      {
        "name": "Kyu Ha Lee",
        "url": {}
      }
    ],
    "date": "2019-08-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-038.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-039/",
    "title": "RSSampling: A Pioneering Package for Ranked Set Sampling ",
    "description": "Ranked set sampling (RSS) is an advanced data collection method when the exact mea surement of an observation is difficult and/or expensive used in a number of research areas, e.g., environment, bioinformatics, ecology, etc. In this method, random sets are drawn from a population and the units in sets are ranked with a ranking mechanism which is based on a visual inspection or a concomitant variable. Because of the importance of working with a good design and easy analysis, there is a need for a software tool which provides sampling designs and statistical inferences based on RSS and its modifications. This paper introduces an R package as a free and easy-to-use analysis tool for both sampling processes and statistical inferences based on RSS and its modified versions. For researchers, the RSSampling package provides a sample with RSS, extreme RSS, median RSS, percentile RSS, balanced groups RSS, double versions of RSS, L-RSS, truncation-based RSS, and robust extreme RSS when the judgment rankings are both perfect and imperfect. Researchers can also use this new package to make parametric inferences for the population mean and the variance where the sample is obtained via classical RSS. Moreover, this package includes applications of the nonparametric methods which are one sample sign test, Mann-Whitney-Wilcoxon test, and Wilcoxon signed-rank test procedures. The package is available as RSSampling on CRAN.",
    "author": [
      {
        "name": "Busra Sevinc",
        "url": {}
      },
      {
        "name": "Bekir Cetintav",
        "url": {}
      },
      {
        "name": "Melek Esemen",
        "url": {}
      },
      {
        "name": "Selma Gurler",
        "url": {}
      }
    ],
    "date": "2019-08-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-039.zip\nCRAN packages used\nNSM3, RSSampling, stats, LearnBayes\nCRAN Task Views implied by cited packages\nBayesian, Distributions, Survival, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-040/",
    "title": "unival: An FA-based R Package For Assessing Essential Unidimensionality Using External Validity Information",
    "description": "The unival package is designed to help researchers decide between unidimensional and correlated factors solutions in the factor analysis of psychometric measures. The novelty of the approach is its use of external information, in which multiple factor scores and general factor scores are related to relevant external variables or criteria. The unival package’s implementation comes from a series of procedures put forward by Ferrando and Lorenzo-Seva (2019) and new methodological developments proposed in this article. We assess models fitted using unival by means of a simulation study extending the results obtained in the original proposal. Its usefulness is also assessed through a real-world data example. Based on these results, we conclude unival is a valuable tool for use in applications in which the dimensionality of an item set is to be assessed.",
    "author": [
      {
        "name": "Pere J. Ferrando",
        "url": {}
      },
      {
        "name": "Urbano Lorenzo-Seva",
        "url": {}
      },
      {
        "name": "David Navarro-Gonzalez",
        "url": {}
      }
    ],
    "date": "2019-08-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-040.zip\nCRAN packages used\nunival, stats, optimbase, psych, mirt\nCRAN Task Views implied by cited packages\nPsychometrics, MissingData\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-035/",
    "title": "BINCOR: An R package for Estimating the Correlation between Two Unevenly Spaced Time Series",
    "description": "This paper presents a computational program named BINCOR (BINned CORrelation) for estimating the correlation between two unevenly spaced time series. This program is also applicable to the situation of two evenly spaced time series not on the same time grid. BINCOR is based on a novel estimation approach proposed by Mudelsee (2010) for estimating the correlation between two climate time series with different timescales. The idea is that autocorrelation (e.g. an AR1 process) means that memory enables values obtained on different time points to be correlated. Binned correlation is performed by resampling the time series under study into time bins on a regular grid, assigning the mean values of the variable under scrutiny within those bins. We present two examples of our BINCOR package with real data: instrumental and paleoclimatic time series. In both applications BINCOR works properly in detecting well-established relationships between the climate records compared.",
    "author": [
      {
        "name": "Josue M. Polanco-Martinez",
        "url": {}
      },
      {
        "name": "Martin A. Medina-Elizalde",
        "url": {}
      },
      {
        "name": "Maria Fernanda Sanchez Goni",
        "url": {}
      },
      {
        "name": "Manfred            Mudelsee",
        "url": {}
      }
    ],
    "date": "2019-08-18",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-035.zip\nCRAN packages used\nBINCOR, dplR, pracma, TSdist\nCRAN Task Views implied by cited packages\nDifferentialEquations, NumericalMathematics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-036/",
    "title": "auditor: an R Package for Model-Agnostic Visual Validation and Diagnostics",
    "description": "Machine learning models have successfully been applied to challenges in applied in biology, medicine, finance, physics, and other fields. With modern software it is easy to train even a complex model that fits the training data and results in high accuracy on test set. However, problems often arise when models are confronted with the real-world data. This paper describes methodology and tools for model-agnostic auditing. It provides functinos for assessing and comparing the goodness of fit and performance of models. In addition, the package may be used for analysis of the similarity of residuals and for identification of outliers and influential observations. The examination is carried out by diagnostic scores and visual verification. The code presented in this paper are implemented in the auditor package. Its flexible and consistent grammar facilitates the validation models of a large class of models.",
    "author": [
      {
        "name": "Alicja Gosiewska",
        "url": {}
      },
      {
        "name": "Przemysław Biecek",
        "url": {}
      }
    ],
    "date": "2019-08-18",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-036.zip\nCRAN packages used\nauditor\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-024/",
    "title": "shadow: R Package for Geometric Shadow Calculations in an Urban Environment",
    "description": "This paper introduces the shadow package for R. The package provides functions for shadow related calculations in the urban environment, namely shadow height, shadow footprint and Sky View Factor (SVF) calculations, as well as a wrapper function to estimate solar radiation while taking shadow effects into account. All functions operate on a layer of polygons with a height attribute, also known as “extruded polygons” or 2.5D vector data. Such data are associated with accuracy limitations in representing urban environments. However, unlike 3D models, polygonal layers of building outlines along with their height are abundantly available and their processing does not require specialized closed-source 3D software. The present package thus brings spatio-temporal shadow, SVF and solar radiation calculation capabilities to the open-source spatial analysis workflow in R. Package functionality is demonstrated using small reproducible examples for each function. Wider potential use cases include urban environment applications such as evaluation of micro-climatic influence for urban planning, studying urban climatic comfort and estimating photovoltaic energy production potential.",
    "author": [
      {
        "name": "Michael Dorman",
        "url": {}
      },
      {
        "name": "Evyatar Erell",
        "url": {}
      },
      {
        "name": "Adi Vulkan",
        "url": {}
      },
      {
        "name": "Itai Kloog",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-024.zip\nCRAN packages used\ninsol, shadow, sp, threejs, raster, rgeos, maptools, parallel, plot3D, sf\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-025/",
    "title": "Integration of networks and pathways with StarBioTrek package",
    "description": "High-throughput genomic technologies bring to light a comprehensive hallmark of molecular changes of a disease. It is increasingly evident that genes are not isolated from each other and the identification of a gene signature can only partially elucidate the de-regulated biological functions in a disease. The comprehension of how groups of genes (pathways) are related to each other (pathway cross talk) could explain biological mechanisms causing diseases. Biological pathways are important tools to identify gene interactions and decrease the large number of genes to be studied by partitioning them into smaller groups. Furthermore, recent scientific studies have demonstrated that an integration of pathways and networks, instead of a single component of the pathway or a single network, could lead to a deeper understanding of the pathology. StarBioTrek is an R package for the integration of biological pathways and networks which provides a series of functions to support the user in their analyses. In particular, it implements algorithms to identify pathways cross-talk networks and gene network drivers in pathways. It is available as open source and open development software in the Bioconductor platform.",
    "author": [
      {
        "name": "Claudia Cava",
        "url": {}
      },
      {
        "name": "Isabella Castiglioni",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-025.zip\nCRAN packages used\nXGR, qgraph, GOplot\nCRAN Task Views implied by cited packages\nPsychometrics\nBioconductor packages used\nNetPathMiner, ToPASeq, StarBioTrek, graphite\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-026/",
    "title": "ciuupi: An R package for Computing Confidence Intervals that Utilize Uncertain Prior Information",
    "description": "We have created the R package ciuupi to compute confidence intervals that utilize uncertain prior information in linear regression. Unlike post-model-selection confidence intervals, the confidence interval that utilizes uncertain prior information (CIUUPI) implemented in this package has, to an excellent approximation, coverage probability throughout the parameter space that is very close to the desired minimum coverage probability. Furthermore, when the uncertain prior information is correct, the CIUUPI is, on average, shorter than the standard confidence interval constructed using the full linear regression model. In this paper we provide motivating examples of scenarios where the CIUUPI may be used. We then give a detailed description of this interval and the numerical constrained optimization method implemented in R to obtain it. Lastly, using a real data set as an illustrative example, we show how to use the functions in ciuupi.",
    "author": [
      {
        "name": "Rheanna Mainzer",
        "url": {}
      },
      {
        "name": "Paul Kabaila",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-026.zip\nCRAN packages used\nciuupi, nloptr, statmod\nCRAN Task Views implied by cited packages\nDistributions, NumericalMathematics, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-027/",
    "title": "cvcrand: A Package for Covariate-constrained Randomization and the Clustered Permutation Test for Cluster Randomized Trials",
    "description": "The cluster randomized trial (CRT) is a randomized controlled trial in which randomization is conducted at the cluster level (e.g., school or hospital) and outcomes are measured for each individual within a cluster. Often, the number of clusters available to randomize is small (≤ 20), which increases the chance of baseline covariate imbalance between comparison arms. Such imbalance is particularly problematic when the covariates are predictive of the outcome because it can threaten the internal validity of the CRT. Pair-matching and stratification are two restricted randomization approaches that are frequently used to ensure balance at the design stage. An alternative, less commonly-used restricted randomization approach is covariate-constrained randomization. Covariate-constrained randomization quantifies baseline imbalance of cluster-level covariates using a balance metric and randomly selects a randomization scheme from those with acceptable balance by the balance metric. It is able to accommodate multiple covariates, both categorical and continuous. To facilitate imple mentation of covariate-constrained randomization for the design of two-arm parallel CRTs, we have developed the cvcrand R package. In addition, cvcrand also implements the clustered permutation test for analyzing continuous and binary outcomes collected from a CRT designed with covariate constrained randomization. We used a real cluster randomized trial to illustrate the functions included in the package.",
    "author": [
      {
        "name": "Hengshi Yu",
        "url": {}
      },
      {
        "name": "Fan Li",
        "url": {}
      },
      {
        "name": "John A. Gallis",
        "url": {}
      },
      {
        "name": "Elizabeth L. Turner",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-027.zip\nCRAN packages used\ncvcrand\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-028/",
    "title": "jomo: A Flexible Package for Two-level Joint Modelling Multiple Imputation",
    "description": "Multiple imputation is a tool for parameter estimation and inference with partially observed data, which is used increasingly widely in medical and social research. When the data to be imputed are correlated or have a multilevel structure — repeated observations on patients, school children nested in classes within schools within educational districts — the imputation model needs to include this structure. Here we introduce our joint modelling package for multiple imputation of multilevel data, jomo, which uses a multivariate normal model fitted by Markov Chain Monte Carlo (MCMC). Compared to previous packages for multilevel imputation, e.g. pan, jomo adds the facility to (i) handle and impute categorical variables using a latent normal structure, (ii) impute level-2 variables, and (iii) allow for cluster-specific covariance matrices, including the option to give them an inverse-Wishart distribution at level 2. The package uses C routines to speed up the computations and has been extensively validated in simulation studies both by ourselves and others.",
    "author": [
      {
        "name": "Matteo Quartagno",
        "url": {}
      },
      {
        "name": "Simon Grund",
        "url": {}
      },
      {
        "name": "James Carpenter",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-028.zip\nCRAN packages used\njomo, pan, norm, cat, mix, R2MLwiN, mitools, mice, semTools, BaBooN, mitml, Amelia, mi, lavaan.survey, dummies, nlme\nCRAN Task Views implied by cited packages\nMissingData, OfficialStatistics, SocialSciences, Multivariate, Psychometrics, Bayesian, ChemPhys, Econometrics, Environmetrics, Finance, Spatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-029/",
    "title": "ipwErrorY: An R Package for Estimation of Average Treatment Effect with Misclassified Binary Outcome",
    "description": "It has been well documented that ignoring measurement error may result in severely biased inference results. In recent years, there has been limited but increasing research on causal inference with measurement error. In the presence of misclassified binary outcome variable, Shu and Yi (2017) considered the inverse probability weighted estimation of the average treatment effect and proposed valid estimation methods to correct for misclassification effects for various settings. To expedite the application of those methods for situations where misclassification in the binary outcome variable is a real concern, we implement correction methods proposed by Shu and Yi (2017) and develop an R package ipwErrorY for general users. Simulated datasets are used to illustrate the use of the developed package.",
    "author": [
      {
        "name": "Di Shu",
        "url": {}
      },
      {
        "name": "Grace Y. Yi",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-029.zip\nCRAN packages used\nipwErrorY, nleqslv\nCRAN Task Views implied by cited packages\nNumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-030/",
    "title": "optimParallel: An R Package Providing a Parallel Version of the L-BFGS-B Optimization Method",
    "description": "The R package optimParallel provides a parallel version of the L-BFGS-B optimization method of optim(). The main function of the package is optimParallel(), which has the same usage and output as optim(). Using optimParallel() can significantly reduce the optimization time, especially when the evaluation time of the objective function is large and no analytical gradient is available. We introduce the R package and illustrate its implementation, which takes advantage of the lexical scoping mechanism of R.",
    "author": [
      {
        "name": "Florian Gerber",
        "url": {}
      },
      {
        "name": "Reinhard Furrer",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-030.zip\nCRAN packages used\nlbfgsb3, lbfgsb3c, optimParallel, testthat, microbenchmark\nCRAN Task Views implied by cited packages\nOptimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-031/",
    "title": "swgee: An R Package for Analyzing Longitudinal Data with Response Missingness and Covariate Measurement Error",
    "description": "Though longitudinal data often contain missing responses and error-prone covariates, relatively little work has been available to simultaneously correct for the effects of response missingness and covariate measurement error on analysis of longitudinal data. Yi (2008) proposed a simulation based marginal method to adjust for the bias induced by measurement error in covariates as well as by missingness in response. The proposed method focuses on modeling the marginal mean and variance structures, and the missing at random mechanism is assumed. Furthermore, the distribution of covariates are left unspecified. These features make the proposed method applicable to a broad settings. In this paper, we develop an R package, called swgee, which implements the method proposed by Yi (2008). Moreover, our package includes additional implementation steps which extend the setting considered by Yi (2008). To describe the use of the package and its main features, we report simulation studies and analyses of a data set arising from the Framingham Heart Study.",
    "author": [
      {
        "name": "Juan Xiong",
        "url": {}
      },
      {
        "name": "Grace Y. Yi",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-031.zip\nCRAN packages used\ngee, yags, wgeesel, geepack, mvtnorm\nCRAN Task Views implied by cited packages\nSocialSciences, Distributions, Econometrics, Finance, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-032/",
    "title": "roahd Package: Robust Analysis of High Dimensional Data",
    "description": "The focus of this paper is on the open-source R package roahd (RObust Analysis of High dimensional Data), see Tarabelloni et al. (2017). roahd has been developed to gather recently proposed statistical methods that deal with the robust inferential analysis of univariate and multivariate functional data. In particular, efficient methods for outlier detection and related graphical tools, methods to represent and simulate functional data, as well as inferential tools for testing differences and dependency among families of curves will be discussed, and the associated functions of the package will be described in details.",
    "author": [
      {
        "name": "Francesca Ieva",
        "url": {}
      },
      {
        "name": "Anna Maria Paganoni",
        "url": {}
      },
      {
        "name": "Juan Romo",
        "url": {}
      },
      {
        "name": "Nicholas Tarabelloni",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-032.zip\nCRAN packages used\nR\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-033/",
    "title": "The Landscape of R Packages for Automated Exploratory Data Analysis",
    "description": "The increasing availability of large but noisy data sets with a large number of heterogeneous variables leads to the increasing interest in the automation of common tasks for data analysis. The most time-consuming part of this process is the Exploratory Data Analysis, crucial for better domain understanding, data cleaning, data validation, and feature engineering. There is a growing number of libraries that attempt to automate some of the typical Exploratory Data Analysis tasks to make the search for new insights easier and faster. In this paper, we present a systematic review of existing tools for Automated Exploratory Data Analysis (autoEDA). We explore the features of fifteen popular R packages to identify the parts of analysis that can be effectively automated with the current tools and to point out new directions for further autoEDA development.",
    "author": [
      {
        "name": "Mateusz Staniak",
        "url": {}
      },
      {
        "name": "Przemysław Biecek",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-033.zip\nCRAN packages used\ncranlogs, radiant, visdat, archivist, xtable, arsenal, DataExplorer, dataMaid, dlookr, ExPanDaR, explore, shiny, exploreR, funModeling, inspectdf, RtutoR, SmartEDA, data.table, summarytools, knitr, ggplot2, xray, tableone, describer, skimr, prettyR, Hmisc, ggfortify, autoplotly, gpairs, GGally, survminer, cr17, DALEX, iml\nCRAN Task Views implied by cited packages\nReproducibleResearch, TeachingStatistics, MissingData, WebTechnologies, Bayesian, ClinicalTrials, Econometrics, Finance, Graphics, HighPerformanceComputing, Multivariate, OfficialStatistics, Phylogenetics, SocialSciences, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-019/",
    "title": "MDFS: MultiDimensional Feature Selection in R",
    "description": "Identification of informative variables in an information system is often performed using simple one-dimensional filtering procedures that discard information about interactions between variables. Such an approach may result in removing some relevant variables from consideration. Here we present an R package MDFS (MultiDimensional Feature Selection) that performs identification of informative variables taking into account synergistic interactions between multiple descriptors and the decision variable. MDFS is an implementation of an algorithm based on information theory (Mnich and Rudnicki, 2017). The computational kernel of the package is implemented in C++. A high-performance version implemented in CUDA C is also available. The application of MDFS is demonstrated using the well-known Madelon dataset, in which a decision variable is generated from synergistic interactions between descriptor variables. It is shown that the application of multidimen sional analysis results in better sensitivity and ranking of importance.",
    "author": [
      {
        "name": "Radosław Piliszek",
        "url": {}
      },
      {
        "name": "Krzysztof Mnich",
        "url": {}
      },
      {
        "name": "Szymon Migacz",
        "url": {}
      },
      {
        "name": "Paweł Tabaszewski",
        "url": {}
      },
      {
        "name": "Andrzej Sułecki",
        "url": {}
      },
      {
        "name": "Aneta            Polewko-Klim",
        "url": {}
      },
      {
        "name": "Witold Rudnicki",
        "url": {}
      }
    ],
    "date": "2019-08-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-019.zip\nCRAN packages used\nMDFS, Rfast\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-020/",
    "title": "Nowcasting: An R Package for Predicting Economic Variables Using Dynamic Factor Models",
    "description": "The nowcasting package provides the tools to make forecasts of monthly or quarterly economic variables using dynamic factor models. The objective is to help the user at each step of the forecasting process, starting with the construction of a database, all the way to the interpretation of the forecasts. The dynamic factor model adopted in this package is based on the articles from Giannone et al. (2008) and Banbura et al. (2011). Although there exist several other dynamic factor model packages available for R, ours provides an environment to easily forecast economic variables and interpret results.",
    "author": [
      {
        "name": "Serge de Valk",
        "url": {}
      },
      {
        "name": "Daiane de Mattos",
        "url": {}
      },
      {
        "name": "Pedro Ferreira",
        "url": {}
      }
    ],
    "date": "2019-08-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-020.zip\nCRAN packages used\nnowcasting\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-021/",
    "title": "ConvergenceClubs: A Package for Performing the Phillips and Sul's Club Convergence Clustering Procedure",
    "description": "This paper introduces package ConvergenceClubs, which implements functions to perform the Phillips and Sul (2007, 2009) club convergence clustering procedure in a simple and reproducible manner. The approach proposed by Phillips and Sul to analyse the convergence patterns of groups of economies is formulated as a nonlinear time varying factor model that allows for different time paths as well as individual heterogeneity. Unlike other approaches in which economies are grouped a priori, it also allows the endogenous determination of convergence clubs. The algorithm, usage, and implementation details are discussed.",
    "author": [
      {
        "name": "Roberto Sichera",
        "url": {}
      },
      {
        "name": "Pietro Pizzuto",
        "url": {}
      }
    ],
    "date": "2019-08-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-021.zip\nCRAN packages used\nConvergenceClubs, mFilter\nCRAN Task Views implied by cited packages\nTimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-022/",
    "title": "SimCorrMix: Simulation of Correlated Data with Multiple Variable Types Including Continuous and Count Mixture Distributions",
    "description": "The SimCorrMix package generates correlated continuous (normal, non-normal, and mix ture), binary, ordinal, and count (regular and zero-inflated, Poisson and Negative Binomial) variables that mimic real-world data sets. Continuous variables are simulated using either Fleishman’s third order or Headrick’s fifth-order power method transformation. Simulation occurs at the component level for continuous mixture distributions, and the target correlation matrix is specified in terms of correlations with components. However, the package contains functions to approximate expected correlations with continuous mixture variables. There are two simulation pathways which calculate intermediate correlations involving count variables differently, increasing accuracy under a wide range of parameters. The package also provides functions to calculate cumulants of continuous mixture distributions, check parameter inputs, calculate feasible correlation boundaries, and summarize and plot simulated variables. SimCorrMix is an important addition to existing R simulation packages because it is the first to include continuous mixture and zero-inflated count variables in correlated data sets.",
    "author": [
      {
        "name": "Allison Fialkowski",
        "url": {}
      },
      {
        "name": "Hemant Tiwari",
        "url": {}
      }
    ],
    "date": "2019-08-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-022.zip\nCRAN packages used\nAdaptGauss, DPP, bgmm, ClusterR, mclust, mixture, AdMit, bimixt, bmixture, CAMAN, flexmix, mixdist, mixtools, nspmix, MixtureInf, Rmixmod, hurdlr, zic, mixpack, distr, stats, rebmix, SimCorrMix, SimMultiCorrData, GenOrd, VGAM, Matrix, ggplot2, mvtnorm\nCRAN Task Views implied by cited packages\nCluster, Distributions, Multivariate, Bayesian, Environmetrics, Econometrics, Psychometrics, ExtremeValue, Finance, Graphics, MetaAnalysis, NumericalMathematics, Phylogenetics, Robust, SocialSciences, Survival, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-023/",
    "title": "Time-Series Clustering in R Using the dtwclust Package",
    "description": "Most clustering strategies have not changed considerably since their initial definition. The common improvements are either related to the distance measure used to assess dissimilarity, or the function used to calculate prototypes. Time-series clustering is no exception, with the Dynamic Time Warping distance being particularly popular in that context. This distance is computationally expensive, so many related optimizations have been developed over the years. Since no single clustering algorithm can be said to perform best on all datasets, different strategies must be tested and compared, so a common infrastructure can be advantageous. In this manuscript, a general overview of shape-based time-series clustering is given, including many specifics related to Dynamic Time Warping and associated techniques. At the same time, a description of the dtwclust package for the R statistical software is provided, showcasing how it can be used to evaluate many different time-series clustering procedures.",
    "author": [
      {
        "name": "Alexis Sardá-Espinosa",
        "url": {}
      }
    ],
    "date": "2019-08-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-023.zip\nCRAN packages used\ndtwclust, flexclust, cluster, TSdist, TSclust, pdc, dtw, proxy, clue, foreach, RcppParallel, doParallel\nCRAN Task Views implied by cited packages\nTimeSeries, Cluster, Multivariate, HighPerformanceComputing, Environmetrics, Optimization, Robust\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-009/",
    "title": "mixedsde: A Package to Fit Mixed Stochastic Differential Equations",
    "description": "Stochastic differential equations (SDEs) are useful to model continuous stochastic processes. When (independent) repeated temporal data are available, variability between the trajectories can be modeled by introducing random effects in the drift of the SDEs. These models are useful to analyze neuronal data, crack length data, pharmacokinetics, financial data, to cite some applications among other. The R package focuses on the estimation of SDEs with linear random effects in the drift. The goal is to estimate the common density of the random effects from repeated discrete observations of the SDE. The package mixedsde proposes three estimation methods: a Bayesian parametric, a frequentist parametric and a frequentist nonparametric method. The three procedures are described as well as the main functions of the package. Illustrations are presented on simulated and real data.",
    "author": [
      {
        "name": "Charlotte Dion",
        "url": {}
      },
      {
        "name": "Simone Hermann",
        "url": {}
      },
      {
        "name": "Adeline Samson",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-009.zip\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-010/",
    "title": "Indoor Positioning and Fingerprinting: The R Package ipft",
    "description": "Methods based on Received Signal Strength Indicator (RSSI) fingerprinting are in the forefront among several techniques being proposed for indoor positioning. This paper introduces the R package ipft, which provides algorithms and utility functions for indoor positioning using fingerprinting techniques. These functions are designed for manipulation of RSSI fingerprint data sets, estimation of positions, comparison of the performance of different positioning models, and graphical visualization of data. Well-known machine learning algorithms are implemented in this package to perform analysis and estimations over RSSI data sets. The paper provides a description of these algorithms and functions, as well as examples of its use with real data. The ipft package provides a base that we hope to grow into a comprehensive library of fingerprinting-based indoor positioning methodologies.",
    "author": [
      {
        "name": "Emilio Sansano",
        "url": {}
      },
      {
        "name": "Raúl Montoliu",
        "url": {}
      },
      {
        "name": "Óscar Belmonte",
        "url": {}
      },
      {
        "name": "Joaquín Torres-Sospedra",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-010.zip\nCRAN packages used\nipft, ggplot2\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-011/",
    "title": "RobustGaSP: Robust Gaussian Stochastic Process Emulation in R",
    "description": "Gaussian stochastic process (GaSP) emulation is a powerful tool for approximating compu tationally intensive computer models. However, estimation of parameters in the GaSP emulator is a challenging task. No closed-form estimator is available and many numerical problems arise with standard estimates, e.g., the maximum likelihood estimator. In this package, we implement a marginal posterior mode estimator, for special priors and parameterizations. This estimation method that meets the robust parameter estimation criteria was discussed in Gu et al. (2018); mathematical reasons are provided therein to explain why robust parameter estimation can greatly improve predictive performance of the emulator. In addition, inert inputs (inputs that almost have no effect on the variability of a function) can be identified from the marginal posterior mode estimation at no extra computational cost. The package also implements the parallel partial Gaussian stochastic process (PP GaSP) emulator (Gu and Berger (2016)) for the scenario where the computer model has multiple outputs on, for example, spatial-temporal coordinates. The package can be operated in a default mode, but also allows numerous user specifications, such as the capability of specifying trend functions and noise terms. Examples are studied herein to highlight the performance of the package in terms of out-of-sample prediction.",
    "author": [
      {
        "name": "Mengyang Gu",
        "url": {}
      },
      {
        "name": "Jesus Palomo",
        "url": {}
      },
      {
        "name": "James O. Berger",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-011.zip\nCRAN packages used\nDiceKriging, GPfit, mleGP, spatial, fields, RobustGaSP, lhs, sensitivity, nloptr\nCRAN Task Views implied by cited packages\nExperimentalDesign, Spatial, Distributions, Environmetrics, Optimization, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-012/",
    "title": "Whats for dynr: A Package for Linear and Nonlinear Dynamic Modeling in R",
    "description": "Intensive longitudinal data in the behavioral sciences are often noisy, multivariate in nature, and may involve multiple units undergoing regime switches by showing discontinuities interspersed with continuous dynamics. Despite increasing interest in using linear and nonlinear differential/difference equation models with regime switches, there has been a scarcity of software packages that are fast and freely accessible. We have created an R package called dynr that can handle a broad class of linear and nonlinear discreteand continuous-time models, with regime-switching properties and linear Gaussian measurement functions, in C, while maintaining simple and easy-to learn model specification functions in R. We present the mathematical and computational bases used by the dynr R package, and present two illustrative examples to demonstrate the unique features of dynr.",
    "author": [
      {
        "name": "Lu Ou",
        "url": {}
      },
      {
        "name": "Michael D. Hunter",
        "url": {}
      },
      {
        "name": "Sy-Miin Chow",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-012.zip\nCRAN packages used\ndynr, dlm, KFAS, dse, OpenMx, ctsem, depmixS4, RHmm, MSwM, MSBVAR, MSGARCH, pomp, stats, Rcpp, RcppGSL, mice\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, MissingData, Psychometrics, Bayesian, Cluster, DifferentialEquations, Environmetrics, HighPerformanceComputing, Multivariate, NumericalMathematics, OfficialStatistics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-016/",
    "title": "Identifying and Testing Recursive vs. Interdependent Links in Simultaneous Equation Models via the SIRE Package",
    "description": "Simultaneous equation models (SEMs) are composed of relations which either represent by Gianmarco Vacca and Maria Grazia Zoia Package Equation Models via the SIRE Interdependent Links in Simultaneous Identifying and Testing Recursive vs. Contributed research article 1",
    "author": [
      {
        "name": "Gianmarco Vacca",
        "url": {}
      },
      {
        "name": "Maria Grazia Zoia",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-016.zip\nCRAN packages used\nSIRE, igraph, systemfit, Rsolnp\nCRAN Task Views implied by cited packages\nOptimization, Econometrics, gR, Graphics, Psychometrics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-017/",
    "title": "fclust: An R Package for Fuzzy Clustering",
    "description": "Fuzzy clustering methods discover fuzzy partitions where observations can be softly assigned to more than one cluster. The package fclust is a toolbox for fuzzy clustering in the R programming language. It not only implements the widely used fuzzy k-means (FkM) algorithm, but also many FkM variants. Fuzzy cluster similarity measures, cluster validity indices and cluster visualization tools are also offered. In the current version, all the functions are rewritten in the C++ language allowing their application in large-size problems. Moreover, new fuzzy relational clustering algorithms for partitioning qualitative/mixed data are provided together with an improved version of the so-called Gustafson-Kessel algorithm to avoid singularity in the cluster covariance matrices. Finally, it is now possible to automatically select the number of clusters by means of the available fuzzy cluster validity indices.",
    "author": [
      {
        "name": "Maria Brigida Ferraro",
        "url": {}
      },
      {
        "name": "Paolo Giordani",
        "url": {}
      },
      {
        "name": "Alessio Serafini",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-017.zip\nCRAN packages used\nfclust, cluster, clue, e1071, skmeans, vegclust, ppclust, Rcpp, RcppArmadillo, smacof, MASS\nCRAN Task Views implied by cited packages\nCluster, Multivariate, Environmetrics, NumericalMathematics, Psychometrics, Distributions, Robust, Econometrics, HighPerformanceComputing, MachineLearning, NaturalLanguageProcessing, Optimization, SocialSciences, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-018/",
    "title": "Matching with Clustered Data: the CMatching Package in R",
    "description": "Matching is a well known technique to balance covariates distribution between treated and control units in non-experimental studies. In many fields, clustered data are a very common occurrence in the analysis of observational data and the clustering can add potentially interesting information. Matching algorithms should be adapted to properly exploit the hierarchical structure. In this article we present the CMatching package implementing matching algorithms for clustered data. The package provides functions for obtaining a matched dataset along with estimates of most common parameters of interest and model-based standard errors. A propensity score matching analysis, relating math proficiency with homework completion for students belonging to different schools (based on the NELS-88 data), illustrates in detail the use of the algorithms.",
    "author": [
      {
        "name": "Massimo Cannas",
        "url": {}
      },
      {
        "name": "Bruno Arpino",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-018.zip\nCRAN packages used\nCMatching, Matching, designmatch, optmatch, MatchIT, quickmatch, multiwayvcov\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, ExperimentalDesign, HighPerformanceComputing, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-002/",
    "title": "Connecting R with D3 for dynamic graphics, to explore multivariate data with tours",
    "description": "The tourr package in R has several algorithms and displays for showing multivariate data as a sequence of low-dimensional projections. It can display as a movie but has no capacity for interaction, such as stop/go, change tour type, drop/add variables. The tourrGui package provides these sorts of controls, but the interface is programmed with the dated RGtk2 package. This work explores using custom messages to pass data from R to D3 for viewing, using the Shiny framework. This is an approach that can be generally used for creating all sorts of interactive graphics.",
    "author": [
      {
        "name": "Michael Kipp",
        "url": {}
      },
      {
        "name": "Ursula Laa",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntourr, tourrGui, RGtk2\nCRAN Task Views implied by cited packages\nGraphics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-003/",
    "title": "dr4pl: A Stable Convergence Algorithm for the 4 Parameter Logistic Model",
    "description": "The 4 Parameter Logistic (4PL) model has been recognized as a major tool to analyze the relationship between doses and responses in pharmacological experiments. A main strength of this model is that each parameter contributes an intuitive meaning enhancing interpretability of a fitted model. However, implementing the 4PL model using conventional statistical software often encounters numerical errors. This paper highlights the issue of convergence failure and presents several causes with solutions. These causes include outliers and a non-logistic data shape, so useful remedies such as robust estimation, outlier diagnostics and constrained optimization are proposed. These features are implemented in a new R package dr4pl (Dose-Response analysis using the 4 Parameter Logistic model) whose code examples are presented as a separate section. Our R package dr4pl is shown to work well for data sets where the traditional dose-response modelling packages drc and nplr fail.",
    "author": [
      {
        "name": "Hyowon An",
        "url": {}
      },
      {
        "name": "Justin T. Landis",
        "url": {}
      },
      {
        "name": "Aubrey G. Bailey",
        "url": {}
      },
      {
        "name": "James S. Marron",
        "url": {}
      },
      {
        "name": "Dirk P. Dittmer",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-003.zip\nCRAN packages used\ndr4pl, drc, nplr\nCRAN Task Views implied by cited packages\nChemPhys\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-004/",
    "title": "Time Series Forecasting with KNN in R: the tsfknn Package",
    "description": "In this paper the tsfknn package for time series forecasting using k-nearest neighbor regres sion is described. This package allows users to specify a KNN model and to generate its forecasts. The user can choose among different multi-step ahead strategies and among different functions to aggregate the targets of the nearest neighbors. It is also possible to assess the forecast accuracy of the KNN model.",
    "author": [
      {
        "name": "Francisco Martínez",
        "url": {}
      },
      {
        "name": "María P. Frías",
        "url": {}
      },
      {
        "name": "Francisco Charte",
        "url": {}
      },
      {
        "name": "Antonio J. Rivera",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-004.zip\nCRAN packages used\nforecast, caret, nnfor, tsfknn, forecastHybrid, GMDH, NTS, tsDyn, nnet, neuralnet\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance, MachineLearning, Environmetrics, HighPerformanceComputing, MissingData, Multivariate, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-005/",
    "title": "rollmatch: An R Package for Rolling Entry Matching",
    "description": "The gold standard of experimental research is the randomized control trial. However, interventions are often implemented without a randomized control group for practical or ethical reasons. Propensity score matching (PSM) is a popular method for minimizing the effects of a randomized experiment from observational data by matching members of a treatment group to similar candidates that did not receive the intervention. Traditional PSM is not designed for studies that enroll participants on a rolling basis and does not provide a solution for interventions in which the baseline and intervention period are undefined in the comparison group. Rolling Entry Matching (REM) is a new matching method that addresses both issues. REM selects comparison members who are similar to intervention members with respect to both static (e.g., race) and dynamic (e.g., health conditions) characteristics. This paper will discuss the key components of REM and introduce the rollmatch R package.",
    "author": [
      {
        "name": "Kasey Jones",
        "url": {}
      },
      {
        "name": "Rob Chew",
        "url": {}
      },
      {
        "name": "Allison Witman",
        "url": {}
      },
      {
        "name": "Yiyan Liu",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-005.zip\nCRAN packages used\nrollmatch, CBPS, ipw, MatchIt, Matching, optmatch\nCRAN Task Views implied by cited packages\nSocialSciences, HighPerformanceComputing, MissingData, OfficialStatistics, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-006/",
    "title": "orthoDr: Semiparametric Dimension Reduction via Orthogonality Constrained Optimization",
    "description": "orthoDr is a package in R that solves dimension reduction problems using orthogonality constrained optimization approach. The package serves as a unified framework for many regression and survival analysis dimension reduction models that utilize semiparametric estimating equations. The main computational machinery of orthoDr is a first-order algorithm developed by Wen and Yin (2012) for optimization within the Stiefel manifold. We implement the algorithm through Rcpp and OpenMP for fast computation. In addition, we developed a general-purpose solver for such constrained problems with user-specified objective functions, which works as a drop-in version of optim(). The package also serves as a platform for future methodology developments along this line of work.",
    "author": [
      {
        "name": "Ruoqing Zhu",
        "url": {}
      },
      {
        "name": "Jiyang Zhang",
        "url": {}
      },
      {
        "name": "Ruilin Zhao",
        "url": {}
      },
      {
        "name": "Peng Xu",
        "url": {}
      },
      {
        "name": "Wenzhuo Zhou",
        "url": {}
      },
      {
        "name": "Xin Zhang",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-006.zip\nCRAN packages used\northoDr, Rcpp, RcppArmadillo, ManifoldOpthm\nCRAN Task Views implied by cited packages\nNumericalMathematics, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-007/",
    "title": "Modeling regimes with extremes: the bayesdfa package for identifying and forecasting common trends and anomalies in multivariate time-series data",
    "description": "The bayesdfa package provides a flexible Bayesian modeling framework for applying dy namic factor analysis (DFA) to multivariate time-series data as a dimension reduction tool. The core estimation is done with the Stan probabilistic programming language. In addition to being one of the few Bayesian implementations of DFA, novel features of this model include (1) optionally modeling latent process deviations as drawn from a Student-t distribution to better model extremes, and (2) optionally including autoregressive and moving-average components in the latent trends. Besides estimation, we provide a series of plotting functions to visualize trends, loadings, and model pre dicted values. A secondary analysis for some applications is to identify regimes in latent trends. We provide a flexible Bayesian implementation of a Hidden Markov Model — also written with Stan — to characterize regime shifts in latent processes. We provide simulation testing and details on parameter sensitivities in supplementary information.",
    "author": [
      {
        "name": "Eric J. Ward",
        "url": {}
      },
      {
        "name": "Sean C. Anderson",
        "url": {}
      },
      {
        "name": "Luis A. Damiano",
        "url": {}
      },
      {
        "name": "Mary E. Hunsicker",
        "url": {}
      },
      {
        "name": "Michael A. Litzow",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndlm, KFAS, MARSS, tsfa, rstan, heavy, bsts, stochvol, loo, depmixS4, HMM, msm\nCRAN Task Views implied by cited packages\nTimeSeries, Bayesian, Finance, Cluster, Distributions, Econometrics, Multivariate, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-008/",
    "title": "Optimization Routines for Enforcing One-to-One Matches in Record Linkage Problems",
    "description": "Record linkage aims at quickly and accurately identifying if two records represent the same real world entity. In many applications, we are interested in restricting the linkage results to \"1 to 1\" links, that is a single record does not appear more than once in the output. This can be dealt with the transport algorithm. The optimization problem, however, grows quadratically in the size of the input, quickly becoming untreatable for cases with a few thousand records. This paper compares different solutions, provided by some R packages for linear programming solvers. The comparison is done in terms of memory usage and execution time. The aim is to overcome the current implementation in the toolkit RELAIS, specifically developed for record linkage problems. The results highlight improvements beyond expectations. In fact the tested solutions allow successfully executing the \"1 to 1\" reduction for large size datasets up to the largest sample surveys at National Statistical Institutes.",
    "author": [
      {
        "name": "Diego Moretti",
        "url": {}
      },
      {
        "name": "Luca Valentino",
        "url": {}
      },
      {
        "name": "Tiziana Tuoto",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-008.zip\nCRAN packages used\nlpSolve, Rglpk, ROI.plugin.clp, intpoint, slam\nCRAN Task Views implied by cited packages\nOptimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-041/",
    "title": "Using Web Services to Work with Geodata in R",
    "description": "Through collaborative mapping, a massive amount of data is accessible. Many individuals contribute information each day. The growing amount of geodata is gathered by volunteers or obtained via crowd-sourcing. One outstanding example of this is the OpenStreetMap (OSM) Project which provides access to big data in geography. Another online mapping service that enables the integration of geodata into the analysis is Google Maps. The expanding content and the availability of geographic information radically changes the perspective on geodata (Chilton 2009). Recently many application programming interfaces (APIs) have been built on OSM and Google Maps. That leads to a point where it is possible to access sections of geographical information without the usage of a complex database solution, especially if one only requires a small data section for a visualization. First tools for spatial analysis have been included in the R language very early (Bivand and Gebhardt, 2000) and this development will continue to accelerate, underpinning a continual change. Notably, in recent years many tools have been developed to enable the usage of R as a geographic information system (GIS). With a GIS it is possible to process spatial data. QuantumGIS (QGIS) is a free software solution for these tasks, and a user interface is available for this purpose. R is, therefore, an alternative to geographic information systems like QGIS (QGIS Development Team 2009). Besides, add-ins for QGIS and R-packages (RQGIS) are available, that enables the combination of R and QGIS (Muenchow and Schratz 2017). It is the target of this article to present some of the most important R-functionalities to download and process geodata from OSM and the Google Maps API. The focus of this paper is on functions that enable the natural usage of these APIs.",
    "author": [
      {
        "name": "Jan-Philipp Kolb",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-041.zip\nCRAN packages used\nRQGIS, osmdata, OpenStreetMap, ggplot2, ggmap, RgoogleMaps, leaflet, magrittr, tmap, mapview, mapdeck, lawn, googleway, RDSTK, RJSONIO, jsonlite, tmaptools, sf, OpenStreeetMap, mapmisc, opencage, curl, XML, osmplotr, sp, igraph\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, Graphics, SpatioTemporal, gR, OfficialStatistics, Optimization, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:55+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-001/",
    "title": "atable: Create Tables for Clinical Trial Reports",
    "description": "Examining distributions of variables is the first step in the analysis of a clinical trial before more specific modelling can begin. Reporting these results to stakeholders of the trial is an essential part of a statistician’s work. The atable package facilitates these steps by offering easy-to-use but still flexible functions.",
    "author": [
      {
        "name": "Armin Ströbel",
        "url": {}
      }
    ],
    "date": "2019-07-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-001.zip\nCRAN packages used\nmultgee, Hmisc, knitr, xtable, flextable, settings, survival, furniture, tableone, stargazer, DescTools, margrittr, dplyr\nCRAN Task Views implied by cited packages\nReproducibleResearch, SocialSciences, ClinicalTrials, Econometrics, MissingData, Bayesian, ModelDeployment, Multivariate, OfficialStatistics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-081/",
    "title": "idmTPreg: Regression Model for Progressive Illness Death Data",
    "description": "The progressive illness-death model is frequently used in medical applications. For example, the model may be used to describe the disease process in cancer studies. We have developed a new R package called idmTPreg to estimate regression coefficients in datasets that can be described by the progressive illness-death model. The motivation for the development of the package is a recent contribution that enables the estimation of possibly time-varying covariate effects on the transition probabilities for a progressive illness-death data. The main feature of the package is that it befits both non-Markov and Markov progressive illness-death data. The package implements the introduced estimators obtained using a direct binomial regression approach. Also, variance estimates and confidence bands are implemented in the package. This article presents guidelines for the use of the package.",
    "author": [
      {
        "name": "Leyla Azarang",
        "url": {}
      },
      {
        "name": "Manuel Oviedo de la Fuente",
        "url": {}
      }
    ],
    "date": "2019-02-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-081.zip\nCRAN packages used\nidmTPreg, mstate, msm, p3state.msm, doParallel, foreach, survival\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Distributions, Econometrics, HighPerformanceComputing, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-076/",
    "title": "Dynamic Simulation and Testing for Single-Equation Cointegrating and Stationary Autoregressive Distributed Lag Models",
    "description": "While autoregressive distributed lag models allow for extremely flexible dynamics, interpret ing the substantive significance of complex lag structures remains difficult. In this paper we discuss dynamac (dynamic autoregressive and cointegrating models), an R package designed to assist users in estimating, dynamically simulating, and plotting the results of a variety of autoregressive distributed lag models. It also contains a number of post-estimation diagnostics, including a test for cointegration for when researchers are estimating the error-correction variant of the autoregressive distributed lag model.",
    "author": [
      {
        "name": "Soren Jordan",
        "url": {}
      },
      {
        "name": "Andrew Q. Philips",
        "url": {}
      }
    ],
    "date": "2018-12-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-076.zip\nCRAN packages used\ndynsim, Zelig, urca, MASS\nCRAN Task Views implied by cited packages\nEconometrics, Finance, SocialSciences, Distributions, Environmetrics, Multivariate, NumericalMathematics, Psychometrics, Robust, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-074/",
    "title": "ShinyItemAnalysis for Teaching Psychometrics and to Enforce Routine Analysis of Educational Tests",
    "description": "This work introduces ShinyItemAnalysis, an R package and an online shiny application for psychometric analysis of educational tests and items. ShinyItemAnalysis covers a broad range of psy chometric methods and offers data examples, model equations, parameter estimates, interpretation of results, together with a selected R code, and is therefore suitable for teaching psychometric concepts with R. Furthermore, the application aspires to be an easy-to-use tool for analysis of educational tests by allowing the users to upload and analyze their own data and to automatically generate analysis reports in PDF or HTML. We argue that psychometric analysis should be a routine part of test development in order to gather proofs of reliability and validity of the measurement, and we demonstrate how ShinyItemAnalysis may help enforce this goal.",
    "author": [
      {
        "name": "Patrícia Martinková",
        "url": {}
      },
      {
        "name": "Adéla Drabinová",
        "url": {}
      }
    ],
    "date": "2018-12-13",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-074.zip\nCRAN packages used\npsych, ltm, difR, lavaan, ShinyItemAnalysis, shiny, mirt, corrplot, cowplot, CTT, data.table, deltaPlotR, difNLR, DT, ggdendro, ggplot2, gridExtra, knitr, latticeExtra, moments, msm, nnet, plotly, psychometric, reshape2, rmarkdown, shinyBS, shinydashboard, shinyjs, stringr, xtable\nCRAN Task Views implied by cited packages\nPsychometrics, ReproducibleResearch, MissingData, Distributions, Econometrics, Graphics, WebTechnologies, Finance, HighPerformanceComputing, MachineLearning, MetaAnalysis, Multivariate, OfficialStatistics, Phylogenetics, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-075/",
    "title": "Measurement Errors in R",
    "description": "This paper presents an R package to handle and represent measurements with errors in a very simple way. We briefly introduce the main concepts of metrology and propagation of uncertainty, and discuss related R packages. Building upon this, we introduce the errors package, which provides a class for associating uncertainty metadata, automated propagation and reporting. Working with errors enables transparent, lightweight, less error-prone handling and convenient representation of measurements with errors. Finally, we discuss the advantages, limitations and future work of computing with errors.",
    "author": [
      {
        "name": "Iñaki Ucar",
        "url": {}
      },
      {
        "name": "Edzer Pebesma",
        "url": {}
      },
      {
        "name": "Arturo Azcorra",
        "url": {}
      }
    ],
    "date": "2018-12-13",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-075.zip\nCRAN packages used\nunits, errors, car, msm, metRology, propagate, spup, distr, distrEllipse, distrEx, distrMod, distrRmetrics, distrSim, distrTeach, magrittr, ggplot2, tibble\nCRAN Task Views implied by cited packages\nDistributions, ChemPhys, Econometrics, Finance, Graphics, Multivariate, Phylogenetics, Robust, SocialSciences, Survival, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-070/",
    "title": "SARIMA Analysis and Automated Model Reports with BETS, an R Package",
    "description": "This article aims to demonstrate how the powerful features of the R package BETS can be applied to SARIMA time series analysis. BETS provides not only thousands of Brazilian economic time series from different institutions, but also a range of analytical tools, and educational resources. In particular, BETS is capable of generating automated model reports for any given time series. These reports rely on a single function call and are able to build three types of models (SARIMA being one of them). The functions need few inputs and output rich content. The output varies according to the inputs and usually consists of a summary of the series properties, step-by-step explanations on how the model was developed, predictions made by the model, and a file containing these predictions. This work focuses on this feature and several other BETS functions that are designed to help in modeling time series. We present them in a thorough case study: the SARIMA approach to model and forecast the Brazilian production of intermediate goods index series.",
    "author": [
      {
        "name": "Talitha F. Speranza",
        "url": {}
      },
      {
        "name": "Pedro C. Ferreira",
        "url": {}
      },
      {
        "name": "Jonatha A. da Costa",
        "url": {}
      }
    ],
    "date": "2018-12-11",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nBETS, forecast, mFilter, urca, seasonal, httr, rvest, RMySQL, rmarkdown, stats, dygraphs\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance, WebTechnologies, Databases, Environmetrics, MissingData, OfficialStatistics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-072/",
    "title": "Explanations of Model Predictions with live and breakDown Packages ",
    "description": "Complex models are commonly used in predictive modeling. In this paper we present R packages that can be used for explaining predictions from complex black box models and attributing parts of these predictions to input features. We introduce two new approaches and corresponding packages for such attribution, namely live and breakDown. We also compare their results with existing implementations of state-of-the-art solutions, namely, lime (Pedersen and Benesty, 2018) which implements Locally Interpretable Model-agnostic Explanations and iml (Molnar et al., 2018) which implements Shapley values.",
    "author": [
      {
        "name": "Mateusz Staniak",
        "url": {}
      },
      {
        "name": "Przemysław Biecek",
        "url": {}
      }
    ],
    "date": "2018-12-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-072.zip\nCRAN packages used\npdp, lime, caret, mlr, DALEX, iml, live, breakDown, archivist, xgboost, party, data.table, e1071, glmnet, randomForest\nCRAN Task Views implied by cited packages\nMachineLearning, Environmetrics, HighPerformanceComputing, Multivariate, Survival, Cluster, Distributions, Finance, MissingData, ModelDeployment, Psychometrics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-073/",
    "title": "bnclassify: Learning Bayesian Network Classifiers",
    "description": "The bnclassify package provides state-of-the art algorithms for learning Bayesian network classifiers from data. For structure learning it provides variants of the greedy hill-climbing search, a well-known adaptation of the Chow-Liu algorithm and averaged one-dependence estimators. It provides Bayesian and maximum likelihood parameter estimation, as well as three naive-Bayes specific methods based on discriminative score optimization and Bayesian model averaging. The implementation is efficient enough to allow for time-consuming discriminative scores on medium sized data sets. The bnclassify package provides utilities for model evaluation, such as cross-validated accuracy and penalized log-likelihood scores, and analysis of the underlying networks, including network plotting via the Rgraphviz package. It is extensively tested, with over 200 automated tests that give a code coverage of 94%. Here we present the main functionalities, illustrate them with a number of data sets, and comment on related software.",
    "author": [
      {
        "name": "Bojan Mihaljević",
        "url": {}
      },
      {
        "name": "Concha Bielza",
        "url": {}
      },
      {
        "name": "Pedro Larrañaga",
        "url": {}
      }
    ],
    "date": "2018-12-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-073.zip\nCRAN packages used\nbnlearn, bnclassify, caret, mlr, gRain, deal\nCRAN Task Views implied by cited packages\nBayesian, gR, HighPerformanceComputing, MachineLearning, Multivariate\nBioconductor packages used\nRgraphviz\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-053/",
    "title": "stplanr: A Package for Transport Planning",
    "description": "Tools for transport planning should be flexible, scalable, and transparent. The stplanr package demonstrates and provides a home for such tools, with an emphasis on spatial transport data and non-motorized modes. The stplanr package facilitates common transport planning tasks including: downloading and cleaning transport datasets; creating geographic “desire lines” from origin-destination (OD) data; route assignment, locally and interfaces to routing services such as CycleStreets.net; calculation of route segment attributes such as bearing and aggregate flow; and ‘travel watershed’ analysis. This paper demonstrates this functionality using reproducible examples on real transport datasets. More broadly, the experience of developing and using R functions for transport applications shows that open source software can form the basis of a reproducible transport planning workflow. The stplanr package, alongside other packages and open source projects, could provide a more transparent and democratically accountable alternative to the current approach, which is heavily reliant on proprietary and relatively inaccessible software.",
    "author": [
      {
        "name": "Robin Lovelace",
        "url": {}
      },
      {
        "name": "Richard Ellison",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-053.zip\nCRAN packages used\nsp, rgeos, rgdal, sf, SpatialEpi, diseasemapping, leaflet, tmap, mapview, mapmisc, XML, twitteR, ggplot2, muStat, mgcv, shiny, haven, rio, dplyr, osmdata, stats19, bikedata, stplanr, nycflights, nycflights13, cyclestreets, igraph, Rcpp, aspace, MCI\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, Graphics, OfficialStatistics, SpatioTemporal, Bayesian, Databases, Econometrics, Environmetrics, gR, HighPerformanceComputing, ModelDeployment, NumericalMathematics, Optimization, Phylogenetics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-054/",
    "title": "rcss: R package for optimal convex stochastic switching",
    "description": "The R package rcss provides users with a tool to approximate the value functions in the Bellman recursion under certain assumptions that guarantee desirable convergence properties. This R package represents the first software implementation of these methods using matrices and nearest neighbors. This package also employs a pathwise dynamic method to gauge the quality of these value function approximations. Statistical analysis can be performed on the results to obtain other useful practical insights. This paper describes rcss version 1.6.",
    "author": [
      {
        "name": "Juri Hinz",
        "url": {}
      },
      {
        "name": "Jeremy Yee",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-054.zip\nCRAN packages used\nrcss, Rcpp, OpenMp\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-055/",
    "title": "addhaz: Contribution of Chronic Diseases to the Disability Burden Using R",
    "description": "The increase in life expectancy followed by the burden of chronic diseases contributes to disability at older ages. The estimation of how much chronic conditions contribute to disability can be useful to develop public health strategies to reduce the burden. This paper introduces the R package addhaz, which is based on the attribution method (Nusselder and Looman, 2004) to partition disability into the additive contributions of diseases using cross-sectional data. The R package includes tools to fit the additive hazard model, the core of the attribution method, to binary and multinomial outcomes. The models are fitted by maximizing the binomial and multinomial log-likelihood functions using constrained optimization. Wald and bootstrap confidence intervals can be obtained for the parameter estimates. Also, the contribution of diseases to the disability prevalence and their bootstrap confidence intervals can be estimated. An additional feature is the possibility to use parallel computing to obtain the bootstrap confidence intervals. In this manuscript, we illustrate the use of addhaz with several examples for the binomial and multinomial models, using the data from the Brazilian National Health Survey, 2013.",
    "author": [
      {
        "name": "Renata Tiene de Carvalho Yokota",
        "url": {}
      },
      {
        "name": "Caspar WN Looman",
        "url": {}
      },
      {
        "name": "Wilma Johanna Nusselder",
        "url": {}
      },
      {
        "name": "Herman Van            Oyen",
        "url": {}
      },
      {
        "name": "Geert Molenberghs",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-055.zip\nCRAN packages used\naddhaz, boot, stats, logbin, VGAM\nCRAN Task Views implied by cited packages\nEconometrics, SocialSciences, Survival, Distributions, Environmetrics, ExtremeValue, Multivariate, Optimization, Psychometrics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-056/",
    "title": "Snowboot: Bootstrap Methods for Network Inference",
    "description": "Complex networks are used to describe a broad range of disparate social systems and natural phenomena, from power grids to customer segmentation to human brain connectome. Challenges of parametric model specification and validation inspire a search for more data-driven and flexible nonparametric approaches for inference of complex networks. In this paper we discuss methodology and R implementation of two bootstrap procedures on random networks, that is, patchwork bootstrap of Thompson et al. (2016) and Gel et al. (2017) and vertex bootstrap of Snijders and Borgatti (1999). To our knowledge, the new R package snowboot is the first implementation of the vertex and patchwork bootstrap inference on networks in R. Our new package is accompanied with a detailed user’s manual, and is compatible with the popular R package on network studies igraph. We evaluate the patchwork bootstrap and vertex bootstrap with extensive simulation studies and illustrate their utility in an application to analysis of real world networks.",
    "author": [
      {
        "name": "Yuzhou Chen",
        "url": {}
      },
      {
        "name": "Yulia R. Gel",
        "url": {}
      },
      {
        "name": "Vyacheslav Lyubchich",
        "url": {}
      },
      {
        "name": "Kusha Nezafati",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-056.zip\nCRAN packages used\nsnowboot, bootnet, sna, graphics, igraph, parallel, Rcpp, Rdpack, stats, VGAM\nCRAN Task Views implied by cited packages\nOptimization, SocialSciences, Bayesian, Distributions, Econometrics, Environmetrics, ExtremeValue, gR, Graphics, HighPerformanceComputing, Multivariate, NumericalMathematics, Psychometrics, Spatial, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-057/",
    "title": "testforDEP: An R Package for Modern Distribution-free Tests and Visualization Tools for Independence",
    "description": "This article introduces testforDEP, a portmanteau R package implementing for the first time several modern tests and visualization tools for independence between two variables. While classical tests for independence are in the base R packages, there have been several recently developed tests for independence that are not available in R. This new package combines the classical tests including Pearson’s product moment correlation coefficient method, Kendall’s τ rank correlation coefficient method and Spearman’s ρ rank correlation coefficient method with modern tests consisting of an empirical likelihood based test, a density-based empirical likelihood ratio test, Kallenberg data driven test, maximal information coefficient test, Hoeffding’s independence test and the continuous analysis of variance test. For two input vectors of observations, the function testforDEP provides a common interface for each of the tests and returns test statistics, corresponding p values and bootstrap confidence intervals as output. The function AUK provides an interface to visualize Kendall plots and computes the area under the Kendall plot similar to computing the area under a receiver operating characteristic (ROC) curve.",
    "author": [
      {
        "name": "Jeffrey C. Miecznikowski",
        "url": {}
      },
      {
        "name": "En-shuo Hsu",
        "url": {}
      },
      {
        "name": "Yanhua Chen",
        "url": {}
      },
      {
        "name": "Albert Vexler",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-057.zip\nCRAN packages used\ntestforDEP, Hmisc, minerva\nCRAN Task Views implied by cited packages\nBayesian, ClinicalTrials, Econometrics, MissingData, Multivariate, OfficialStatistics, ReproducibleResearch, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-058/",
    "title": "Navigating the R Package Universe",
    "description": "Today, the enormous number of contributed packages available to R users outstrips any given user’s ability to understand how these packages work, their relative merits, or how they are related to each other. We organized a plenary session at useR!2017 in Brussels for the R community to think through these issues and ways forward. This session considered three key points of discussion. Users can navigate the universe of R packages with (1) capabilities for directly searching for R packages, (2) guidance for which packages to use, e.g., from CRAN Task Views and other sources, and (3) access to common interfaces for alternative approaches to essentially the same problem.",
    "author": [
      {
        "name": "Julia Silge",
        "url": {}
      },
      {
        "name": "John C. Nash",
        "url": {}
      },
      {
        "name": "Spencer Graves",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-058.zip\nCRAN packages used\nsos, CRANsearcher, utils, pkgdown, lfe, optimx\nCRAN Task Views implied by cited packages\nEconometrics, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-059/",
    "title": "rFSA: An R Package for Finding Best Subsets and Interactions",
    "description": "Herein we present the R package rFSA, which implements an algorithm for improved variable selection. The algorithm searches a data space for models of a user-specified form that are statistically optimal under a measure of model quality. Many iterations afford a set of feasible solutions (or candidate models) that the researcher can evaluate for relevance to his or her questions of interest. The algorithm can be used to formulate new or to improve upon existing models in bioinformatics, health care, and myriad other fields in which the volume of available data has outstripped researchers’ practical and computational ability to explore larger subsets or higher-order interaction terms. The package accommodates linear and generalized linear models, as well as a variety of criterion functions such as Allen’s PRESS and AIC. New modeling strategies and criterion functions can be adapted easily to work with rFSA.",
    "author": [
      {
        "name": "Joshua Lambert",
        "url": {}
      },
      {
        "name": "Liyu Gong",
        "url": {}
      },
      {
        "name": "Corrine F. Elliott",
        "url": {}
      },
      {
        "name": "Katherine Thompson",
        "url": {}
      },
      {
        "name": "Arnold Stromberg",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-059.zip\nCRAN packages used\nrFSA, leaps, glmulti, glmnet, hierNet, hashmap, geepack, devtools\nCRAN Task Views implied by cited packages\nSocialSciences, ChemPhys, Econometrics, MachineLearning, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-060/",
    "title": "lmridge: A Comprehensive R Package for Ridge Regression",
    "description": "The ridge regression estimator, one of the commonly used alternatives to the conventional ordinary least squares estimator, avoids the adverse effects in the situations when there exists some considerable degree of multicollinearity among the regressors. There are many software packages available for estimation of ridge regression coefficients. However, most of them display limited methods to estimate the ridge biasing parameters without testing procedures. Our developed package, lmridge can be used to estimate ridge coefficients considering a range of different existing biasing parameters, to test these coefficients with more than 25 ridge related statistics, and to present different graphical displays of these statistics.",
    "author": [
      {
        "name": "Muhammad Imdad Ullah",
        "url": {}
      },
      {
        "name": "Muhammad Aslam",
        "url": {}
      },
      {
        "name": "Saima Altaf",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-060.zip\nCRAN packages used\nlmridge, ridge, MASS, lrmest, ltsbase, penalized, glmnet, RXshrink, rrBLUP, RidgeFusion, bigRR, lpridge, genridge, CoxRidge\nCRAN Task Views implied by cited packages\nMachineLearning, Survival, Distributions, Econometrics, Environmetrics, Multivariate, NumericalMathematics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-061/",
    "title": "Geospatial Point Density",
    "description": "This paper introduces a spatial point density algorithm designed to be explainable, meaning ful, and efficient. Originally designed for military applications, this technique applies to any spatial point process where there is a desire to clearly understand the measurement of density and maintain fidelity of the point locations. Typical spatial density plotting algorithms, such as kernel density estimation, implement some type of smoothing function that often results in a density value that is difficult to interpret. The purpose of the visualization method in this paper is to understand spatial point activity density with precision and meaning. The temporal tendency of the point process as an extension of the point density methodology is also discussed and displayed. Applications include visualization and measurement of any type of spatial point process. Visualization techniques integrate ggmap with examples from San Diego crime data.",
    "author": [
      {
        "name": "Paul F. Evangelista",
        "url": {}
      },
      {
        "name": "David Beskow",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-061.zip\nCRAN packages used\npointdensityP, spatstat, kde2d, bkde2D, ggplot2, ggmap, data.table\nCRAN Task Views implied by cited packages\nSpatial, Finance, Graphics, HighPerformanceComputing, Phylogenetics, SpatioTemporal, Survival, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-063/",
    "title": "sdpt3r: Semidefinite Quadratic Linear Programming in R",
    "description": "We present the package sdpt3r, an R implementation of the Matlab package SDPT3 (Toh et al., 1999). The purpose of the software is to solve semidefinite quadratic linear programming (SQLP) problems, which encompasses problems such as D-optimal experimental design, the nearest correlation matrix problem, and distance weighted discrimination, as well as problems in graph theory",
    "author": [
      {
        "name": "Adam Rahman",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-063.zip\nCRAN packages used\nsdpt3r, Rdsdp, Rcsdp, cccp, scs, Rmosek, quantmod\nCRAN Task Views implied by cited packages\nOptimization, Finance\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-064/",
    "title": "Downside Risk Evaluation with the R Package GAS",
    "description": "Financial risk managers routinely use non–linear time series models to predict the downside risk of the capital under management. They also need to evaluate the adequacy of their model using so–called backtesting procedures. The latter involve hypothesis testing and evaluation of loss functions. This paper shows how the R package GAS can be used for both the dynamic prediction and the evaluation of downside risk. Emphasis is given to the two key financial downside risk measures: Value-at-Risk (VaR) and Expected Shortfall (ES). High-level functions for: (i) prediction, (ii) backtesting, and (iii) model comparison are discussed, and code examples are provided. An illustration using the series of log–returns of the Dow Jones Industrial Average constituents is reported.",
    "author": [
      {
        "name": "David Ardia",
        "url": {}
      },
      {
        "name": "Kris Boudt",
        "url": {}
      },
      {
        "name": "Leopoldo Catania",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-064.zip\nCRAN packages used\nGAS, cubature\nCRAN Task Views implied by cited packages\nNumericalMathematics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-065/",
    "title": "NetworkToolbox: Methods and Measures for Brain, Cognitive, and Psychometric Network Analysis in R",
    "description": "This article introduces the NetworkToolbox package for R. Network analysis offers an intuitive perspective on complex phenomena via models depicted by nodes (variables) and edges (correlations). The ability of networks to model complexity has made them the standard approach for modeling the intricate interactions in the brain. Similarly, networks have become an increasingly attractive model for studying the complexity of psychological and psychopathological phenomena. NetworkToolbox aims to provide researchers with state-of-the-art methods and measures for es timating and analyzing brain, cognitive, and psychometric networks. In this article, I introduce NetworkToolbox and provide a tutorial for applying some the package’s functions to personality data.",
    "author": [
      {
        "name": "Alexander P. Christensen",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-065.zip\nCRAN packages used\nNetworkToolbox, statnet, igraph, sna, brainGraph, qgraph, IsingFit, bootnet, glasso, psych, MVN, EGA, lavaan\nCRAN Task Views implied by cited packages\nPsychometrics, Optimization, SocialSciences, Bayesian, Econometrics, gR, Graphics, MissingData, OfficialStatistics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-066/",
    "title": "jsr223: A Java Platform Integration for R with Programming Languages Groovy, JavaScript, JRuby, Jython, and Kotlin",
    "description": "The R package jsr223 is a high-level integration for five programming languages in the Java platform: Groovy, JavaScript, JRuby, Jython, and Kotlin. Each of these languages can use Java objects in their own syntax. Hence, jsr223 is also an integration for R and the Java platform. It enables developers to leverage Java solutions from within R by embedding code snippets or evaluating script files. This approach is generally easier than rJava’s low-level approach that employs the Java Native Interface. jsr223’s multi-language support is dependent on the Java Scripting API: an implementation of “JSR-223: Scripting for the Java Platform” that defines a framework to embed scripts in Java applications. The jsr223 package also features extensive data exchange capabilities and a callback interface that allows embedded scripts to access the current R session. In all, jsr223 makes solutions developed in Java or any of the jsr223-supported languages easier to use in R.",
    "author": [
      {
        "name": "Floid R. Gilbert",
        "url": {}
      },
      {
        "name": "David B. Dahl",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-066.zip\nCRAN packages used\nrJava, cranlogs, jsr223, rscala, jdx, V8, R6, Rserve, opencpu, rGroovy, jsonlite, reticulate, rJython, PythonInR, rjson\nCRAN Task Views implied by cited packages\nWebTechnologies, ModelDeployment, NumericalMathematics, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-068/",
    "title": "RcppMsgPack: MessagePack Headers and Interface Functions for R",
    "description": "MessagePack, or MsgPack for short, or when referring to the implementation, is an efficient binary serialization format for exchanging data between different programming languages. The RcppMsgPack package provides R with both the MessagePack C++ header files, and the ability to access, create and alter MessagePack objects directly from R. The main driver functions of the R interface are two functions msgpack_pack and msgpack_unpack. The function msgpack_pack serializes R objects to a raw MessagePack message. The function msgpack_unpack de-serializes MessagePack messages back into R objects. Several helper functions are available to aid in processing and formatting data including msgpack_simplify, msgpack_format and msgpack_map.",
    "author": [
      {
        "name": "Travers Ching",
        "url": {}
      },
      {
        "name": "Dirk Eddelbuettel",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-068.zip\nCRAN packages used\nmongolite, RProtoBuf, RcppRedis, RcppMsgPack, Rcpp, nanotime, httr, feather, data.table\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Databases, NumericalMathematics, Finance, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-069/",
    "title": "BNSP: an R Package for Fitting Bayesian Semiparametric Regression Models and Variable Selection",
    "description": "The R package BNSP provides a unified framework for semiparametric location-scale regression and stochastic search variable selection. The statistical methodology that the package is built upon utilizes basis function expansions to represent semiparametric covariate effects in the mean and variance functions, and spike-slab priors to perform selection and regularization of the estimated effects. In addition to the main function that performs posterior sampling, the package includes functions for assessing convergence of the sampler, summarizing model fits, visualizing covariate effects and obtaining predictions for new responses or their means given feature/covariate vectors.",
    "author": [
      {
        "name": "Georgios Papageorgiou",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-069.zip\nCRAN packages used\nBNSP, bamlss, spikeSlabGAM, brms, gamboostLSS, mgcv, coda, ggplot2, plot3D, threejs, colorspace, np, gamair, lattice\nCRAN Task Views implied by cited packages\nBayesian, Graphics, Econometrics, Environmetrics, Phylogenetics, SocialSciences, gR, MachineLearning, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-079/",
    "title": "The politeness Package: Detecting Politeness in Natural Language",
    "description": "This package provides tools to extract politeness markers in English natural language. It also allows researchers to easily visualize and quantify politeness between groups of documents. This package combines and extends prior research on the linguistic markers of politeness (Brown and Levinson, 1987; Danescu-Niculescu-Mizil et al., 2013; Voigt et al., 2017). We demonstrate two applications for detecting politeness in natural language during consequential social interactions— distributive negotiations, and speed dating.",
    "author": [
      {
        "name": "Michael Yeomans",
        "url": {}
      },
      {
        "name": "Alejandro Kantor",
        "url": {}
      },
      {
        "name": "Dustin Tingley",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-079.zip\nCRAN packages used\ntidytext, tm, quanteda, coreNLP, spacyR, SentimentAnalysis, syuzhet, topicmodeling, stm, glmnet, textir, hunspell, data.table, politeness\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, HighPerformanceComputing, Finance, MachineLearning, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-080/",
    "title": "Consistency Cubes: a fast, efficient method for exact Boolean minimization.",
    "description": "A lot of effort has been spent over the past few decades in the QCA methodology field, to develop efficient Boolean minimization algorithms to derive an exact, and more importantly complete list of minimal prime implicants that explain the initial, observed positive configurations. As the complexity grows exponentially with every new condition, the required computer memory goes past the current computer resources and the polynomial time required to solve this problem quickly grows towards infinity. This paper introduces a new alternative to the existing non-polynomial attempts. It completely solves the memory problem, and preliminary tests show it is exponentially hundreds of time faster than eQMC, the current “best” algorithm for QCA in R, and probes into a territory where it competes and even outperforms engineering algorithms such as Espresso, for exact minimizations. While speed is not much of an issue now (eQMC is fast enough for simple data), it might prove to be essential when further developing towards all possible temporal orders, or searching for configurations in panel data over time, combined with / or automatic detection of difficult counterfactuals etc.",
    "author": [
      {
        "name": "Adrian Dusa",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-080.zip\nCRAN packages used\nQCA, lpSolve, venn, LogicOpt\nCRAN Task Views implied by cited packages\nOptimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-044/",
    "title": "revengc: An R package to Reverse Engineer Summarized Data",
    "description": "Decoupled (e.g. separate averages) and censored (e.g. > 100 species) variables are continually reported by many well-established organizations, such as the World Health Organization (WHO), Centers for Disease Control and Prevention (CDC), and World Bank. The challenge therefore is to infer what the original data could have been given summarized information. We present an R package that reverse engineers censored and/or decoupled data with two main functions. The cnbinom.pars() function estimates the average and dispersion parameter of a censored univariate frequency table. The rec() function reverse engineers summarized data into an uncensored bivariate table of probabilities.",
    "author": [
      {
        "name": "Samantha Duchscherer",
        "url": {}
      },
      {
        "name": "Robert Stewart",
        "url": {}
      },
      {
        "name": "Marie Urban",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-044.zip\nCRAN packages used\nrevengc, truncdist, mipfp\nCRAN Task Views implied by cited packages\nOfficialStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-045/",
    "title": "Basis-Adaptive Selection Algorithm in dr-package",
    "description": "Sufficient dimension reduction (SDR) turns out to be a useful dimension reduction tool in high-dimensional regression analysis. Weisberg (2002) developed the dr-package to implement the four most popular SDR methods. However, the package does not provide any clear guidelines as to which method should be used given a data. Since the four methods may provide dramatically different dimension reduction results, the selection in the dr-package is problematic for statistical practitioners. In this paper, a basis-adaptive selection algorithm is developed in order to relieve this issue. The basic idea is to select an SDR method that provides the highest correlation between the basis estimates obtained by the four classical SDR methods. A real data example and numerical studies confirm the practical usefulness of the developed algorithm.",
    "author": [
      {
        "name": "Jae Keun Yoo",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-045.zip\nCRAN packages used\ndr\nCRAN Task Views implied by cited packages\nMultivariate, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-046/",
    "title": "fICA: FastICA Algorithms and Their Improved Variants",
    "description": "In independent component analysis (ICA) one searches for mutually independent non gaussian latent variables when the components of the multivariate data are assumed to be linear combinations of them. Arguably, the most popular method to perform ICA is FastICA. There are two classical versions, the deflation-based FastICA where the components are found one by one, and the symmetric FastICA where the components are found simultaneously. These methods have been implemented previously in two R packages, fastICA and ica. We present the R package fICA and compare it to the other packages. Additional features in fICA include optimization of the extraction order in the deflation-based version, possibility to use any nonlinearity function, and improvement to convergence of the deflation-based algorithm. The usage of the package is demonstrated by applying it to the real ECG data of a pregnant woman.",
    "author": [
      {
        "name": "Jari Miettinen",
        "url": {}
      },
      {
        "name": "Klaus Nordhausen",
        "url": {}
      },
      {
        "name": "Sara Taskinen",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-046.zip\nCRAN packages used\nfastICA, ica, fICA, BSSasymp\nCRAN Task Views implied by cited packages\nPsychometrics, ChemPhys, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-047/",
    "title": "Spatial Uncertainty Propagation Analysis with the spup R Package",
    "description": "Many environmental and geographical models, such as those used in land degradation, agro ecological and climate studies, make use of spatially distributed inputs that are known imperfectly. The R package spup provides functions for examining the uncertainty propagation from input data and model parameters onto model outputs via the environmental model. The functions include uncertainty model specification, stochastic simulation and propagation of uncertainty using Monte Carlo (MC) techniques. Uncertain variables are described by probability distributions. Both numerical and categorical data types are handled. The package also accommodates spatial auto-correlation within a variable and cross-correlation between variables. The MC realizations may be used as input to the environmental models written in or called from R. This article provides theoretical background and three worked examples that guide users through the application of spup.",
    "author": [
      {
        "name": "Kasia Sawicka",
        "url": {}
      },
      {
        "name": "Gerard B.M. Heuvelink",
        "url": {}
      },
      {
        "name": "Dennis J.J. Walvoort",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-047.zip\nCRAN packages used\npropagate, errors, metRology, spup, gstat, stats, mvtnorm, whisker, shiny\nCRAN Task Views implied by cited packages\nChemPhys, WebTechnologies, Distributions, Finance, Multivariate, Spatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-048/",
    "title": "clustMixType: User-Friendly Clustering of Mixed-Type Data in R",
    "description": "Clustering algorithms are designed to identify groups in data where the traditional emphasis has been on numeric data. In consequence, many existing algorithms are devoted to this kind of data even though a combination of numeric and categorical data is more common in most business applications. Recently, new algorithms for clustering mixed-type data have been proposed based on Huang’s k-prototypes algorithm. This paper describes the R package clustMixType which provides an implementation of k-prototypes in R.",
    "author": [
      {
        "name": "Gero Szepannek",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-048.zip\nCRAN packages used\ngower, cluster, CluMix, flexclust, fpc, clustMD, kamila, clustMixType, klaR, wesanderson, clusteval\nCRAN Task Views implied by cited packages\nCluster, Multivariate, Environmetrics, Graphics, MachineLearning, Robust\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-049/",
    "title": "Stilt: Easy Emulation of Time Series AR(1) Computer Model Output in Multidimensional Parameter Space",
    "description": "Statistically approximating or “emulating” time series model output in parameter space is a common problem in climate science and other fields. There are many packages for spatio-temporal modeling. However, they often lack focus on time series, and exhibit statistical complexity. Here, we present the R package stilt designed for simplified AR(1) time series Gaussian process emulation, and provide examples relevant to climate modelling. Notably absent is Markov chain Monte Carlo estimation – a challenging concept to many scientists. We keep the number of user choices to a minimum. Hence, the package can be useful pedagogically, while still applicable to real life emulation problems. We provide functions for emulator cross-validation, empirical coverage, prediction, as well as response surface plotting. While the examples focus on climate model emulation, the emulator is general and can be also used for kriging spatio-temporal data.",
    "author": [
      {
        "name": "Roman Olson",
        "url": {}
      },
      {
        "name": "Kelsey L. Ruckert",
        "url": {}
      },
      {
        "name": "Won Chang",
        "url": {}
      },
      {
        "name": "Klaus Keller",
        "url": {}
      },
      {
        "name": "Murali Haran",
        "url": {}
      },
      {
        "name": "Soon-Il An",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-049.zip\nCRAN packages used\ngstat, mlegp, spBayes, ramps, spTimer, RandomFields, stilt, fields, maps, spam, dotCall64\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Bayesian, Multivariate, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-050/",
    "title": "SMM: An R Package for Estimation and Simulation of Discrete-time semi-Markov Models",
    "description": "Semi-Markov models, independently introduced by Lévy (1954), Smith (1955) and Takacs (1954), are a generalization of the well-known Markov models. For semi-Markov models, sojourn times can be arbitrarily distributed, while sojourn times of Markov models are constrained to be exponentially distributed (in continuous time) or geometrically distributed (in discrete time). The aim of this paper is to present the R package SMM, devoted to the simulation and estimation of discrete time multi-state semi-Markov and Markov models. For the semi-Markov case we have considered: parametric and non-parametric estimation; with and without censoring at the beginning and/or at the end of sample paths; one or several independent sample paths. Several discrete-time distributions are considered for the parametric estimation of sojourn time distributions of semi-Markov chains: Uniform, Geometric, Poisson, Discrete Weibull and Binomial Negative.",
    "author": [
      {
        "name": "Vlad Stefan Barbu",
        "url": {}
      },
      {
        "name": "Caroline Bérard",
        "url": {}
      },
      {
        "name": "Dominique Cellier",
        "url": {}
      },
      {
        "name": "Mathilde Sautreuil",
        "url": {}
      },
      {
        "name": "Nicolas Vergne",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSMM, semiMarkov, hsmm, mhsmm\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-051/",
    "title": "ggplot2 Compatible Quantile-Quantile Plots in R",
    "description": "Q-Q plots allow us to assess univariate distributional assumptions by comparing a set of quantiles from the empirical and the theoretical distributions in the form of a scatterplot. To aid in the interpretation of Q-Q plots, reference lines and confidence bands are often added. We can also detrend the Q-Q plot so the vertical comparisons of interest come into focus. Various implementations of Q-Q plots exist in R, but none implements all of these features. qqplotr extends ggplot2 to provide a complete implementation of Q-Q plots. This paper introduces the plotting framework provided by qqplotr and provides multiple examples of how it can be used.",
    "author": [
      {
        "name": "Alexandre Almeida",
        "url": {}
      },
      {
        "name": "Adam Loy",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-051.zip\nCRAN packages used\nbase, lattice, car, ggplot2, qqplotr, stats, robustbase, boot\nCRAN Task Views implied by cited packages\nMultivariate, Econometrics, Graphics, Robust, SocialSciences, Finance, Optimization, Phylogenetics, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-052/",
    "title": "Forecast Combinations in R using the ForecastComb Package",
    "description": "This paper introduces the R package ForecastComb. The aim is to provide researchers and practitioners with a comprehensive implementation of the most common ways in which forecasts can be combined. The package in its current version covers 15 popular estimation methods for creating a combined forecasts – including simple methods, regression-based methods, and eigenvector-based methods. It also includes useful tools to deal with common challenges of forecast combination (e.g., missing values in component forecasts, or multicollinearity), and to rationalize and visualize the combination results.",
    "author": [
      {
        "name": "Christoph E. Weiss",
        "url": {}
      },
      {
        "name": "Eran Raviv",
        "url": {}
      },
      {
        "name": "Gernot Roetzer",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-052.zip\nCRAN packages used\nBMA, opera, forecastHybrid, ForecastCombinations, GeomComb, quadprog, mtsdi, forecTheta\nCRAN Task Views implied by cited packages\nTimeSeries, Bayesian, Econometrics, OfficialStatistics, Optimization, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-040/",
    "title": "Profile Likelihood Estimation of the Correlation Coefficient in the Presence of Left, Right or Interval Censoring and Missing Data",
    "description": "We discuss implementation of a profile likelihood method for estimating a Pearson correla tion coefficient from bivariate data with censoring and/or missing values. The method is implemented in an R package clikcorr which calculates maximum likelihood estimates of the correlation coefficient when the data are modeled with either a Gaussian or a Student t-distribution, in the presence of left, right, or interval censored and/or missing data. The R package includes functions for conducting inference and also provides graphical functions for visualizing the censored data scatter plot and profile log likelihood function. The performance of clikcorr in a variety of circumstances is evaluated through extensive simulation studies. We illustrate the package using two dioxin exposure datasets.",
    "author": [
      {
        "name": "Yanming Li",
        "url": {}
      },
      {
        "name": "Brenda W. Gillespie",
        "url": {}
      },
      {
        "name": "Kerby Shedden",
        "url": {}
      },
      {
        "name": "John A. Gillespie",
        "url": {}
      }
    ],
    "date": "2018-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-040.zip\nCRAN packages used\nclikcorr, survival, mvtnorm\nCRAN Task Views implied by cited packages\nClinicalTrials, Distributions, Econometrics, Finance, Multivariate, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-041/",
    "title": "The utiml Package: Multi-label Classification in R",
    "description": "Learning classification tasks in which each instance is associated with one or more labels are known as multi-label learning. The implementation of multi-label algorithms, performed by different researchers, have several specificities, like input/output format, different internal functions, distinct programming language, to mention just some of them. As a result, current machine learning tools include only a small subset of multi-label decomposition strategies. The utiml package is a framework for the application of classification algorithms to multi-label data. Like the well known MULAN used with Weka, it provides a set of multi-label procedures such as sampling methods, transformation strategies, threshold functions, pre-processing techniques and evaluation metrics. The package was designed to allow users to easily perform complete multi-label classification experiments in the R environment. This paper describes the utiml API and illustrates its use in different multi-label classification scenarios.",
    "author": [
      {
        "name": "Adriano Rivolli",
        "url": {}
      },
      {
        "name": "Andre C. P. L. F. de Carvalho",
        "url": {}
      }
    ],
    "date": "2018-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-041.zip\nCRAN packages used\nmldr, mlr, MLPUGS, utiml, randomForest, C50, e1071, parallel\nCRAN Task Views implied by cited packages\nMachineLearning, Environmetrics, Cluster, Distributions, MissingData, Multivariate, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-042/",
    "title": "Dot-Pipe: an S3 Extensible Pipe for R",
    "description": "Pipe notation is popular with a large league of R users, with magrittr being the dominant realization. However, this should not be enough to consider piping in R as a settled topic that is not subject to further discussion, experimentation, or possibility for improvement. To promote innovation opportunities, we describe the wrapr R package and “dot-pipe” notation, a well behaved sequencing operator with S3 extensibility. We include a number of examples of using this pipe to interact with and extend other R packages.",
    "author": [
      {
        "name": "John Mount",
        "url": {}
      },
      {
        "name": "Nina Zumel",
        "url": {}
      }
    ],
    "date": "2018-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndata.table, magrittr, dplyr, future, rmonad, pipeR, backpipe, drake, wrapr, ggplot2, rquery\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Databases, Finance, Graphics, ModelDeployment, Phylogenetics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-043/",
    "title": "nsROC: An R package for Non-Standard ROC Curve Analysis",
    "description": "The receiver operating characteristic (ROC) curve is a graphical method which has become standard in the analysis of diagnostic markers, that is, in the study of the classification ability of a numerical variable. Most of the commercial statistical software provide routines for the standard ROC curve analysis. Of course, there are also many R packages dealing with the ROC estimation as well as other related problems. In this work we introduce the nsROC package which incorporates some new ROC curve procedures. Particularly: ROC curve comparison based on general distances among functions for both paired and unpaired designs; efficient confidence bands construction; a generalization of the curve considering different classification subsets than the one involved in the classical defini tion of the ROC curve; a procedure to deal with censored data in cumulative-dynamic ROC curve estimation for time-to-event outcomes; and a non-parametric ROC curve method for meta-analysis. This is the only R package which implements these particular procedures.",
    "author": [
      {
        "name": "Sonia Pérez-Fernández",
        "url": {}
      },
      {
        "name": "Pablo Martínez-Camblor",
        "url": {}
      },
      {
        "name": "Peter Filzmoser",
        "url": {}
      },
      {
        "name": "Norberto Corral",
        "url": {}
      }
    ],
    "date": "2018-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-043.zip\nCRAN packages used\npROC, ROCR, plotROC, fbroc, OptimalCutpoints, timeROC, survivalROC, HSROC, nsROC, sde, tdROC, survival\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, DifferentialEquations, Econometrics, Finance, MachineLearning, Multivariate, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-038/",
    "title": "mmpf: Monte-Carlo Methods for Prediction Functions",
    "description": "Machine learning methods can often learn high-dimensional functions which generalize well but are not human interpretable. The mmpf package marginalizes prediction functions using Monte-Carlo methods, allowing users to investigate the behavior of these learned functions, as on a lower dimensional subset of input features: partial dependence and variations thereof. This makes machine learning methods more useful in situations where accurate prediction is not the only goal, such as in the social sciences where linear models are commonly used because of their interpretability. Many methods for estimating prediction functions produce estimated functions which are not directly human-interpretable because of their complexity: for example, they may include high dimensional interactions and/or complex nonlinearities. While a learning method’s capacity to automatically learn interactions and nonlinearities is attractive when the goal is prediction, there are many cases where users want good predictions and the ability to understand how predictions depend on the features. mmpf implements general methods for interpreting prediction functions using Monte-Carlo methods. These methods allow any function which generates predictions to be be interpreted. mmpf is currently used in other packages for machine learning like edarf and mlr (Jones and Linder, 2016; Bischl et al., 2016).",
    "author": [
      {
        "name": "Zachary M. Jones",
        "url": {}
      }
    ],
    "date": "2018-06-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmmpf, edarf\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-039/",
    "title": "dimRed and coRanking - Unifying Dimensionality Reduction in R",
    "description": "“Dimensionality reduction” (DR) is a widely used approach to find low dimensional and interpretable representations of data that are natively embedded in high-dimensional spaces. DR can be realized by a plethora of methods with different properties, objectives, and, hence, (dis)advantages. The resulting low-dimensional data embeddings are often difficult to compare with objective criteria. Here, we introduce the dimRed and coRanking packages for the R language. These open source software packages enable users to easily access multiple classical and advanced DR methods using a common interface. The packages also provide quality indicators for the embeddings and easy visualization of high dimensional data. The coRanking package provides the functionality for assessing DR methods in the co-ranking matrix framework. In tandem, these packages allow for uncovering complex structures high dimensional data. Currently 15 DR methods are available in the package, some of which were not previously available to R users. Here, we outline the dimRed and coRanking packages and make the implemented methods understandable to the interested reader.",
    "author": [
      {
        "name": "Guido Kraemer",
        "url": {}
      },
      {
        "name": "Markus Reichstein",
        "url": {}
      },
      {
        "name": "Miguel D. Mahecha",
        "url": {}
      }
    ],
    "date": "2018-06-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndimRed, coRanking, kernlab, vegan, RANN, igraph, lle, diffusionMap, MASS, igraph, Rtsne, fastICA, DRR\nCRAN Task Views implied by cited packages\nMultivariate, Optimization, Psychometrics, Spatial, Environmetrics, gR, Graphics, ChemPhys, Cluster, Distributions, Econometrics, MachineLearning, NaturalLanguageProcessing, NumericalMathematics, Phylogenetics, Robust, SocialSciences\nBioconductor packages used\npcaMethods\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-037/",
    "title": "Collections in R: Review and Proposal",
    "description": "R is a powerful tool for data processing, visualization, and modeling. However, R is slower than other languages used for similar purposes, such as Python. One reason for this is that R lacks base support for collections, abstract data types that store, manipulate, and return data (e.g., sets, maps, stacks). An exciting recent trend in the R extension ecosystem is the development of collection packages, packages that provide classes that implement common collections. At least 12 collection packages are available across the two major R extension repositories, the Comprehensive R Archive Network (CRAN) and Bioconductor. In this article, we compare collection packages in terms of their features, design philosophy, ease of use, and performance on benchmark tests. We demonstrate that, when used well, the data structures provided by collection packages are in many cases significantly faster than the data structures provided by base R. We also highlight current deficiencies among R collection packages and propose avenues of possible improvement. This article provides useful recommendations to R programmers seeking to speed up their programs and aims to inform the development of future collection-oriented software for R.",
    "author": [
      {
        "name": "Timothy Barry",
        "url": {}
      }
    ],
    "date": "2018-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRcpp, hashr, hashFunction, filehashSQLite, tictoc, DSL, bit64, bit, Oarray, sets, filehash, hash, hashmap, rstackdeque, rstack, liqueueR, dequer, flifo, listenv, stdvectors, microbenchmark, neuroim, FindMinIC\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MedicalImaging, NumericalMathematics\nBioconductor packages used\nS4Vectors\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-036/",
    "title": "Small Area Disease Risk Estimation and Visualization Using R",
    "description": "Small area disease risk estimation is essential for disease prevention and control. In this paper, we demonstrate how R can be used to obtain disease risk estimates and quantify risk factors using areal data. We explain how to define disease risk models and how to perform Bayesian inference using the INLA package. We also show how to make interactive maps of estimates using the leaflet package to better understand the disease spatial patterns and communicate the results. We show an example of lung cancer risk in Pennsylvania, United States, in year 2002, and demonstrate that R represents an excellent tool for disease surveillance by enabling reproducible health data analysis.",
    "author": [
      {
        "name": "Paula Moraga",
        "url": {}
      }
    ],
    "date": "2018-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-036.zip\nCRAN packages used\nleaflet, SpatialEpi, spdep, ggplot2, flexdashboard, shiny, SpatialEpiApp, dygraphs, DT, rmarkdown\nCRAN Task Views implied by cited packages\nReproducibleResearch, Spatial, Econometrics, Graphics, Phylogenetics, TimeSeries, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-035/",
    "title": "RatingScaleReduction package: stepwise rating scale item reduction without predictability loss",
    "description": "This study presents an innovative method for reducing the number of rating scale items without predictability loss. The “area under the receiver operator curve” method (AUC ROC) is used for the stepwise method of reducing items of a rating scale. RatingScaleReduction R package contains the presented implementation. Differential evolution (a metaheuristic for optimization) was applied to one of the analyzed datasets to illustrate that the presented stepwise method can be used with other classifiers to reduce the number of rating scale items (variables). The targeted areas of application are decision making, data mining, machine learning, and psychometrics. Keywords: rating scale, receiver operator characteristic, ROC, AUC, scale reduction.",
    "author": [
      {
        "name": "Waldemar W. Koczkodaj",
        "url": {}
      },
      {
        "name": "Feng Li",
        "url": {}
      },
      {
        "name": "Alicja Wolny–Dominiak",
        "url": {}
      }
    ],
    "date": "2018-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npROC, ROCR, RatingScaleReduction, DEoptim\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-034/",
    "title": "ICSOutlier: Unsupervised Outlier Detection for Low-Dimensional Contamination Structure",
    "description": "Detecting outliers in a multivariate and unsupervised context is an important and ongoing problem notably for quality control. Many statistical methods are already implemented in R and are briefly surveyed in the present paper. But only a few lead to the accurate identification of potential outliers in the case of a small level of contamination. In this particular context, the Invariant Coordinate Selection (ICS) method shows remarkable properties for identifying outliers that lie on a low-dimensional subspace in its first invariant components. It is implemented in the ICSOutlier package. The main function of the package, ics.outlier, offers the possibility of labelling potential outliers in a completely automated way. Four examples, including two real examples in quality control, illustrate the use of the function. Comparing with several other approaches, it appears that ICS is generally as efficient as its competitors and shows an advantage in the context of a small proportion of outliers lying in a low-dimensional subspace. In quality control, the method may help in properly identifying some defective products while not detecting too many false positives.",
    "author": [
      {
        "name": "Aurore Archimbaud",
        "url": {}
      },
      {
        "name": "Klaus Nordhausen",
        "url": {}
      },
      {
        "name": "Anne Ruiz-Gazen",
        "url": {}
      }
    ],
    "date": "2018-05-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-034.zip\nCRAN packages used\nmvoutlier, CerioliOutlierDetection, rrcovHD, faoutlier, abodOutlier, HighDimOut, alphaOutlier, extremevalues, HDoutliers, outliers, DMwR2, ldbod, Rlof, depth, REPPlab, OutlierDC, pcadapt, rrcov, ICSOutlier, ICS, robustbase\nCRAN Task Views implied by cited packages\nRobust, Multivariate, OfficialStatistics, Psychometrics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-033/",
    "title": "RealVAMS: An R Package for Fitting a Multivariate Value-added Model (VAM)",
    "description": "We present RealVAMS, an R package for fitting a generalized linear mixed model to multimembership data with partially crossed and partially nested random effects. RealVAMS utilizes a multivariate generalized linear mixed model with pseudo-likelihood approximation for fitting normally distributed continuous response(s) jointly with a binary outcome. In an educational context, the model is referred to as a multidimensional value-added model, which extends previous theory to estimate the relationships between potential teacher contributions toward different student outcomes and to allow the consideration of a binary, real-world outcome such as graduation. The simultaneous joint modeling of continuous and binary outcomes was not available prior to RealVAMS due to computational difficulties. In this paper, we discuss the multidimensional model, describe RealVAMS, and demonstrate the use of this package and its modeling options with an educational data set.",
    "author": [
      {
        "name": "Jennifer Broatch",
        "url": {}
      },
      {
        "name": "Jennifer Green",
        "url": {}
      },
      {
        "name": "Andrew Karl",
        "url": {}
      }
    ],
    "date": "2018-05-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-033.zip\nCRAN packages used\nRealVAMS, lme4\nCRAN Task Views implied by cited packages\nBayesian, Econometrics, Environmetrics, OfficialStatistics, Psychometrics, SocialSciences, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-018/",
    "title": "PanJen: An R package for Ranking Transformations in a Linear Regression",
    "description": "PanJen is an R-package for ranking transformations in linear regressions. It provides users with the ability to explore the relationship between a dependent variable and its independent variables. The package offers an easy and data-driven way to choose a functional form in multiple linear regression models by comparing a range of parametric transformations. The parametric functional forms are benchmarked against each other and a non-parametric transformation. The package allows users to generate plots that show the relation between a covariate and the dependent variable. Furthermore, PanJen will enable users to specify specific functional transformations, driven by a priori and theory-based hypotheses. The package supplies both model fits and plots that allow users to make informed choices on the functional forms in their regression. We show that the ranking in PanJen outperforms the Box-Tidwell transformation, especially in the presence of inefficiency, heteroscedasticity or endogeneity.",
    "author": [
      {
        "name": "Cathrine Ulla Jensen",
        "url": {}
      },
      {
        "name": "Toke Emil Panduro",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-018.zip\nCRAN packages used\nPanJen, mgcv\nCRAN Task Views implied by cited packages\nBayesian, Econometrics, Environmetrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-019/",
    "title": "Tackling Uncertainties of Species Distribution Model Projections with Package mopa",
    "description": "Species Distribution Models (SDMs) constitute an important tool to assist decision-making in environmental conservation and planning in the context of climate change. Nevertheless, SDM pro jections are affected by a wide range of uncertainty factors (related to training data, climate projections and SDM techniques), which limit their potential value and credibility. The new package mopa pro vides tools for designing comprehensive multi-factor SDM ensemble experiments, combining multiple sources of uncertainty (e.g. baseline climate, pseudo-absence realizations, SDM techniques, future projections) and allowing to assess their contribution to the overall spread of the ensemble projection. In addition, mopa is seamlessly integrated with the climate4R bundle and allows straightforward retrieval and post-processing of state-of-the-art climate datasets (including observations and climate change projections), thus facilitating the proper analysis of key uncertainty factors related to climate data.",
    "author": [
      {
        "name": "M. Iturbide",
        "url": {}
      },
      {
        "name": "J. Bedia",
        "url": {}
      },
      {
        "name": "J.M. Gutiérrez",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-019.zip\nCRAN packages used\nmopa, sdm, biomod2, dismo, SDMTools, raster, sp, e1071, stats, ranger, earth, tree, rpart, caret\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, Environmetrics, Spatial, SpatioTemporal, Survival, Cluster, Distributions, HighPerformanceComputing, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-020/",
    "title": "FHDI: An R Package for Fractional Hot Deck Imputation",
    "description": "Fractional hot deck imputation (FHDI), proposed by Kalton and Kish (1984) and investigated by Kim and Fuller (2004), is a tool for handling item nonresponse in survey sampling. In FHDI, each missing item is filled with multiple observed values yielding a single completed data set for subsequent analyses. An R package FHDI is developed to perform FHDI and also the fully efficient fractional imputation (FEFI) method of (Fuller and Kim, 2005) to impute multivariate missing data with arbitrary missing patterns. FHDI substitutes missing items with a few observed values jointly obtained from a set of donors whereas the FEFI uses all the possible donors. This paper introduces FHDI as a tool for implementing the multivariate version of fractional hot deck imputation discussed in Im et al. (2015) as well as FEFI. For variance estimation of FHDI and FEFI, the Jackknife method is implemented, and replicated weights are provided as a part of the output.",
    "author": [
      {
        "name": "Jongho Im",
        "url": {}
      },
      {
        "name": "In Ho Cho",
        "url": {}
      },
      {
        "name": "Jae Kwang Kim",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-020.zip\nCRAN packages used\nmice, mi, Amelia, VIM, FHDI\nCRAN Task Views implied by cited packages\nOfficialStatistics, SocialSciences, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-021/",
    "title": "Bayesian Testing, Variable Selection and Model Averaging in Linear Models using R with BayesVarSel",
    "description": "In this paper, objective Bayesian methods for hypothesis testing and variable selection in linear models are considered. The focus is on BayesVarSel, an R package that computes posterior probabilities of hypotheses/models and provides a suite of tools to properly summarize the results. We introduce the usage of specific functions to compute several types of model averaging estimations and predictions weighted by posterior probabilities. BayesVarSel contains exact algorithms to perform fast computations in problems of small to moderate size and heuristic sampling methods to solve large problems. We illustrate the functionalities of the package with several data examples.",
    "author": [
      {
        "name": "Gonzalo Garcia-Donato",
        "url": {}
      },
      {
        "name": "Anabel Forte",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-021.zip\nCRAN packages used\nBayesVarSel, faraway, BayesFactor, BMS, mombf, BAS, BMA\nCRAN Task Views implied by cited packages\nBayesian, Econometrics, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-022/",
    "title": "onewaytests: An R Package for One-Way Tests in Independent Groups Designs",
    "description": "One-way tests in independent groups designs are the most commonly utilized statistical methods with applications on the experiments in medical sciences, pharmaceutical research, agri culture, biology, engineering, social sciences and so on. In this paper, we present the onewaytests package to investigate treatment effects on the dependent variable. The package offers the one-way tests in independent groups designs, which include ANOVA, Welch’s heteroscedastic F test, Welch’s heteroscedastic F test with trimmed means and Winsorized variances, Brown-Forsythe test, Alexander Govern test, James second order test and Kruskal-Wallis test. The package also provides pairwise comparisons, graphical approaches, and assesses variance homogeneity and normality of data in each group via tests and plots. A simulation study is also conducted to give recommendations for applied researchers on the selection of appropriate one-way tests under assumption violations. Furthermore, especially for non-R users, a user-friendly web application of the package is provided. This application is available at http://www.softmed.hacettepe.edu.tr/onewaytests.",
    "author": [
      {
        "name": "Osman Dag",
        "url": {}
      },
      {
        "name": "Anil Dolgun",
        "url": {}
      },
      {
        "name": "Naime Meric Konar",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-022.zip\nCRAN packages used\nonewaytests, onewaytests, stats\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-023/",
    "title": "Inventorymodel: an R Package for Centralized Inventory Problems",
    "description": "Inventory management of goods is an integral part of logistics systems; common to various economic sectors such as industry, agriculture and trade; and independent of production volume. In general, as companies seek to minimize economic losses, studies on problems of multi-agent inventory have increased in recent years. A multi-agent inventory problem is a situation in which several agents face individual inventory problems and agree to coordinate their orders with the objective of reducing their costs. The R package Inventorymodel allows the determination of both the optimal policy for some inventory situations with deterministic demands and the allocation of costs from a game-theoretic perspective. The required calculations may be computed for any number of agents although the computational complexity of this class of problems when the involved agents enlarge is not reduced. In this work, the different possibilities that the package offers are described and some examples of usage are also demonstrated.",
    "author": [
      {
        "name": "Alejandro Saavedra-Nieves",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-023.zip\nCRAN packages used\nInventorymodel, e1071, GameTheoryAllocation\nCRAN Task Views implied by cited packages\nCluster, Distributions, Environmetrics, MachineLearning, Multivariate, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-024/",
    "title": "R Package imputeTestbench to Compare Imputation Methods for Univariate Time Series",
    "description": "Missing observations are common in time series data and several methods are available to impute these values prior to analysis. Variation in statistical characteristics of univariate time series can have a profound effect on characteristics of missing observations and, therefore, the accuracy of different imputation methods. The imputeTestbench package can be used to compare the prediction accuracy of different methods as related to the amount and type of missing data for a user-supplied dataset. Missing data are simulated by removing observations completely at random or in blocks of different sizes depending on characteristics of the data. Several imputation algorithms are included with the package that vary from simple replacement with means to more complex interpolation methods. The testbench is not limited to the default functions and users can add or remove methods as needed. Plotting functions also allow comparative visualization of the behavior and effectiveness of different algorithms. We present example applications that demonstrate how the package can be used to understand differences in prediction accuracy between methods as affected by characteristics of a dataset and the nature of missing data.",
    "author": [
      {
        "name": "Marcus W Beck",
        "url": {}
      },
      {
        "name": "Neeraj Bokde",
        "url": {}
      },
      {
        "name": "Gualberto Asencio-Cortés",
        "url": {}
      },
      {
        "name": "Kishore Kulat",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-024.zip\nCRAN packages used\nimputeTestbench, dplyr, reshape2, tidyr, ggplot2, forecast, imputeTS, zoo, stats, datasets, Rcpp, matlabr\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Environmetrics, Finance, Graphics, HighPerformanceComputing, ModelDeployment, NumericalMathematics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-025/",
    "title": "rpostgis: Linking R with a PostGIS Spatial Database",
    "description": "With the proliferation of sensors and the ease of data collection from online sources, large datasets have become the norm in many scientific disciplines, and efficient data storage, management, and retrival is imperative for large research projects. Relational databases provide a solution, but in order to be useful, must be able to be linked to analysis and visualization tools, such as R. Here, we present a package intended to facilitate integration of R with the open-source database software PostgreSQL, with a focus on its spatial extension, PostGIS. The package rpostgis (version 1.4.1) provides methods for spatial data handling (vector and raster) between PostGIS-enabled databases and R, methods for R \"data.frame\"s storage in PostgreSQL, and a set of convenient wrappers for common database procedures. We thus expect rpostgis to be useful for both (1) existing users of spatial data in R and/or PostGIS, and (2) R users who have yet to adopt relational databases for their projects.",
    "author": [
      {
        "name": "David Bucklin",
        "url": {}
      },
      {
        "name": "Mathieu Basille",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-025.zip\nCRAN packages used\nrgdal, maptools, raster, RPostgreSQL, DBI, rpostgis, sp, rgeos, wkb, sf, rpostgisLT, adehabitatLT\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-026/",
    "title": "lba: An R Package for Latent Budget Analysis",
    "description": "The latent budget model is a mixture model for compositional data sets in which the entries, a contingency table, may be either realizations from a product multinomial distribution or distribution free. Based on this model, the latent budget analysis considers the interactions of two variables; the ex planatory (row) and the response (column) variables. The package lba uses expectation-maximization and active constraints method (ACM) to carry out, respectively, the maximum likelihood and the least squares estimation of the model parameters. It contains three main functions, lba which performs the analysis, goodnessfit for model selection and goodness of fit and the plotting functions plotcorr and plotlba used as a help in the interpretation of the results.",
    "author": [
      {
        "name": "Enio G. Jelihovschi",
        "url": {}
      },
      {
        "name": "Ivan Bezerra Allaman",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-026.zip\nCRAN packages used\nlba, alabama, plotrix, scatterplot3d, rgl, MASS\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, Psychometrics, Distributions, Econometrics, Environmetrics, NumericalMathematics, Optimization, Robust, SocialSciences, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-027/",
    "title": "Semiparametric Generalized Linear Models with the gldrm Package",
    "description": "This paper introduces a new algorithm to estimate and perform inferences on a recently proposed and developed semiparametric generalized linear model (glm). Rather than selecting a particular parametric exponential family model, such as the Poisson distribution, this semiparametric glm assumes that the response is drawn from the more general exponential tilt family. The regression coefficients and unspecified reference distribution are estimated by maximizing a semiparametric like lihood. The new algorithm incorporates several computational stability and efficiency improvements over the algorithm originally proposed. In particular, the new algorithm performs well for either small or large support for the nonparametric response distribution. The algorithm is implemented in a new R package called gldrm.",
    "author": [
      {
        "name": "Michael J. Wurm",
        "url": {}
      },
      {
        "name": "Paul J. Rathouz",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-027.zip\nCRAN packages used\ngldrm\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-028/",
    "title": "LP Algorithms for Portfolio Optimization: The PortfolioOptim Package",
    "description": "The paper describes two algorithms for financial portfolio optimization with the following risk measures: CVaR, MAD, LSAD and dispersion CVaR. These algorithms can be applied to discrete distributions of asset returns since then the optimization problems can be reduced to linear programs. The first algorithm solves a simple recourse problem as described by Haneveld using Benders de composition method. The second algorithm finds an optimal portfolio with the smallest distance to a given benchmark portfolio and is an adaptation of the least norm solution (called also normal solution) of linear programs due to Zhao and Li. The algorithms are implemented in R in the package PortfolioOptim.",
    "author": [
      {
        "name": "Andrzej Palczewski",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-028.zip\nCRAN packages used\nfPortfolio, PortfolioAnalytics, Rglpk, quadprog, DEoptim, GenSA, psoptim, parma, nloptr, PortfolioOptim\nCRAN Task Views implied by cited packages\nOptimization, Finance\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-029/",
    "title": "Welfare, Inequality and Poverty Analysis with rtip: An Approach Based on Stochastic Dominance",
    "description": "Disparities in economic welfare, inequality and poverty across and within countries are of great interest to sociologists, economists, researchers, social organizations and political scientists. Information about these topics is commonly based on surveys. We present a package called rtip that implements techniques based on stochastic dominance to make unambiguous comparisons, in terms of welfare, poverty and inequality, among income distributions. Besides providing point estimates and confidence intervals for the most commonly used indicators of these characteristics, the package rtip estimates the usual Lorenz curve, the generalized Lorenz curve, the TIP (Three I’s of Poverty) curve and allows to test statistically whether one curve is dominated by another.",
    "author": [
      {
        "name": "Angel Berihuete",
        "url": {}
      },
      {
        "name": "Carmen D. Ramos",
        "url": {}
      },
      {
        "name": "Miguel A. Sordo",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-029.zip\nCRAN packages used\nrtip, IC2, ineq, laeken, boot\nCRAN Task Views implied by cited packages\nOfficialStatistics, Econometrics, Optimization, SocialSciences, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-031/",
    "title": "SetMethods: an Add-on R Package for Advanced QCA",
    "description": "This article presents the functionalities of the R package SetMethods, aimed at performing advanced set-theoretic analyses. This includes functions for performing set-theoretic multi-method research, set-theoretic theory evaluation, Enhanced Standard Analysis, diagnosing the impact of temporal, spatial, or substantive clusterings of the data on the results obtained via Qualitative Com parative Analysis (QCA), indirect calibration, and visualising QCA results via XY plots or radar charts. Each functionality is presented in turn, the conceptual idea and the logic behind the procedure being first summarized, and afterwards illustrated with data from Schneider et al. (2010).",
    "author": [
      {
        "name": "Ioana-Elena Oana",
        "url": {}
      },
      {
        "name": "Carsten Q. Schneider",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-031.zip\nCRAN packages used\nQCA, SetMethods\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-032/",
    "title": "HRM: An R Package for Analysing High-dimensional Multi-factor Repeated Measures",
    "description": "High-dimensional longitudinal data pose a serious challenge for statistical inference as many test statistics cannot be computed for high-dimensional data, or they do not maintain the nominal type-I error rate, or have very low power. Therefore, it is necessary to derive new inference methods capable of dealing with high dimensionality, and to make them available to statistics practitioners. One such method is implemented in the package HRM described in this article. This new method uses a similar approach as the Welch-Satterthwaite t-test approximation and works very well for high-dimensional data as long as the data distribution is not too skewed or heavy-tailed. The package also provides a GUI to offer an easy way to apply the methods.",
    "author": [
      {
        "name": "Martin Happ",
        "url": {}
      },
      {
        "name": "Solomon W. Harrar",
        "url": {}
      },
      {
        "name": "Arne C. Bathke",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nHRM, ggplot2, data.table, RGtk2, RGtk2Extras, cairoDevice, xtable, longitudinal, MANOVA.RM\nCRAN Task Views implied by cited packages\nGraphics, Finance, HighPerformanceComputing, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-017/",
    "title": "Advanced Bayesian Multilevel Modeling with the R Package brms",
    "description": "The brms package allows R users to easily specify a wide range of Bayesian single-level and multilevel models which are fit with the probabilistic programming language Stan behind the scenes. Several response distributions are supported, of which all parameters (e.g., location, scale, and shape) can be predicted. Non-linear relationships may be specified using non-linear predictor terms or semi-parametric approaches such as splines or Gaussian processes. Multivariate models can be fit as well. To make all of these modeling options possible in a multilevel framework, brms provides an intuitive and powerful formula syntax, which extends the well known formula syntax of lme4. The purpose of the present paper is to introduce this syntax in detail and to demonstrate its usefulness with four examples, each showing relevant aspects of the syntax.",
    "author": [
      {
        "name": "Paul-Christian Bürkner",
        "url": {}
      }
    ],
    "date": "2018-05-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbrms, lme4, rstanarm, MCMCglmm, mgcv, nlme, afex, loo, gamlss.data, bridgesampling\nCRAN Task Views implied by cited packages\nBayesian, SocialSciences, Econometrics, Environmetrics, Psychometrics, OfficialStatistics, SpatioTemporal, ChemPhys, Finance, Phylogenetics, Spatial, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-005/",
    "title": "Support Vector Machines for Survival Analysis with R",
    "description": "This article introduces the R package survivalsvm, implementing support vector machines for survival analysis. Three approaches are available in the package: The regression approach takes censoring into account when formulating the inequality constraints of the support vector problem. In the ranking approach, the inequality constraints set the objective to maximize the concordance index for comparable pairs of observations. The hybrid approach combines the regression and ranking constraints in a single model. We describe survival support vector machines and their implementation, provide examples and compare the prediction performance with the Cox proportional hazards model, random survival forests and gradient boosting using several real datasets. On these datasets, survival support vector machines perform on par with the reference methods.",
    "author": [
      {
        "name": "Césaire J. K. Fouodo",
        "url": {}
      },
      {
        "name": "Inke R. König",
        "url": {}
      },
      {
        "name": "Claus Weihs",
        "url": {}
      },
      {
        "name": "Andreas Ziegler",
        "url": {}
      },
      {
        "name": "Marvin N. Wright",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-005.zip\nCRAN packages used\nsurvivalsvm, kernlab, pracma, quadprog, Matrix, randomForestSRC, mboost, mlr, ggplot2, tikzDevice\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, NumericalMathematics, Optimization, Survival, Cluster, DifferentialEquations, Econometrics, Graphics, HighPerformanceComputing, NaturalLanguageProcessing, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-008/",
    "title": "Nonparametric Independence Tests and k-sample Tests for Large Sample Sizes Using Package HHG",
    "description": "Nonparametric tests of independence and k-sample tests are ubiquitous in modern applica tions, but they are typically computationally expensive. We present a family of nonparametric tests that are computationally efficient and powerful for detecting any type of dependence between a pair of univariate random variables. The computational complexity of the suggested tests is sub-quadratic in sample size, allowing calculation of test statistics for millions of observations. We survey both algorithms and the HHG package in which they are implemented, with usage examples showing the implementation of the proposed tests for both the independence case and the k-sample problem. The tests are compared to existing nonparametric tests via several simulation studies comparing both runtime and power. Special focus is given to the design of data structures used in implementation of the tests. These data structures can be useful for developers of nonparametric distribution-free tests.",
    "author": [
      {
        "name": "Barak Brill",
        "url": {}
      },
      {
        "name": "Yair Heller",
        "url": {}
      },
      {
        "name": "Ruth Heller",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-008.zip\nCRAN packages used\nHmisc, infotheo, entropy, minerva, dHSIC, energy, HHG, kernlab, dslice, rbenchmark, doRNG\nCRAN Task Views implied by cited packages\nMultivariate, Bayesian, ClinicalTrials, Cluster, Econometrics, HighPerformanceComputing, MachineLearning, NaturalLanguageProcessing, OfficialStatistics, Optimization, ReproducibleResearch, SocialSciences\nBioconductor packages used\nminet\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-009/",
    "title": "Simple Features for R: Standardized Support for Spatial Vector Data",
    "description": "Simple features are a standardized way of encoding spatial vector data (points, lines, polygons) in computers. The sf package implements simple features in R, and has roughly the same capacity for spatial vector data as packages sp, rgeos, and rgdal. We describe the need for this package, its place in the R package ecosystem, and its potential to connect R to other computer systems. We illustrate this with examples of its use.",
    "author": [
      {
        "name": "Edzer Pebesma",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-009.zip\nCRAN packages used\nsf, sp, rgdal, rgeos, tidyverse, dplyr, ggplot2, lwgeom, geosphere, s2, raster, Rcpp\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Graphics, HighPerformanceComputing, ModelDeployment, NumericalMathematics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-010/",
    "title": "Pstat: An R Package to Assess Population Differentiation in Phenotypic Traits",
    "description": "The package Pstat calculates PST values to assess differentiation among populations from a set of quantitative traits and provides bootstrapped distributions and confidence intervals for PST . Variations of PST as a function of the parameter c/h2 are studied as well. The package implements different transformations of the measured phenotypic traits to eliminate variation resulting from allometric growth, including calculation of residuals from linear regression, Reist standardization, and the Aitchison transformation.",
    "author": [
      {
        "name": "Stéphane Blondeau Da Silva",
        "url": {}
      },
      {
        "name": "Anne Da Silva",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-010.zip\nCRAN packages used\nPstat, diveRsity, hierfstat\nCRAN Task Views implied by cited packages\nGenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-011/",
    "title": "Approximating the Sum of Independent Non-Identical Binomial Random Variables",
    "description": "The distribution of the sum of independent non-identical binomial random variables is frequently encountered in areas such as genomics, healthcare, and operations research. Analytical solutions for the density and distribution are usually cumbersome to find and difficult to compute. Several methods have been developed to approximate the distribution, among which is the saddlepoint approximation. However, implementation of the saddlepoint approximation is non-trivial. In this paper, we implement the saddlepoint approximation in the sinib package and provide two examples to illustrate its usage. One example uses simulated data while the other uses real-world healthcare data. The sinib package addresses the gap between the theory and the implementation of approximating the sum of independent non-identical binomials.",
    "author": [
      {
        "name": "Boxiang Liu",
        "url": {}
      },
      {
        "name": "Thomas Quertermous",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-011.zip\nCRAN packages used\nstats, EQL, sinib\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-012/",
    "title": "cchs: An R Package for Stratified Case-Cohort Studies",
    "description": "The cchs package contains a function, also called cchs, for analyzing data from a stratified case-cohort study, as used in epidemiology. For data from this type of study, cchs calculates Estimator III of Borgan et al. (2000), which is a score-unbiased estimator for the regression coefficients in the Cox proportional hazards model. From the user’s point of view, the function is similar to coxph (in the survival package) and other widely used model-fitting functions. Convenient software has not previously been available for Estimator III since it is complicated to calculate. SAS and S-Plus code-fragments for the calculation have been published, but cchs is easier to use and more efficient in terms of time and memory, and can cope with much larger datasets. It also avoids several minor approximations and simplifications.",
    "author": [
      {
        "name": "Edmund Jones",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-012.zip\nCRAN packages used\ncchs, survival, cchs, survival, survey, NestedCohort\nCRAN Task Views implied by cited packages\nSurvival, SocialSciences, ClinicalTrials, Econometrics, OfficialStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-013/",
    "title": "InfoTrad: An R package for estimating the probability of informed trading",
    "description": "The purpose of this paper is to introduce the R package InfoTrad for estimating the proba bility of informed trading (PIN) initially proposed by Easley et al. (1996). PIN is a popular information asymmetry measure that proxies the proportion of informed traders in the market. This study provides a short survey on alternative estimation techniques for the PIN. There are many problems documented in the existing literature in estimating PIN. InfoTrad package aims to address two problems. First, the sequential trading structure proposed by Easley et al. (1996) and later extended by Easley et al. (2002) is prone to sample selection bias for stocks with large trading volumes, due to floating point exception. This problem is solved by different factorizations provided by Easley et al. (2010) (EHO factorization) and Lin and Ke (2011) (LK factorization). Second, the estimates are prone to bias due to boundary solutions. A grid-search algorithm (YZ algorithm) is proposed by Yan and Zhang (2012) to overcome the bias introduced due to boundary estimates. In recent years, clustering algorithms have become popular due to their flexibility in quickly handling large data sets. Gan et al. (2015) propose an algorithm (GAN algorithm) to estimate PIN using hierarchical agglomerative clustering which is later extended by Ersan and Alici (2016) (EA algorithm). The package InfoTrad offers LK and EHO factorizations given an input matrix and initial parameter vector. In addition, these factorizations can be used to estimate PIN through YZ algorithm, GAN algorithm and EA algorithm.",
    "author": [
      {
        "name": "Duygu Çelik",
        "url": {}
      },
      {
        "name": "Murat Tiniç",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-013.zip\nCRAN packages used\nInfoTrad, FinAsym, PIN, nloptr\nCRAN Task Views implied by cited packages\nFinance, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-014/",
    "title": "Generalized Additive Model Multiple Imputation by Chained Equations With Package ImputeRobust",
    "description": "Data analysis, common to all empirical sciences, often requires complete data sets. Unfortu nately, real world data collection will usually result in data values not being observed. We present a package for robust multiple imputation (the ImputeRobust package) that allows the use of generalized additive models for location, scale, and shape in the context of chained equations. The paper describes the basics of the imputation technique which builds on a semi-parametric regression model (GAMLSS) and the algorithms and functions provided with the corresponding package. Furthermore, some illustrative examples are provided.",
    "author": [
      {
        "name": "Daniel Salfran",
        "url": {}
      },
      {
        "name": "Martin Spiess",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-014.zip\nCRAN packages used\nImputeRobust, mice, gamlss\nCRAN Task Views implied by cited packages\nEconometrics, Multivariate, OfficialStatistics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-015/",
    "title": "MGLM: An R Package for Multivariate Categorical Data Analysis",
    "description": "Data with multiple responses is ubiquitous in modern applications. However, few tools are available for regression analysis of multivariate counts. The most popular multinomial-logit model has a very restrictive mean-variance structure, limiting its applicability to many data sets. This article introduces an R package MGLM, short for multivariate response generalized linear models, that expands the current tools for regression analysis of polytomous data. Distribution fitting, random number generation, regression, and sparse regression are treated in a unifying framework. The algorithm, usage, and implementation details are discussed.",
    "author": [
      {
        "name": "Juhyun Kim",
        "url": {}
      },
      {
        "name": "Yiwen Zhang",
        "url": {}
      },
      {
        "name": "Joshua Day",
        "url": {}
      },
      {
        "name": "Hua Zhou",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-015.zip\nCRAN packages used\nMGLM, VGAM, glmnet, dirmult, parallel, isoform, glmc\nCRAN Task Views implied by cited packages\nDistributions, Survival, Econometrics, Environmetrics, ExtremeValue, MachineLearning, Multivariate, Psychometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-016/",
    "title": "ArCo: An R package to Estimate Artificial Counterfactuals",
    "description": "In this paper we introduce the ArCo package for R which consists of a set of functions to implement the the Artificial Counterfactual (ArCo) methodology to estimate causal effects of an intervention (treatment) on aggregated data and when a control group is not necessarily available. The ArCo method is a two-step procedure, where in the first stage a counterfactual is estimated from a large panel of time series from a pool of untreated peers. In the second-stage, the average treatment effect over the post-intervention sample is computed. Standard inferential procedures are available. The package is illustrated with both simulated and real datasets.",
    "author": [
      {
        "name": "Yuri R. Fonseca",
        "url": {}
      },
      {
        "name": "Ricardo P. Masini",
        "url": {}
      },
      {
        "name": "Marcelo C. Medeiros",
        "url": {}
      },
      {
        "name": "Gabriel F. R. Vasconcelos",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-016.zip\nCRAN packages used\nArCo, boot, glmnet, Synth\nCRAN Task Views implied by cited packages\nSurvival, Econometrics, MachineLearning, Optimization, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-001/",
    "title": "A System for an Accountable Data Analysis Process in R",
    "description": "Efficiently producing transparent analyses may be difficult for beginners or tedious for the experienced. This implies a need for computing systems and environments that can efficiently satisfy reproducibility and accountability standards. To this end, we have developed a system, R package, and R Shiny application called adapr (Accountable Data Analysis Process in R) that is built on the principle of accountable units. An accountable unit is a data file (statistic, table or graphic) that can be associated with a provenance, meaning how it was created, when it was created and who created it, and this is similar to the ’verifiable computational results’ (VCR) concept proposed by Gavish and Donoho. Both accountable units and VCRs are version controlled, sharable, and can be incorporated into a collaborative project. However, accountable units use file hashes and do not involve watermarking or public repositories like VCRs. Reproducing collaborative work may be highly complex, requiring repeating computations on multiple systems from multiple authors; however, determining the provenance of each unit is simpler, requiring only a search using file hashes and version control systems.",
    "author": [
      {
        "name": "Jonathan Gelfond",
        "url": {}
      },
      {
        "name": "Martin Goros",
        "url": {}
      },
      {
        "name": "Brian Hernandez",
        "url": {}
      },
      {
        "name": "Alex Bokov",
        "url": {}
      }
    ],
    "date": "2018-05-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nknitr, rmarkdown, cacher, archivist, adapr, packrat\nCRAN Task Views implied by cited packages\nReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-002/",
    "title": "GrpString: An R Package for Analysis of Groups of Strings",
    "description": "The R package GrpString was developed as a comprehensive toolkit for quantitatively analyzing and comparing groups of strings. It offers functions for researchers and data analysts to prepare strings from event sequences, extract common patterns from strings, and compare patterns be tween string vectors. The package also finds transition matrices and complexity of strings, determines clusters in a string vector, and examines the statistical difference between two groups of strings.",
    "author": [
      {
        "name": "Hui Tang",
        "url": {}
      },
      {
        "name": "Elizabeth L. Day",
        "url": {}
      },
      {
        "name": "Molly B. Atkinson",
        "url": {}
      },
      {
        "name": "Norbert J. Pienta",
        "url": {}
      }
    ],
    "date": "2018-05-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-002.zip\nCRAN packages used\nstringr, stringb, stringi, gsubfn, uniqtag, stringdist, TraMineR, informR, GrpString, entropy\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, OfficialStatistics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-003/",
    "title": "Epistemic Game Theory: Putting Algorithms to Work",
    "description": "The aim of this study is to construct an epistemic model in which each rational choice under common belief in rationality is supplemented by a type which expresses such a belief. In practice, the finding of type depends on manual solution approach with some mathematical operations in scope of the theory. This approach becomes less convenient with the growth of the size of the game. To solve this difficulty, a linear programming model is constructed for two-player, static and non-cooperative games to find the type that is supporting that player’s rational choice is optimal under common belief in rationality and maximizing the utility of the game. Since the optimal choice would only be made from rational choices, it is first necessary to eliminate all strictly dominated choices. In real life, the games are usually large sized. Therefore, the elimination process should be performed in a computer environment. Since software related to game theory was mostly prepared with a result-oriented approach for some types of games, it was necessary to develop software to execute the iterated elimination method. With this regard, a program has been developed that determines the choices that are strictly dominated by pure and randomized choices in two-player games. Two functions named “esdc” and “type” are created by using R statistical programming language for the operations performed in both parts, and these functions are added to the content of an R package after its creation with the name EpistemicGameTheory.",
    "author": [
      {
        "name": "Bilge Başer",
        "url": {}
      },
      {
        "name": "Nalan Cinemre",
        "url": {}
      }
    ],
    "date": "2018-05-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-003.zip\nCRAN packages used\nEpistemicGameTheory, roxygen2, lpSolve\nCRAN Task Views implied by cited packages\nOptimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-004/",
    "title": "Residuals and Diagnostics for Binary and Ordinal Regression Models: An Introduction to the sure Package",
    "description": "Residual diagnostics is an important topic in the classroom, but it is less often used in practice by Brandon M. Greenwell, Andrew J. McCarthy, Bradley C. Boehmke, and Dungang Liu Introduction to the sure Package Ordinal Regression Models: An",
    "author": [
      {
        "name": "Brandon M. Greenwell",
        "url": {}
      },
      {
        "name": "Andrew J. McCarthy",
        "url": {}
      },
      {
        "name": "Bradley C. Boehmke",
        "url": {}
      },
      {
        "name": "Dungang Liu",
        "url": {}
      }
    ],
    "date": "2018-05-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-004.zip\nCRAN packages used\nMASS, VGAM, ordinal, rms, PResiduals, sure, ggplot2\nCRAN Task Views implied by cited packages\nEconometrics, Psychometrics, SocialSciences, Distributions, Environmetrics, Multivariate, Survival, ExtremeValue, Graphics, NumericalMathematics, Phylogenetics, ReproducibleResearch, Robust\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:54+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-067/",
    "title": "RQGIS: Integrating R with QGIS for Statistical Geocomputing",
    "description": "Integrating R with Geographic Information Systems (GIS) extends R’s statistical capabilities with numerous geoprocessing and data handling tools available in a GIS. QGIS is one of the most popular open-source GIS, and it furthermore integrates other GIS programs such as the System for Automated Geoscientific Analyses (SAGA) GIS and the Geographic Resources Analysis Support System (GRASS) GIS within a single software environment. This and its QGIS Python API makes it a perfect candidate for console-based geoprocessing. By establishing an interface, the R package RQGIS makes it possible to use QGIS as a geoprocessing workhorse from within R. Compared to other packages building a bridge to GIS (e.g., rgrass7, RSAGA, RPyGeo), RQGIS offers a wider range of geoalgorithms, and is often easier to use due to various convenience functions. Finally, RQGIS supports the seamless integration of Python code using reticulate from within R for improved extendability.",
    "author": [
      {
        "name": "Jannes Muenchow",
        "url": {}
      },
      {
        "name": "Patrick Schratz",
        "url": {}
      },
      {
        "name": "Alexander Brenning",
        "url": {}
      }
    ],
    "date": "2017-12-04",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-067.zip\nCRAN packages used\nmaptools, raster, sp, sf, mapview, mapmisc, osmar, dodgr, RArcInfo, rgrass7, mapedit, rgdal, rgeos, RSAGA, RPyGeo, RQGIS, reticulate, rPython, sperrorest, nlme, mgcv, spgrass6, leaflet\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Econometrics, Environmetrics, NumericalMathematics, SocialSciences, Bayesian, ChemPhys, Finance, HighPerformanceComputing, OfficialStatistics, Psychometrics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-068/",
    "title": "rpsftm: An R Package for Rank Preserving Structural Failure Time Models",
    "description": "Treatment switching in a randomised controlled trial occurs when participants change from their randomised treatment to the other trial treatment during the study. Failure to account for treatment switching in the analysis (i.e. by performing a standard intention-to-treat analysis) can lead to biased estimates of treatment efficacy. The rank preserving structural failure time model (RPSFTM) is a method used to adjust for treatment switching in trials with survival outcomes. The RPSFTM is due to Robins and Tsiatis (1991) and has been developed by White et al. (1997, 1999). The method is randomisation based and uses only the randomised treatment group, observed event times, and treatment history in order to estimate a causal treatment effect. The treatment effect, ψ, is estimated by balancing counter-factual event times (that would be observed if no treatment were received) between treatment groups. G-estimation is used to find the value of ψ such that a test statistic Z (ψ) = 0. This is usually the test statistic used in the intention-to-treat analysis, for example, the log rank test statistic. We present an R package, rpsftm, that implements the method.",
    "author": [
      {
        "name": "Annabel Allison",
        "url": {}
      },
      {
        "name": "Ian R White",
        "url": {}
      },
      {
        "name": "Simon Bond",
        "url": {}
      }
    ],
    "date": "2017-12-04",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-068.zip\nCRAN packages used\nipw, rpsftm, eha\nCRAN Task Views implied by cited packages\nSurvival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-066/",
    "title": "glmmTMB Balances Speed and Flexibility Among Packages for Zero-inflated Generalized Linear Mixed Modeling",
    "description": "Count data can be analyzed using generalized linear mixed models when observations are correlated in ways that require random effects. However, count data are often zero-inflated, containing more zeros than would be expected from the typical error distributions. We present a new package, glmmTMB, and compare it to other R packages that fit zero-inflated mixed models. The glmmTMB package fits many types of GLMMs and extensions, including models with continuously distributed responses, but here we focus on count responses. glmmTMB is faster than glmmADMB, MCMCglmm, and brms, and more flexible than INLA and mgcv for zero-inflated modeling. One unique feature of glmmTMB (among packages that fit zero-inflated mixed models) is its ability to estimate the Conway-Maxwell-Poisson distribution parameterized by the mean. Overall, its most appealing features for new users may be the combination of speed, flexibility, and its interface’s similarity to lme4.",
    "author": [
      {
        "name": "Mollie E. Brooks",
        "url": {}
      },
      {
        "name": "Kasper Kristensen",
        "url": {}
      },
      {
        "name": "Koen J. van Benthem",
        "url": {}
      },
      {
        "name": "Arni Magnusson",
        "url": {}
      },
      {
        "name": "Casper W. Berg",
        "url": {}
      },
      {
        "name": "Anders Nielsen",
        "url": {}
      },
      {
        "name": "Hans J. Skaug",
        "url": {}
      },
      {
        "name": "Martin Mächler",
        "url": {}
      },
      {
        "name": "Benjamin M. Bolker",
        "url": {}
      }
    ],
    "date": "2017-12-01",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-066.zip\nCRAN packages used\nglmmTMB, pscl, MCMCglmm, mgcv, brms, gamlss, flexmix, MXM, VGAM, mgcv, TMB, devtools\nCRAN Task Views implied by cited packages\nEconometrics, Environmetrics, SocialSciences, Bayesian, Psychometrics, Survival, Cluster, Distributions, ExtremeValue, gR, MachineLearning, Multivariate, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-064/",
    "title": "carx: an R Package to Estimate Censored Autoregressive Time Series with Exogenous Covariates ",
    "description": "We implement in the R package carx a novel and computationally efficient quasi-likelihood method for estimating a censored autoregressive model with exogenous covariates. The proposed quasi-likelihood method reduces to maximum likelihood estimation in absence of censoring. The carx package contains many useful functions for practical data analysis with censored stochastic regression, including functions for outlier detection, model diagnostics, and prediction with censored time series data. We illustrate the capabilities of the carx package with simulations and an elaborate real data analysis.",
    "author": [
      {
        "name": "Chao Wang",
        "url": {}
      },
      {
        "name": "Kung-Sik Chan",
        "url": {}
      }
    ],
    "date": "2017-11-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-064.zip\nCRAN packages used\ncensReg, AER, NADA, VGAM, MCMCpack, cents, ARCensReg, carx, xts, TSA\nCRAN Task Views implied by cited packages\nSurvival, TimeSeries, Econometrics, Distributions, Multivariate, Psychometrics, Bayesian, Environmetrics, ExtremeValue, Finance, SocialSciences, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-065/",
    "title": "An Introduction to Rocker: Docker Containers for R",
    "description": "We describe the Rocker project, which provides a widely-used suite of Docker images with customized R environments for particular tasks. We discuss how this suite is organized, and how these tools can increase portability, scaling, reproducibility, and convenience of R users and developers.",
    "author": [
      {
        "name": "Carl Boettiger",
        "url": {}
      },
      {
        "name": "Dirk Eddelbuettel",
        "url": {}
      }
    ],
    "date": "2017-11-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npackrat, rhub, tidyverse\nCRAN Task Views implied by cited packages\nReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-059/",
    "title": "Simulating Probabilistic Long-Term Effects in Models with Temporal Dependence",
    "description": "The R package pltesim calculates and depicts probabilistic long-term effects in binary models with temporal dependence variables. The package performs two tasks. First, it calculates the change in the probability of the event occurring given a change in a theoretical variable. Second, it calculates the rolling difference in the future probability of the event for two scenarios: one where the event occurred at a given time and one where the event does not occur. The package is consistent with the recent movement to depict meaningful and easy-to-interpret quantities of interest with the requisite measures of uncertainty. It is the first to make it easy for researchers to interpret shortand long-term effects of explanatory variables in binary autoregressive models, which can have important implications for the correct interpretation of these models.",
    "author": [
      {
        "name": "Christopher Gandrud",
        "url": {}
      },
      {
        "name": "Laron K. Williams",
        "url": {}
      }
    ],
    "date": "2017-11-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-059.zip\nCRAN packages used\npltesim, ggplot2, DAMisc\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-060/",
    "title": "ManlyMix: An R Package for Manly Mixture Modeling",
    "description": "Model-based clustering is a popular technique for grouping objects based on a finite mixture model. It has countless applications in different fields of study. The R package ManlyMix implements the Manly mixture model that allows modeling skewness within data groups and performs cluster analysis. ManlyMix is a powerful diagnostics tool that is capable of conducting investigation con cerning the normality of variables upon fitting of a Manly forward or backward model. Theoretical foundations as well as description of functions are provided. All features of the package are illus trated with examples in great detail. The analysis of real-life datasets demonstrates the flexibility and usefulness of the package.",
    "author": [
      {
        "name": "Xuwen Zhu",
        "url": {}
      },
      {
        "name": "Volodymyr Melnykov",
        "url": {}
      }
    ],
    "date": "2017-11-22",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nflowClust, mixsmsn, EMMIXskew, EMMIXuskew, mixsmsn, EMMIXskew, EMMIXuskew, flowClust, ManlyMix, ManlyMix\nCRAN Task Views implied by cited packages\nCluster\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-061/",
    "title": "Partial Rank Data with the hyper2 Package: Likelihood Functions for Generalized Bradley-Terry Models",
    "description": "Here I present the hyper2 package for generalized Bradley-Terry models and give examples from two competitive situations: single scull rowing, and the competitive cooking game show Mas terChef Australia. A number of natural statistical hypotheses may be tested straightforwardly using the software.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2017-11-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-061.zip\nCRAN packages used\nhyper2, aylmer\nCRAN Task Views implied by cited packages\nDistributions\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-062/",
    "title": "riskRegression: Predicting the Risk of an Event using Cox Regression Models",
    "description": "In the presence of competing risks a prediction of the time-dynamic absolute risk of an event can be based on cause-specific Cox regression models for the event and the competing risks (Benichou and Gail, 1990). We present computationally fast and memory optimized C++ functions with an R inter face for predicting the covariate specific absolute risks, their confidence intervals, and their confidence bands based on right censored time to event data. We provide explicit formulas for our implementation of the estimator of the (stratified) baseline hazard function in the presence of tied event times. As a by-product we obtain fast access to the baseline hazards (compared to survival::basehaz()) and predictions of survival probabilities, their confidence intervals and confidence bands. Confidence intervals and confidence bands are based on point-wise asymptotic expansions of the corresponding statistical functionals. The software presented here is implemented in the riskRegression package.",
    "author": [
      {
        "name": "Brice Ozenne",
        "url": {}
      },
      {
        "name": "Anne Lyngholm Sørensen",
        "url": {}
      },
      {
        "name": "Thomas Scheike",
        "url": {}
      },
      {
        "name": "Christian Torp-Pedersen",
        "url": {}
      },
      {
        "name": "Thomas            Alexander Gerds",
        "url": {}
      }
    ],
    "date": "2017-11-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-062.zip\nCRAN packages used\nsurvival, rms, riskRegression, mstate, rbenchmark, profvis, mets\nCRAN Task Views implied by cited packages\nSurvival, Econometrics, SocialSciences, ClinicalTrials, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-063/",
    "title": "openEBGM: An R Implementation of the Gamma-Poisson Shrinker Data Mining Model",
    "description": "We introduce the R package openEBGM, an implementation of the Gamma-Poisson Shrinker (GPS) model for identifying unexpected counts in large contingency tables using an empirical Bayes approach. The Empirical Bayes Geometric Mean (EBGM) and quantile scores are obtained from the GPS model estimates. openEBGM provides for the evaluation of counts using a number of different methods, including the model-based disproportionality scores, the relative reporting ratio (RR), and the proportional reporting ratio (PRR). Data squashing for computational efficiency and stratification for confounding variable adjustment are included. Application to adverse event detection is discussed.",
    "author": [
      {
        "name": "Travis Canida",
        "url": {}
      },
      {
        "name": "John Ihrie",
        "url": {}
      }
    ],
    "date": "2017-11-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-063.zip\nCRAN packages used\nopenEBGM, PhViD, mederrRank, tidyr, ggplot2, data.table\nCRAN Task Views implied by cited packages\nBayesian, Finance, Graphics, HighPerformanceComputing, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-057/",
    "title": "Allele Imputation and Haplotype Determination from Databases Composed of Nuclear Families",
    "description": "The alleHap package is designed for imputing genetic missing data and reconstruct non recombinant haplotypes from pedigree databases in a deterministic way. When genotypes of related individuals are available in a number of linked genetic markers, the program starts by identifying haplotypes compatible with the observed genotypes in those markers without missing values. If haplotypes are identified in parents or offspring, missing alleles can be imputed in subjects containing missing values. Several scenarios are analyzed: family completely genotyped, children partially genotyped and parents completely genotyped, children fully genotyped and parents containing entirely or partially missing genotypes, and founders and their offspring both only partially genotyped. The alleHap package also has a function to simulate pedigrees including all these scenarios. This article describes in detail how our package works for the desired applications, including illustrated explanations and easily reproducible examples.",
    "author": [
      {
        "name": "Nathan Medina-Rodríguez",
        "url": {}
      },
      {
        "name": "Ángelo Santana",
        "url": {}
      }
    ],
    "date": "2017-11-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhaplo.ccs, haplo.stats, hsphase, linkim, rrBLUP, synbreed, alleHap\nCRAN Task Views implied by cited packages\nGenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-058/",
    "title": "rentrez: An R package for the NCBI eUtils API",
    "description": "The USA National Center for Biotechnology Information (NCBI) is one of the world’s most important sources of biological information. NCBI databases like PubMed and GenBank contain mil lions of records describing bibliographic, genetic, genomic, and medical data. Here I present rentrez, a package which provides an R interface to 50 NCBI databases. The package is well-documented, contains an extensive suite of unit tests and has an active user base. The programmatic interface to the NCBI provided by rentrez allows researchers to query databases and download or import particular records into R sessions for subsequent analysis. The complete nature of the package, its extensive test-suite and the fact the package implements the NCBI’s usage policies all make rentrez a powerful aid to developers of new packages that perform more specific tasks.",
    "author": [
      {
        "name": "David J. Winter",
        "url": {}
      }
    ],
    "date": "2017-11-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-058.zip\nCRAN packages used\nape, RISmed, pubmed.mineR, rentrez, reutils, rotl, fulltext, treemap\nCRAN Task Views implied by cited packages\nPhylogenetics, Environmetrics, Genetics, Graphics, OfficialStatistics, WebTechnologies\nBioconductor packages used\ngenomes, RMassBank, MeSHSim, genbankr\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-056/",
    "title": "Splitting It Up: The spduration Split-Population Duration Regression Package for Time-Varying Covariates",
    "description": "We present an implementation of split-population duration regression in the spduration (Beger et al., 2017) package for R that allows for time-varying covariates. The statistical model accounts for units that are immune to a certain outcome and are not part of the duration process the researcher is primarily interested in. We provide insights for when immune units exist, that can significantly increase the predictive performance compared to standard duration models. The package includes estimation and several post-estimation methods for split-population Weibull and log-logistic models. We provide an empirical application to data on military coups.",
    "author": [
      {
        "name": "Andreas Beger",
        "url": {}
      },
      {
        "name": "Daniel W. Hill",
        "url": {}
      },
      {
        "name": "Jr.",
        "url": {}
      },
      {
        "name": "Nils. W. Metternich",
        "url": {}
      },
      {
        "name": "Shahryar Minhas",
        "url": {}
      },
      {
        "name": "Michael D. Ward",
        "url": {}
      }
    ],
    "date": "2017-11-05",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspduration, survival, smcure\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-055/",
    "title": "mle.tools: An R Package for Maximum Likelihood Bias Correction",
    "description": "Recently, Mazucheli (2017) uploaded the package mle.tools to CRAN. It can be used for bias corrections of maximum likelihood estimates through the methodology proposed by Cox and Snell (1968). The main function of the package, coxsnell.bc(), computes the bias corrected maximum likelihood estimates. Although in general, the bias corrected estimators may be expected to have better sampling properties than the uncorrected estimators, analytical expressions from the formula proposed by Cox and Snell (1968) are either tedious or impossible to obtain. The purpose of this paper is twofolded: to introduce the mle.tools package, especially the coxsnell.bc() function; secondly, to compare, for thirty one continuous distributions, the bias estimates from the coxsnell.bc() function and the bias estimates from analytical expressions available in the literature. We also compare, for five distributions, the observed and expected Fisher information. Our numerical experiments show that the functions are efficient to estimate the biases by the Cox-Snell formula and for calculating the observed and expected Fisher information.",
    "author": [
      {
        "name": "Josmar Mazucheli",
        "url": {}
      },
      {
        "name": "André Felipe B. Menezes",
        "url": {}
      },
      {
        "name": "Saralees Nadarajah",
        "url": {}
      }
    ],
    "date": "2017-11-01",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-055.zip\nCRAN packages used\nmle.tools, fitdistrplus\nCRAN Task Views implied by cited packages\nDistributions, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-054/",
    "title": "ider: Intrinsic Dimension Estimation with R",
    "description": "In many data analyses, the dimensionality of the observed data is high while its intrinsic dimension remains quite low. Estimating the intrinsic dimension of an observed dataset is an essential preliminary step for dimensionality reduction, manifold learning, and visualization. This paper introduces an R package, named ider, that implements eight intrinsic dimension estimation methods, including a recently proposed method based on a second-order expansion of a probability mass function and a generalized linear model. The usage of each function in the package is explained with datasets generated using a function that is also included in the package.",
    "author": [
      {
        "name": "Hideitsu Hino",
        "url": {}
      }
    ],
    "date": "2017-10-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-054.zip\nCRAN packages used\nider, ider, ider, fractal, nonlinearTseries, tseriesChaos, fractal, nonlinearTseries, tseriesChaos, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, Rcpp\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, HighPerformanceComputing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-052/",
    "title": "BayesBD: An R Package for Bayesian Inference on Image Boundaries",
    "description": "We present the BayesBD package providing Bayesian inference for boundaries of noisy images. The BayesBD package implements flexible Gaussian process priors indexed by the circle to recover the boundary in a binary or Gaussian noised image. The boundary recovered by BayesBD has the practical advantages of guaranteed geometric restrictions and convenient joint inferences under certain assumptions, in addition to its desirable theoretical property of achieving (nearly) minimax optimal rate in a way that is adaptive to the unknown smoothness. The core sampling tasks for our model have linear complexity, and are implemented in C++ for computational efficiency using packages Rcpp and RcppArmadillo. Users can access the full functionality of the package in both the command line and the corresponding shiny application. Additionally, the package includes numerous utility functions to aid users in data preparation and analysis of results. We compare BayesBD with selected existing packages using both simulations and real data applications, demonstrating the excellent performance and flexibility of BayesBD even when the observation contains complicated structural information that may violate its assumptions.",
    "author": [
      {
        "name": "Nicholas Syring",
        "url": {}
      },
      {
        "name": "Meng Li",
        "url": {}
      }
    ],
    "date": "2017-10-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nBayesBD, RcppArmadillo, shiny\nCRAN Task Views implied by cited packages\nNumericalMathematics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-053/",
    "title": "Simulating Noisy, Nonparametric, and Multivariate Discrete Patterns",
    "description": "Requiring no analytical forms, nonparametric discrete patterns are flexible in representing complex relationships among random variables. This makes them increasingly useful for data-driven applications. However, there appears to be no software tools for simulating nonparametric discrete patterns, which prevents objective evaluation of statistical methods that discover discrete relationships from data. We present a simulator to generate nonparametric discrete functions as contingency tables. User can request strictly many-to-one functional patterns. The simulator can also produce contingency tables representing dependent non-functional and independent relationships. An option is provided to apply random noise to contingency tables. We demonstrate the utility of the simulator by showing the advantage of the FunChisq test over Pearson’s chi-square test in detecting functional patterns. This simulator, implemented in the function simulate_tables in the R package FunChisq (version 2.4.0 or greater), offers an important means to evaluate the performance of nonparametric statistical pattern discovery methods.",
    "author": [
      {
        "name": "Ruby Sharma",
        "url": {}
      },
      {
        "name": "Sajal Kumar",
        "url": {}
      },
      {
        "name": "Hua Zhong",
        "url": {}
      },
      {
        "name": "Mingzhou Song",
        "url": {}
      }
    ],
    "date": "2017-10-25",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-053.zip\nCRAN packages used\nrTableICC, FunChisq\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-046/",
    "title": "Visualization of Regression Models Using visreg",
    "description": "Regression models allow one to isolate the relationship between the outcome and an ex planatory variable while the other variables are held constant. Here, we introduce an R package, visreg, for the convenient visualization of this relationship via short, simple function calls. In addition to estimates of this relationship, the package also provides pointwise confidence bands and partial residuals to allow assessment of variability as well as outliers and other deviations from modeling assumptions. The package provides several options for visualizing models with interactions, including lattice plots, contour plots, and both static and interactive perspective plots. The implementation of the package is designed to be fully object-oriented and interface seamlessly with R’s rich collection of model classes, allowing a consistent interface for visualizing not only linear models, but generalized linear models, proportional hazards models, generalized additive models, robust regression models, and many more.",
    "author": [
      {
        "name": "Patrick Breheny",
        "url": {}
      },
      {
        "name": "Woodrow Burchett",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nvisreg, rms, rockchalk, car, effects, plotmo, lattice, ggplot2, splines, rgl, MASS, mgcv, locfit, randomForest, e1071, gbm, lme4\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, Environmetrics, MachineLearning, Multivariate, Graphics, Psychometrics, Survival, Bayesian, Distributions, SpatioTemporal, Cluster, Finance, NumericalMathematics, OfficialStatistics, Phylogenetics, ReproducibleResearch, Robust\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-047/",
    "title": "arulesViz: Interactive Visualization of Association Rules with R",
    "description": "Association rule mining is a popular data mining method to discover interesting relation ships between variables in large databases. An extensive toolbox is available in the R-extension package arules. However, mining association rules often results in a vast number of found rules, leaving the analyst with the task to go through a large set of rules to identify interesting ones. Sifting manually through extensive sets of rules is time-consuming and strenuous. Visualization and espe cially interactive visualization has a long history of making large amounts of data better accessible. The R-extension package arulesViz provides most popular visualization techniques for association rules. In this paper, we discuss recently added interactive visualizations to explore association rules and demonstrate how easily they can be used in arulesViz via a unified interface. With examples, we help to guide the user in selecting appropriate visualizations and interpreting the results.",
    "author": [
      {
        "name": "Michael Hahsler",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\narulesViz, arules, DT, plotly, grid, visNetwork\nCRAN Task Views implied by cited packages\nMachineLearning, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-048/",
    "title": "liureg: A Comprehensive R Package for the Liu Estimation of Linear Regression Model with Collinear Regressors",
    "description": "The Liu regression estimator is now a commonly used alternative to the conventional ordinary least squares estimator that avoids the adverse effects in the situations when there exists a considerable degree of multicollinearity among the regressors. There are only a few software packages available for estimation of the Liu regression coefficients, though with limited methods to estimate the Liu biasing parameter without addressing testing procedures. Our liureg package can be used to estimate the Liu regression coefficients utilizing a range of different existing biasing parameters, to test these coefficients with more than 15 Liu related statistics, and to present different graphical displays of these statistics.",
    "author": [
      {
        "name": "Muhammad Imdadullah",
        "url": {}
      },
      {
        "name": "Muhammad Aslam",
        "url": {}
      },
      {
        "name": "Saima Altaf",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-048.zip\nCRAN packages used\nlrmest, ltsbase, liureg, lmridge, MASS, mctest\nCRAN Task Views implied by cited packages\nDistributions, Econometrics, Environmetrics, Multivariate, NumericalMathematics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-049/",
    "title": "The welchADF Package for Robust Hypothesis Testing in Unbalanced Multivariate Mixed Models with Heteroscedastic and Non-normal Data",
    "description": "A new R package is presented for dealing with non-normality and variance heterogeneity of sample data when conducting hypothesis tests of main effects and interactions in mixed models. The proposal departs from an existing SAS program which implements Johansen’s general formulation of Welch-James’s statistic with approximate degrees of freedom, which makes it suitable for testing any linear hypothesis concerning cell means in univariate and multivariate mixed model designs when the data pose non-normality and non-homogeneous variance. Improved type I error rate control is obtained using bootstrapping for calculating an empirical critical value, whereas robustness against non-normality is achieved through trimmed means and Winsorized variances. A wrapper function eases the application of the test in common situations, such as performing omnibus tests on all effects and interactions, pairwise contrasts, and tetrad contrasts of two-way interactions. The package is demonstrated in several problems including unbalanced univariate and multivariate designs.",
    "author": [
      {
        "name": "Pablo J. Villacorta",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-049.zip\nCRAN packages used\nART, WRS2, robustbase, robust, robustlmm, nlme, lme4, welchADF, gamm4, mgcv\nCRAN Task Views implied by cited packages\nRobust, Econometrics, Environmetrics, SocialSciences, Bayesian, OfficialStatistics, Psychometrics, SpatioTemporal, ChemPhys, Finance, Multivariate, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-050/",
    "title": "Bayesian Regression Models for Interval-censored Data in R",
    "description": "The package icenReg provides classic survival regression models for interval-censored data. We present an update to the package that extends the parametric models into the Bayesian framework. Core additions include functionality to define the regression model with the standard regression syntax while providing a custom prior function. Several other utility functions are presented that allow for simplified examination of the posterior distribution.",
    "author": [
      {
        "name": "Clifford Anderson-Bergman",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-050.zip\nCRAN packages used\nicenReg, foreach, doParallel, coda, Rcpp, RcppEigen\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics, Bayesian, gR, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-051/",
    "title": "queueing: A Package For Analysis Of Queueing Networks and Models in R",
    "description": "queueing is a package that solves and provides the main performance measures for both basic Markovian queueing models and single and multiclass product-form queueing networks. It can be used both in education and for professional purposes. It provides an intuitive, straightforward way to build queueing models using S3 methods. The package solves Markovian models of the form M/M/c/K/M/FCFS, open and closed single class Jackson networks, open and closed multiclass networks and mixed networks. Markovian models are used when both the customer inter-arrival time and the server processing time are exponentially distributed. Queueing network solvers are useful for modelling situations in which more than one station must be visited.",
    "author": [
      {
        "name": "Pedro Cañadilla Jiménez",
        "url": {}
      },
      {
        "name": "Yolanda Román Montoya",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-051.zip\nCRAN packages used\nsimmer, queuecomputer, queueing\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-044/",
    "title": "fourierin: An R package to compute Fourier integrals",
    "description": "We present the R package fourierin (Basulto-Elias, 2017) for evaluating functions defined as Fourier-type integrals over a collection of argument values. The integrals are finitely supported with integrands involving continuous functions of one or two variables. As an important application, such Fourier integrals arise in so-called “inversion formulas”, where one seeks to evaluate a probability density at a series of points from a given characteristic function (or vice versa) through Fourier transforms. This paper intends to fill a gap in current R software, where tools for repeated evaluation of functions as Fourier integrals are not directly available. We implement two approaches for such computations with numerical integration. In particular, if the argument collection for evaluation corresponds to a regular grid, then an algorithm from Inverarity (2002) may be employed based on a fast Fourier transform, which creates significant improvements in the speed over a second approach to numerical Fourier integration (where the latter also applies to cases where the points for evaluation are not on a grid). We illustrate the package with the computation of probability densities and characteristic functions through Fourier integrals/transforms, for both univariate and bivariate examples.",
    "author": [
      {
        "name": "Guillermo Basulto-Elias",
        "url": {}
      },
      {
        "name": "Alicia Carriquiry",
        "url": {}
      },
      {
        "name": "Kris De Brabanter",
        "url": {}
      },
      {
        "name": "Daniel J. Nordman",
        "url": {}
      }
    ],
    "date": "2017-10-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfourierin, RcppArmadillo\nCRAN Task Views implied by cited packages\nNumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-045/",
    "title": "afmToolkit: an R Package for Automated AFM Force-Distance Curves Analysis",
    "description": "Atomic force microscopy (AFM) is widely used to measure molecular and colloidal inter actions as well as mechanical properties of biomaterials. In this paper the afmToolkit R package is introduced. This package allows the user to automatically batch process AFM force-distance and force-time curves. afmToolkit capabilities range from importing ASCII files and preprocessing the curves (contact point detection, baseline correction. . . ) for finding relevant physical information, such as Young’s modulus, adhesion energies and exponential decay for force relaxation and creep experiments. This package also contains plotting, summary and feature extraction functions. The package also comes with several data sets so the user can test the aforementioned features with ease. The package afmToolkit eases the basic processing of large amount of AFM F-d/t curves at once. It is also flexible enough to easily incorporate new functions as they are needed and can be seen as a programming infrastructure for further algorithm development.",
    "author": [
      {
        "name": "Rafael Benítez",
        "url": {}
      },
      {
        "name": "Vicente J. Bolós",
        "url": {}
      },
      {
        "name": "José-Luis Toca-Herrera",
        "url": {}
      }
    ],
    "date": "2017-10-12",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-045.zip\nCRAN packages used\nafmToolkit, devtools, ggplot2, minpack.lm, gridExtra, scales, dplyr\nCRAN Task Views implied by cited packages\nChemPhys, Graphics, Optimization, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-042/",
    "title": "adegraphics: An S4 Lattice-Based Package for the Representation of Multivariate Data",
    "description": "The ade4 package provides tools for multivariate analyses. Whereas new statistical methods have been added regularly in the package since its first release in 2002, the graphical functions, that are used to display the main outputs of an analysis, have not benefited from such enhancements. In this context, the adegraphics package, available on CRAN since 2015, is a complete reimplementation of the ade4 graphical functionalities but with large improvements. The package uses the S4 object system (each graph is an object) and is based on the graphical framework provided by lattice and grid. We give a brief description of the package and illustrate some important functionalities to build elegant graphs.",
    "author": [
      {
        "name": "Aurélie Siberchicot",
        "url": {}
      },
      {
        "name": "Alice Julien-Laferrière",
        "url": {}
      },
      {
        "name": "Anne-Béatrice Dufour",
        "url": {}
      },
      {
        "name": "Jean Thioulouse",
        "url": {}
      },
      {
        "name": "Stéphane            Dray",
        "url": {}
      }
    ],
    "date": "2017-10-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-042.zip\nCRAN packages used\nvegan, MASS, FactoMineR, ade4, lattice, adegraphics, sp, spdep, RColorBrewer, ggplot2\nCRAN Task Views implied by cited packages\nMultivariate, Spatial, Graphics, Psychometrics, Environmetrics, Econometrics, Phylogenetics, Distributions, NumericalMathematics, Robust, SocialSciences, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-043/",
    "title": "LeArEst: Length and Area Estimation from Data Measured with Additive Error",
    "description": "This paper describes an R package LeArEst that can be used for estimating object dimensions from a noisy image. The package is based on a simple parametric model for data that are drawn from uniform distribution contaminated by an additive error. Our package is able to estimate the length of the object of interest on a given straight line that intersects it, as well as to estimate the object area when it is elliptically shaped. The input data may be a numerical vector or an image in JPEG format. In this paper, background statistical models and methods for the package are summarized, and the algorithms and key functions implemented are described. Also, examples that demonstrate its usage are provided. Availability: LeArEst is available on CRAN.",
    "author": [
      {
        "name": "Mirta Benšić",
        "url": {}
      },
      {
        "name": "Petar Taler",
        "url": {}
      },
      {
        "name": "Safet Hamedović",
        "url": {}
      },
      {
        "name": "Emmanuel Karlo Nyarko",
        "url": {}
      },
      {
        "name": "Kristian Sabo",
        "url": {}
      }
    ],
    "date": "2017-10-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-043.zip\nCRAN packages used\nLeArEst, decon, deamer, conicfit, jpeg, opencpu, shiny\nCRAN Task Views implied by cited packages\nWebTechnologies, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-040/",
    "title": "dGAselID: An R Package for Selecting a Variable Number of Features in High Dimensional Data",
    "description": "The dGAselID package proposes an original approach to feature selection in high dimen sional data. The method is built upon a diploid genetic algorithm. The genotype to phenotype mapping is modeled after the Incomplete Dominance Inheritance, overpassing the necessity to define a dominance scheme. The fitness evaluation is done by user selectable supervised classifiers, from a broad range of options. Cross validation options are also accessible. A new approach to crossover, inspired from the random assortment of chromosomes during meiosis is included. Several mutation operators, inspired from genetics, are also proposed. The package is fully compatible with the data formats used in Bioconductor and MLInterfaces package, readily applicable to microarray studies, but is flexible to other feature selection applications from high dimensional data. Several options for the visualization of evolution and outcomes are implemented to facilitate the interpretation of results. The package’s functionality is illustrated by examples.",
    "author": [
      {
        "name": "Nicolae Teodor Melita",
        "url": {}
      },
      {
        "name": "Stefan Holban",
        "url": {}
      }
    ],
    "date": "2017-08-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndGAselID, genalg, GA, nsga2R, gaoptim, STPGA, kofnGA, mogavs, gaselect, scales\nCRAN Task Views implied by cited packages\nOptimization\nBioconductor packages used\nMLInterfaces, MLInterfaces, ALL, genefilter, hgu95av2.db\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-041/",
    "title": "CRTgeeDR: an R Package for Doubly Robust Generalized Estimating Equations Estimations in Cluster Randomized Trials with Missing Data",
    "description": "Semi-parametric approaches based on generalized estimating equations (GEE) are widely used to analyze correlated outcomes in longitudinal settings. In this paper, we present a package CRTgeeDR developed for cluster randomized trials with missing data (CRTs). For use of inverse probability weighting to adjust for missing data in cluster randomized trials, we show that other software lead to biased estimation for non-independence working correlation structure. CRTgeeDR solves this problem. We also extend the ability of existing packages to allow augmented Doubly Robust GEE estimation (DR). Simulation studies demonstrate the consistency of estimators implemented in CRTgeeDR compared to packages such as geepack and the gains associated with the use of the DR for analyzing a binary outcome using a logistic regression. Finally, we illustrate the method on data from a sanitation CRT in developing countries.",
    "author": [
      {
        "name": "Melanie Prague",
        "url": {}
      },
      {
        "name": "Rui Wang",
        "url": {}
      },
      {
        "name": "Victor De Gruttola",
        "url": {}
      }
    ],
    "date": "2017-08-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCRTgeeDR, gee, geepack, geeM, ipw, drgee, CausalGAM, tmle, tmlenet, numDeriv, geesmv\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, NumericalMathematics, Robust\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-039/",
    "title": "anomalyDetection: Implementation of Augmented Network Log Anomaly Detection Procedures",
    "description": "As the number of cyber-attacks continues to grow on a daily basis, so does the delay in threat detection. For instance, in 2015, the Office of Personnel Management discovered that approximately 21.5 million individual records of Federal employees and contractors had been stolen. On average, the time between an attack and its discovery is more than 200 days. In the case of the OPM breach, the attack had been going on for almost a year. Currently, cyber analysts inspect numerous potential incidents on a daily basis, but have neither the time nor the resources available to perform such a task. anomalyDetection aims to curtail the time frame in which anomalous cyber activities go unnoticed and to aid in the efficient discovery of these anomalous transactions among the millions of daily logged events by i) providing an efficient means for pre-processing and aggregating cyber data for analysis by employing a tabular vector transformation and handling multicollinearity concerns; ii) offering numerous built-in multivariate statistical functions such as Mahalanobis distance, factor analysis, principal components analysis to identify anomalous activity, iii) incorporating the pipe operator (%>%) to allow it to work well in the tidyverse workflow. Combined, anomalyDetection offers cyber analysts an efficient and simplified approach to break up network events into time-segment blocks and identify periods associated with suspected anomalies for further evaluation.",
    "author": [
      {
        "name": "Robert J. Gutierrez",
        "url": {}
      },
      {
        "name": "Bradley C. Boehmke",
        "url": {}
      },
      {
        "name": "Kenneth W. Bauer",
        "url": {}
      },
      {
        "name": "Cade M. Saie",
        "url": {}
      },
      {
        "name": "Trevor J. Bihl",
        "url": {}
      }
    ],
    "date": "2017-08-04",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-039.zip\nCRAN packages used\nanomalyDetection, magrittr, tidyverse\nCRAN Task Views implied by cited packages\nWebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-038/",
    "title": "ctmcd: An R Package for Estimating the Parameters of a Continuous-Time Markov Chain from Discrete-Time Data",
    "description": "This article introduces the R package ctmcd, which provides an implementation of methods for the estimation of the parameters of a continuous-time Markov chain given that data are only available on a discrete-time basis. This data consists of partial observations of the state of the chain, which are made without error at discrete times, an issue also known as the embedding problem for Markov chains. The functions provided comprise matrix logarithm based approximations as described in Israel et al. (2001), as well as Kreinin and Sidelnikova (2001), an expectation-maximization algorithm and a Gibbs sampling approach, both introduced by Bladt and Sørensen (2005). For the expectation maximization algorithm Wald confidence intervals based on the Fisher information estimation method of Oakes (1999) are provided. For the Gibbs sampling approach, equal-tailed credibility intervals can be obtained. In order to visualize the parameter estimates, a matrix plot function is provided. The methods described are illustrated by Standard and Poor’s discrete-time corporate credit rating transition data.",
    "author": [
      {
        "name": "Marius Pfeuffer",
        "url": {}
      }
    ],
    "date": "2017-07-28",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-038.zip\nCRAN packages used\nmsm, ctmcd, coda, foreach, doParallel\nCRAN Task Views implied by cited packages\nBayesian, Distributions, gR, HighPerformanceComputing, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-036/",
    "title": "Discrete Time Markov Chains with R",
    "description": "The markovchain package aims to provide S4 classes and methods to easily handle Discrete Time Markov Chains (DTMCs), filling the gap with what is currently available in the CRAN repository. In this work, I provide an exhaustive description of the main functions included in the package, as well as hands-on examples.",
    "author": [
      {
        "name": "Giorgio Alfredo Spedicato",
        "url": {}
      }
    ],
    "date": "2017-07-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmarkovchain, clickstream, DTMCPack, MTCM, FuzzyStatProb, depmixS4, HMM, msm, heemod, TPmsm, Rcpp, igraph, matlab, Matrix, expm, method, expm, RcppParallel\nCRAN Task Views implied by cited packages\nNumericalMathematics, HighPerformanceComputing, Survival, Cluster, Distributions, Econometrics, Finance, gR, Graphics, Multivariate, Optimization, Spatial, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-037/",
    "title": "Furniture for Quantitative Scientists",
    "description": "A basic understanding of the distributions of study variables and the relationships among them is essential to inform statistical modeling. This understanding is achieved through the com putation of summary statistics and exploratory data analysis. Unfortunately, this step tends to be under-emphasized in the research process, in part because of the often tedious nature of thorough exploratory data analysis. The table1() function in the furniture package streamlines much of the exploratory data analysis process, making the computation and communication of summary statistics simple and beautiful while offering significant time-savings to the researcher.",
    "author": [
      {
        "name": "Tyson S. Barrett",
        "url": {}
      },
      {
        "name": "Emily Brignone",
        "url": {}
      }
    ],
    "date": "2017-07-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfurniture\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-034/",
    "title": "anchoredDistr: a Package for the Bayesian Inversion of Geostatistical Parameters with Multi-type and Multi-scale Data",
    "description": "The Method of Anchored Distributions (MAD) is a method for Bayesian inversion designed for inferring both local (e.g. point values) and global properties (e.g. mean and variogram parameters) of spatially heterogenous fields using multi-type and multi-scale data. Software implementations of MAD exist in C++ and C# to import data, execute an ensemble of forward model simulations, and perform basic post-processing of calculating likelihood and posterior distributions for a given application. This article describes the R package anchoredDistr that has been built to provide an R based environment for this method. In particular, anchoredDistr provides a range of post-processing capabilities for MAD software by taking advantage of the statistical capabilities and wide use of the R language. Two examples from stochastic hydrogeology are provided to highlight the features of the package for MAD applications in inferring anchored distributions of local parameters (e.g. point values of transmissivity) as well as global parameters (e.g. the mean of the spatial random function for hydraulic conductivity).",
    "author": [
      {
        "name": "Heather Savoy",
        "url": {}
      },
      {
        "name": "Falk Heße",
        "url": {}
      },
      {
        "name": "Yoram Rubin",
        "url": {}
      }
    ],
    "date": "2017-06-28",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-034.zip\nCRAN packages used\ngstat, spBayes, spTimer, anchoredDistr, devtools, RSQLite, np, plyr, dplyr, ggplot2\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Bayesian, Econometrics, Graphics, Phylogenetics, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-035/",
    "title": "A Tidy Data Model for Natural Language Processing using cleanNLP",
    "description": "Recent advances in natural language processing have produced libraries that extract low level features from a collection of raw texts. These features, known as annotations, are usually stored internally in hierarchical, tree-based data structures. This paper proposes a data model to represent annotations as a collection of normalized relational data tables optimized for exploratory data analysis and predictive modeling. The R package cleanNLP, which calls one of two state of the art NLP libraries (CoreNLP or spaCy), is presented as an implementation of this data model. It takes raw text as an input and returns a list of normalized tables. Specific annotations provided include tokenization, part of speech tagging, named entity recognition, sentiment analysis, dependency parsing, coreference resolution, and word embeddings. The package currently supports input text in English, German, French, and Spanish.",
    "author": [
      {
        "name": "Taylor Arnold",
        "url": {}
      }
    ],
    "date": "2017-06-28",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-035.zip\nCRAN packages used\ndplyr, ggplot2, magrittr, broom, janitor, tidyr, cleanNLP, tidytext, StanfordCoreNLP, coreNLP, XML, spacyr, NLP, cleanNLP, lda, lsa, topicmodels, sqliter, rJava, sotu, glmnet\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, WebTechnologies, Graphics, HighPerformanceComputing, MachineLearning, Phylogenetics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-030/",
    "title": "PGEE: An R Package for Analysis of Longitudinal Data with High-Dimensional Covariates",
    "description": "We introduce an R package PGEE that implements the penalized generalized estimating equations (GEE) procedure proposed by Wang et al. (2012) to analyze longitudinal data with a large number of covariates. The PGEE package includes three main functions: CVfit, PGEE, and MGEE. The CVfit function computes the cross-validated tuning parameter for penalized generalized estimating equations. The function PGEE performs simultaneous estimation and variable selection for longitudinal data with high-dimensional covariates; whereas the function MGEE fits unpenalized GEE to the data for comparison. The R package PGEE is illustrated using a yeast cell-cycle gene expression data set.",
    "author": [
      {
        "name": "Gul Inan",
        "url": {}
      },
      {
        "name": "Lan Wang",
        "url": {}
      }
    ],
    "date": "2017-06-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngee, geepack, PGEE, MASS, mvtnorm, ncvreg, penalized, glmnet, rqPen\nCRAN Task Views implied by cited packages\nMachineLearning, SocialSciences, Distributions, Econometrics, Multivariate, Survival, Environmetrics, Finance, NumericalMathematics, Psychometrics, Robust\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-031/",
    "title": "minval: An R package for MINimal VALidation of Stoichiometric Reactions",
    "description": "A genome-scale metabolic reconstruction is a compilation of all stoichiometric reactions that can describe the entire cellular metabolism of an organism, and they have become an indispensable tool for our understanding of biological phenomena, covering fields that range from systems biology to bioengineering. Interrogation of metabolic reconstructions are generally carried through Flux Balance Analysis, an optimization method in which the biological sense of the optimal solution is highly sensitive to thermodynamic unbalance caused by the presence of stoichiometric reactions whose compounds are not produced or consumed in any other reaction (orphan metabolites) and by mass unbalance. The minval package was designed as a tool to identify orphan metabolites and evaluate the mass and charge balance of stoichiometric reactions. The package also includes functions to characterize and write models in TSV and SBML formats, extract all reactants, products, metabolite names and compartments from a metabolic reconstruction.",
    "author": [
      {
        "name": "Daniel Osorio",
        "url": {}
      },
      {
        "name": "Janneth González",
        "url": {}
      },
      {
        "name": "Andrés Pinzón",
        "url": {}
      }
    ],
    "date": "2017-06-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsybil, abcdeFBA, minval, gdata, readxl, xlsx, sybilSBML, sybil\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-032/",
    "title": "Working with Daily Climate Model Output Data in R and the futureheatwaves Package",
    "description": "Research on climate change impacts can require extensive processing of climate model output, especially when using ensemble techniques to incorporate output from multiple climate models and multiple simulations of each model. This processing can be particularly extensive when identifying and characterizing multi-day extreme events like heat waves and frost day spells, as these must be processed from model output with daily time steps. Further, climate model output is in a format and follows standards that may be unfamiliar to most R users. Here, we provide an overview of working with daily climate model output data in R. We then present the futureheatwaves package, which we developed to ease the process of identifying, characterizing, and exploring multi-day extreme events in climate model output. This package can input a directory of climate model output files, identify all extreme events using customizable event definitions, and summarize the output using user-specified functions.",
    "author": [
      {
        "name": "G. Brooke Anderson",
        "url": {}
      },
      {
        "name": "Colin Eason",
        "url": {}
      },
      {
        "name": "Elizabeth A. Barnes",
        "url": {}
      }
    ],
    "date": "2017-06-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfutureheatwaves, ggplot2, ncdf4, RNetCDF, ncdf4.helpers, PCICt, ncdf4.helpers, RCMIP5, wux, ggplot2, Rcpp, leaflet\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics, Spatial, SpatioTemporal, HighPerformanceComputing, NumericalMathematics\nBioconductor packages used\nncdfFlow\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-033/",
    "title": "Counterfactual: An R Package for Counterfactual Analysis",
    "description": "The Counterfactual package implements the estimation and inference methods of Cher nozhukov et al. (2013) for counterfactual analysis. The counterfactual distributions considered are the result of changing either the marginal distribution of covariates related to the outcome variable of interest, or the conditional distribution of the outcome given the covariates. They can be applied to estimate quantile treatment effects and wage decompositions. This paper serves as an introduction to the package and displays basic functionality of the commands contained within.",
    "author": [
      {
        "name": "Mingli Chen",
        "url": {}
      },
      {
        "name": "Victor Chernozhukov",
        "url": {}
      },
      {
        "name": "Iván Fernández-Val",
        "url": {}
      },
      {
        "name": "Blaise Melly",
        "url": {}
      }
    ],
    "date": "2017-06-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCounterfactual, quantreg, survival\nCRAN Task Views implied by cited packages\nEconometrics, SocialSciences, Survival, ClinicalTrials, Environmetrics, Optimization, ReproducibleResearch, Robust\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-029/",
    "title": "flan: An R Package for Inference on Mutation Models.",
    "description": "This paper describes flan, a package providing tools for fluctuation analysis of mutant cell counts. It includes functions dedicated to the distribution of final numbers of mutant cells. Parametric estimation and hypothesis testing are also implemented, enabling inference on different sorts of data with several possible methods. An overview of the subject is proposed. The general form of mutation models is described, including the classical models as particular cases. Estimating from a model, when the data have been generated by another, induces different possible biases, which are identified and discussed. The three estimation methods available in the package are described, and their mean squared errors are compared. Finally, implementation is discussed, and a few examples of usage on real data sets are given.",
    "author": [
      {
        "name": "Adrien Mazoyer",
        "url": {}
      },
      {
        "name": "Rémy Drouilhet",
        "url": {}
      },
      {
        "name": "Stéphane Despréaux",
        "url": {}
      },
      {
        "name": "Bernard Ycart",
        "url": {}
      }
    ],
    "date": "2017-05-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nflan, Rcpp, ggplot2, RcppGSL, polynom, RcppArmadillo, lbfgsb3\nCRAN Task Views implied by cited packages\nNumericalMathematics, Graphics, HighPerformanceComputing, Optimization, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-028/",
    "title": "checkmate: Fast Argument Checks for Defensive R Programming",
    "description": "Dynamically typed programming languages like R allow programmers to write generic, flexible and concise code and to interact with the language using an interactive Read-eval-print-loop (REPL). However, this flexibility has its price: As the R interpreter has no information about the expected variable type, many base functions automatically convert the input instead of raising an exception. Unfortunately, this frequently leads to runtime errors deeper down the call stack which obfuscates the original problem and renders debugging challenging. Even worse, unwanted conver sions can remain undetected and skew or invalidate the results of a statistical analysis. As a resort, assertions can be employed to detect unexpected input during runtime and to signal understandable and traceable errors. The package checkmate provides a plethora of functions to check the type and related properties of the most frequently used R objects and variable types. The package is mostly written in C to avoid any unnecessary performance overhead. Thus, the programmer can conveniently write concise, well-tested assertions which outperforms custom R code for many applications. Fur thermore, checkmate simplifies writing unit tests using the framework testthat (Wickham, 2011) by extending it with plenty of additional expectation functions, and registered C routines are available for package developers to perform assertions on arbitrary SEXPs (internal data structure for R objects implemented as struct in C) in compiled code.",
    "author": [
      {
        "name": "Michel Lang",
        "url": {}
      }
    ],
    "date": "2017-05-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncheckmate, assertthat, assertive, assertive.numbers, assertive.sets, assertr, magrittr, dplyr, testthat, data.table, tibble, microbenchmark, mlr, BatchJobs, batchtools\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Finance, MachineLearning, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-001/",
    "title": "iotools: High-Performance I/O Tools for R",
    "description": "The iotools package provides a set of tools for input and output intensive data processing in R. The functions chunk.apply and read.chunk are supplied to allow for iteratively loading contiguous blocks of data into memory as raw vectors. These raw vectors can then be efficiently converted into matrices and data frames with the iotools functions mstrsplit and dstrsplit. These functions minimize copying of data and avoid the use of intermediate strings in order to drastically improve performance. Finally, we also provide read.csv.raw to allow users to read an entire dataset into memory with the same efficient parsing code. In this paper, we present these functions through a set of examples with an emphasis on the flexibility provided by chunk-wise operations. We provide benchmarks comparing the speed of read.csv.raw to data loading functions provided in base R and other contributed packages.",
    "author": [
      {
        "name": "Taylor Arnold",
        "url": {}
      },
      {
        "name": "Michael J. Kane",
        "url": {}
      },
      {
        "name": "Simon Urbanek",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbigmemory, ff, readr, foreach, iterators, iotools, Matrix\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Econometrics, Multivariate, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-002/",
    "title": "IsoGeneGUI: Multiple Approaches for Dose-Response Analysis of Microarray Data Using R",
    "description": "The analysis of transcriptomic experiments with ordered covariates, such as dose-response data, has become a central topic in bioinformatics, in particular in omics studies. Consequently, multiple R packages on CRAN and Bioconductor are designed to analyse microarray data from various perspectives under the assumption of order restriction. We introduce the new R package IsoGene Graphical User Interface (IsoGeneGUI), an extension of the original IsoGene package that includes methods from most of available R packages designed for the analysis of order restricted microarray data, namely orQA, ORIClust, goric and ORCME. The methods included in the new IsoGeneGUI range from inference and estimation to model selection and clustering tools. The IsoGeneGUI is not only the most complete tool for the analysis of order restricted microarray experiments available in R but also it can be used to analyse other types of dose-response data. The package provides all the methods in a user friendly fashion, so analyses can be implemented by users with limited knowledge of R programming.",
    "author": [
      {
        "name": "Martin Otava",
        "url": {}
      },
      {
        "name": "Rudradev Sengupta",
        "url": {}
      },
      {
        "name": "Ziv Shkedy",
        "url": {}
      },
      {
        "name": "Dan Lin",
        "url": {}
      },
      {
        "name": "Setia Pramana",
        "url": {}
      },
      {
        "name": "Tobias Verbeke",
        "url": {}
      },
      {
        "name": "Philippe            Haldermans",
        "url": {}
      },
      {
        "name": "Ludwig A. Hothorn",
        "url": {}
      },
      {
        "name": "Daniel Gerhard",
        "url": {}
      },
      {
        "name": "Rebecca M. Kuiper",
        "url": {}
      },
      {
        "name": "Florian Klinglmueller",
        "url": {}
      },
      {
        "name": "           Adetayo Kasim",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\ntitle: ‘IsoGeneGUI: Multiple Approaches for Dose-Response Analysis of Microarray Data\nUsing R’\ndescription: The analysis of transcriptomic experiments with ordered covariates, such\nas dose-response data, has become a central topic in bioinformatics, in particular\nin omics studies. Consequently, multiple R packages on CRAN and Bioconductor are\ndesigned to analyse microarray data from various perspectives under the assumption\nof order restriction. We introduce the new R package IsoGene Graphical User Interface\n(IsoGeneGUI), an extension of the original IsoGene package that includes methods\nfrom most of available R packages designed for the analysis of order restricted\nmicroarray data, namely orQA, ORIClust, goric and ORCME. The methods included in\nthe new IsoGeneGUI range from inference and estimation to model selection and clustering\ntools. The IsoGeneGUI is not only the most complete tool for the analysis of order\nrestricted microarray experiments available in R but also it can be used to analyse\nother types of dose-response data. The package provides all the methods in a user\nfriendly fashion, so analyses can be implemented by users with limited knowledge\nof R programming.\nauthor:\n- Martin Otava\n- Rudradev Sengupta\n- Ziv Shkedy\n- Dan Lin\n- Setia Pramana\n- Tobias Verbeke\n- Philippe Haldermans\n- Ludwig A. Hothorn\n- Daniel Gerhard\n- Rebecca M. Kuiper\n- Florian Klinglmueller\n- ’ Adetayo Kasim’\ndate: ‘2017-05-10’\ndate_received: ‘2015-07-22’\njournal:\ntitle: The R Journal\nissn: 2073-4859\nfirstpage: 14\nlastpage: 26\nvolume: 9\nissue: 1\nslug: RJ-2017-002\npackages:\ncran:\n- IsoGene\n- orQA\n- goric\n- ORCME\n- ORIClust\n- limma\n- mratios\nbioc: ~\npreview: preview.png\nCTV: Cluster\noutput:\ndistill::distill_article:\nself_contained: no\ntoc: no\nlegacy_pdf: yes\npdf_url: RJ-2017-002.pdf\ncitation_url: https://doi.org/10.32614/RJ-2017-002\ndoi: 10.32614/RJ-2017-002\ncreative_commons: CC BY\ncsl: /home/mitchell/R/x86_64-pc-linux-gnu-library/4.1/rjtools/rjournal.csl\n\n\n\nCRAN packages used\nIsoGene, orQA, goric, ORCME, ORIClust, limma, mratios\nCRAN Task Views implied by cited packages\nCluster\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-003/",
    "title": "OrthoPanels: An R Package for Estimating a Dynamic Panel Model with Fixed Effects Using the Orthogonal Reparameterization Approach",
    "description": "This article describes the R package OrthoPanels, which includes the function opm(). This function implements the orthogonal reparameterization approach recommended by Lancaster (2002) to estimate dynamic panel models with fixed effects (and optionally: wave specific intercepts). This article provides a statistical description of the orthogonal reparameterization approach, a demonstration of the package using real-world data, and simulations comparing the estimator to the known-to-be-biased OLS estimator and the commonly used GMM estimator.",
    "author": [
      {
        "name": "Mark Pickup",
        "url": {}
      },
      {
        "name": "Paul Gustafson",
        "url": {}
      },
      {
        "name": "Davor Cubranic",
        "url": {}
      },
      {
        "name": "Geoffrey Evans",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nOrthoPanels, plm\nCRAN Task Views implied by cited packages\nEconometrics, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-004/",
    "title": "smoof: Single- and Multi-Objective Optimization Test Functions",
    "description": "Benchmarking algorithms for optimization problems usually is carried out by running the algorithms under consideration on a diverse set of benchmark or test functions. A vast variety of test functions was proposed by researchers and is being used for investigations in the literature. The smoof package implements a large set of test functions and test function generators for both the single and multi-objective case in continuous optimization and provides functions to easily create own test functions. Moreover, the package offers some additional helper methods, which can be used in the context of optimization.",
    "author": [
      {
        "name": "Jakob Bossek",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nemoa, mco, ecr, cec2005benchmark, cec2013, globalOptTests, soobench, smoof, ParamHelpers, ggplot2\nCRAN Task Views implied by cited packages\nOptimization, Graphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-005/",
    "title": "alineR: an R Package for Optimizing Feature-Weighted Alignments and Linguistic Distances",
    "description": "Linguistic distance measurements are commonly used in anthropology and biology when quantitative and statistical comparisons between words are needed. This is common, for example, when analyzing linguistic and genetic data. Such comparisons can provide insight into historical population patterns and evolutionary processes. However, the most commonly used linguistic distances are derived from edit distances, which do not weight phonetic features that may, for example, represent smaller-scale patterns in linguistic evolution. Thus, computational methods for calculating feature-weighted linguistic distances are needed for linguistic, biological, and evolutionary applications; additionally, the linguistic distances presented here are generic and may have broader applications in fields such as text mining and search, as well as applications in psycholinguistics and morphology. To facilitate this research, we are making available an open-source R software package that performs feature-weighted linguistic distance calculations. The package also includes a supervised learning methodology that uses a genetic algorithm and manually determined alignments to estimate 13 linguistic parameters including feature weights and a skip penalty. Here we present the package and use it to demonstrate the supervised learning methodology by estimating the optimal linguistic parameters for both simulated data and for a sample of Austronesian languages. Our results show that the methodology can estimate these parameters for both simulated and real language data, that optimizing feature weights improves alignment accuracy by approximately 29%, and that optimization significantly affects the resulting distance measurements. Availability: alineR is available on CRAN.",
    "author": [
      {
        "name": "Sean S. Downey",
        "url": {}
      },
      {
        "name": "Guowei Sun",
        "url": {}
      },
      {
        "name": "Peter Norquest",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nalineR, stringdist, RecordLinkage, doMC\nCRAN Task Views implied by cited packages\nOfficialStatistics, HighPerformanceComputing\nBioconductor packages used\nBiostrings\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-006/",
    "title": "Implementing a Metapopulation Bass Diffusion Model using the R Package deSolve",
    "description": "Diffusion is a fundamental process in physical, biological, social and economic settings. Consumer products often go viral, with sales driven by the word of mouth effect, as their adoption spreads through a population. The classic diffusion model used for product adoption is the Bass diffusion model, and this divides a population into two groups of people: potential adopters who are likely to adopt a product, and adopters who have purchased the product, and influence others to adopt. The Bass diffusion model is normally captured in an aggregate form, where no significant consumer differences are modeled. This paper extends the Bass model to capture a spatial perspective, using metapopulation equations from the field of infectious disease modeling. The paper’s focus is on simulation of deterministic models by solving ordinary differential equations, and does not encompass parameter estimation. The metapopulation model in implemented in R using the deSolve package, and shows the potential of using the R framework to implement large-scale integral equation models, with applications in the field of marketing and consumer behaviour.",
    "author": [
      {
        "name": "Jim Duggan",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndeSolve, EpiModel, ggplot2, scales\nCRAN Task Views implied by cited packages\nDifferentialEquations, Graphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-007/",
    "title": "MDplot: Visualise Molecular Dynamics",
    "description": "The MDplot package provides plotting functions to allow for automated visualisation of molecular dynamics simulation output. It is especially useful in cases where the plot generation is rather tedious due to complex file formats or when a large number of plots are generated. The graphs that are supported range from those which are standard, such as RMSD/RMSF (root-mean-square deviation and root-mean-square fluctuation, respectively) to less standard, such as thermodynamic integration analysis and hydrogen bond monitoring over time. All told, they address many com monly used analyses. In this article, we set out the MDplot package’s functions, give examples of the function calls, and show the associated plots. Plotting and data parsing is separated in all cases, i.e. the respective functions can be used independently. Thus, data manipulation and the integration of additional file formats is fairly easy. Currently, the loading functions support GROMOS, GROMACS, and AMBER file formats. Moreover, we also provide a Bash interface that allows simple embedding of MDplot into Bash scripts as the final analysis step. Availability: The package can be obtained in the latest major version from CRAN (https://cran.r project.org/package=MDplot) or in the most recent version from the project’s GitHub page at https://github.com/MDplot/MDplot, where feedback is also most welcome. MDplot is published under the GPL-3 license.",
    "author": [
      {
        "name": "Christian Margreitter",
        "url": {}
      },
      {
        "name": "Chris Oostenbrink",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMDplot, bio3d, Rknots\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-008/",
    "title": "On Some Extensions to GA Package: Hybrid Optimisation, Parallelisation and Islands EvolutionOn some extensions to GA package: hybrid optimisation, parallelisation and islands evolution",
    "description": "Genetic algorithms are stochastic iterative algorithms in which a population of individuals evolve by emulating the process of biological evolution and natural selection. The R package GA provides a collection of general purpose functions for optimisation using genetic algorithms. This paper describes some enhancements recently introduced in version 3 of the package. In particular, hybrid GAs have been implemented by including the option to perform local searches during the evolution. This allows to combine the power of genetic algorithms with the speed of a local optimiser. Another major improvement is the provision of facilities for parallel computing. Parallelisation has been implemented using both the master-slave approach and the islands evolution model. Several examples of usage are presented, with both real-world data examples and benchmark functions, showing that often high-quality solutions can be obtained more efficiently.",
    "author": [
      {
        "name": "Luca Scrucca",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrgenoud, Rmalschains, DEoptim, GenSA, pso, cmaes, tabuSearch, GA, quantmod, doParallel, foreach, iterators, doRNG, forecast, astsa, globalOptTests, Rcpp, memoise\nCRAN Task Views implied by cited packages\nOptimization, HighPerformanceComputing, Finance, MachineLearning, TimeSeries, Econometrics, Environmetrics, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-009/",
    "title": "imputeTS: Time Series Missing Value Imputation in R",
    "description": "The imputeTS package specializes on univariate time series imputation. It offers multiple state-of-the-art imputation algorithm implementations along with plotting functions for time series missing data statistics. While imputation in general is a well-known problem and widely covered by R packages, finding packages able to fill missing values in univariate time series is more complicated. The reason for this lies in the fact, that most imputation algorithms rely on inter-attribute correlations, while univariate time series imputation instead needs to employ time dependencies. This paper provides an introduction to the imputeTS package and its provided algorithms and tools. Furthermore, it gives a short overview about univariate time series imputation in R.",
    "author": [
      {
        "name": "Steffen Moritz",
        "url": {}
      },
      {
        "name": "Thomas Bartz-Beielstein",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nAMELIA, mice, VIM, missMDA, imputeTS, zoo, forecast, spacetime, timeSeries, xts\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, Econometrics, OfficialStatistics, Environmetrics, Multivariate, SocialSciences, SpatioTemporal, Psychometrics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-010/",
    "title": "Update of the nlme Package to Allow a Fixed Standard Deviation of the Residual Error",
    "description": "The use of linear and non-linear mixed models in the life sciences and pharmacometrics is common practice. Estimation of the parameters of models not involving a system of differential equations is often done by the R or S-Plus software with the nonlinear mixed effects nlme package. The estimated residual error may be used for diagnosis of the fitted model, but not whether the model correctly describes the relation between response and included variables including the true covariance structure. The latter is only true if the residual error is known in advance. Therefore, it may be necessary or more appropriate to fix the residual error a priori instead of estimate its value. This can be the case if one wants to include evidence from past studies or a theoretical derivation; e.g., when using a binomial model. S-Plus has an option to fix this residual error to a constant, in contrast to R. For convenience, the nlme package was customized to offer this option as well. In this paper, we derived the log-likelihoods for the mixed models using a fixed residual error. By using some well-known examples from mixed models, we demonstrated the equivalence of R and S-Plus with respect to the estimates. The updated package has been accepted by the Comprehensive R Archive Network (CRAN) team and will be available at the CRAN website.",
    "author": [
      {
        "name": "Simon H. Heisterkamp",
        "url": {}
      },
      {
        "name": "Engelbertus van Willigen",
        "url": {}
      },
      {
        "name": "Paul-Matthias Diderichsen",
        "url": {}
      },
      {
        "name": "John Maringwa",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nnlme\nCRAN Task Views implied by cited packages\nChemPhys, Econometrics, Environmetrics, Finance, OfficialStatistics, Psychometrics, SocialSciences, Spatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-011/",
    "title": "EMSaov: An R Package for the Analysis of Variance with the Expected Mean Squares and its Shiny Application",
    "description": "EMSaov is a new R package that we developed to provide users with an analysis of variance table including the expected mean squares (EMS) for various types of experimental design. It is not easy to find the appropriate test, particularly the denominator for the F statistic that depends on the EMS, when some variables exhibit random effects or when we use a special experimental design such as nested design, repeated measures design, or split-plot design. With EMSaov, a user can easily find the F statistic denominator and can determine how to analyze the data when using a special experimental design. We also develop a web application with a GUI interface using the shiny package in R . We expect that our application can contribute to the efficient and easy analysis of experimental data.",
    "author": [
      {
        "name": "Hye-Min Choe",
        "url": {}
      },
      {
        "name": "Mijeong Kim",
        "url": {}
      },
      {
        "name": "Eun-Kyung Lee",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nnlme, afex, EMSaov, shiny\nCRAN Task Views implied by cited packages\nChemPhys, Econometrics, Environmetrics, Finance, OfficialStatistics, Psychometrics, SocialSciences, Spatial, SpatioTemporal, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-012/",
    "title": "Multilabel Classification with R Package mlr",
    "description": "We implemented several multilabel classification algorithms in the machine learning package mlr. The implemented methods are binary relevance, classifier chains, nested stacking, dependent binary relevance and stacking, which can be used with any base learner that is accessible in mlr. Moreover, there is access to the multilabel classification versions of randomForestSRC and rFerns. All these methods can be easily compared by different implemented multilabel performance measures and resampling methods in the standardized mlr framework. In a benchmark experiment with several multilabel datasets, the performance of the different methods is evaluated.",
    "author": [
      {
        "name": "Philipp Probst",
        "url": {}
      },
      {
        "name": "Quay Au",
        "url": {}
      },
      {
        "name": "Giuseppe Casalicchio",
        "url": {}
      },
      {
        "name": "Clemens Stachl",
        "url": {}
      },
      {
        "name": "Bernd Bischl",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmldr, rFerns, randomForestSRC, randomForestSRC, ada, batchtools\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MachineLearning, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-013/",
    "title": "milr: Multiple-Instance Logistic Regression with Lasso Penalty",
    "description": "The purpose of the milr package is to analyze multiple-instance data. Ordinary multiple instance data consists of many independent bags, and each bag is composed of several instances. The statuses of bags and instances are binary. Moreover, the statuses of instances are not observed, whereas the statuses of bags are observed. The functions in this package are applicable for analyzing multiple-instance data, simulating data via logistic regression, and selecting important covariates in the regression model. To this end, maximum likelihood estimation with an expectation-maximization algorithm is implemented for model estimation, and a lasso penalty added to the likelihood function is applied for variable selection. Additionally, an \"milr\" object is applicable to generic functions fitted, predict and summary. Simulated data and a real example are given to demonstrate the features of this package.",
    "author": [
      {
        "name": "Ping-Yang Chen",
        "url": {}
      },
      {
        "name": "Ching-Chuan Chen",
        "url": {}
      },
      {
        "name": "Chun-Hao Yang",
        "url": {}
      },
      {
        "name": "Sheng-Mao Chang",
        "url": {}
      },
      {
        "name": "Kuo-Jung Lee",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmilr\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-014/",
    "title": "spcadjust: An R Package for Adjusting for Estimation Error in Control Charts",
    "description": "In practical applications of control charts the in-control state and the corresponding chart parameters are usually estimated based on some past in-control data. The estimation error then needs to be accounted for. In this paper we present an R package, spcadjust, which implements a bootstrap based method for adjusting monitoring schemes to take into account the estimation error. By bootstrapping the past data this method guarantees, with a certain probability, a conditional performance of the chart. In spcadjust the method is implement for various types of Shewhart, CUSUM and EWMA charts, various performance criteria, and both parametric and non-parametric bootstrap schemes. In addition to the basic charts, charts based on linear and logistic regression models for risk adjusted monitoring are included, and it is easy for the user to add further charts. Use of the package is demonstrated by examples.",
    "author": [
      {
        "name": "Axel Gandy",
        "url": {}
      },
      {
        "name": "Jan Terje Kvaløy",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspcadjust, surveillance, spc, qcc, IQCC, qcr, edcc, MSQC\nCRAN Task Views implied by cited packages\nEnvironmetrics, SpatioTemporal, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-015/",
    "title": "GsymPoint: An R Package to Estimate the Generalized Symmetry Point, an Optimal Cut-off Point for Binary Classification in Continuous Diagnostic Tests",
    "description": "In clinical practice, it is very useful to select an optimal cutpoint in the scale of a continuous biomarker or diagnostic test for classifying individuals as healthy or diseased. Several methods for choosing optimal cutpoints have been presented in the literature, depending on the ultimate goal. One of these methods, the generalized symmetry point, recently introduced, generalizes the symmetry point by incorporating the misclassification costs. Two statistical approaches have been proposed in the literature for estimating this optimal cutpoint and its associated sensitivity and specificity measures, a parametric method based on the generalized pivotal quantity and a nonparametric method based on empirical likelihood. In this paper, we introduce GsymPoint, an R package that implements these methods in a user-friendly environment, allowing the end-user to calculate the generalized symmetry point depending on the levels of certain categorical covariates. The practical use of this package is illustrated using three real biomedical datasets.",
    "author": [
      {
        "name": "Mónica López-Ratón",
        "url": {}
      },
      {
        "name": "Elisa M. Molanes-López",
        "url": {}
      },
      {
        "name": "Emilio Letón",
        "url": {}
      },
      {
        "name": "Carmen Cadarso-Suárez",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nGsymPoint, PresenceAbsence, DiagnosisMed, pROC, OptimalCutpoints, GsymPoint\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-016/",
    "title": "pdp: An R Package for Constructing Partial Dependence Plots",
    "description": "Complex nonparametric models—like neural networks, random forests, and support vector machines—are more common than ever in predictive analytics, especially when dealing with large observational databases that don’t adhere to the strict assumptions imposed by traditional statistical techniques (e.g., multiple linear regression which assumes linearity, homoscedasticity, and normality). Unfortunately, it can be challenging to understand the results of such models and explain them to management. Partial dependence plots offer a simple solution. Partial dependence plots are low dimensional graphical renderings of the prediction function so that the relationship between the outcome and predictors of interest can be more easily understood. These plots are especially useful in explaining the output from black box models. In this paper, we introduce pdp, a general R package for constructing partial dependence plots.",
    "author": [
      {
        "name": "Brandon M. Greenwell",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrandomForest, gbm, party, partykit, pdp, plotmo, lattice, ICEbox, car, effects, ggplot2, grid, latticeExtra, gridExtra, nnet, C50, rpart, adabag, ipred, adabag, xgboost, Cubist, MASS, earth, mda, ranger, e1071, kernlab, caret, magrittr, foreach, viridis, plyr, doMC, doParallel, dplyr\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, Environmetrics, Survival, Econometrics, SocialSciences, Graphics, HighPerformanceComputing, Cluster, Distributions, Psychometrics, Finance, NaturalLanguageProcessing, NumericalMathematics, Optimization, Phylogenetics, Robust, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-017/",
    "title": "Weighted Effect Coding for Observational Data with wec",
    "description": "Weighted effect coding refers to a specific coding matrix to include factor variables in generalised linear regression models. With weighted effect coding, the effect for each category represents the deviation of that category from the weighted mean (which corresponds to the sample mean). This technique has particularly attractive properties when analysing observational data, that commonly are unbalanced. The wec package is introduced, that provides functions to apply weighted effect coding to factor variables, and to interactions between (a.) a factor variable and a continuous variable and between (b.) two factor variables.",
    "author": [
      {
        "name": "Rense Nieuwenhuis",
        "url": {}
      },
      {
        "name": "Manfred te Grotenhuis",
        "url": {}
      },
      {
        "name": "Ben Pelzer",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nwec\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-018/",
    "title": "coxphMIC: An R Package for Sparse Estimation of Cox Proportional Hazards Models via Approximated Information Criteria",
    "description": "In this paper, we describe an R package named coxphMIC, which implements the sparse estimation method for Cox proportional hazards models via approximated information criterion (Su et al., 2016). The developed methodology is named MIC which stands for “Minimizing approximated Information Criteria\". A reparameterization step is introduced to enforce sparsity while at the same time keeping the objective function smooth. As a result, MIC is computationally fast with a superior performance in sparse estimation. Furthermore, the reparameterization tactic yields an additional advantage in terms of circumventing post-selection inference (Leeb and Pötscher, 2005). The MIC method and its R implementation are introduced and illustrated with the PBC data.",
    "author": [
      {
        "name": "Razieh Nabi",
        "url": {}
      },
      {
        "name": "Xiaogang Su",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-019/",
    "title": "Retrieval and Analysis of Eurostat Open Data with the eurostat Package",
    "description": "The increasing availability of open statistical data resources is providing novel opportunities for research and citizen science. Efficient algorithmic tools are needed to realize the full potential of the new information resources. We introduce the eurostat R package that provides a collection of custom tools for the Eurostat open data service, including functions to query, download, manipulate, and visualize these data sets in a smooth, automated and reproducible manner. The online documentation provides detailed examples on the analysis of these spatio-temporal data collections. This work provides substantial improvements over the previously available tools, and has been extensively tested by an active user community. The eurostat R package contributes to the growing open source ecosystem dedicated to reproducible research in computational social science and digital humanities.",
    "author": [
      {
        "name": "Leo Lahti",
        "url": {}
      },
      {
        "name": "Janne Huovari",
        "url": {}
      },
      {
        "name": "Markus Kainu",
        "url": {}
      },
      {
        "name": "Przemysław Biecek",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nFAOSTAT, WDI, pxweb, osmar, eurostat, smarterpoland, rsdmx, datamart, quandl, pdfetch, classInt, httr, jsonlite, readr, sp, stringi, tibble, plotrix, maptools, rgdal, rgeos, scales, stringr, countrycode\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, Graphics, NaturalLanguageProcessing, SpatioTemporal, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-020/",
    "title": "Market Area Analysis for Retail and Service Locations with MCI",
    "description": "In retail location analysis, marketing research and spatial planning, the market areas of stores and/or locations are a frequent subject. Market area analyses consist of empirical observations and modeling via theoretical and/or econometric models such as the Huff Model or the Multiplicative Competitive Interaction Model. The authors’ package MCI implements the steps of market area analysis into R with a focus on fitting the models and data preparation and processing.",
    "author": [
      {
        "name": "Thomas Wieland",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMCI, SpatialPosition, ggmap, osmar, osrm, car, spgwr\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, Econometrics, Finance, Multivariate, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-021/",
    "title": "PSF: Introduction to R Package for Pattern Sequence Based Forecasting Algorithm",
    "description": "This paper introduces the R package that implements the Pattern Sequence based Forecasting (PSF) algorithm, which was developed for univariate time series forecasting. This algorithm has been successfully applied to many different fields. The PSF algorithm consists of two major parts: clustering and prediction. The clustering part includes selection of the optimum number of clusters. It labels time series data with reference to such clusters. The prediction part includes functions like optimum window size selection for specific patterns and prediction of future values with reference to past pattern sequences. The PSF package consists of various functions to implement the PSF algorithm. It also contains a function which automates all other functions to obtain optimized prediction results. The aim of this package is to promote the PSF algorithm and to ease its usage with minimum efforts. This paper describes all the functions in the PSF package with their syntax. It also provides a simple example. Finally, the usefulness of this package is discussed by comparing it to auto.arima and ets, well-known time series forecasting functions available on CRAN repository.",
    "author": [
      {
        "name": "Neeraj Bokde",
        "url": {}
      },
      {
        "name": "Gualberto Asencio-Cortés",
        "url": {}
      },
      {
        "name": "Francisco Martínez-Álvarez",
        "url": {}
      },
      {
        "name": "Kishore Kulat",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPSF, cluster, data.table, forecast\nCRAN Task Views implied by cited packages\nEnvironmetrics, Finance, Cluster, Econometrics, HighPerformanceComputing, Multivariate, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-022/",
    "title": "BayesBinMix: an R Package for Model Based Clustering of Multivariate Binary Data",
    "description": "The BayesBinMix package offers a Bayesian framework for clustering binary data with or without missing values by fitting mixtures of multivariate Bernoulli distributions with an unknown number of components. It allows the joint estimation of the number of clusters and model parameters using Markov chain Monte Carlo sampling. Heated chains are run in parallel and accelerate the convergence to the target posterior distribution. Identifiability issues are addressed by implementing label switching algorithms. The package is demonstrated and benchmarked against the Expectation Maximization algorithm using a simulation study as well as a real dataset.",
    "author": [
      {
        "name": "Panagiotis Papastamoulis",
        "url": {}
      },
      {
        "name": "Magnus Rattray",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nBayesBinMix, label.switching, foreach, doParallel, coda, FlexMix, flexclust\nCRAN Task Views implied by cited packages\nBayesian, Cluster, gR, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-023/",
    "title": "Network Visualization with ggplot2",
    "description": "This paper explores three different approaches to visualize networks by building on the grammar of graphics framework implemented in the ggplot2 package. The goal of each approach is to provide the user with the ability to apply the flexibility of ggplot2 to the visualization of network data, including through the mapping of network attributes to specific plot aesthetics. By incorporating networks in the ggplot2 framework, these approaches (1) allow users to enhance networks with additional information on edges and nodes, (2) give access to the strengths of ggplot2, such as layers and facets, and (3) convert network data objects to the more familiar data frames.",
    "author": [
      {
        "name": "Sam Tyner",
        "url": {}
      },
      {
        "name": "François Briatte",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nigraph, sna, network, statnet, ggplot2, ggnetwork, geomnet, ggmap, ggfortify, GGally, gcookbook, intergraph, grid, ggrepel, ndtv, gridExtra, tnet, ggCompNet, tidyverse, plyr, dplyr\nCRAN Task Views implied by cited packages\ngR, SocialSciences, Graphics, Optimization, Spatial, Bayesian, Phylogenetics, WebTechnologies\nBioconductor packages used\nggbio, ggtree\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-024/",
    "title": "The mosaic Package: Helping Students to Think with Data Using R",
    "description": "The mosaic package provides a simplified and systematic introduction to the core functional ity related to descriptive statistics, visualization, modeling, and simulation-based inference required in first and second courses in statistics. This introduction to the package describes some of the guiding principles behind the design of the package and provides illustrative examples of several of the most important functions it implements. These can be combined to help students “think with data\" using R in their early course work, starting with simple, yet powerful, declarative commands.",
    "author": [
      {
        "name": "Randall Pruim",
        "url": {}
      },
      {
        "name": "Daniel T Kaplan",
        "url": {}
      },
      {
        "name": "Nicholas J Horton",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmosaic, lattice, mosaic, mosaicData, ggplot2, ggplot2, dplyr, parallel, MASS\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, Phylogenetics, Distributions, Econometrics, Environmetrics, NumericalMathematics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-025/",
    "title": "autoimage: Multiple Heat Maps for Projected Coordinates",
    "description": "Heat maps are commonly used to display the spatial distribution of a response observed on a two-dimensional grid. The autoimage package provides convenient functions for constructing multiple heat maps in unified, seamless way, particularly when working with projected coordinates. The autoimage package natively supports: 1. automatic inclusion of a color scale with the plotted image, 2. construction of heat maps for responses observed on regular or irregular grids, as well as non-gridded data, 3. construction of a matrix of heat maps with a common color scale, 4. construction of a matrix of heat maps with individual color scales, 5. projecting coordinates before plotting, 6. easily adding geographic borders, points, and other features to the heat maps. After comparing the autoimage package’s capabilities for constructing heat maps to those of existing tools, a carefully selected set of examples is used to highlight the capabilities of the autoimage package.",
    "author": [
      {
        "name": "Joshua P. French",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nautoimage, fields, lattice, sp, ggplot2, spatstat, gridExtra, cowplot, akima, mapproj, gear, viridisLite, maps\nCRAN Task Views implied by cited packages\nSpatial, Graphics, SpatioTemporal, Multivariate, NumericalMathematics, Phylogenetics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-026/",
    "title": "Hosting Data Packages via drat: A Case Study with Hurricane Exposure Data",
    "description": "Data-only packages offer a way to provide extended functionality for other R users. However, such packages can be large enough to exceed the package size limit (5 megabytes) for the Comprehen sive R Archive Network (CRAN). As an alternative, large data packages can be posted to additional repostiories beyond CRAN itself in a way that allows smaller code packages on CRAN to access and use the data. The drat package facilitates creation and use of such alternative repositories and makes it particularly simple to host them via GitHub. CRAN packages can draw on packages posted to drat repositories through the use of the ‘Additonal_repositories’ field in the DESCRIPTION file. This paper describes how R users can create a suite of coordinated packages, in which larger data packages are hosted in an alternative repository created with drat, while a smaller code package that interacts with this data is created that can be submitted to CRAN.",
    "author": [
      {
        "name": "G. Brooke Anderson",
        "url": {}
      },
      {
        "name": "Dirk Eddelbuettel",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nNMMAPSlite, stashR, rnoaa, tigris, UScensus2000, drat, grattan, hurricaneexposure, devtools, rcmdcheck, git2r, littler, knitr, roxygen2\nCRAN Task Views implied by cited packages\nReproducibleResearch, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-027/",
    "title": "The NoiseFiltersR Package: Label Noise Preprocessing in R",
    "description": "In Data Mining, the value of extracted knowledge is directly related to the quality of the used data. This makes data preprocessing one of the most important steps in the knowledge discovery process. A common problem affecting data quality is the presence of noise. A training set with label noise can reduce the predictive performance of classification learning techniques and increase the overfitting of classification models. In this work we present the NoiseFiltersR package. It contains the first extensive R implementation of classical and state-of-the-art label noise filters, which are the most common techniques for preprocessing label noise. The algorithms used for the implementation of the label noise filters are appropriately documented and referenced. They can be called in a R-user-friendly manner, and their results are unified by means of the \"filter\" class, which also benefits from adapted print and summary methods.",
    "author": [
      {
        "name": "Pablo Morales",
        "url": {}
      },
      {
        "name": "Julián Luengo",
        "url": {}
      },
      {
        "name": "Luís P.F. Garcia",
        "url": {}
      },
      {
        "name": "Ana C. Lorena",
        "url": {}
      },
      {
        "name": "André C.P.L.F. de Carvalho",
        "url": {}
      },
      {
        "name": "           Francisco Herrera",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\ntitle: ‘The NoiseFiltersR Package: Label Noise Preprocessing in R’\ndescription: In Data Mining, the value of extracted knowledge is directly related\nto the quality of the used data. This makes data preprocessing one of the most important\nsteps in the knowledge discovery process. A common problem affecting data quality\nis the presence of noise. A training set with label noise can reduce the predictive\nperformance of classification learning techniques and increase the overfitting of\nclassification models. In this work we present the NoiseFiltersR package. It contains\nthe first extensive R implementation of classical and state-of-the-art label noise\nfilters, which are the most common techniques for preprocessing label noise. The\nalgorithms used for the implementation of the label noise filters are appropriately\ndocumented and referenced. They can be called in a R-user-friendly manner, and their\nresults are unified by means of the “filter” class, which also benefits from adapted\nprint and summary methods.\nauthor:\n- Pablo Morales\n- Julián Luengo\n- Luís P.F. Garcia\n- Ana C. Lorena\n- André C.P.L.F. de Carvalho\n- ’ Francisco Herrera’\ndate: ‘2017-05-10’\ndate_received: ‘2016-07-12’\njournal:\ntitle: The R Journal\nissn: 2073-4859\nfirstpage: 219\nlastpage: 228\nvolume: 9\nissue: 1\nslug: RJ-2017-027\npackages:\ncran:\n- MICE\n- Amelia\n- caret\n- FSelector\n- mvoutlier\n- robustDA\n- probFDA\n- NoiseFiltersR\n- unbalanced\n- RWeka\nbioc: ~\npreview: preview.png\nCTV:\n- MachineLearning\n- Multivariate\n- Robust\n- HighPerformanceComputing\n- NaturalLanguageProcessing\n- OfficialStatistics\n- SocialSciences\noutput:\ndistill::distill_article:\nself_contained: no\ntoc: no\nlegacy_pdf: yes\npdf_url: RJ-2017-027.pdf\ncitation_url: https://doi.org/10.32614/RJ-2017-027\ndoi: 10.32614/RJ-2017-027\ncreative_commons: CC BY\ncsl: /home/mitchell/R/x86_64-pc-linux-gnu-library/4.1/rjtools/rjournal.csl\n\n\n\nCRAN packages used\nMICE, Amelia, caret, FSelector, mvoutlier, robustDA, probFDA, NoiseFiltersR, unbalanced, RWeka\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, Robust, HighPerformanceComputing, NaturalLanguageProcessing, OfficialStatistics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-036/",
    "title": "rnrfa: An R package to Retrieve, Filter and Visualize Data from the UK National River Flow Archive",
    "description": "The UK National River Flow Archive (NRFA) stores several types of hydrological data and metadata: daily river flow and catchment rainfall time series, gauging station and catchment informa tion. Data are served through the NRFA web services via experimental RESTful APIs. Obtaining NRFA data can be unwieldy due to complexities in handling HTTP GET requests and parsing responses in JSON and XML formats. The rnrfa package provides a set of functions to programmatically access, filter, and visualize NRFA data using simple R syntax. This paper describes the structure of the rnrfa package, including examples using the main functions gdf() and cmr() for flow and rainfall data, respectively. Visualization examples are also provided with a shiny web application and functions provided in the package. Although this package is regional specific, the general framework and structure could be applied to similar databases.",
    "author": [
      {
        "name": "Claudia Vitolo",
        "url": {}
      },
      {
        "name": "Matthew Fry",
        "url": {}
      },
      {
        "name": "Wouter Buytaert",
        "url": {}
      }
    ],
    "date": "2017-01-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrnrfa, rnoaa, waterData, RNCEP, shiny, leaflet, rmarkdown, DT, dplyr, cowplot, plyr, httr, xml2, stringr, xts, rjson, ggmap, ggplot2, rgdal, sp, ggrepel, devtools, microbenchmark, cranlogs, evd, outliers, spacetime, sos4R\nCRAN Task Views implied by cited packages\nWebTechnologies, Spatial, SpatioTemporal, ReproducibleResearch, Distributions, Econometrics, Environmetrics, ExtremeValue, Finance, Graphics, Phylogenetics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-041/",
    "title": "Normal Tolerance Interval Procedures in the tolerance Package",
    "description": "Statistical tolerance intervals are used for a broad range of applications, such as quality control, engineering design tests, environmental monitoring, and bioequivalence testing. tolerance is the only R package devoted to procedures for tolerance intervals and regions. Perhaps the most commonly-employed functions of the package involve normal tolerance intervals. A number of new procedures for this setting have been included in recent versions of tolerance. In this paper, we discuss and illustrate the functions that implement these normal tolerance interval procedures, one of which is a new, novel type of operating characteristic curve.",
    "author": [
      {
        "name": "Derek S. Young",
        "url": {}
      }
    ],
    "date": "2017-01-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntolerance, cranlogs, rgl\nCRAN Task Views implied by cited packages\nDistributions, Graphics, Multivariate, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-054/",
    "title": "Computing Pareto Frontiers and Database Preferences with the rPref Package",
    "description": "The concept of Pareto frontiers is well-known in economics. Within the database community there exist many different solutions for the specification and calculation of Pareto frontiers, also called Skyline queries in the database context. Slight generalizations like the combination of the Pareto operator with the lexicographical order have been established under the term database preferences. In this paper we present the rPref package which allows to efficiently deal with these concepts within R. With its help, database preferences can be specified in a very similar way as in a state-of-the-art database management system. Our package provides algorithms for an efficient calculation of the Pareto-optimal set and further functionalities for visualizing and analyzing the induced preference order.",
    "author": [
      {
        "name": "Patrick Roocks",
        "url": {}
      }
    ],
    "date": "2017-01-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrPref, emoa, mco, TunePareto, dplyr, lazyeval, RcppParallel, igraph, ggplot2\nCRAN Task Views implied by cited packages\nGraphics, Optimization, gR, HighPerformanceComputing, Phylogenetics, Spatial\nBioconductor packages used\nRgraphviz\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-057/",
    "title": "Weighted Distance Based Discriminant Analysis: The R Package WeDiBaDis",
    "description": "The WeDiBaDis package provides a user friendly environment to perform discriminant analysis (supervised classification). WeDiBaDis is an easy to use package addressed to the biological and medical communities, and in general, to researchers interested in applied studies. It can be suitable when the user is interested in the problem of constructing a discriminant rule on the basis of distances between a relatively small number of instances or units of known unbalanced-class membership measured on many (possibly thousands) features of any type. This is a current situation when analyzing genetic biomedical data. This discriminant rule can then be used both, as a means of explaining differences among classes, but also in the important task of assigning the class membership for new unlabeled units. Our package implements two discriminant analysis procedures in an R environment: the well-known distance-based discriminant analysis (DB-discriminant) and a weighted distance-based discriminant (WDB-discriminant), a novel classifier rule that we introduce. This new procedure is based on an improvement of the DB rule taking into account the statistical depth of the units. This article presents both classifying procedures and describes the implementation of each in detail. We illustrate the use of the package using an ecological and a genetic experimental example. Finally, we illustrate the effectiveness of the new proposed procedure (WDB), as compared with DB. This comparison is carried out using thirty-eight, high-dimensional, class-unbalanced, cancer data sets, three of which include clinical features.",
    "author": [
      {
        "name": "Itziar Irigoien",
        "url": {}
      },
      {
        "name": "Francesc Mestres",
        "url": {}
      },
      {
        "name": "Concepcion Arenas",
        "url": {}
      }
    ],
    "date": "2017-01-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncluster, ICGE, vegan\nCRAN Task Views implied by cited packages\nEnvironmetrics, Multivariate, Cluster, Phylogenetics, Psychometrics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-059/",
    "title": "condSURV: An R Package for the Estimation of the Conditional Survival Function for Ordered Multivariate Failure Time Data",
    "description": "One major goal in clinical applications of time-to-event data is the estimation of survival with censored data. The usual nonparametric estimator of the survival function is the time-honored Kaplan-Meier product-limit estimator. Though this estimator has been implemented in several R packages, the development of the condSURV R package has been motivated by recent contributions that allow the estimation of the survival function for ordered multivariate failure time data. The condSURV package provides three different approaches all based on the Kaplan-Meier estimator. In one of these approaches these quantities are estimated conditionally on current or past covariate measures. Illustration of the software usage is included using real data.",
    "author": [
      {
        "name": "Luis Meira-Machado",
        "url": {}
      },
      {
        "name": "Marta Sestelo",
        "url": {}
      }
    ],
    "date": "2017-01-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsurvival, prodlim, condSURV\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-042/",
    "title": "easyROC: An Interactive Web-tool for ROC Curve Analysis Using R Language Environment",
    "description": "ROC curve analysis is a fundamental tool for evaluating the performance of a marker in a number of research areas, e.g., biomedicine, bioinformatics, engineering etc., and is frequently used for discriminating cases from controls. There are a number of analysis tools which are used to guide researchers through their analysis. Some of these tools are commercial and provide basic methods for ROC curve analysis while others offer advanced analysis techniques and a command-based user interface, such as the R environment. The R environmentg includes comprehensive tools for ROC curve analysis; however, using a command-based interface might be challenging and time consuming when a quick evaluation is desired; especially for non-R users, physicians etc. Hence, a quick, comprehensive, free and easy-to-use analysis tool is required. For this purpose, we developed a user-friendly web tool based on the R language. This tool provides ROC statistics, graphical tools, optimal cutpoint calculation, comparison of several markers, and sample size estimation to support researchers in their decisions without writing R codes. easyROC can be used via any device with an internet connection independently of the operating system. The web interface of easyROC is constructed with the R package shiny. This tool is freely available through www.biosoft.hacettepe.edu.tr/easyROC.",
    "author": [
      {
        "name": "Dincer Goksuluk",
        "url": {}
      },
      {
        "name": "Selcuk Korkmaz",
        "url": {}
      },
      {
        "name": "Gokmen Zararsiz",
        "url": {}
      },
      {
        "name": "A. Ergun Karaagaoglu",
        "url": {}
      }
    ],
    "date": "2016-12-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nROCR, pROC, OptimalCutpoints, shiny, plotROC, plyr\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, WebTechnologies\nBioconductor packages used\nROC\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-033/",
    "title": "diverse: an R Package to Analyze Diversity in Complex Systems",
    "description": "The package diverse provides an easy-to-use interface to calculate and visualize different aspects of diversity in complex systems. In recent years, an increasing number of research projects in social and interdisciplinary sciences, including fields like innovation studies, scientometrics, economics, and network science have emphasized the role of diversification and sophistication of socioeconomic systems. However, so far no dedicated package exists that covers the needs of these emerging fields and interdisciplinary teams. Most packages about diversity tend to be created according to the demands and terminology of particular areas of natural and biological sciences. The package diverse uses interdisciplinary concepts of diversity—like variety, disparity and balance— as well as ubiquity and revealed comparative advantages, that are relevant to many fields of science, but are in particular useful for interdisciplinary research on diversity in socioeconomic systems. The package diverse provides a toolkit for social scientists, interdisciplinary researcher, and beginners in ecology to (i) import data, (ii) calculate different data transformations and normalization like revealed comparative advantages, (iii) calculate different diversity measures, and (iv) connect diverse to other specialized R packages on similarity measures, data visualization techniques, and statistical significance tests. The comprehensiveness of the package, from matrix import and transformations options, over similarity and diversity measures, to data visualization methods, makes it a useful package to explore different dimensions of diversity in complex systems.",
    "author": [
      {
        "name": "Miguel R. Guevara",
        "url": {}
      },
      {
        "name": "Dominik Hartmann",
        "url": {}
      },
      {
        "name": "Marcelo Mendoza",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nentropart, vegan, biodiversityR, Blaunet, diveRsity, divo, FD, hierDiversity, simboot, treescape, SYNCSA, diverse, proxy, pheatmap, treemap, igraph, foreign, spadeR\nCRAN Task Views implied by cited packages\nEnvironmetrics, Multivariate, OfficialStatistics, Phylogenetics, Spatial, gR, Graphics, Optimization, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-037/",
    "title": "Qtools: A Collection of Models and Tools for Quantile Inference",
    "description": "Quantiles play a fundamental role in statistics. The quantile function defines the distribution of a random variable and, thus, provides a way to describe the data that is specular but equivalent to that given by the corresponding cumulative distribution function. There are many advantages in working with quantiles, starting from their properties. The renewed interest in their usage seen in the last years is due to the theoretical, methodological, and software contributions that have broadened their applicability. This paper presents the R package Qtools, a collection of utilities for unconditional and conditional quantiles.",
    "author": [
      {
        "name": "Marco Geraci",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nquantreg, bayesQR, BSquare, lqmm, Qtools, boot, Rearrangement, mice\nCRAN Task Views implied by cited packages\nSocialSciences, Bayesian, Econometrics, Optimization, Robust, Survival, Environmetrics, Multivariate, OfficialStatistics, ReproducibleResearch, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-045/",
    "title": "Ake: An R Package for Discrete and Continuous Associated Kernel Estimations",
    "description": "Kernel estimation is an important technique in exploratory data analysis. Its utility relies on its ease of interpretation, especially based on graphical means. The Ake package is introduced for univariate density or probability mass function estimation and also for continuous and discrete regression functions using associated kernel estimators. These associated kernels have been proposed due to their specific features of variables of interest. The package focuses on associated kernel methods appropriate for continuous (bounded, positive) or discrete (count, categorical) data often found in applied settings. Furthermore, optimal bandwidths are selected by cross-validation for any associated kernel and by Bayesian methods for the binomial kernel. Other Bayesian methods for selecting bandwidths with other associated kernels will complete this package in its future versions; particularly, a Bayesian adaptive method for gamma kernel estimation of density functions is developed. Some practical and theoretical aspects of the normalizing constant in both density and probability mass functions estimations are given.",
    "author": [
      {
        "name": "Wanbitching E. Wansouwé",
        "url": {}
      },
      {
        "name": "Sobom M. Somé",
        "url": {}
      },
      {
        "name": "Célestin C. Kokonendji",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nAke\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-051/",
    "title": "water: Tools and Functions to Estimate Actual Evapotranspiration Using Land Surface Energy Balance Models in R",
    "description": "The crop water requirement is a key factor in the agricultural process. It is usually estimated throughout actual evapotranspiration (ETa ). This parameter is the key to develop irrigation strategies, to improve water use efficiency and to understand hydrological, climatic, and ecosystem processes. Currently, it is calculated with classical methods, which are difficult to extrapolate, or with land surface energy balance models (LSEB), such as METRIC and SEBAL, which are based on remote sensing data. This paper describes water, an open implementation of LSEB. The package provides several functions to estimate the parameters of the LSEB equation from satellite data and proposes a new object class to handle weather station data. One of the critical steps in METRIC is the selection of “cold” and “hot” pixels, which water solves with an automatic method. The water package can process a batch of satellite images and integrates most of the already published sub-models for METRIC. Although water implements METRIC, it will be expandable to SEBAL and others in the near future. Finally, two different procedures are demonstrated using data that is included in water package.",
    "author": [
      {
        "name": "Guillermo Federico Olmedo",
        "url": {}
      },
      {
        "name": "Samuel Ortega-Farías",
        "url": {}
      },
      {
        "name": "Daniel de la Fuente-Sáiz",
        "url": {}
      },
      {
        "name": "David Fonseca-            Luego",
        "url": {}
      },
      {
        "name": "Fernando Fuentes-Peñailillo",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nraster, raster\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-061/",
    "title": "Measurement Units in R",
    "description": "We briefly review SI units, and discuss R packages that deal with measurement units, their compatibility and conversion. Built upon udunits2 and the UNIDATA udunits library, we introduce the package units that provides a class for maintaining unit metadata. When used in expression, it automatically converts units, and simplifies units of results when possible; in case of incompatible units, errors are raised. The class flexibly allows expansion beyond predefined units. Using units may eliminate a whole class of potential scientific programming mistakes. We discuss the potential and limitations of computing with explicit units.",
    "author": [
      {
        "name": "Edzer Pebesma",
        "url": {}
      },
      {
        "name": "Thomas Mailund",
        "url": {}
      },
      {
        "name": "James Hiebert",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlubridate, sp, measurements, NISTunits, udunits2, units, ggplot2, spacetime, h5, RNetCDF, sos4R\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Graphics, Phylogenetics, ReproducibleResearch, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-062/",
    "title": "mctest: An R Package for Detection of Collinearity among Regressors",
    "description": "It is common for linear regression models to be plagued with the problem of multicollinearity when two or more regressors are highly correlated. This problem results in unstable estimates of regression coefficients and causes some serious problems in validation and interpretation of the model. Different diagnostic measures are used to detect multicollinearity among regressors. Many statistical software and R packages provide few diagnostic measures for the judgment of multicollinearity. Most widely used diagnostic measures in these software are: coefficient of determination (R2 ), variance inflation factor/tolerance limit (VIF/TOL), eigenvalues, condition number (CN) and condition index (CI) etc. In this manuscript, we present an R package, mctest, that computes popular and widely used multicollinearity diagnostic measures. The package also indicates which regressors may be the reason of collinearity among regressors.",
    "author": [
      {
        "name": "Muhammad Imdadullah",
        "url": {}
      },
      {
        "name": "Muhammad Aslam",
        "url": {}
      },
      {
        "name": "Saima Altaf",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmctest, perturb, HH, car, fmsb, rms, faraway, usdm, VIF, leaps, bestglm, glmulti, meifly\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, ChemPhys, ClinicalTrials, Finance, Multivariate, ReproducibleResearch, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-044/",
    "title": "Escape from Boxland",
    "description": "A library of common geometric shapes can be used to train our brains for understanding data structure in high-dimensional Euclidean space. This article describes the methods for producing cubes, spheres, simplexes, and tori in multiple dimensions. It also describes new ways to define and generate high-dimensional tori. The algorithms are described, critical code chunks are given, and a large collection of generated data are provided. These are available in the R package geozoo, and selected movies and images, are available on the GeoZoo web site (http://schloerke.github.io/geozoo/).",
    "author": [
      {
        "name": "Barret Schloerke",
        "url": {}
      },
      {
        "name": "Hadley Wickham",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      }
    ],
    "date": "2016-11-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngeozoo, tourr, bitops, geozoo, geozoo\nCRAN Task Views implied by cited packages\nMultivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-046/",
    "title": "An Introduction to Principal Surrogate Evaluation with the pseval Package",
    "description": "We describe a new package called pseval that implements the core methods for the evaluation of principal surrogates in a single clinical trial. It provides a flexible interface for defining models for the risk given treatment and the surrogate, the models for integration over the missing counterfactual surrogate responses, and the estimation methods. Estimated maximum likelihood and pseudo-score can be used for estimation, and the bootstrap for inference. A variety of post-estimation methods are provided, including print, summary, plot, and testing. We summarize the main statistical methods that are implemented in the package and illustrate its use from the perspective of a novice R user.",
    "author": [
      {
        "name": "Michael C. Sachs",
        "url": {}
      },
      {
        "name": "Erin E. Gabriel",
        "url": {}
      }
    ],
    "date": "2016-11-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npseval, survival, survey, ggplot2, lattice, Surrogate\nCRAN Task Views implied by cited packages\nGraphics, SocialSciences, Survival, ClinicalTrials, Econometrics, Multivariate, OfficialStatistics, Pharmacokinetics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-052/",
    "title": "quantreg.nonpar: An R Package for Performing Nonparametric Series Quantile Regression",
    "description": "The R package quantreg.nonpar implements nonparametric quantile regression methods to estimate and make inference on partially linear quantile models. quantreg.nonpar obtains point estimates of the conditional quantile function and its derivatives based on series approximations to the nonparametric part of the model. It also provides pointwise and uniform confidence intervals over a region of covariate values and/or quantile indices for the same functions using analytical and resampling methods. This paper serves as an introduction to the package and displays basic functionality of the functions contained within.",
    "author": [
      {
        "name": "Michael Lipsitz",
        "url": {}
      },
      {
        "name": "Alexandre Belloni",
        "url": {}
      },
      {
        "name": "Victor Chernozhukov",
        "url": {}
      },
      {
        "name": "Iván Fernández-Val",
        "url": {}
      }
    ],
    "date": "2016-11-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nquantreg.nonpar, quantreg, QuantifQuantile, quantregGrowth, fda\nCRAN Task Views implied by cited packages\nEnvironmetrics, Econometrics, Optimization, ReproducibleResearch, Robust, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-053/",
    "title": "nmfgpu4R: GPU-Accelerated Computation of the Non-Negative Matrix Factorization (NMF) Using CUDA Capable Hardware",
    "description": "In this work, a novel package called nmfgpu4R is presented, which offers the computation of Non-negative Matrix Factorization (NMF) on Compute Unified Device Architecture (CUDA) platforms within the R environment. Benchmarks show a remarkable speed-up in terms of time per iteration by utilizing the parallelization capabilities of modern graphics cards. Therefore the application of NMF gets more attractive for real-world sized problems because the time to compute a factorization is reduced by an order of magnitude.",
    "author": [
      {
        "name": "Sven Koitka",
        "url": {}
      },
      {
        "name": "Christoph M. Friedrich",
        "url": {}
      }
    ],
    "date": "2016-11-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nNMF, NMFN, nmfgpu4R, Matrix, SparseM\nCRAN Task Views implied by cited packages\nEconometrics, Multivariate, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-055/",
    "title": "micompr: An R Package for Multivariate Independent Comparison of Observations",
    "description": "The R package micompr implements a procedure for assessing if two or more multivariate samples are drawn from the same distribution. The procedure uses principal component analysis to convert multivariate observations into a set of linearly uncorrelated statistical measures, which are then compared using a number of statistical methods. This technique is independent of the distributional properties of samples and automatically selects features that best explain their differences. The procedure is appropriate for comparing samples of time series, images, spectrometric measures or similar high-dimension multivariate observations.",
    "author": [
      {
        "name": "Nuno Fachada",
        "url": {}
      },
      {
        "name": "João Rodrigues",
        "url": {}
      },
      {
        "name": "Vitor V. Lopes",
        "url": {}
      },
      {
        "name": "Rui C. Martins",
        "url": {}
      },
      {
        "name": "Agostinho C. Rosa",
        "url": {}
      }
    ],
    "date": "2016-11-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmicompr, vegan, Blossom, energy, crossmatch, cramer, ks, ChemoSpec, biotools, MVN, testthat, knitr, roxygen2, deseasonalize\nCRAN Task Views implied by cited packages\nMultivariate, ChemPhys, Environmetrics, Phylogenetics, Psychometrics, ReproducibleResearch, Spatial, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-032/",
    "title": "Design of the TRONCO BioConductor Package for TRanslational ONCOlogy",
    "description": "Models of cancer progression provide insights on the order of accumulation of genetic alterations during cancer development. Algorithms to infer such models from the currently available mutational profiles collected from different cancer patients (cross-sectional data) have been defined in the literature since late the 90s. These algorithms differ in the way they extract a graphical model of the events modelling the progression, e.g., somatic mutations or copy-number alterations. TRONCO is an R package for TRanslational ONcology which provides a series of functions to assist the user in the analysis of cross-sectional genomic data and, in particular, it implements algorithms that aim to model cancer progression by means of the notion of selective advantage. These algorithms are proved to outperform the current state-of-the-art in the inference of cancer progression models. TRONCO also provides functionalities to load input cross-sectional data, set up the execution of the algorithms, assess the statistical confidence in the results, and visualize the models. Availability. Freely available at http://www.bioconductor.org/ under GPL license; project hosted at http://bimib.disco.unimib.it/ and https://github.com/BIMIB-DISCo/TRONCO. Contact. tronco@disco.unimib.it",
    "author": [
      {
        "name": "Marco Antoniotti",
        "url": {}
      },
      {
        "name": "Giulio Caravagna",
        "url": {}
      },
      {
        "name": "Luca De Sano",
        "url": {}
      },
      {
        "name": "Alex Graudenzi",
        "url": {}
      },
      {
        "name": "Giancarlo Mauri",
        "url": {}
      },
      {
        "name": "Bud            Mishra",
        "url": {}
      },
      {
        "name": "Daniele Ramazzotti",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\nBioconductor packages used\nTRONCO\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-039/",
    "title": "Variants of Simple Correspondence Analysis",
    "description": "This paper presents the R package CAvariants (Lombardo and Beh, 2017). The package performs six variants of correspondence analysis on a two-way contingency table. The main function that shares the same name as the package – CAvariants – allows the user to choose (via a series of input parameters) from six different correspondence analysis procedures. These include the classical approach to (symmetrical) correspondence analysis, singly ordered correspondence analysis, doubly ordered correspondence analysis, non symmetrical correspondence analysis, singly ordered non symmetrical correspondence analysis and doubly ordered non symmetrical correspondence analysis. The code provides the flexibility for constructing either a classical correspondence plot or a biplot graphical display. It also allows the user to consider other important features that allow to assess the reliability of the graphical representations, such as the inclusion of algebraically derived elliptical confidence regions. This paper provides R functions that elaborates more fully on the code presented in Beh and Lombardo (2014).",
    "author": [
      {
        "name": "Rosaria Lombardo",
        "url": {}
      },
      {
        "name": "Eric J. Beh",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMASS, ca, anacor, FactoMineR, cabootcrs, CAinterprTools, homals, dualScale, ExPosition, vegan, ade4, cncaGUI, PTAk, CAvariants\nCRAN Task Views implied by cited packages\nPsychometrics, Multivariate, Environmetrics, ChemPhys, Spatial, Distributions, Econometrics, Graphics, MedicalImaging, NumericalMathematics, Pharmacokinetics, Phylogenetics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-047/",
    "title": "Calculating Biological Module Enrichment or Depletion and Visualizing Data on Large-scale Molecular Maps with ACSNMineR and RNaviCell Packages",
    "description": "Biological pathways or modules represent sets of interactions or functional relationships occurring at the molecular level in living cells. A large body of knowledge on pathways is organized in public databases such as the KEGG, Reactome, or in more specialized repositories, the Atlas of Cancer Signaling Network (ACSN) being an example. All these open biological databases facilitate analyses, improving our understanding of cellular systems. We hereby describe ACSNMineR for calculation of enrichment or depletion of lists of genes of interest in biological pathways. ACSNMineR integrates ACSN molecular pathways gene sets, but can use any gene set encoded as a GMT file, for instance sets of genes available in the Molecular Signatures Database (MSigDB). We also present RNaviCell, that can be used in conjunction with ACSNMineR to visualize different data types on web-based, interactive ACSN maps. We illustrate the functionalities of the two packages with biological data taken from large-scale cancer datasets.",
    "author": [
      {
        "name": "Paul Deveau",
        "url": {}
      },
      {
        "name": "Emmanuel Barillot",
        "url": {}
      },
      {
        "name": "Valentina Boeva",
        "url": {}
      },
      {
        "name": "Andrei Zinovyev",
        "url": {}
      },
      {
        "name": "Eric Bonnet",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nACSNMineR, RNaviCell, ggplot2\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics\nBioconductor packages used\nGOstats, clusterProfiler\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-049/",
    "title": "dCovTS: Distance Covariance/Correlation for Time Series",
    "description": "The distance covariance function is a new measure of dependence between random vectors. We drop the assumption of iid data to introduce distance covariance for time series. The R package dCovTS provides functions that compute and plot distance covariance and correlation functions for both univariate and multivariate time series. Additionally it includes functions for testing serial independence based on distance covariance. This paper describes the theoretical background of distance covariance methodology in time series and discusses in detail the implementation of these methods with the R package dCovTS.",
    "author": [
      {
        "name": "Maria Pitsillou",
        "url": {}
      },
      {
        "name": "Konstantinos Fokianos",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nenergy, doParallel, portes, MTS\nCRAN Task Views implied by cited packages\nTimeSeries, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-050/",
    "title": "comf: An R Package for Thermal Comfort Studies",
    "description": "The field of thermal comfort generated a number of thermal comfort indices. Their code implementation needs to be done by individual researchers. This paper presents the R package, comf, which includes functions for common and new thermal comfort indices. Additional functions allow comparisons between the predictive performance of these indices. This paper reviews existing thermal comfort indices and available code implementations. This is followed by the description of the R package and an example how to use the R package for the comparison of different thermal comfort indices on data from a thermal comfort study.",
    "author": [
      {
        "name": "Marcel Schweiker",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncomf\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-056/",
    "title": "mixtox: An R Package for Mixture Toxicity Assessment",
    "description": "Mixture toxicity assessment is indeed necessary for humans and ecosystems that are contin ually exposed to a variety of chemical mixtures. This paper describes an R package, called mixtox, which offers a general framework of curve fitting, mixture experimental design, and mixture toxicity prediction for practitioners in toxicology. The unique features of mixtox include: (1) constructing a uniform table for mixture experimental design; and (2) predicting toxicity of a mixture with multiple components based on reference models such as concentration addition, independent action, and generalized concentration addition. We describe the various functions of the package and provide examples to illustrate their use and show the collaboration of mixtox with other existing packages (e.g., drc) in predicting toxicity of chemical mixtures.",
    "author": [
      {
        "name": "Xiang-Wei Zhu",
        "url": {}
      },
      {
        "name": "Jian-Yi Chen",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-060/",
    "title": "ggfortify: Unified Interface to Visualize Statistical Results of Popular R Packages",
    "description": "The ggfortify package provides a unified interface that enables users to use one line of code to visualize statistical results of many R packages using ggplot2 idioms. With the help of ggfortify, statisticians, data scientists, and researchers can avoid the sometimes repetitive work of using the ggplot2 syntax to achieve what they need.",
    "author": [
      {
        "name": "Yuan Tang",
        "url": {}
      },
      {
        "name": "Masaaki Horikoshi",
        "url": {}
      },
      {
        "name": "Wenxuan Li",
        "url": {}
      }
    ],
    "date": "2016-09-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlattice, ggplot2, ggfortify, cluster, lfda, zoo, xts, timeSeries, forecast, changepoint, strucchange, dlm, dplyr, tidyr, gridExtra, scales\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, Econometrics, Environmetrics, Graphics, Multivariate, Bayesian, Cluster, Pharmacokinetics, Phylogenetics, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-031/",
    "title": "QPot: An R Package for Stochastic Differential Equation Quasi-Potential Analysis",
    "description": "QPot (pronounced kyoo + p ät) is an R package for analyzing two-dimensional systems of stochastic differential equations. It provides users with a wide range of tools to simulate, analyze, and visualize the dynamics of these systems. One of QPot’s key features is the computation of the quasi-potential, an important tool for studying stochastic systems. Quasi-potentials are particularly useful for comparing the relative stabilities of equilibria in systems with alternative stable states. This paper describes QPot’s primary functions, and explains how quasi-potentials can yield insights about the dynamics of stochastic systems. Three worked examples guide users through the application of QPot’s functions.",
    "author": [
      {
        "name": "Christopher M. Moore",
        "url": {}
      },
      {
        "name": "Christopher R. Stieha",
        "url": {}
      },
      {
        "name": "Ben C. Nolting",
        "url": {}
      },
      {
        "name": "Maria K. Cameron",
        "url": {}
      },
      {
        "name": "Karen C.            Abbott",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nQPot, rootSolve, deSolve, phaseR, Sim.DiffProc, yuima, viridis, plot3D, rgl\nCRAN Task Views implied by cited packages\nDifferentialEquations, TimeSeries, Finance, Graphics, Multivariate, Pharmacokinetics, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-034/",
    "title": "Simulating Correlated Binary and Multinomial Responses under Marginal Model Specification: The SimCorMultRes Package",
    "description": "We developed the R package SimCorMultRes to facilitate simulation of correlated categori cal (binary and multinomial) responses under a desired marginal model specification. The simulated correlated categorical responses are obtained by applying threshold approaches to correlated contin uous responses of underlying regression models and the dependence structure is parametrized in terms of the correlation matrix of the latent continuous responses. This article provides an elaborate introduction to the SimCorMultRes package demonstrating its design and usage via three examples. The package can be obtained via CRAN.",
    "author": [
      {
        "name": "Anestis Touloumis",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSimCorMultRes, GenOrd, MultiOrd, mvtBinaryEP, multgee\nCRAN Task Views implied by cited packages\nDistributions, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-035/",
    "title": "eiCompare: Comparing Ecological Inference Estimates across EI and EI:RC",
    "description": "Social scientists and statisticians often use aggregate data to predict individual-level behavior because the latter are not always available. Various statistical techniques have been developed to make inferences from one level (e.g., precinct) to another level (e.g., individual voter) that minimize errors associated with ecological inference. While ecological inference has been shown to be highly problematic in a wide array of scientific fields, many political scientists and analysis employ the techniques when studying voting patterns. Indeed, federal voting rights lawsuits now require such an analysis, yet expert reports are not consistent in which type of ecological inference is used. This is especially the case in the analysis of racially polarized voting when there are multiple candidates and multiple racial groups. The eiCompare package was developed to easily assess two of the more common ecological inference methods: EI and EI:R×C. The package facilitates a seamless comparison between these methods so that scholars and legal practitioners can easily assess the two methods and whether they produce similar or disparate findings.",
    "author": [
      {
        "name": "Loren Collingwood",
        "url": {}
      },
      {
        "name": "Kassra Oskooii",
        "url": {}
      },
      {
        "name": "Sergio Garcia-Rios",
        "url": {}
      },
      {
        "name": "Matt Barreto",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nei, eiPack, eiCompare\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-038/",
    "title": "Two-Tier Latent Class IRT Models in R",
    "description": "In analyzing data deriving from the administration of a questionnaire to a group of individu als, Item Response Theory (IRT) models provide a flexible framework to account for several aspects involved in the response process, such as the existence of multiple latent traits. In this paper, we focus on a class of semi-parametric multidimensional IRT models, in which these traits are represented through one or more discrete latent variables; these models allow us to cluster individuals into homo geneous latent classes and, at the same time, to properly study item characteristics. In particular, we follow a within-item multidimensional formulation similar to that adopted in the two-tier models, with each item measuring one or two latent traits. The proposed class of models may be estimated through the package MLCIRTwithin, whose functioning is illustrated in this paper with examples based on data about quality-of-life measurement and about the propensity to commit a crime.",
    "author": [
      {
        "name": "Silvia Bacci",
        "url": {}
      },
      {
        "name": "Francesco Bartolucci",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMLCIRTwithin, MultiLCIRT, CDM, mirt, flirt, covLCA, lavaan, OpenMx, LMest\nCRAN Task Views implied by cited packages\nPsychometrics, Econometrics, OfficialStatistics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-040/",
    "title": "hdm: High-Dimensional Metrics",
    "description": "In this article the package High-dimensional Metrics hdm is introduced. It is a collection of statistical methods for estimation and quantification of uncertainty in high-dimensional approximately sparse models. It focuses on providing confidence intervals and significance testing for (possibly many) low-dimensional subcomponents of the high-dimensional parameter vector. Efficient estimators and uniformly valid confidence intervals for regression coefficients on target variables (e.g., treatment or policy variable) in a high-dimensional approximately sparse regression model, for average treatment effect (ATE) and average treatment effect for the treated (ATET), as well for extensions of these param eters to the endogenous setting are provided. Theory grounded, data-driven methods for selecting the penalization parameter in Lasso regressions under heteroscedastic and non-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence intervals for regression coefficients of a high-dimensional sparse regression are implemented. Data sets which have been used in the literature and might be useful for classroom demonstration and for testing new estimators are included.",
    "author": [
      {
        "name": "Victor Chernozhukov",
        "url": {}
      },
      {
        "name": "Chris Hansen",
        "url": {}
      },
      {
        "name": "Martin Spindler",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nglmnet, lars, hdm\nCRAN Task Views implied by cited packages\nMachineLearning, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-048/",
    "title": "Subgroup Discovery with Evolutionary Fuzzy Systems in R: The SDEFSR Package",
    "description": "Subgroup discovery is a data mining task halfway between descriptive and predictive data mining. Nowadays it is very relevant for researchers due to the fact that the knowledge extracted is simple and interesting. For this task, evolutionary fuzzy systems are well suited algorithms because they can find a good trade-off between multiple objectives in large search spaces. In fact, this paper presents the SDEFSR package, which contains all the evolutionary fuzzy systems for subgroup discovery presented throughout the literature. It is a package without dependencies on other software, providing functions with recommended default parameters. In addition, it brings a graphical user interface to avoid the user having to know all the parameters of the algorithms.",
    "author": [
      {
        "name": "Ángel M. García",
        "url": {}
      },
      {
        "name": "Francisco Charte",
        "url": {}
      },
      {
        "name": "Pedro González",
        "url": {}
      },
      {
        "name": "Cristóbal J. Carmona",
        "url": {}
      },
      {
        "name": "María J. del Jesus",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrsubgroup, SDEFSR, devtools, mldr\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-058/",
    "title": "Distance Measures for Time Series in R: The TSdist Package",
    "description": "The definition of a distance measure between time series is crucial for many time series data mining tasks, such as clustering and classification. For this reason, a vast portfolio of time series distance measures has been published in the past few years. In this paper, the TSdist package is presented, a complete tool which provides a unified framework to calculate the largest variety of time series dissimilarity measures available in R at the moment, to the best of our knowledge. The package implements some popular distance measures which were not previously available in R, and moreover, it also provides wrappers for measures already included in other R packages. Additionally, the application of these distance measures to clustering and classification tasks is also supported in TSdist, directly enabling the evaluation and comparison of their performance within these two frameworks.",
    "author": [
      {
        "name": "Usue Mori",
        "url": {}
      },
      {
        "name": "Alexander Mendiburu",
        "url": {}
      },
      {
        "name": "Jose A. Lozano",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nTSdist, dtw, pdc, proxy, longitudinalData, TSclust, zoo, xts\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance, Environmetrics, Multivariate, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-030/",
    "title": "multipleNCC: Inverse Probability Weighting of Nested Case-Control Data",
    "description": "Reuse of controls from nested case-control designs can increase efficiency in many situations, for instance with competing risks or in other multiple endpoints situations. The matching between cases and controls must be broken when controls are to be used for other endpoints. A weighted analysis can then be performed to take care of the biased sampling from the cohort. We present the R package multipleNCC for reuse of controls in nested case-control studies by inverse probability weighting of the partial likelihood. The package handles right-censored, left-truncated and additionally matched data, and varying numbers of sampled controls and the whole analysis is carried out using one simple command. Four weight estimators are presented and variance estimation is explained. The package is illustrated by analyzing health survey data from three counties in Norway for two causes of death: cardiovascular disease and death from alcohol abuse, liver disease, and accidents and violence. The data set is included in the package.",
    "author": [
      {
        "name": "Nathalie C. Støer",
        "url": {}
      },
      {
        "name": "Sven Ove Samuelsen",
        "url": {}
      }
    ],
    "date": "2016-08-11",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmultipleNCC, survival, mgcv, ipw, MatchIt, NestedCohort, survey, Epi, gam\nCRAN Task Views implied by cited packages\nSocialSciences, Survival, Econometrics, Environmetrics, OfficialStatistics, Bayesian, ClinicalTrials\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-043/",
    "title": "tigris: An R Package to Access and Work with Geographic Data from the US Census Bureau",
    "description": "TIGER/Line shapefiles from the United States Census Bureau are commonly used for the mapping and analysis of US demographic trends. The tigris package provides a uniform interface for R users to download and work with these shapefiles. Functions in tigris allow R users to request Census geographic datasets using familiar geographic identifiers and return those datasets as objects of class \"Spatial*DataFrame\". In turn, tigris ensures consistent and high-quality spatial data for R users’ cartographic and spatial analysis projects that involve US Census data. This article provides an overview of the functionality of the tigris package, and concludes with an applied example of a geospatial workflow using data retrieved with tigris.",
    "author": [
      {
        "name": "Kyle Walker",
        "url": {}
      }
    ],
    "date": "2016-08-11",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntigris, rgdal, sp, UScensus2010, USABoundaries, choroplethr, ggplot2, sp, rappdirs, dplyr, tmap, shiny, leaflet, devtools\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Graphics, OfficialStatistics, Phylogenetics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-005/",
    "title": "Maps, Coordinate Reference Systems and Visualising Geographic Data with mapmisc",
    "description": "The mapmisc package provides functions for visualising geospatial data, including fetching background map layers, producing colour scales and legends, and adding scale bars and orientation arrows to plots. Background maps are returned in the coordinate reference system of the dataset supplied, and inset maps and direction arrows reflect the map projection being plotted. This is a “light weight” package having an emphasis on simplicity and ease of use.",
    "author": [
      {
        "name": "Patrick E. Brown",
        "url": {}
      }
    ],
    "date": "2016-07-28",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsp, raster, mapmisc, rgdal, RColorBrewer, classInt, rgeos, geosphere, dismo, maptools, R.utils, geostatsp, knitr, ggplot2, leaflet\nCRAN Task Views implied by cited packages\nSpatial, Graphics, SpatioTemporal, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-024/",
    "title": "statmod: Probability Calculations for the Inverse Gaussian Distribution",
    "description": "The inverse Gaussian distribution (IGD) is a well known and often used probability dis tribution for which fully reliable numerical algorithms have not been available. We develop fast, reliable basic probability functions (dinvgauss, pinvgauss, qinvgauss and rinvgauss) for the IGD that work for all possible parameter values and which achieve close to full machine accuracy. The most challenging task is to compute quantiles for given cumulative probabilities and we develop a simple but elegant mathematical solution to this problem. We show that Newton’s method for finding the quantiles of a IGD always converges monotonically when started from the mode of the distribution. Simple Taylor series expansions are used to improve accuracy on the log-scale. The IGD probability functions provide the same options and obey the same conventions as do probability functions provided in the stats package.",
    "author": [
      {
        "name": "Göknur Giner",
        "url": {}
      },
      {
        "name": "Gordon K. Smyth",
        "url": {}
      }
    ],
    "date": "2016-07-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSuppDists, STAR, statmod\nCRAN Task Views implied by cited packages\nDistributions, HighPerformanceComputing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-027/",
    "title": "Nonparametric Tests for the Interaction in Two-way Factorial Designs Using R",
    "description": "An increasing number of R packages include nonparametric tests for the interaction in two-way factorial designs. This paper briefly describes the different methods of testing and reports the resulting p-values of such tests on datasets for four types of designs: between, within, mixed, and pretest-posttest designs. Potential users are advised only to apply tests they are quite familiar with and not be guided by p-values for selecting packages and tests.",
    "author": [
      {
        "name": "Jos Feys",
        "url": {}
      }
    ],
    "date": "2016-07-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nWRS2, nparLD, coin, lmPerm, perm, ez, boot, ART, ARTool, npIntFactRep, Rfit, StatMethRank, outliers, npsm, cocor\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, ExperimentalDesign, Optimization, Psychometrics, Robust, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-002/",
    "title": "Gender Prediction Methods Based on First Names with genderizeR",
    "description": "In recent years, there has been increased interest in methods for gender prediction based on first names that employ various open data sources. These methods have applications from bibliometric studies to customizing commercial offers for web users. Analysis of gender disparities in science based on such methods are published in the most prestigious journals, although they could be improved by choosing the most suited prediction method with optimal parameters and performing validation studies using the best data source for a given purpose. There is also a need to monitor and report how well a given prediction method works in comparison to others. In this paper, the author recommends a set of tools (including one dedicated to gender prediction, the R package called genderizeR), data sources (including the genderize.io API), and metrics that could be fully reproduced and tested in order to choose the optimal approach suitable for different gender analyses.",
    "author": [
      {
        "name": "Kamil Wais",
        "url": {}
      }
    ],
    "date": "2016-07-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngenderizeR, qdap, gender, babynames, sortinghat, stringr, tm, ROCR, verification, data.table, dplyr\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NaturalLanguageProcessing, Finance, MachineLearning, Multivariate, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-029/",
    "title": "sbtools: A Package Connecting R to Cloud-based Data for Collaborative Online Research",
    "description": "The adoption of high-quality tools for collaboration and reproducibile research such as R and Github is becoming more common in many research fields. While Github and other version management systems are excellent resources, they were originally designed to handle code and scale poorly to large text-based or binary datasets. A number of scientific data repositories are coming online and are often focused on dataset archival and publication. To handle collaborative workflows using large scientific datasets, there is increasing need to connect cloud-based online data storage to R. In this article, we describe how the new R package sbtools enables direct access to the advanced online data functionality provided by ScienceBase, the U.S. Geological Survey’s online scientific data storage platform.",
    "author": [
      {
        "name": "Luke A Winslow",
        "url": {}
      },
      {
        "name": "Scott Chamberlain",
        "url": {}
      },
      {
        "name": "Alison P Appling",
        "url": {}
      },
      {
        "name": "Jordan S Read",
        "url": {}
      }
    ],
    "date": "2016-07-23",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-001/",
    "title": "metaplus: An R Package for the Analysis of Robust Meta-Analysis and Meta-Regression",
    "description": "The metaplus package is described with examples of its use for fitting meta-analysis and meta-regression. For either meta-analysis or meta-regression it is possible to fit one of three models: standard normal random effect, t-distribution random effect or mixture of normal random effects. The latter two models allow for robustness by allowing for a random effect distribution with heavier tails than the normal distribution, and for both robust models the presence of outliers may be tested using the parametric bootstrap. For the mixture of normal random effects model the outlier studies may be identified through their posterior probability of membership in the outlier component of the mixture. Plots allow the results of the different models to be compared. The package is demonstrated on three examples: a meta-analysis with no outliers, a meta-analysis with an outlier and a meta-regression with an outlier.",
    "author": [
      {
        "name": "Ken J. Beath",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmetaplus, metafor, bbmle, forestplot, extrafont\nCRAN Task Views implied by cited packages\nMetaAnalysis, ClinicalTrials, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-014/",
    "title": "Spatio-Temporal Interpolation using gstat",
    "description": "We present new spatio-temporal geostatistical modelling and interpolation capabilities of the R package gstat. Various spatio-temporal covariance models have been implemented, such as the separable, product-sum, metric and sum-metric models. In a real-world application we compare spatio temporal interpolations using these models with a purely spatial kriging approach. The target variable of the application is the daily mean PM10 concentration measured at rural air quality monitoring stations across Germany in 2005. R code for variogram fitting and interpolation is presented in this paper to illustrate the workflow of spatio-temporal interpolation using gstat. We conclude that the system works properly and that the extension of gstat facilitates and eases spatio-temporal geostatistical modelling and prediction for R users.",
    "author": [
      {
        "name": "Benedikt Gräler",
        "url": {}
      },
      {
        "name": "Edzer Pebesma",
        "url": {}
      },
      {
        "name": "Gerard Heuvelink",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspacetime, gstat, RandomFields, spTimer, spBayes, spate, FNN\nCRAN Task Views implied by cited packages\nSpatioTemporal, Spatial, Bayesian, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-020/",
    "title": "Crowdsourced Data Preprocessing with R and Amazon Mechanical Turk",
    "description": "This article introduces the use of the Amazon Mechanical Turk (MTurk) crowdsourcing platform as a resource for R users to leverage crowdsourced human intelligence for preprocessing “messy” data into a form easily analyzed within R. The article first describes MTurk and the MTurkR package, then outlines how to use MTurkR to gather and manage crowdsourced data with MTurk using some of the package’s core functionality. Potential applications of MTurkR include construction of manually coded training sets, human transcription and translation, manual data scraping from scanned documents, content analysis, image classification, and the completion of online survey questionnaires, among others. As an example of massive data preprocessing, the article describes an image rating task involving 225 crowdsourced workers and more than 5500 images using just three MTurkR function calls.",
    "author": [
      {
        "name": "Thomas J. Leeper",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMTurkR, MTurkRGUI, tcltk, curl, XML\nCRAN Task Views implied by cited packages\nWebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-021/",
    "title": "mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models",
    "description": "Finite mixture models are being used increasingly to model a wide variety of random phenomena for clustering, classification and density estimation. mclust is a powerful and popular package which allows modelling of data as a Gaussian finite mixture with different covariance structures and different numbers of mixture components, for a variety of purposes of analysis. Recently, version 5 of the package has been made available on CRAN. This updated version adds new covariance structures, dimension reduction capabilities for visualisation, model selection criteria, initialisation strategies for the EM algorithm, and bootstrap-based inference, making it a full-featured R package for data analysis via finite mixture modelling.",
    "author": [
      {
        "name": "Luca Scrucca",
        "url": {}
      },
      {
        "name": "Michael Fop",
        "url": {}
      },
      {
        "name": "T. Brendan Murphy",
        "url": {}
      },
      {
        "name": "Adrian E. Raftery",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmclust, cranlogs, Rmixmod, mixture, EMCluster, mixtools, bgmm, flexmix, igraph, gclus, rrcov, tourr, fpc\nCRAN Task Views implied by cited packages\nCluster, Multivariate, Distributions, Environmetrics, Graphics, gR, Optimization, Psychometrics, Robust, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-026/",
    "title": "R Packages to Aid in Handling Web Access Logs",
    "description": "Web access logs contain information on HTTP(S) requests and form a key part of both industry and academic explorations of human behaviour on the internet. But the preparation (reading, parsing and manipulation) of that data is just unique enough to make generalized tools unfit for the task, both in programming time and processing time which are compounded when dealing with large data sets common with web access logs. In this paper we explain and demonstrate a series of packages designed to efficiently read in, parse and munge access log data, allowing researchers to handle URLs and IP addresses easily. These packages are substantially faster than existing R methods from a 3-500% speedup for file reading to a 57,000% speedup in URL parsing.",
    "author": [
      {
        "name": "Oliver Keyes",
        "url": {}
      },
      {
        "name": "Bob Rudis",
        "url": {}
      },
      {
        "name": "Jay Jacobs",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhttr, ApacheLogProcessor, webreadr, readr, microbenchmark, urltools, httr, XML, lubridate, iptools, rgeolocate, Rcpp\nCRAN Task Views implied by cited packages\nWebTechnologies, HighPerformanceComputing, NumericalMathematics, ReproducibleResearch, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-028/",
    "title": "GMDH: An R Package for Short Term Forecasting via GMDH-Type Neural Network Algorithms",
    "description": "Group Method of Data Handling (GMDH)-type neural network algorithms are the heuristic self organization method for the modelling of complex systems. GMDH algorithms are utilized for a variety of purposes, examples include identification of physical laws, the extrapolation of physical fields, pattern recognition, clustering, the approximation of multidimensional processes, forecasting without models, etc. In this study, the R package GMDH is presented to make short term forecasting through GMDH-type neural network algorithms. The GMDH package has options to use different transfer functions (sigmoid, radial basis, polynomial, and tangent functions) simultaneously or separately. Data on cancer death rate of Pennsylvania from 1930 to 2000 are used to illustrate the features of the GMDH package. The results based on ARIMA models and exponential smoothing methods are included for comparison.",
    "author": [
      {
        "name": "Osman Dag",
        "url": {}
      },
      {
        "name": "Ceylan Yozgatligil",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nglarma, ftsa, MARSS, ensembleBMA, ProbForecastGOP, forecast\nCRAN Task Views implied by cited packages\nTimeSeries, Bayesian, Econometrics, Environmetrics, Finance\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-018/",
    "title": "keyplayer: An R Package for Locating Key Players in Social Networks",
    "description": "Interest in social network analysis has exploded in the past few years, partly thanks to the advancements in statistical methods and computing for network analysis. A wide range of the methods for network analysis is already covered by existent R packages. However, no comprehensive packages are available to calculate group centrality scores and to identify key players (i.e., those players who constitute the most central group) in a network. These functionalities are important because, for example, many social and health interventions rely on key players to facilitate the intervention. Identifying key players is challenging because players who are individually the most central are not necessarily the most central as a group due to redundancy in their connections. In this paper we develop methods and tools for computing group centrality scores and for identifying key players in social networks. We illustrate the methods using both simulated and empirical examples. The package keyplayer providing the presented methods is available from Comprehensive R Archive Network (CRAN).",
    "author": [
      {
        "name": "Weihua An",
        "url": {}
      },
      {
        "name": "Yu-Hsin Liu",
        "url": {}
      }
    ],
    "date": "2016-05-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nnetwork, sna, igraph, statnet, RSiena, keyplayer, influenceR\nCRAN Task Views implied by cited packages\nSocialSciences, gR, Optimization, Bayesian, Graphics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-022/",
    "title": "clustering.sc.dp: Optimal Clustering with Sequential Constraint by Using Dynamic Programming",
    "description": "The general clustering algorithms do not guarantee optimality because of the hardness of the problem. Polynomial-time methods can find the clustering corresponding to the exact optimum only in special cases. For example, the dynamic programming algorithm can solve the one-dimensional clustering problem, i.e., when the items to be clustered can be characterised by only one scalar number. Optimal one-dimensional clustering is provided by package Ckmeans.1d.dp in R. The paper shows a possible generalisation of the method implemented in this package to multidimensional data: the dynamic programming method can be applied to find the optimum clustering of vectors when only subsequent items may form a cluster. Sequential data are common in various fields including telecommunication, bioinformatics, marketing, transportation etc. The proposed algorithm can determine the optima for a range of cluster numbers in order to support the case when the number of clusters is not known in advance.",
    "author": [
      {
        "name": "Tibor Szkaliczki",
        "url": {}
      }
    ],
    "date": "2016-05-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCkmeans.1d.dp, clustering.sc.dp\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-023/",
    "title": "progenyClust: an R package for Progeny Clustering",
    "description": "Identifying the optimal number of clusters is a common problem faced by data scientists in various research fields and industry applications. Though many clustering evaluation techniques have been developed to solve this problem, the recently developed algorithm Progeny Clustering is a much faster alternative and one that is relevant to biomedical applications. In this paper, we introduce an R package progenyClust that implements and extends the original Progeny Clustering algorithm for evaluating clustering stability and identifying the optimal cluster number. We illustrate its applicability using two examples: a simulated test dataset for proof-of-concept, and a cell imaging dataset for demonstrating its application potential in biomedical research. The progenyClust package is versatile in that it offers great flexibility for picking methods and tuning parameters. In addition, the default parameter setting as well as the plot and summary methods offered in the package make the application of Progeny Clustering straightforward and coherent.",
    "author": [
      {
        "name": "Chenyue W. Hu",
        "url": {}
      },
      {
        "name": "Amina A. Qutub",
        "url": {}
      }
    ],
    "date": "2016-05-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncclust, clusterSim, cluster, Nbclust, fpc, progenyClust, stat, Hmisc\nCRAN Task Views implied by cited packages\nCluster, Multivariate, Bayesian, ClinicalTrials, Econometrics, Environmetrics, OfficialStatistics, ReproducibleResearch, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-025/",
    "title": "Using DECIPHER v2.0 to Analyze Big Biological Sequence Data in R",
    "description": "In recent years, the cost of DNA sequencing has decreased at a rate that has outpaced improvements in memory capacity. It is now common to collect or have access to many gigabytes of biological sequences. This has created an urgent need for approaches that analyze sequences in subsets without requiring all of the sequences to be loaded into memory at one time. It has also opened opportunities to improve the organization and accessibility of information acquired in sequencing projects. The DECIPHER package offers solutions to these problems by assisting in the curation of large sets of biological sequences stored in compressed format inside a database. This approach has many practical advantages over standard bioinformatics workflows, and enables large analyses that would otherwise be prohibitively time consuming.",
    "author": [
      {
        "name": "Erik S. Wright",
        "url": {}
      }
    ],
    "date": "2016-05-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRSQLite\nCRAN Task Views implied by cited packages\nDatabases\nBioconductor packages used\nBiostrings, DECIPHER\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-009/",
    "title": "FWDselect: An R Package for Variable Selection in Regression Models",
    "description": "In multiple regression models, when there are a large number (p) of explanatory variables which may or may not be relevant for predicting the response, it is useful to be able to reduce the model. To this end, it is necessary to determine the best subset of q (q ≤ p) predictors which will establish the model with the best prediction capacity. FWDselect package introduces a new forward stepwise based selection procedure to select the best model in different regression frameworks (parametric or nonparametric). The developed methodology, which can be equally applied to linear models, generalized linear models or generalized additive models, aims to introduce solutions to the following two topics: i) selection of the best combination of q variables by using a step-by-step method; and, perhaps, most importantly, ii) search for the number of covariates to be included in the model based on bootstrap resampling techniques. The software is illustrated using real and simulated data.",
    "author": [
      {
        "name": "Marta Sestelo",
        "url": {}
      },
      {
        "name": "Nora M. Villanueva",
        "url": {}
      },
      {
        "name": "Luis Meira-Machado",
        "url": {}
      },
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      }
    ],
    "date": "2016-04-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmeifly, leaps, subselect, leaps, subselect, lars, glmnet, glmulti, bestglm, mgcv, FWDselect\nCRAN Task Views implied by cited packages\nChemPhys, SocialSciences, MachineLearning, Bayesian, Econometrics, Environmetrics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-003/",
    "title": "Conditional Fractional Gaussian Fields with the Package FieldSim",
    "description": "We propose an effective and fast method to simulate multidimensional conditional fractional Gaussian fields with the package FieldSim. Our method is valid not only for conditional simulations associated to fractional Brownian fields, but to any Gaussian field and on any (non regular) grid of points.",
    "author": [
      {
        "name": "Alexandre Brouste",
        "url": {}
      },
      {
        "name": "Jacques Istas",
        "url": {}
      },
      {
        "name": "Sophie Lambert-Lacroix",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nFieldSim, RandomFields\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-006/",
    "title": "Variable Clustering in High-Dimensional Linear Regression: The R Package clere",
    "description": "Dimension reduction is one of the biggest challenges in high-dimensional regression models. We recently introduced a new methodology based on variable clustering as a means to reduce dimen sionality. We present here the R package clere that implements some refinements of this methodology. An overview of the package functionalities as well as examples to run an analysis are described. Numerical experiments on real data were performed to illustrate the good predictive performance of our parsimonious method compared to standard dimension reduction approaches.",
    "author": [
      {
        "name": "Loïc Yengo",
        "url": {}
      },
      {
        "name": "Julien Jacques",
        "url": {}
      },
      {
        "name": "Christophe Biernacki",
        "url": {}
      },
      {
        "name": "Mickael Canouil",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nglmnet, spikeslab, clere, Rcpp, RcppEigen, lasso2, flare\nCRAN Task Views implied by cited packages\nMachineLearning, NumericalMathematics, Bayesian, HighPerformanceComputing, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-011/",
    "title": "Exploring Interaction Effects in Two-Factor Studies using the hiddenf Package in R.",
    "description": "In crossed, two-factor studies with one observation per factor-level combination, interaction effects between factors can be hard to detect and can make the choice of a suitable statistical model difficult. This article describes hiddenf, an R package that enables users to quantify and characterize a certain form of interaction in two-factor layouts. When effects of one factor (a) fall into two groups depending on the level of another factor, and (b) are constant within these groups, the interaction pattern is deemed \"hidden additivity\" because within groups, the effects of the two factors are additive, while between groups the factors are allowed to interact. The hiddenf software can be used to estimate, test, and report an appropriate factorial effects model corresponding to hidden additivity, which is intermediate between the unavailable full factorial model and the overly-simplistic additive model. Further, the software also conducts five statistical tests for interaction proposed between 1949 and 2014. A collection of 17 datasets is used for illustration.",
    "author": [
      {
        "name": "Christopher T. Franck",
        "url": {}
      },
      {
        "name": "Jason A. Osborne",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhiddenf, additivityTests\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-013/",
    "title": "Model Builder for Item Factor Analysis with OpenMx",
    "description": "We introduce a shiny web application to facilitate the construction of Item Factor Analysis (a.k.a. Item Response Theory) models using the OpenMx package. The web application assists with importing data, outcome recoding, and model specification. However, the app does not conduct any analysis but, rather, generates an analysis script. Generated Rmarkdown output serves dual purposes: to analyze a data set and demonstrate good programming practices. The app can be used as a teaching tool or as a starting point for custom analysis scripts.",
    "author": [
      {
        "name": "Joshua N. Pritikin",
        "url": {}
      },
      {
        "name": "Karen M. Schmidt",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nOpenMx, shiny, Rmarkdown, ifaTools, rpf\nCRAN Task Views implied by cited packages\nPsychometrics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-015/",
    "title": "SWMPr: An R Package for Retrieving, Organizing, and Analyzing Environmental Data for Estuaries",
    "description": "The System-Wide Monitoring Program (SWMP) was implemented in 1995 by the US National Estuarine Research Reserve System. This program has provided two decades of continuous monitoring data at over 140 fixed stations in 28 estuaries. However, the increasing quantity of data provided by the monitoring network has complicated broad-scale comparisons between systems and, in some cases, prevented simple trend analysis of water quality parameters at individual sites. This article describes the SWMPr package that provides several functions that facilitate data retrieval, organization, and analysis of time series data in the reserve estuaries. Previously unavailable functions for estuaries are also provided to estimate rates of ecosystem metabolism using the open-water method. The SWMPr package has facilitated a cross-reserve comparison of water quality trends and links quantitative information with analysis tools that have use for more generic applications to environmental time series.",
    "author": [
      {
        "name": "Marcus W Beck",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSWMPr, shiny, cents, wq, ggmap, StreamMetabolism\nCRAN Task Views implied by cited packages\nWebTechnologies, Environmetrics, Spatial, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-016/",
    "title": "CryptRndTest: An R Package for Testing the Cryptographic Randomness",
    "description": "In this article, we introduce the R package CryptRndTest that performs eight statistical randomness tests on cryptographic random number sequences. The purpose of the package is to provide software implementing recently proposed cryptographic randomness tests utilizing goodness of-fit tests superior to the usual chi-square test in terms of statistical performance. Most of the tests included in package CryptRndTest are not available in other software packages such as the R package RDieHarder or the C library TestU01. Chi-square, Anderson-Darling, Kolmogorov-Smirnov, and Jarque-Bera goodness-of-fit procedures are provided along with cryptographic randomness tests. CryptRndTest utilizes multiple precision floating numbers for sequences longer than 64-bit based on the package Rmpfr. By this way, included tests are applied precisely for higher bit-lengths. In addition CryptRndTest provides a user friendly interface to these cryptographic randomness tests. As an illustrative application, CryptRndTest is used to test available random number generators in R.",
    "author": [
      {
        "name": "Haydar Demirhan",
        "url": {}
      },
      {
        "name": "Nihan Bitirim",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRDieHarder, randtests, DescTools, CryptRndTest, Rmpfr, kSamples, tseries, copula, gmp\nCRAN Task Views implied by cited packages\nDistributions, Finance, NumericalMathematics, Econometrics, Environmetrics, ExtremeValue, Multivariate, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-019/",
    "title": "SchemaOnRead: A Package for Schema-on-Read in R",
    "description": "SchemaOnRead is a CRAN package that provides an extensible mechanism for importing a wide range of file types into R as well as support for the emerging schema-on-read paradigm in R. The schema-on-read tools within the package include a single function call that recursively reads folders with text, comma separated value, raster image, R data, HDF5, NetCDF, spreadsheet, Weka, Epi Info, Pajek network, R network, HTML, SPSS, Systat, and Stata files. It also recursively reads folders (e.g., schemaOnRead(\"folder\")), returning a nested list of the contained elements. The provided tools can be used as-is or easily customized to implement tool chains in R. This paper’s contribution is that it introduces and describes the SchemaOnRead package and compares it to related R packages.",
    "author": [
      {
        "name": "Michael J. North",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSchemaOnRead, rio, readbitmap, foreign, testthat\nCRAN Task Views implied by cited packages\nOfficialStatistics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-004/",
    "title": "rTableICC: An R Package for Random Generation of 22K and RC Contingency Tables",
    "description": "In this paper, we describe the R package rTableICC that provides an interface for random generation of 2×2×K and R×C contingency tables constructed over either intraclass-correlated or uncorrelated individuals. Intraclass correlations arise in studies where sampling units include more than one individual and these individuals are correlated. The package implements random generation of contingency tables over individuals with or without intraclass correlations under various sampling plans. The package include two functions for the generation of K 2×2 tables over product-multinomial sampling schemes and that of 2×2×K tables under Poisson or multinomial sampling plans. It also contains two functions that generate R×C tables under product-multinomial, multinomial or Poisson sampling plans with or without intraclass correlations. The package also includes a function for random number generation from a given probability distribution. In addition to the contingency table format, the package also provides raw data required for further estimation purposes.",
    "author": [
      {
        "name": "Haydar Demirhan",
        "url": {}
      }
    ],
    "date": "2016-04-02",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrTableICC, partitions\nCRAN Task Views implied by cited packages\nNumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-007/",
    "title": "Stylometry with R: A Package for Computational Text Analysis",
    "description": "This software paper describes ‘Stylometry with R’ (stylo), a flexible R package for the high level analysis of writing style in stylometry. Stylometry (computational stylistics) is concerned with the quantitative study of writing style, e.g. authorship verification, an application which has considerable potential in forensic contexts, as well as historical research. In this paper we introduce the possibilities of stylo for computational text analysis, via a number of dummy case studies from English and French literature. We demonstrate how the package is particularly useful in the exploratory statistical analysis of texts, e.g. with respect to authorial writing style. Because stylo provides an attractive graphical user interface for high-level exploratory analyses, it is especially suited for an audience of novices, without programming skills (e.g. from the Digital Humanities). More experienced users can benefit from our implementation of a series of standard pipelines for text processing, as well as a number of similarity metrics.",
    "author": [
      {
        "name": "Maciej Eder",
        "url": {}
      },
      {
        "name": "Jan Rybicki",
        "url": {}
      },
      {
        "name": "Mike Kestemont",
        "url": {}
      }
    ],
    "date": "2015-12-22",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nstylo\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-035/",
    "title": "Generalized Hermite Distribution Modelling with the R Package hermite",
    "description": "The Generalized Hermite distribution (and the Hermite distribution as a particular case) is often used for fitting count data in the presence of overdispersion or multimodality. Despite this, to our knowledge, no standard software packages have implemented specific functions to compute basic probabilities and make simple statistical inference based on these distributions. We present here a set of computational tools that allows the user to face these difficulties by modelling with the Generalized Hermite distribution using the R package hermite. The package can also be used to generate random deviates from a Generalized Hermite distribution and to use basic functions to compute probabilities (density, cumulative density and quantile functions are available), to estimate parameters using the maximum likelihood method and to perform the likelihood ratio test for Poisson assumption against a Generalized Hermite alternative. In order to improve the density and quantile functions performance when the parameters are large, Edgeworth and Cornish-Fisher expansions have been used. Hermite regression is also a useful tool for modeling inflated count data, so its inclusion to a commonly used software like R will make this tool available to a wide range of potential users. Some examples of usage in several fields of application are also given.",
    "author": [
      {
        "name": "David Moriña",
        "url": {}
      },
      {
        "name": "Manuel Higueras",
        "url": {}
      },
      {
        "name": "Pedro Puig",
        "url": {}
      },
      {
        "name": "María Oliveira",
        "url": {}
      }
    ],
    "date": "2015-12-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmaxLik, radir\nCRAN Task Views implied by cited packages\nOptimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-034/",
    "title": "Open-Channel Computation with R",
    "description": "The rivr package provides a computational toolset for simulating steady and unsteady one dimensional flows in open channels. It is designed primarily for use by instructors of undergraduate and graduate-level open-channel hydrodynamics courses in such diverse fields as river engineering, physical geography and geophysics. The governing equations used to describe open-channel flows are briefly presented, followed by example applications. These include the computation of gradually varied flows and two examples of unsteady flows in channels—namely, the tracking of the evolution of a flood wave in a channel and the prediction of extreme variation in the water-surface profile that results when a sluice gate is abruptly closed. Model results for the unsteady flow examples are validated against standard benchmarks. The article concludes with a discussion of potential modifications and extensions to the package.",
    "author": [
      {
        "name": "Michael C. Koohafkan",
        "url": {}
      },
      {
        "name": "Bassam A. Younis",
        "url": {}
      }
    ],
    "date": "2015-11-28",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrivr, knitr, shiny, Rcpp\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics, ReproducibleResearch, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-017/",
    "title": "scmamp: Statistical Comparison of Multiple Algorithms in Multiple Problems",
    "description": "Comparing the results obtained by two or more algorithms in a set of problems is a central task in areas such as machine learning or optimization. Drawing conclusions from these comparisons may require the use of statistical tools such as hypothesis testing. There are some interesting papers that cover this topic. In this manuscript we present scmamp, an R package aimed at being a tool that simplifies the whole process of analyzing the results obtained when comparing algorithms, from loading the data to the production of plots and tables. Comparing the performance of different algorithms is an essential step in many research and practical computational works. When new algorithms are proposed, they have to be compared with the state of the art. Similarly, when an algorithm is used for a particular problem, its performance with different sets of parameters has to be compared, in order to tune them for the best results. When the differences are very clear (e.g., when an algorithm is the best in all the problems used in the comparison), the direct comparison of the results may be enough. However, this is an unusual situation and, thus, in most situations a direct comparison may be misleading and not enough to draw sound conclusions; in those cases, the statistical assessment of the results is advisable. The statistical comparison of algorithms in the context of machine learning has been covered in several papers. In particular, the tools implemented in this package are those presented in Demšar (2006); García and Herrera (2008); García et al. (2010). Another good review that covers, among other aspects, the statistical assessment of the results in the context of supervised classification can be found in Santafé et al. (2015).",
    "author": [
      {
        "name": "Borja Calvo",
        "url": {}
      },
      {
        "name": "Guzmán Santafé",
        "url": {}
      }
    ],
    "date": "2015-11-26",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nscmamp\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-022/",
    "title": "Numerical Evaluation of the Gauss Hypergeometric Function with the hypergeo Package",
    "description": "This paper introduces the hypergeo package of R routines for numerical calculation of hypergeometric functions. The package is focussed on efficient and accurate evaluation of the Gauss hypergeometric function over the whole of the complex plane within the constraints of fixed-precision arithmetic. The hypergeometric series is convergent only within the unit circle, so analytic continuation must be used to define the function outside the unit circle. This short document outlines the numerical and conceptual methods used in the package; and justifies the package philosophy, which is to maintain transparent and verifiable links between the software and Abramowitz and Stegun (1965). Most of the package functionality is accessed via the single function hypergeo(), which dispatches to one of several methods depending on the value of its arguments. The package is demonstrated in the context of game theory.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2015-11-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngsl, appell, hypergeo\nCRAN Task Views implied by cited packages\nNumericalMathematics, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-036/",
    "title": "Code Profiling in R: A Review of Existing Methods and an Introduction to Package GUIProfiler",
    "description": "Code analysis tools are crucial to understand program behavior. Profile tools use the results of time measurements in the execution of a program to gain this understanding and thus help in the optimization of the code. In this paper, we review the different available packages to profile R code and show the advantages and disadvantages of each of them. In additon, we present GUIProfiler, a package that fulfills some unmet needs. Package GUIProfiler generates an HTML report with the timing for each code line and the relationships between different functions. This package mimics the behavior of the MATLAB profiler. The HTML report includes information on the time spent on each of the lines of the profiled code (the slowest code is highlighted). If the package is used within the RStudio environment, the user can navigate across the bottlenecks in the code and open the editor to modify the lines of code where more time is spent. It is also possible to edit the code using Notepad++ (a free editor for Windows) by simply clicking on the corresponding line. The graphical user interface makes it easy to identify the specific lines which slow down the code. The integration in RStudio and the generation of an HTML report makes GUIProfiler a very convenient tool to perform code optimization.",
    "author": [
      {
        "name": "Angel Rubio",
        "url": {}
      },
      {
        "name": "Fernando de Villar",
        "url": {}
      }
    ],
    "date": "2015-11-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\naprof, proftools, profr, microbenchmark, Nozzle.R1, knitr, GUIProfiler, stringr, plyr, devtools, shiny\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, ReproducibleResearch, WebTechnologies\nBioconductor packages used\nRgraphviz, graph\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-024/",
    "title": "An R Package for the Panel Approach Method for Program Evaluation: pampe",
    "description": "The pampe package for R implements the panel data approach method for program evalua tion designed to estimate the causal effects of political interventions or treatments. This procedure exploits the dependence among cross-sectional units to construct a counterfactual of the treated unit(s), and it is an appropriate method for research events that occur at an aggregate level like countries or regions and that affect only one or a small number of units. The implementation of the pampe package is illustrated using data from Hong Kong and 24 other units, by examining the economic impact of the political and economic integration of Hong Kong with mainland China in 1997 and 2004 respectively.",
    "author": [
      {
        "name": "Ainhoa Vega-Bayo",
        "url": {}
      }
    ],
    "date": "2015-11-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npampe, leaps, xtable\nCRAN Task Views implied by cited packages\nChemPhys, Econometrics, ReproducibleResearch, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-018/",
    "title": "VSURF: An R Package for Variable Selection Using Random Forests",
    "description": "This paper describes the R package VSURF. Based on random forests, and for both regression and classification problems, it returns two subsets of variables. The first is a subset of important variables including some redundancy which can be relevant for interpretation, and the second one is a smaller subset corresponding to a model trying to avoid redundancy focusing more closely on the prediction objective. The two-stage strategy is based on a preliminary ranking of the explanatory variables using the random forests permutation-based score of importance and proceeds using a stepwise forward strategy for variable introduction. The two proposals can be obtained automatically using data-driven default values, good enough to provide interesting results, but strategy can also be tuned by the user. The algorithm is illustrated on a simulated example and its applications to real datasets are presented.",
    "author": [
      {
        "name": "Robin Genuer",
        "url": {}
      },
      {
        "name": "Jean-Michel Poggi",
        "url": {}
      },
      {
        "name": "Christine Tuleau-Malot",
        "url": {}
      }
    ],
    "date": "2015-11-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nVSURF, rpart, randomForest, party, ipred, Boruta, varSelRF, spikeSlabGAM, BioMark, mlbench, mixOmics\nCRAN Task Views implied by cited packages\nMachineLearning, Environmetrics, Survival, ChemPhys, Multivariate, Bayesian, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-008/",
    "title": "quickpsy: An R Package to Fit Psychometric Functions for Multiple Groups",
    "description": "quickpsy is a package to parametrically fit psychometric functions. In comparison with previous R packages, quickpsy was built to easily fit and plot data for multiple groups. Here, we describe the standard parametric model used to fit psychometric functions and the standard estimation of its parameters using maximum likelihood. We also provide examples of usage of quickpsy, including how allowing the lapse rate to vary can sometimes eliminate the bias in parameter estimation, but not in general. Finally, we describe some implementation details, such as how to avoid the problems associated to round-off errors in the maximisation of the likelihood or the use of closures and non-standard evaluation functions.",
    "author": [
      {
        "name": "Daniel Linares",
        "url": {}
      },
      {
        "name": "Joan López-Moliner",
        "url": {}
      }
    ],
    "date": "2015-11-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nquickpsy, psyphy, modelfree, MPDiR, gridExtra, dplyr, ggplot2\nCRAN Task Views implied by cited packages\nPsychometrics, Graphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-010/",
    "title": "An Interactive Survey Application for Validating Social Network Analysis Techniques",
    "description": "Social network analysis is extremely well supported by the R community and is routinely used for studying the relationships between people engaged in collaborative activities. While there has been rapid development of new approaches and metrics in this field, the challenging question of validity (how well insights derived from social networks agree with reality) is often difficult to address. We propose the use of several R packages to generate interactive surveys that are specifically well suited for validating social network analyses. Using our web-based survey application, we were able to validate the results of applying community-detection algorithms to infer the organizational structure of software developers contributing to open-source projects.",
    "author": [
      {
        "name": "Mitchell Joblin",
        "url": {}
      },
      {
        "name": "Wolfgang Mauerer",
        "url": {}
      }
    ],
    "date": "2015-11-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nigraph, sna, twitteR, Rfacebook, shiny\nCRAN Task Views implied by cited packages\nWebTechnologies, Optimization, Bayesian, gR, Graphics, SocialSciences, Spatial\nBioconductor packages used\ngraph\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-021/",
    "title": "QuantifQuantile: An R Package for Performing Quantile Regression Through Optimal Quantization",
    "description": "In quantile regression, various quantiles of a response variable Y are modelled as func tions of covariates (rather than its mean). An important application is the construction of reference curves/surfaces and conditional prediction intervals for Y. Recently, a nonparametric quantile regres sion method based on the concept of optimal quantization was proposed. This method competes very well with k-nearest neighbor, kernel, and spline methods. In this paper, we describe an R package, called QuantifQuantile, that allows to perform quantization-based quantile regression. We describe the various functions of the package and provide examples.",
    "author": [
      {
        "name": "Isabelle Charlier",
        "url": {}
      },
      {
        "name": "Davy Paindaveine",
        "url": {}
      },
      {
        "name": "Jérôme Saracco",
        "url": {}
      }
    ],
    "date": "2015-10-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nquantreg, quantregGrowth, QuantifQuantile, rgl, quantregGrowth\nCRAN Task Views implied by cited packages\nEnvironmetrics, Econometrics, Graphics, Multivariate, Optimization, ReproducibleResearch, Robust, SocialSciences, SpatioTemporal, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-026/",
    "title": "ClustVarLV: An R Package for the Clustering of Variables Around Latent Variables",
    "description": "The clustering of variables is a strategy for deciphering the underlying structure of a data set. Adopting an exploratory data analysis point of view, the Clustering of Variables around Latent Variables (CLV) approach has been proposed by Vigneau and Qannari (2003). Based on a family of optimization criteria, the CLV approach is adaptable to many situations. In particular, constraints may be introduced in order to take account of additional information about the observations and/or the variables. In this paper, the CLV method is depicted and the R package ClustVarLV including a set of functions developed so far within this framework is introduced. Considering successively different types of situations, the underlying CLV criteria are detailed and the various functions of the package are illustrated using real case studies.",
    "author": [
      {
        "name": "Evelyne Vigneau",
        "url": {}
      },
      {
        "name": "Mingkun Chen",
        "url": {}
      },
      {
        "name": "El Mostafa Qannari",
        "url": {}
      }
    ],
    "date": "2015-10-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncluster, ClustVarLV, ClustOfVar, clere, biclust, pvclust, Hmisc, FactoMineR, plsgenomics, Rcpp, ClustVarLV\nCRAN Task Views implied by cited packages\nMultivariate, Cluster, Psychometrics, Environmetrics, HighPerformanceComputing, Bayesian, ClinicalTrials, Econometrics, Graphics, NumericalMathematics, OfficialStatistics, ReproducibleResearch, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-012/",
    "title": "Heteroscedastic Censored and Truncated Regression with crch",
    "description": "The crch package provides functions for maximum likelihood estimation of censored or truncated regression models with conditional heteroscedasticity along with suitable standard methods to summarize the fitted models and compute predictions, residuals, etc. The supported distributions include leftor right-censored or truncated Gaussian, logistic, or student-t distributions with potentially different sets of regressors for modeling the conditional location and scale. The models and their R implementation are introduced and illustrated by numerical weather prediction tasks using precipitation data for Innsbruck (Austria).",
    "author": [
      {
        "name": "Jakob W. Messner",
        "url": {}
      },
      {
        "name": "Georg J. Mayr",
        "url": {}
      },
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2015-10-14",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndglm, glmx, gamlss, betareg, crch, Formula, gamlss.cens, gamlss.tr, sampleSelection, mhurdle\nCRAN Task Views implied by cited packages\nEconometrics, Psychometrics, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-031/",
    "title": "mtk: A General-Purpose and Extensible R Environment for Uncertainty and Sensitivity Analyses of Numerical Experiments",
    "description": "Along with increased complexity of the models used for scientific activities and engineering come diverse and greater uncertainties. Today, effectively quantifying the uncertainties contained in a model appears to be more important than ever. Scientific fellows know how serious it is to calibrate their model in a robust way, and decision-makers describe how critical it is to keep the best effort to reduce the uncertainties about the model. Effectively accessing the uncertainties about the model requires mastering all the tasks involved in the numerical experiments, from optimizing the experimental design to managing the very time consuming aspect of model simulation and choosing the adequate indicators and analysis methods. In this paper, we present an open framework for organizing the complexity associated with numerical model simulation and analyses. Named mtk (Mexico Toolkit), the developed system aims at providing practitioners from different disciplines with a systematic and easy way to compare and to find the best method to effectively uncover and quantify the uncertainties contained in the model and further to evaluate their impact on the performance of the model. Such requirements imply that the system must be generic, universal, homogeneous, and extensible. This paper discusses such an implementation using the R scientific computing platform and demonstrates its functionalities with examples from agricultural modeling. The package mtk is of general purpose and easy to extend. Numerous methods are already available in the actual release version, including Fast, Sobol, Morris, Basic Monte-Carlo, Regression, LHS (Latin Hypercube Sampling), PLMM (Polynomial Linear metamodel). Most of them are compiled from available R packages with extension tools delivered by package mtk.",
    "author": [
      {
        "name": "Juhui Wang",
        "url": {}
      },
      {
        "name": "Robert Faivre",
        "url": {}
      },
      {
        "name": "Hervé Richard",
        "url": {}
      },
      {
        "name": "Hervé Monod",
        "url": {}
      }
    ],
    "date": "2015-10-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsensitivity, spartan, diceDesign, planor, mtk, ff\nCRAN Task Views implied by cited packages\nEnvironmetrics, ExperimentalDesign, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-029/",
    "title": "ALTopt: An R Package for Optimal Experimental Design of Accelerated Life Testing",
    "description": "The R package ALTopt has been developed with the aim of creating and evaluating optimal experimental designs of censored accelerated life tests (ALTs). This package takes the generalized linear model approach to ALT planning, because this approach can easily handle censoring plans and derive information matrices for evaluating designs. Three types of optimality criteria are considered: D-optimality for model parameter estimation, U-optimality for reliability prediction at a single use condition, and I-optimality for reliability prediction over a region of use conditions. The Weibull distribution is assumed for failure time data and more than one stress factor can be specified in the package. Several graphical evaluation tools are also provided for the comparison of different ALT test plans.",
    "author": [
      {
        "name": "Kangwon Seo",
        "url": {}
      },
      {
        "name": "Rong Pan",
        "url": {}
      }
    ],
    "date": "2015-09-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nALTopt\nCRAN Task Views implied by cited packages\nExperimentalDesign\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-033/",
    "title": "mmpp: A Package for Calculating Similarity and Distance Metrics for Simple and Marked Temporal Point Processes",
    "description": "A simple temporal point process (SPP) is an important class of time series, where the sample realization of the process is solely composed of the times at which events occur. Particular examples of point process data are neuronal spike patterns or spike trains, and a large number of distance and similarity metrics for those data have been proposed. A marked point process (MPP) is an extension of a simple temporal point process, in which a certain vector valued mark is associated with each of the temporal points in the SPP. Analyses of MPPs are of practical importance because instances of MPPs include recordings of natural disasters such as earthquakes and tornadoes. In this paper, we introduce the R package mmpp, which implements a number of distance and similarity metrics for SPPs, and also extends those metrics for dealing with MPPs.",
    "author": [
      {
        "name": "Hideitsu Hino",
        "url": {}
      },
      {
        "name": "Ken Takano",
        "url": {}
      },
      {
        "name": "Noboru Murata",
        "url": {}
      }
    ],
    "date": "2015-09-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsplancs, spatstat, PtProcess, stpp, mmpp, SAPP, etasFLP\nCRAN Task Views implied by cited packages\nSpatioTemporal, Spatial, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-027/",
    "title": "Working with Multilabel Datasets in R: The mldr Package",
    "description": "Most classification algorithms deal with datasets which have a set of input features, the variables to be used as predictors, and only one output class, the variable to be predicted. However, in late years many scenarios in which the classifier has to work with several outputs have come to life. Automatic labeling of text documents, image annotation or protein classification are among them. Multilabel datasets are the product of these new needs, and they have many specific traits. The mldr package allows the user to load datasets of this kind, obtain their characteristics, produce specialized plots, and manipulate them. The goal is to provide the exploratory tools needed to analyze multilabel datasets, as well as the transformation and manipulation functions that will make possible to apply binary and multiclass classification models to this data or the development of new multilabel classifiers. Thanks to its integrated user interface, the exploratory functions will be available even to non-specialized R users.",
    "author": [
      {
        "name": "Francisco Charte",
        "url": {}
      },
      {
        "name": "David Charte",
        "url": {}
      }
    ],
    "date": "2015-09-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRWeka, mldr, shiny, Rcmdr, rattle, XML, circlize, devtools, pROC, shiny\nCRAN Task Views implied by cited packages\nWebTechnologies, MachineLearning, Finance, NaturalLanguageProcessing\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-032/",
    "title": "treeClust: An R Package for Tree-Based Clustering Dissimilarities",
    "description": "This paper describes treeClust, an R package that produces dissimilarities useful for cluster ing. These dissimilarities arise from a set of classification or regression trees, one with each variable in the data acting in turn as a the response, and all others as predictors. This use of trees produces dissim ilarities that are insensitive to scaling, benefit from automatic variable selection, and appear to perform well. The software allows a number of options to be set, affecting the set of objects returned in the call; the user can also specify a clustering algorithm and, optionally, return only the clustering vector. The package can also generate a numeric data set whose inter-point distances relate to the treeClust ones; such a numeric data set can be much smaller than the vector of inter-point dissimilarities, a useful feature in big data sets.",
    "author": [
      {
        "name": "Samuel E. Buttrey",
        "url": {}
      },
      {
        "name": "Lyn R. Whitaker",
        "url": {}
      }
    ],
    "date": "2015-09-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntreeClust, cluster, rpart, tree\nCRAN Task Views implied by cited packages\nCluster, Environmetrics, MachineLearning, Multivariate, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-017/",
    "title": "Fitting Conditional and Simultaneous Autoregressive Spatial Models in hglm",
    "description": "We present a new version (> 2.0) of the hglm package for fitting hierarchical generalized linear models (HGLMs) with spatially correlated random effects. CAR() and SAR() families for con ditional and simultaneous autoregressive random effects were implemented. Eigen decomposition of the matrix describing the spatial structure (e.g., the neighborhood matrix) was used to transform the CAR/SAR random effects into an independent, but heteroscedastic, Gaussian random effect. A linear predictor is fitted for the random effect variance to estimate the parameters in the CAR and SAR models. This gives a computationally efficient algorithm for moderately sized problems.",
    "author": [
      {
        "name": "Moudud Alam",
        "url": {}
      },
      {
        "name": "Lars Rönnegård",
        "url": {}
      },
      {
        "name": "Xia Shen",
        "url": {}
      }
    ],
    "date": "2015-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhglm, spaMM, HGLMMM\nCRAN Task Views implied by cited packages\nSpatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-020/",
    "title": "apc: An R Package for Age-Period-Cohort Analysis",
    "description": "The apc package includes functions for age-period-cohort analysis based on the canonical parametrisation of Kuang et al. (2008a). The package includes functions for organizing the data, descriptive plots, a deviance table, estimation of (sub-models of) the age-period-cohort model, a plot for specification testing, plots of estimated parameters, and sub-sample analysis.",
    "author": [
      {
        "name": "Bent Nielsen",
        "url": {}
      }
    ],
    "date": "2015-08-05",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\napc, Epi\nCRAN Task Views implied by cited packages\nSurvival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-025/",
    "title": "BSGS: Bayesian Sparse Group Selection",
    "description": "An R package BSGS is provided for the integration of Bayesian variable and sparse group selection separately proposed by Chen et al. (2011) and Chen et al. (in press) for variable selection problems, even in the cases of large p and small n. This package is designed for variable selection problems including the identification of the important groups of variables and the active variables within the important groups. This article introduces the functions in the BSGS package that can be used to perform sparse group selection as well as variable selection through simulation studies and real data.",
    "author": [
      {
        "name": "Kuo-Jung Lee",
        "url": {}
      },
      {
        "name": "Ray-Bing Chen",
        "url": {}
      }
    ],
    "date": "2015-08-05",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nBSGS\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-023/",
    "title": "SRCS: Statistical Ranking Color Scheme for Visualizing Parameterized Multiple Pairwise Comparisons with R",
    "description": "The problem of comparing a new solution method against existing ones to find statistically significant differences arises very often in sciences and engineering. When the problem instance being solved is defined by several parameters, assessing a number of methods with respect to many problem configurations simultaneously becomes a hard task. Some visualization technique is required for presenting a large number of statistical significance results in an easily interpretable way. Here we review an existing color-based approach called Statistical Ranking Color Scheme (SRCS) for displaying the results of multiple pairwise statistical comparisons between several methods assessed separately on a number of problem configurations. We introduce an R package implementing SRCS, which performs all the pairwise statistical tests from user data and generates customizable plots. We demonstrate its applicability on two examples from the areas of dynamic optimization and machine learning, in which several algorithms are compared on many problem instances, each defined by a combination of parameters.",
    "author": [
      {
        "name": "Pablo J. Villacorta",
        "url": {}
      },
      {
        "name": "José A. Sáez",
        "url": {}
      }
    ],
    "date": "2015-07-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfactorplot, SRCS, e1071, RWeka\nCRAN Task Views implied by cited packages\nMachineLearning, Cluster, Distributions, Environmetrics, Multivariate, NaturalLanguageProcessing, Psychometrics\nBioconductor packages used\npaircompviz\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-030/",
    "title": "abctools: An R Package for Tuning Approximate Bayesian Computation Analyses",
    "description": "Approximate Bayesian computation (ABC) is a popular family of algorithms which perform approximate parameter inference when numerical evaluation of the likelihood function is not possible but data can be simulated from the model. They return a sample of parameter values which produce simulations close to the observed dataset. A standard approach is to reduce the simulated and observed datasets to vectors of summary statistics and accept when the difference between these is below a specified threshold. ABC can also be adapted to perform model choice. In this article, we present a new software package for R, abctools which provides methods for tuning ABC algorithms. This includes recent dimension reduction algorithms to tune the choice of summary statistics, and coverage methods to tune the choice of threshold. We provide several illustrations of these routines on applications taken from the ABC literature.",
    "author": [
      {
        "name": "Matthew A. Nunes",
        "url": {}
      },
      {
        "name": "Dennis Prangle",
        "url": {}
      }
    ],
    "date": "2015-07-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nabctools, abc, easyABC, MASS\nCRAN Task Views implied by cited packages\nBayesian, Distributions, Econometrics, Environmetrics, Multivariate, NumericalMathematics, Pharmacokinetics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-019/",
    "title": "zoib: An R Package for Bayesian Inference for Beta Regression and Zero/One Inflated Beta Regression",
    "description": "The beta distribution is a versatile function that accommodates a broad range of probability distribution shapes. Beta regression based on the beta distribution can be used to model a response variable y that takes values in open unit interval (0, 1). Zero/one inflated beta (ZOIB) regression models can be applied when y takes values from closed unit interval [0, 1]. The ZOIB model is based a piecewise distribution that accounts for the probability mass at 0 and 1, in addition to the probability density within (0, 1). This paper introduces an R package – zoib that provides Bayesian inferences for a class of ZOIB models. The statistical methodology underlying the zoib package is discussed, the functions covered by the package are outlined, and the usage of the package is illustrated with three examples of different data and model types. The package is comprehensive and versatile in that it can model data with or without inflation at 0 or 1, accommodate clustered and correlated data via latent variables, perform penalized regression as needed, and allow for model comparison via the computation of the DIC criterion.",
    "author": [
      {
        "name": "Fang Liu",
        "url": {}
      },
      {
        "name": "Yunchuan Kong",
        "url": {}
      }
    ],
    "date": "2015-07-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbetareg, Bayesianbetareg, zoib, coda, rjags\nCRAN Task Views implied by cited packages\nBayesian, gR, Cluster, Econometrics, Psychometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-028/",
    "title": "PracTools: Computations for Design of Finite Population Samples",
    "description": "PracTools is an R package with functions that compute sample sizes for various types of finite population sampling designs when totals or means are estimated. One-, two-, and three-stage designs are covered as well as allocations for stratified sampling and probability proportional to size sampling. Sample allocations can be computed that minimize the variance of an estimator subject to a budget constraint or that minimize cost subject to a precision constraint. The package also contains some specialized functions for estimating variance components and design effects. Several finite populations are included that are useful for classroom instruction.",
    "author": [
      {
        "name": "Richard Valliant",
        "url": {}
      },
      {
        "name": "Jill A. Dever",
        "url": {}
      },
      {
        "name": "Frauke Kreuter",
        "url": {}
      }
    ],
    "date": "2015-06-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npps, sampling, samplingbook, simFrame, survey, PracTools, stratification, alabama, Rsolnp, SamplingStrata\nCRAN Task Views implied by cited packages\nOfficialStatistics, Optimization, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-011/",
    "title": "R as an Environment for Reproducible Analysis of DNA Amplification Experiments",
    "description": "There is an ever-increasing number of applications, which use quantitative PCR (qPCR) or digital PCR (dPCR) to elicit fundamentals of biological processes. Moreover, quantitative isother mal amplification (qIA) methods have become more prominent in life sciences and point-of-care diagnostics. Additionally, the analysis of melting data is essential during many experiments. Several software packages have been developed for the analysis of such datasets. In most cases, the software is either distributed as closed source software or as monolithic block with little freedom to perform highly customized analysis procedures. We argue, among others, that R is an excellent foundation for reproducible and transparent data analysis in a highly customizable cross-platform environment. However, for novices it is often challenging to master R or learn capabilities of the vast number of packages available. In the paper, we describe exemplary workflows for the analysis of qPCR, qIA or dPCR experiments including the analysis of melting curve data. Our analysis relies entirely on R packages available from public repositories. Additionally, we provide information related to standardized and reproducible research.",
    "author": [
      {
        "name": "Stefan Rödiger",
        "url": {}
      },
      {
        "name": "Michał Burdukiewicz",
        "url": {}
      },
      {
        "name": "Konstantin Blagodatskikh",
        "url": {}
      },
      {
        "name": "Michael Jahn",
        "url": {}
      },
      {
        "name": "Peter Schierack",
        "url": {}
      }
    ],
    "date": "2015-06-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndpcR, kulife, MCMC.qpcr, qPCR.CT, DivMelt, qpcR, chipPCR, MBmca, RDML, RNetCDF, archivist, settings, shiny, rateratio.test\nCRAN Task Views implied by cited packages\nReproducibleResearch, Spatial, SpatioTemporal, WebTechnologies\nBioconductor packages used\nnondetects, qpcrNorm, HTqPCR, SLqPCR, ddCt, EasyqpcR, unifiedWMWqPCR, ReadqPCR, NormqPCR\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-009/",
    "title": "Implementing Persistent O(1) Stacks and Queues in R",
    "description": "True to their functional roots, most R functions are side-effect-free, and users expect datatypes to be persistent. However, these semantics complicate the creation of efficient and dynamic data structures. Here, we describe the implementation of stack and queue data structures satisfying these conditions in R, available in the CRAN package rstackdeque. Guided by important work in purely functional languages, we look at both partiallyand fully-persistent versions of queues, comparing their performance characteristics. Finally, we illustrate the usefulness of such dynamic structures with examples of generating and solving mazes.",
    "author": [
      {
        "name": "Shawn T. O’Neil",
        "url": {}
      }
    ],
    "date": "2015-06-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrstackdeque, dplyr, microbenchmark, ggplot2, hash, Rcpp\nCRAN Task Views implied by cited packages\nGraphics, HighPerformanceComputing, NumericalMathematics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-007/",
    "title": "sae: An R Package for Small Area Estimation",
    "description": "We describe the R package sae for small area estimation. This package can be used to obtain model-based estimates for small areas based on a variety of models at the area and unit levels, along with basic direct and indirect estimates. Mean squared errors are estimated by analytical approximations in simple models and applying bootstrap procedures in more complex models. We describe the package functions and show how to use them through examples.",
    "author": [
      {
        "name": "Isabel Molina",
        "url": {}
      },
      {
        "name": "Yolanda Marhuenda",
        "url": {}
      }
    ],
    "date": "2015-06-02",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsae, nlme, MASS, survey, sampling, rsae, JoSae, hbsae, mme, saery, sae2\nCRAN Task Views implied by cited packages\nOfficialStatistics, SocialSciences, Econometrics, Environmetrics, Pharmacokinetics, Psychometrics, Bayesian, ChemPhys, Distributions, Finance, Multivariate, NumericalMathematics, Robust, Spatial, SpatioTemporal, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-010/",
    "title": "Correspondence Analysis on Generalised Aggregated Lexical Tables (CA-GALT) in the FactoMineR Package",
    "description": "Correspondence analysis on generalised aggregated lexical tables (CA-GALT) is a method that generalizes classical CA-ALT to the case of several quantitative, categorical and mixed variables. It aims to establish a typology of the external variables and a typology of the events from their mutual relationships. In order to do so, the influence of external variables on the lexical choices is untangled cancelling the associations among them, and to avoid the instability issued from multicollinearity, they are substituted by their principal components. The CaGalt function, implemented in the FactoMineR package, provides numerous numerical and graphical outputs. Confidence ellipses are also provided to validate and improve the representation of words and variables. Although this methodology was developed mainly to give an answer to the problem of analyzing open-ended questions, it can be applied to any kind of frequency/contingency table with external variables.",
    "author": [
      {
        "name": "Belchin Kostov",
        "url": {}
      },
      {
        "name": "Mónica Bécue-Bertaut",
        "url": {}
      },
      {
        "name": "François Husson",
        "url": {}
      }
    ],
    "date": "2015-06-02",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nFactoMineR\nCRAN Task Views implied by cited packages\nMultivariate, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-013/",
    "title": "fslr: Connecting the FSL Software with R",
    "description": "We present the package fslr, a set of R functions that interface with FSL (FMRIB Software Library), a commonly-used open-source software package for processing and analyzing neuroimaging data. The fslr package performs operations on ‘nifti’ image objects in R using command-line functions from FSL, and returns R objects back to the user. fslr allows users to develop image processing and analysis pipelines based on FSL functionality while interfacing with the functionality provided by R. We present an example of the analysis of structural magnetic resonance images, which demonstrates how R users can leverage the functionality of FSL without switching to shell commands. Glossary of acronyms MRI Magnetic Resonance Imaging/Image FSL FMRIB Software Library PD Proton Density FAST FMRIB’s Automated Segmentation Tool FLAIR Fluid-Attenuated Inversion Recovery FLIRT FMRIB’s Linear Image Registration Tool MS Multiple Sclerosis BET Brain Extraction Tool FMRIB Functional MRI of the Brain Group FNIRT FMRIB’s Nonlinear Image Registration Tool MNI Montreal Neurological Institute",
    "author": [
      {
        "name": "John Muschelli",
        "url": {}
      },
      {
        "name": "Elizabeth Sweeney",
        "url": {}
      },
      {
        "name": "Martin Lindquist",
        "url": {}
      },
      {
        "name": "Ciprian Crainiceanu",
        "url": {}
      }
    ],
    "date": "2015-06-02",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nAnalyzeFMRI, RNiftyReg, fmri, fslr, oro.nifti, ggplot2, ggplot2, mgcv\nCRAN Task Views implied by cited packages\nMedicalImaging, ChemPhys, Graphics, Phylogenetics, Bayesian, Econometrics, Environmetrics, SocialSciences\nBioconductor packages used\nEBImage\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-003/",
    "title": "sparkTable: Generating Graphical Tables for Websites and Documents with R",
    "description": "Visual analysis of data is important to understand the main characteristics, main trends and relationships in data sets and it can be used to assess the data quality. Using the R package sparkTable, statistical tables holding quantitative information can be enhanced by including spark-type graphs such as sparklines and sparkbars . These kind of graphics are well-known in literature and are considered as simple, intense and illustrative graphs that are small enough to fit in a single line. Thus, they can easily enrich tables and texts with additional information in a comprehensive visual way. The R package sparkTable uses a clean S4 class design and provides methods to create different types of sparkgraphs that can be used in websites, presentations and documents. We also implemented an easy way for non-experts to create highly complex tables. In this case, graphical parameters can be interactively changed, variables can be sorted, graphs can be added and removed in an interactive manner. Thereby it is possible to produce custom-tailored graphical tables – standard tables that are enriched with graphs – that can be displayed in a browser and exported to various formats.",
    "author": [
      {
        "name": "Alexander Kowarik",
        "url": {}
      },
      {
        "name": "Bernhard Meindl",
        "url": {}
      },
      {
        "name": "Matthias Templ",
        "url": {}
      }
    ],
    "date": "2015-05-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsparkTable, knitr, brew, xtable, shiny\nCRAN Task Views implied by cited packages\nReproducibleResearch, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-008/",
    "title": "showtext: Using System Fonts in R Graphics",
    "description": "This article introduces the showtext package that makes it easy to use system fonts in R graphics. Unlike other methods to embed fonts into graphics, showtext converts text into raster images or polygons, and then adds them to the plot canvas. This method produces platform-independent image files that do not rely on the fonts that create them. It supports a large number of font formats and R graphics devices, and meanwhile provides convenient features such as using web fonts and integrating with knitr. This article provides an elaborate introduction to the showtext package, including its design, usage, and examples.",
    "author": [
      {
        "name": "Yixuan Qiu",
        "url": {}
      }
    ],
    "date": "2015-05-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nextrafont, showtext, knitr, Cairo, Rttf2pt1, sysfonts, RCurl, jsonlite, ggplot2, xkcd, RSvgDevice\nCRAN Task Views implied by cited packages\nGraphics, WebTechnologies, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-016/",
    "title": "Estimability Tools for Package Developers",
    "description": "When a linear model is rank-deficient, then predictions based on that model become questionable because not all predictions are uniquely estimable. However, some of them are, and the estimability package provides tools that package developers can use to tell which is which. With the use of these tools, a model object’s predict method could return estimable predictions as-is while flagging non-estimable ones in some way, so that the user can know which predictions to believe. The estimability package also provides, as a demonstration, an estimability-enhanced epredict method to use in place of predict for models fitted using the stats package.",
    "author": [
      {
        "name": "Russell V. Lenth",
        "url": {}
      }
    ],
    "date": "2015-05-11",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nestimability\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-015/",
    "title": "Manipulation of Discrete Random Variables with discreteRV",
    "description": "A prominent issue in statistics education is the sometimes large disparity between the theoretical and the computational coursework. discreteRV is an R package for manipulation of discrete random variables which uses clean and familiar syntax similar to the mathematical notation in introductory probability courses. The package offers functions that are simple enough for users with little experience with statistical programming, but has more advanced features which are suitable for a large number of more complex applications. In this paper, we introduce and motivate discreteRV, describe its functionality, and provide reproducible examples illustrating its use.",
    "author": [
      {
        "name": "Eric Hare",
        "url": {}
      },
      {
        "name": "Andreas Buja",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      }
    ],
    "date": "2015-05-06",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndiscreteRV, devtools\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-004/",
    "title": "rdrobust: An R Package for Robust Nonparametric Inference in Regression-Discontinuity Designs",
    "description": "This article describes the R package rdrobust, which provides data-driven graphical and in ference procedures for RD designs. The package includes three main functions: rdrobust, rdbwselect and rdplot. The first function (rdrobust) implements conventional local-polynomial RD treatment effect point estimators and confidence intervals, as well as robust bias-corrected confidence intervals, for average treatment effects at the cutoff. This function covers sharp RD, sharp kink RD, fuzzy RD and fuzzy kink RD designs, among other possibilities. The second function (rdbwselect) implements several bandwidth selectors proposed in the RD literature. The third function (rdplot) provides data-driven optimal choices of evenly-spaced and quantile-spaced partition sizes, which are used to implement several data-driven RD plots.",
    "author": [
      {
        "name": "Sebastian Calonico",
        "url": {}
      },
      {
        "name": "Matias D. Cattaneo",
        "url": {}
      },
      {
        "name": "Rocío Titiunik",
        "url": {}
      }
    ],
    "date": "2015-04-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrdrobust\nCRAN Task Views implied by cited packages\nEconometrics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-005/",
    "title": "Frames2: A Package for Estimation in Dual Frame Surveys",
    "description": "Data from complex survey designs require special consideration with regard to estimation of finite population parameters and corresponding variance estimation procedures, as a consequence of significant departures from the simple random sampling assumption. In the past decade a number of statistical software packages have been developed to facilitate the analysis of complex survey data. All these statistical software packages are able to treat samples selected from one sampling frame containing all population units. Dual frame surveys are very useful when it is not possible to guarantee a complete coverage of the target population and may result in considerable cost savings over a single frame design with comparable precision. There are several estimators available in the statistical literature but no existing software covers dual frame estimation procedures. This gap is now filled by package Frames2. In this paper we highlight the main features of the package. The package includes the main estimators in dual frame surveys and also provides interval confidence estimation.",
    "author": [
      {
        "name": "Antonio Arcos",
        "url": {}
      },
      {
        "name": "David Molina",
        "url": {}
      },
      {
        "name": "Maria Giovanna Ranalli",
        "url": {}
      },
      {
        "name": "María del Mar Rueda",
        "url": {}
      }
    ],
    "date": "2015-04-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsurvey, sampling, laeken, TeachingSampling, Frames2\nCRAN Task Views implied by cited packages\nOfficialStatistics, ReproducibleResearch, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-006/",
    "title": "The Complex Multivariate Gaussian Distribution",
    "description": "Here I introduce package cmvnorm, a complex generalization of the mvtnorm package. A complex generalization of the Gaussian process is suggested and numerical results presented using the package. An application in the context of approximating the Weierstrass σ-function using a complex Gaussian process is given.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2015-04-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncmvnorm, mvtnorm, emulator\nCRAN Task Views implied by cited packages\nDistributions, Finance, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-012/",
    "title": "The gridGraphics Package",
    "description": "The gridGraphics package provides a function, grid.echo(), that can be used to convert a plot drawn with the graphics package to a visually identical plot drawn using grid. This conversion provides access to a variety of grid tools for making customisations and additions to the plot that are not possible with the graphics package.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2015-04-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngridGraphics, lattice, ggplot2, plotrix, maps, gridBase, gridSVG\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, Pharmacokinetics, Phylogenetics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-002/",
    "title": "fanplot: An R Package for Visualising Sequential Distributions",
    "description": "Fan charts, first developed by the Bank of England in 1996, have become a standard method for visualising forecasts with uncertainty. Using shading fan charts focus the attention towards the whole distribution away from a single central measure. This article describes the basics of plotting fan charts using an R add-on package alongside some additional methods for displaying sequential distributions. Examples are based on distributions of both estimated parameters from a time series model and future values with uncertainty.",
    "author": [
      {
        "name": "Guy J. Abel",
        "url": {}
      }
    ],
    "date": "2015-04-07",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nvars, forecast, fanplot, R2OpenBUGS, zoo, tsbugs, RColorBrewer, shiny\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance, Environmetrics, gR, Graphics, Spatial, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-014/",
    "title": "Identifying Complex Causal Dependencies in Configurational Data with Coincidence Analysis",
    "description": "We present cna, a package for performing Coincidence Analysis (CNA). CNA is a config urational comparative method for the identification of complex causal dependencies—in particular, causal chains and common cause structures—in configurational data. After a brief introduction to the method’s theoretical background and main algorithmic ideas, we demonstrate the use of the package by means of an artificial and a real-life data set. Moreover, we outline planned enhancements of the package that will further increase its applicability.",
    "author": [
      {
        "name": "Michael Baumgartner",
        "url": {}
      },
      {
        "name": "Alrik Thiem",
        "url": {}
      }
    ],
    "date": "2015-03-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nQCA, SetMethods, cna\nCRAN Task Views implied by cited packages\nCausalInference\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-001/",
    "title": "Peptides: A Package for Data Mining of Antimicrobial Peptides",
    "description": "Antimicrobial peptides (AMP) are a promising source of antibiotics with a broad spectrum activity against bacteria and low incidence of developing resistance. The mechanism by which an AMP executes its function depends on a set of computable physicochemical properties from the amino acid sequence. The Peptides package was designed to allow the quick and easy computation of ten structural characteristics own of the antimicrobial peptides, with the aim of generating data to increase the accuracy in classification and design of new amino acid sequences. Moreover, the options to read and plot XVG output files from GROMACS molecular dynamics package are included.",
    "author": [
      {
        "name": "Daniel Osorio",
        "url": {}
      },
      {
        "name": "Paola Rondón-Villarreal",
        "url": {}
      },
      {
        "name": "Rodrigo Torres",
        "url": {}
      }
    ],
    "date": "2015-02-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPeptides, caret\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MachineLearning, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-028/",
    "title": "bshazard: A Flexible Tool for Nonparametric Smoothing of the Hazard Function",
    "description": "The hazard function is a key component in the inferential process in survival analysis and relevant for describing the pattern of failures. However, it is rarely shown in research papers due to the difficulties in nonparametric estimation. We developed the bshazard package to facilitate the computation of a nonparametric estimate of the hazard function, with data-driven smoothing. The method accounts for left truncation, right censoring and possible covariates. B-splines are used to estimate the shape of the hazard within the generalized linear mixed models framework. Smoothness is controlled by imposing an autoregressive structure on the baseline hazard coefficients. This perspective allows an ’automatic’ smoothing by avoiding the need to choose the smoothing parameter, which is estimated from the data as a dispersion parameter. A simulation study demonstrates the capability of our software and an application to estimate the hazard of Non-Hodgkin’s lymphoma in Swedish population data shows its potential.",
    "author": [
      {
        "name": "Paola Rebora",
        "url": {}
      },
      {
        "name": "Agus Salim",
        "url": {}
      },
      {
        "name": "Marie Reilly",
        "url": {}
      }
    ],
    "date": "2015-01-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmuhaz, flexsurv, bshazard, Epi, survival, splines\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Distributions, Econometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-030/",
    "title": "Farewell's Linear Increments Model for Missing Data: The FLIM package",
    "description": "Missing data is common in longitudinal studies. We present a package for Farewell’s Linear Increments Model for Missing Data (the FLIM package), which can be used to fit linear models for observed increments of longitudinal processes and impute missing data. The method is valid for data with regular observation patterns. The end result is a list of fitted models and a hypothetical complete dataset corresponding to the data we might have observed had individuals not been missing. The FLIM package may also be applied to longitudinal studies for causal analysis, by considering counterfactual data as missing data for instance to compare the effect of different treatments when only data from observational studies are available. The aim of this article is to give an introduction to the FLIM package and to demonstrate how the package can be applied.",
    "author": [
      {
        "name": "Rune Hoff",
        "url": {}
      },
      {
        "name": "Jon Michael Gran",
        "url": {}
      },
      {
        "name": "Daniel Farewell",
        "url": {}
      }
    ],
    "date": "2015-01-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nzoo\nCRAN Task Views implied by cited packages\nEconometrics, Environmetrics, Finance, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-024/",
    "title": "Flexible R Functions for Processing Accelerometer Data, with Emphasis on NHANES 2003-2006",
    "description": "Accelerometers are a valuable tool for measuring physical activity (PA) in epidemiological studies. However, considerable processing is needed to convert time-series accelerometer data into meaningful variables for statistical analysis. This article describes two recently developed R packages for processing accelerometer data. The package accelerometry contains functions for performing various data processing procedures, such as identifying periods of non-wear time and bouts of activity. The functions are flexible, computationally efficient, and compatible with uniaxial or triaxial data. The package nhanesaccel is specifically for processing data from the National Health and Nutrition Examination Survey (NHANES), years 2003–2006. Its primary function generates measures of PA volume, intensity, frequency, and patterns according to user-specified data processing methods. This function can process the NHANES 2003-2006 dataset in under one minute, which is a drastic improve ment over existing software. This article highlights important features of packages accelerometry and nhanesaccel and demonstrates typical usage for PA researchers.",
    "author": [
      {
        "name": "Dane R. Van Domelen",
        "url": {}
      },
      {
        "name": "W. Stephen Pittard",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\naccelerometry, Rcpp, pawacc, PhysicalActivity, survey, GGIR\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics, OfficialStatistics, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-026/",
    "title": "ngspatial: A Package for Fitting the Centered Autologistic and Sparse Spatial Generalized Linear Mixed Models for Areal Data",
    "description": "Two important recent advances in areal modeling are the centered autologistic model and the sparse spatial generalized linear mixed model (SGLMM), both of which are reparameterizations of traditional models. The reparameterizations improve regression inference by alleviating spatial confounding, and the sparse SGLMM also greatly speeds computing by reducing the dimension of the spatial random effects. Package ngspatial (’ng’ = non-Gaussian) provides routines for fitting these new models. The package supports composite likelihood and Bayesian inference for the centered autologistic model, and Bayesian inference for the sparse SGLMM.",
    "author": [
      {
        "name": "John Hughes",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nngspatial, CARBayes, spdep, Rcpp, RcppArmadillo, batchmeans\nCRAN Task Views implied by cited packages\nSpatial, NumericalMathematics, Econometrics, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-029/",
    "title": "SMR: An R package for computing the externally studentized normal midrange distribution",
    "description": "The main purpose of this paper is to present the main algorithms underlining the con struction and implementation of the SMR package, whose aim to compute studentized normal midrange distribution. Details on the externally studentized normal midrange and standardized normal midrange distributions are also given. The package follows the same structure as the prob ability functions implemented in R. That is: the probability density function (dSMR), the cumulative distribution function (pSMR), the quantile function (qSMR) and the random number generating function (rSMR). The pseudocodes and illustrative examples of how to use the package are presented.",
    "author": [
      {
        "name": "Ben Dêivide Oliveira Batista",
        "url": {}
      },
      {
        "name": "Daniel Furtado Ferreira",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSMR\nCRAN Task Views implied by cited packages\nDistributions\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-031/",
    "title": "MVN: An R Package for Assessing Multivariate Normality",
    "description": "Assessing the assumption of multivariate normality is required by many parametric mul tivariate statistical methods, such as MANOVA, linear discriminant analysis, principal component analysis, canonical correlation, etc. It is important to assess multivariate normality in order to proceed with such statistical methods. There are many analytical methods proposed for checking multivariate normality. However, deciding which method to use is a challenging process, since each method may give different results under certain conditions. Hence, we may say that there is no best method, which is valid under any condition, for normality checking. In addition to numerical results, it is very useful to use graphical methods to decide on multivariate normality. Combining the numerical results from several methods with graphical approaches can be useful and provide more reliable decisions. Here, we present an R package, MVN, to assess multivariate normality. It contains the three most widely used multivariate normality tests, including Mardia’s, Henze-Zirkler’s and Royston’s, and graphical approaches, including chi-square Q-Q, perspective and contour plots. It also includes two multivariate outlier detection methods, which are based on robust Mahalanobis distances. Moreover, this package offers functions to check the univariate normality of marginal distributions through both tests and plots. Furthermore, especially for non-R users, we provide a user-friendly web application of the package. This application is available at http://www.biosoft.hacettepe.edu.tr/MVN/.",
    "author": [
      {
        "name": "Selcuk Korkmaz",
        "url": {}
      },
      {
        "name": "Dincer Goksuluk",
        "url": {}
      },
      {
        "name": "Gokmen Zararsiz",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMASS, FactoMineR, psych, CCA, MVN, shiny\nCRAN Task Views implied by cited packages\nPsychometrics, Multivariate, Distributions, Econometrics, Environmetrics, NumericalMathematics, Pharmacokinetics, Robust, SocialSciences, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-032/",
    "title": "qmethod: A Package to Explore Human Perspectives Using Q Methodology",
    "description": "Q is a methodology to explore the distinct subjective perspectives that exist within a group. It is used increasingly across disciplines. The methodology is semi-qualitative and the data are analysed using data reduction methods to discern the existing patterns of thought. This package is the first to perform Q analysis in R, and it provides many advantages to the existing software: namely, it is fully cross-platform, the algorithms can be transparently examined, it provides results in a clearly structured and tabulated form ready for further exploration and modelling, it produces a graphical summary of the results, and it generates a more concise report of the distinguishing and consensus statements. This paper introduces the methodology and explains how to use the package, its advantages as well as its limitations. I illustrate the main functions with a dataset on value patterns about democracy.",
    "author": [
      {
        "name": "Aiora Zabala",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nqmethod, psych, GPArotation, FactoMineR\nCRAN Task Views implied by cited packages\nPsychometrics, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-033/",
    "title": "gset: An R Package for Exact Sequential Test of Equivalence Hypothesis Based on Bivariate Non-Central t-Statistics",
    "description": "The R package gset calculates equivalence and futility boundaries based on the exact bivariate non-central t test statistics. It is the first R package that targets specifically at the group sequential test of equivalence hypotheses. The exact test approach adopted by gset neither assumes the large-sample normality of the test statistics nor ignores the contribution to the overall Type I error rate from rejecting one out of the two one-sided hypotheses under a null value. The features of gset include: error spending functions, computation of equivalence boundaries and futility boundaries, either binding or nonbinding, depiction of stagewise boundary plots, and operating characteristics of a given group sequential design including empirical Type I error rate, empirical power, expected sample size, and probability of stopping at an interim look due to equivalence or futility.",
    "author": [
      {
        "name": "Fang Liu",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngsDesign, GroupSeq, Hmisc, PwrGSD, AGSDest, clinfun\nCRAN Task Views implied by cited packages\nClinicalTrials, ExperimentalDesign, Bayesian, Econometrics, Multivariate, OfficialStatistics, ReproducibleResearch, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-027/",
    "title": "sgof: An R Package for Multiple Testing Problems",
    "description": "In this paper we present a new R package called sgof for multiple hypothesis testing. The principal aim of this package is to implement SGoF-type multiple testing methods, known to be more powerful than the classical false discovery rate (FDR) and family-wise error rate (FWER) based methods in certain situations, particularly when the number of tests is large. This package includes Bi nomial and Conservative SGoF and the Bayesian and Beta-Binomial SGoF multiple testing procedures, which are adaptations of the original SGoF method to the Bayesian setting and to possibly correlated tests, respectively. The sgof package also implements the Benjamini-Hochberg and Benjamini-Yekutieli FDR controlling procedures. For each method the package provides (among other things) the number of rejected null hypotheses, estimation of the corresponding FDR, and the set of adjusted p values. Some automatic plots of interest are implemented too. Two real data examples are used to illustrate how sgof works.",
    "author": [
      {
        "name": "Irene Castro-Conde",
        "url": {}
      },
      {
        "name": "Jacobo de Uña-Álvarez",
        "url": {}
      }
    ],
    "date": "2014-11-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsgof, mutoss, multcomp\nCRAN Task Views implied by cited packages\nClinicalTrials, SocialSciences, Survival\nBioconductor packages used\nqvalue, HybridMTest, multtest\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-020/",
    "title": "Coordinate-Based Meta-Analysis of fMRI Studies with R ",
    "description": "This paper outlines how to conduct a simple meta-analysis of neuroimaging foci of activation in R. In particular, the first part of this paper reviews the nature of fMRI data, and briefly overviews the existing packages that can be used to analyze fMRI data in R. The second part illustrates how to handle fMRI data by showing how to visualize the results of different neuroimaging studies in a so-called orthographic view, where the spatial distribution of the foci of activation from different fMRI studies can be inspected visually. Functional MRI (fMRI) is one of the most important and powerful tools of neuroscientific research. Although not as commonly used for fMRI analysis as some specific applications such as SPM (Friston et al., 2006), AFNI (Cox and Hyde, 1997), or FSL (Smith et al., 2004), R does provide several packages that can be employed in neuroimaging research. These packages deal with a variety of topics, ranging from reading and manipulating fMRI datasets, to implementing sophisticated statistical models. The goal of this paper is to provide a brief introduction to fMRI analysis, and the various R packages that can be used to carry it out. As an example, it will show how to use simple R commands to read fMRI images and plot results from previous studies, which can then be visually compared. This is a special form of meta-analysis, and a common way to compare results from the existing literature.",
    "author": [
      {
        "name": "Andrea Stocco",
        "url": {}
      }
    ],
    "date": "2014-11-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfmri, RNiftyReg, AnalyzeFMRI, RfmriVC, arf3DS4, BHMSMAfMRI, neuRosim, oro.nifti, spatstat\nCRAN Task Views implied by cited packages\nMedicalImaging, ChemPhys, Spatial, SpatioTemporal, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-023/",
    "title": "phaseR: An R Package for Phase Plane Analysis of Autonomous ODE Systems",
    "description": "When modelling physical systems, analysts will frequently be confronted by differential equations which cannot be solved analytically. In this instance, numerical integration will usually be the only way forward. However, for autonomous systems of ordinary differential equations (ODEs) in one or two dimensions, it is possible to employ an instructive qualitative analysis foregoing this requirement, using so-called phase plane methods. Moreover, this qualitative analysis can even prove to be highly useful for systems that can be solved analytically, or will be solved numerically anyway. The package phaseR allows the user to perform such phase plane analyses: determining the stability of any equilibrium points easily, and producing informative plots.",
    "author": [
      {
        "name": "Michael J. Grayling",
        "url": {}
      }
    ],
    "date": "2014-09-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndeSolve, ReacTran, rootSolve, bvpSolve, sde, phaseR\nCRAN Task Views implied by cited packages\nDifferentialEquations, Finance, Pharmacokinetics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-025/",
    "title": "Applying spartan to Understand Parameter Uncertainty in Simulations",
    "description": "In attempts to further understand the dynamics of complex systems, the application of computer simulation is becoming increasingly prevalent. Whereas a great deal of focus has been placed in the development of software tools that aid researchers develop simulations, similar focus has not been applied in the creation of tools that perform a rigorous statistical analysis of results generated through simulation: vital in understanding how these results offer an insight into the captured system. This encouraged us to develop spartan, a package of statistical techniques designed to assist researchers in understanding the relationship between their simulation and the real system. Previously we have described each technique within spartan in detail, with an accompanying immunology case study examining the development of lymphoid tissue. Here we provide a practical introduction to the package, demonstrating how each technique is run in R, to assist researchers in integrating this package alongside their chosen simulation platform.",
    "author": [
      {
        "name": "Kieran Alden",
        "url": {}
      },
      {
        "name": "Mark Read",
        "url": {}
      },
      {
        "name": "Paul S Andrews",
        "url": {}
      },
      {
        "name": "Jon Timmis",
        "url": {}
      },
      {
        "name": "Mark Coles",
        "url": {}
      }
    ],
    "date": "2014-09-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspartan, lhs, gplots, XML\nCRAN Task Views implied by cited packages\nDistributions, ExperimentalDesign, Graphics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:52+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-022/",
    "title": "Prinsimp",
    "description": "Principal Components Analysis (PCA) is a common way to study the sources of variation in a high-dimensional data set. Typically, the leading principal components are used to understand the variation in the data or to reduce the dimension of the data for subsequent analysis. The remaining principal components are ignored since they explain little of the variation in the data. However, the space spanned by the low variation principal components may contain interesting structure, structure that PCA cannot find. Prinsimp is an R package that looks for interesting structure of low variability. “Interesting” is defined in terms of a simplicity measure. Looking for interpretable structure in a low variability space has particular importance in evolutionary biology, where such structure can signify the existence of a genetic constraint.",
    "author": [
      {
        "name": "Jonathan Zhang",
        "url": {}
      },
      {
        "name": "Nancy Heckman",
        "url": {}
      },
      {
        "name": "Davor Cubranic",
        "url": {}
      },
      {
        "name": "Joel G. Kingsolver",
        "url": {}
      },
      {
        "name": "Travis Gaydos",
        "url": {}
      },
      {
        "name": "J.S.            Marron",
        "url": {}
      }
    ],
    "date": "2014-09-28",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nprinsimp\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-021/",
    "title": "Automatic Conversion of Tables to LongForm Dataframes",
    "description": "TableToLongForm automatically converts hierarchical Tables intended for a human reader into a simple LongForm dataframe that is machine readable, making it easier to access and use the data for analysis. It does this by recognising positional cues present in the hierarchical Table (which would normally be interpreted visually by the human brain) to decompose, then reconstruct the data into a LongForm dataframe. The article motivates the benefit of such a conversion with an example Table, followed by a short user manual, which includes a comparison between the simple one argument call to TableToLongForm, with code for an equivalent manual conversion. The article then explores the types of Tables the package can convert by providing a gallery of all recognised patterns. It finishes with a discussion of available diagnostic methods and future work.",
    "author": [
      {
        "name": "Jimmy Oh",
        "url": {}
      }
    ],
    "date": "2014-09-26",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nTableToLongForm, reshape2, plyr\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-002/",
    "title": "A Multiscale Test of Spatial Stationarity for Textured Images in R",
    "description": "The ability to automatically identify areas of homogeneous texture present within a greyscale image is an important feature of image processing algorithms. This article describes the R package LS2Wstat which employs a recent wavelet-based test of stationarity for locally stationary random fields to assess such spatial homogeneity. By embedding this test within a quadtree image segmentation procedure we are also able to identify texture regions within an image.",
    "author": [
      {
        "name": "Matthew A. Nunes",
        "url": {}
      },
      {
        "name": "Sarah L. Taylor",
        "url": {}
      },
      {
        "name": "Idris A. Eckley",
        "url": {}
      }
    ],
    "date": "2014-06-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nLS2Wstat, LS2W, urca, CADFtest, locits\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-008/",
    "title": "ROSE: a Package for Binary Imbalanced Learning",
    "description": "The ROSE package provides functions to deal with binary classification problems in the presence of imbalanced classes. Artificial balanced samples are generated according to a smoothed bootstrap approach and allow for aiding both the phases of estimation and accuracy evaluation of a binary classifier in the presence of a rare class. Functions that implement more traditional remedies for the class imbalance and different metrics to evaluate accuracy are also provided. These are estimated by holdout, bootstrap or cross-validation methods.",
    "author": [
      {
        "name": "Nicola Lunardon",
        "url": {}
      },
      {
        "name": "Giovanna Menardi",
        "url": {}
      },
      {
        "name": "Nicola Torelli",
        "url": {}
      }
    ],
    "date": "2014-06-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nDMwR, caret, ROSE, ROSE, ROSE, class\nCRAN Task Views implied by cited packages\nMultivariate, HighPerformanceComputing, MachineLearning, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-016/",
    "title": "Web Technologies Task View",
    "description": "This article presents the CRAN Task View on Web Technologies. We describe the most important aspects of Web Technologies and Web Scraping and list some of the packages that are currently available on CRAN. Finally, we plot the network of Web Technology related package dependencies.",
    "author": [
      {
        "name": "Patrick Mair",
        "url": {}
      },
      {
        "name": "Scott Chamberlain",
        "url": {}
      }
    ],
    "date": "2014-06-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nXML, RCurl, rjson, RJSONIO, jsonlite, httr, ROAuth, shiny, rgbif, rfishbase, rfisheries, rsnps, rentrez, crn, RNCEP, WDI, TFX, anametrix, rpubchem, cimis, nhlscrapr, tm, translate, scholar, RgoogleMap, Rfacebook, twitteR, streamR, AWS.tools, MTurkR, GuardianR, igraph\nCRAN Task Views implied by cited packages\nWebTechnologies, Spatial, ChemPhys, Finance, gR, Graphics, HighPerformanceComputing, NaturalLanguageProcessing, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-004/",
    "title": "brainR: Interactive 3 and 4D Images of High Resolution Neuroimage Data",
    "description": "We provide software tools for displaying and publishing interactive 3-dimensional (3D) and 4-dimensional (4D) figures to html webpages, with examples of high-resolution brain imaging. Our framework is based in the R statistical software using the rgl package, a 3D graphics library. We build on this package to allow manipulation of figures including rotation and translation, zooming, coloring of brain substructures, adjusting transparency levels, and addition/or removal of brain structures. The need for better visualization tools of ultra high dimensional data is ever present; we are providing a clean, simple, web-based option. We also provide a package (brainR) for users to readily implement these tools.",
    "author": [
      {
        "name": "John Muschelli",
        "url": {}
      },
      {
        "name": "Elizabeth Sweeney",
        "url": {}
      },
      {
        "name": "Ciprian Crainiceanu",
        "url": {}
      }
    ],
    "date": "2014-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrgl, knitr, Sweave, slidify, misc3d, brainR\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, MedicalImaging, ReproducibleResearch, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-013/",
    "title": "The gridSVG Package",
    "description": "The gridSVG package can be used to generate a grid-based R plot in an SVG format, with the ability to add special effects to the plot. The special effects include animation, interactivity, and advanced graphical features, such as masks and filters. This article provides a basic introduction to important functions in the gridSVG package and discusses the advantages and disadvantages of gridSVG compared to similar R packages.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      },
      {
        "name": "Simon Potter",
        "url": {}
      }
    ],
    "date": "2014-06-02",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngridSVG, lattice, ggplot2, animation, RSVGTipsDevice, googleVis, shiny\nCRAN Task Views implied by cited packages\nGraphics, WebTechnologies, Multivariate, Pharmacokinetics, Phylogenetics, ReproducibleResearch, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-006/",
    "title": "PivotalR: A Package for Machine Learning on Big Data",
    "description": "PivotalR is an R package that provides a front-end to PostgreSQL and all PostgreSQL-like databases such as Pivotal Inc.’s Greenplum Database (GPDB) (Pivotal Inc., 2013a), HAWQ (Pivotal Inc., 2013b). When running on the products of Pivotal Inc., PivotalR utilizes the full power of parallel computation and distributive storage, and thus gives the normal R user access to big data. PivotalR also provides the R wrapper for MADlib. MADlib is an open-source library for scalable in-database analytics. It provides data-parallel implementations of mathematical, statistical and machine-learning algorithms for structured and unstructured data. Thus PivotalR also enables the user to apply machine learning algorithms onto big data.",
    "author": [
      {
        "name": "Hai Qian",
        "url": {}
      }
    ],
    "date": "2014-05-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPivotalR, RPostgreSQL, shiny\nCRAN Task Views implied by cited packages\nWebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-009/",
    "title": "investr: An R Package for Inverse Estimation",
    "description": "Inverse estimation is a classical and well-known problem in regression. In simple terms, it involves the use of an observed value of the response to make inference on the corresponding unknown value of the explanatory variable. To our knowledge, however, statistical software is somewhat lacking the capabilities for analyzing these types of problems. In this paper1 , we introduce investr (which stands for inverse estimation in R), a package for solving inverse estimation problems in both linear and nonlinear regression models.",
    "author": [
      {
        "name": "Brandon M. Greenwell",
        "url": {}
      },
      {
        "name": "Christine M. Schubert Kabban",
        "url": {}
      }
    ],
    "date": "2014-05-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ninvestr, MASS, drc, car, boot\nCRAN Task Views implied by cited packages\nEconometrics, SocialSciences, ChemPhys, Multivariate, Pharmacokinetics, Distributions, Environmetrics, Finance, NumericalMathematics, Optimization, Psychometrics, Robust, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-018/",
    "title": "oligoMask: A Framework for Assessing and Removing the Effect of Genetic Variants on Microarray Probes",
    "description": "As expression microarrays are typically designed relative to a reference genome, any individual genetic variant that overlaps a probe’s genomic position can possibly cause a reduction in hybridization due to the probe no longer being a perfect match to a given sample’s mRNA at that locus. If the samples or groups used in a microarray study differ in terms of genetic variants, the results of the microarray experiment can be negatively impacted. The oligoMask package is an R/SQLite framework which can utilize publicly available genetic variants and works in conjunction with the oligo package to read in the expression data and remove microarray probes which are likely to impact a given microarray experiment prior to analysis. Tools are provided for creating an SQLite database containing the probe and variant annotations and for performing the commonly used RMA preprocessing procedure for Affymetrix microarrays. The oligoMask package is freely available at https://github.com/dbottomly/oligoMask.",
    "author": [
      {
        "name": "Daniel Bottomly",
        "url": {}
      },
      {
        "name": "Beth Wilmot",
        "url": {}
      },
      {
        "name": "Shannon K. McWeeney",
        "url": {}
      }
    ],
    "date": "2014-05-27",
    "categories": [],
    "contents": "\n\n\n\nBioconductor packages used\noligo, xps, maskBAD, VariantAnnotation, BSgenome, Biostrings\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-011/",
    "title": "The stringdist Package for Approximate String Matching",
    "description": "Comparing text strings in terms of distance functions is a common and fundamental task in many statistical text-processing applications. Thus far, string distance functionality has been somewhat scattered around R and its extension packages, leaving users with inconistent interfaces and encoding handling. The stringdist package was designed to offer a low-level interface to several popular string distance algorithms which have been re-implemented in C for this purpose. The package offers distances based on counting q-grams, edit-based distances, and some lesser known heuristic distance functions. Based on this functionality, the package also offers inexact matching equivalents of R’s native exact matching functions match and %in%.",
    "author": [
      {
        "name": "Mark P.J. van der Loo",
        "url": {}
      }
    ],
    "date": "2014-04-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nkernlab, RecordLinkage, MiscPsycho, cba, Mkmisc, deducorrect, vwr, stringdist, textcat, TraMineR\nCRAN Task Views implied by cited packages\nOfficialStatistics, Cluster, NaturalLanguageProcessing, Graphics, MachineLearning, Multivariate, Optimization, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-005/",
    "title": "The RWiener Package: an R Package Providing Distribution Functions for the Wiener Diffusion Model",
    "description": "We present the RWiener package that provides R functions for the Wiener diffusion model. The core of the package are the four distribution functions dwiener, pwiener, qwiener and rwiener, which use up-to-date methods, implemented in C, and provide fast and accurate computation of the density, distribution, and quantile function, as well as a random number generator for the Wiener diffusion model. We used the typical Wiener diffusion model with four parameters: boundary separation, non-decision time, initial bias and drift rate parameter. Beyond the distribution functions, we provide extended likelihood-based functions that can be used for parameter estimation and model selection. The package can be obtained via CRAN.",
    "author": [
      {
        "name": "Dominik Wabersich",
        "url": {}
      },
      {
        "name": "Joachim Vandekerckhove",
        "url": {}
      }
    ],
    "date": "2014-04-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRWiener\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-007/",
    "title": "rotations: An R Package for SO(3) Data",
    "description": "In this article we introduce the rotations package which provides users with the ability to simulate, analyze and visualize three-dimensional rotation data. More specifically it includes four commonly used distributions from which to simulate data, four estimators of the central orientation, six confidence region estimation procedures and two approaches to visualizing rotation data. All of these features are available for two different parameterizations of rotations: three-by-three matrices and quaternions. In addition, two datasets are included that illustrate the use of rotation data in practice.",
    "author": [
      {
        "name": "Bryan Stanfill",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      },
      {
        "name": "Ulrike Genschel",
        "url": {}
      }
    ],
    "date": "2014-04-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\norientlib, onion, circular, SpherWave, rotations, ggplot2, sphereplot, Rcpp, RcppArmadillo\nCRAN Task Views implied by cited packages\nNumericalMathematics, Graphics, Environmetrics, HighPerformanceComputing, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-014/",
    "title": "MRCV: A Package for Analyzing Categorical Variables with Multiple Response Options",
    "description": "Multiple response categorical variables (MRCVs), also known as “pick any” or “choose all that apply” variables, summarize survey questions for which respondents are allowed to select more than one category response option. Traditional methods for analyzing the association between categorical variables are not appropriate with MRCVs due to the within-subject dependence among responses. We have developed the MRCV package as the first R package available to correctly analyze MRCV data. Statistical methods offered by our package include counterparts to traditional Pearson chi-square tests for independence and loglinear models, where bootstrap methods and Rao-Scott adjustments are relied on to obtain valid inferences. We demonstrate the primary functions within the package by analyzing data from a survey assessing the swine waste management practices of Kansas farmers.",
    "author": [
      {
        "name": "Natalie A. Koziol",
        "url": {}
      },
      {
        "name": "Christopher R. Bilder",
        "url": {}
      }
    ],
    "date": "2014-04-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMRCV, geepack\nCRAN Task Views implied by cited packages\nEconometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-019/",
    "title": "sgr: A Package for Simulating Conditional Fake Ordinal Data",
    "description": "Many self-report measures of attitudes, beliefs, personality, and pathology include items that can be easily manipulated by respondents. For example, an individual may deliberately attempt to manipulate or distort responses to simulate grossly exaggerated physical or psychological symptoms in order to reach specific goals such as, for example, obtaining financial compensation, avoiding being charged with a crime, avoiding military duty, or obtaining drugs. This article introduces the package sgr that can be used to perform fake data analysis according to the sample generation by replacement approach. The package includes functions for making simple inferences about discrete/ordinal fake data. The package allows to quantify uncertainty in inferences based on possible fake data as well as to study the implications of fake data for empirical results.",
    "author": [
      {
        "name": "Luigi Lombardi",
        "url": {}
      },
      {
        "name": "Massimiliano Pastore",
        "url": {}
      }
    ],
    "date": "2014-04-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsgr, polycor, MASS\nCRAN Task Views implied by cited packages\nMultivariate, Psychometrics, Distributions, Econometrics, Environmetrics, NumericalMathematics, Pharmacokinetics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-012/",
    "title": "RStorm: Developing and Testing Streaming Algorithms in R",
    "description": "Streaming data, consisting of indefinitely evolving sequences, are becoming ubiquitous in many branches of science and in various applications. Computer scientists have developed streaming applications such as Storm and the S4 distributed stream computing platform1 to deal with data streams. However, in current production packages testing and evaluating streaming algorithms is cumbersome. This paper presents RStorm for the development and evaluation of streaming algorithms analogous to these production packages, but implemented fully in R. RStorm allows developers of streaming algorithms to quickly test, iterate, and evaluate various implementations of streaming algorithms. The paper provides both a canonical computer science example, the streaming word count, and examples of several statistical applications of RStorm.",
    "author": [
      {
        "name": "Maurits Kaptein",
        "url": {}
      }
    ],
    "date": "2014-03-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRStorm, stream\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-001/",
    "title": "Taming PITCHf/x Data with XML2R and pitchRx",
    "description": "XML2R is a framework that reduces the effort required to transform XML content into tables in a way that preserves parent to child relationships. pitchRx applies XML2R’s grammar for XML manipulation to Major League Baseball Advanced Media (MLBAM)’s Gameday data. With pitchRx, one can easily obtain and store Gameday data in a remote database. The Gameday website hosts a wealth of XML data, but perhaps most interesting is PITCHf/x. Among other things, PITCHf/x data can be used to recreate a baseball’s flight path from a pitcher’s hand to home plate. With pitchRx, one can easily create animations and interactive 3D scatterplots of the baseball’s flight path. PITCHf/x data is also commonly used to generate a static plot of baseball locations at the moment they cross home plate. These plots, sometimes called strike-zone plots, can also refer to a plot of event probabilities over the same region. pitchRx provides an easy and robust way to generate strike-zone plots using the ggplot2 package.",
    "author": [
      {
        "name": "Carson Sievert",
        "url": {}
      }
    ],
    "date": "2014-03-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npitchRx, XML2R, ggplot2, rgl, dplyr, mgcv, knitr\nCRAN Task Views implied by cited packages\nGraphics, Bayesian, Econometrics, Environmetrics, Multivariate, Phylogenetics, ReproducibleResearch, SocialSciences, SpatioTemporal, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-003/",
    "title": "Stratified Weibull Regression Model for Interval-Censored Data",
    "description": "Interval censored outcomes arise when a silent event of interest is known to have occurred within a specific time period determined by the times of the last negative and first positive diagnostic tests. There is a rich literature on parametric and non-parametric approaches for the analysis of interval-censored outcomes. A commonly used strategy is to use a proportional hazards (PH) model with the baseline hazard function parameterized. The proportional hazards assumption can be relaxed in stratified models by allowing the baseline hazard function to vary across strata defined by a subset of explanatory variables. In this paper, we describe and implement a new R package straweib, for fitting a stratified Weibull model appropriate for interval censored outcomes. We illustrate the R package straweib by analyzing data from a longitudinal oral health study on the timing of the emergence of permanent teeth in 4430 children.",
    "author": [
      {
        "name": "Xiangdong Gu",
        "url": {}
      },
      {
        "name": "David Shapiro",
        "url": {}
      },
      {
        "name": "Michael D. Hughes",
        "url": {}
      },
      {
        "name": "Raji Balasubramanian",
        "url": {}
      }
    ],
    "date": "2014-03-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsurvival, straweib\nCRAN Task Views implied by cited packages\nClinicalTrials, Econometrics, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-010/",
    "title": "Rankcluster: An R Package for Clustering Multivariate Partial Rankings",
    "description": "The Rankcluster package is the first R package proposing both modeling and clustering tools for ranking data, potentially multivariate and partial. Ranking data are modeled by the Insertion Sorting Rank (ISR) model, which is a meaningful model parametrized by a central ranking and a dispersion parameter. A conditional independence assumption allows multivariate rankings to be taken into account, and clustering is performed by means of mixtures of multivariate ISR models. The parameters of the cluster (central rankings and dispersion parameters) help the practitioners to interpret the clustering. Moreover, the Rankcluster package provides an estimate of the missing ranking positions when rankings are partial. After an overview of the mixture of multivariate ISR models, the Rankcluster package is described and its use is illustrated through the analysis of two real datasets.",
    "author": [
      {
        "name": "Julien Jacques",
        "url": {}
      },
      {
        "name": "Quentin Grimonprez",
        "url": {}
      },
      {
        "name": "Christophe Biernacki",
        "url": {}
      }
    ],
    "date": "2014-03-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRankcluster, pmr, RMallow\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-015/",
    "title": "Archiving Reproducible Research with R and Dataverse",
    "description": "Reproducible research and data archiving are increasingly important issues in research involving statistical analyses of quantitative data. This article introduces the dvn package, which allows R users to publicly archive datasets, analysis files, codebooks, and associated metadata in Dataverse Network online repositories, an open-source data archiving project sponsored by Harvard University. In this article I review the importance of data archiving in the context of reproducible research, introduces the Dataverse Network, explain the implementation of the dvn package, and provide example code for archiving and releasing data using the package.",
    "author": [
      {
        "name": "Thomas J. Leeper",
        "url": {}
      }
    ],
    "date": "2014-03-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndvn, knitr, rfigshare, RCurl, XML, rfigshare, rdryad, OAIHarvester\nCRAN Task Views implied by cited packages\nWebTechnologies, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-017/",
    "title": "Addendum to ``Statistical Software from a Blind Person's Perspective''",
    "description": "This short note explains a solution to a problem for blind users when using the R terminal under Windows Vista or Windows 7, as identified in Godfrey (2013). We note the way the solution was discovered and subsequent confirmatory experiments. As part of his preparations for teaching a blind student in a statistics course, the second author",
    "author": [
      {
        "name": "A. Jonathan R. Godfrey",
        "url": {}
      },
      {
        "name": "Robert Erhardt",
        "url": {}
      }
    ],
    "date": "2014-03-03",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-029/",
    "title": "Dynamic Parallelization of R Functions",
    "description": "R offers several extension packages that allow it to perform parallel computations. These operate on fixed points in the program flow and make it difficult to deal with nested parallelism and to organize parallelism in complex computations in general. In this article we discuss, first, of how to detect parallelism in functions, and second, how to minimize user intervention in that process. We present a solution that requires minimal code changes and enables to flexibly and dynamically choose the degree of parallelization in the resulting computation. An implementation is provided by the R package parallelize.dynamic and practical issues are discussed with the help of examples.",
    "author": [
      {
        "name": "Stefan Böhringer",
        "url": {}
      }
    ],
    "date": "2013-12-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRsge, foreach, boot, snow, parallelize.dynamic\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Econometrics, Optimization, SocialSciences, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-034/",
    "title": "betategarch: Simulation, Estimation and Forecasting of Beta-Skew-t-EGARCH Models",
    "description": "This paper illustrates the usage of the betategarch package, a package for the simulation, estimation and forecasting of Beta-Skew-t-EGARCH models. The Beta-Skew-t-EGARCH model is a dynamic model of the scale or volatility of financial returns. The model is characterised by its robustness to jumps or outliers, and by its exponential specification of volatility. The latter enables richer dynamics, since parameters need not be restricted to be positive to ensure positivity of volatility. In addition, the model also allows for heavy tails and skewness in the conditional return (i.e. scaled return), and for leverage and a time-varying long-term component in the volatility specification. More generally, the model can be viewed as a model of the scale of the error in a dynamic regression.",
    "author": [
      {
        "name": "Genaro Sucarrat",
        "url": {}
      }
    ],
    "date": "2013-12-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntseries, fGarch, rugarch, AutoSEARCH, zoo\nCRAN Task Views implied by cited packages\nFinance, TimeSeries, Econometrics, Environmetrics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-032/",
    "title": "The R in Robotics",
    "description": "The aim of this contribution is to connect two previously separated worlds: robotic application development with the Robot Operating System (ROS) and statistical programming with R. This fruitful combination becomes apparent especially in the analysis and visualization of sensory data. We therefore introduce a new language extension for ROS that allows to implement nodes in pure R. All relevant aspects are described in a step-by-step development of a common sensor data transformation node. This includes the reception of raw sensory data via the ROS network, message interpretation, bag-file analysis, transformation and visualization, as well as the transmission of newly generated messages back into the ROS network.",
    "author": [
      {
        "name": "André Dietrich",
        "url": {}
      },
      {
        "name": "Sebastian Zug",
        "url": {}
      },
      {
        "name": "Jörg Kaiser",
        "url": {}
      }
    ],
    "date": "2013-12-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRcpp\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-021/",
    "title": "factorplot: Improving Presentation of Simple Contrasts in Generalized Linear Models",
    "description": "Recent statistical literature has paid attention to the presentation of pairwise comparisons either from the point of view of the reference category problem in generalized linear models (GLMs) or in terms of multiple comparisons. Both schools of thought are interested in the parsimonious presentation of sufficient information to enable readers to evaluate the significance of contrasts resulting from the inclusion of qualitative variables in GLMs. These comparisons also arise when trying to interpret multinomial models where one category of the dependent variable is omitted as a reference. While considerable advances have been made, opportunities remain to improve the presentation of this information, especially in graphical form. The factorplot package provides new functions for graphically and numerically presenting results of hypothesis tests related to pairwise comparisons resulting from qualitative covariates in GLMs or coefficients in multinomial logistic regression models.",
    "author": [
      {
        "name": "David A. Armstrong II",
        "url": {}
      }
    ],
    "date": "2013-11-22",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmultcomp, qvcalc, Epi, car, multcompView, factorplot\nCRAN Task Views implied by cited packages\nSocialSciences, Survival, ClinicalTrials, Econometrics, Finance, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-030/",
    "title": "CompLognormal: An R Package for Composite Lognormal Distributions",
    "description": "In recent years, composite models based on the lognormal distribution have become popular in actuarial sciences and related areas. In this short note, we present a new R package for computing the probability density function, cumulative density function, and quantile function, and for generating random numbers of any composite model based on the lognormal distribution. The use of the package is illustrated using a real data set.",
    "author": [
      {
        "name": "S. Nadarajah",
        "url": {}
      },
      {
        "name": "S. A. A. Bakar",
        "url": {}
      }
    ],
    "date": "2013-11-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCompLognormal, CRAN, poweRlaw, SMPracticals, MASS, fitdistrplus, distrMod\nCRAN Task Views implied by cited packages\nDistributions, Survival, Econometrics, Environmetrics, Multivariate, NumericalMathematics, Pharmacokinetics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-031/",
    "title": "lfe: Linear Group Fixed Effects",
    "description": "Linear models with fixed effects and many dummy variables are common in some fields. Such models are straightforward to estimate unless the factors have too many levels. The R package lfe solves this problem by implementing a generalization of the within transformation to multiple factors, tailored for large problems.",
    "author": [
      {
        "name": "Simen Gaure",
        "url": {}
      }
    ],
    "date": "2013-11-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMatrix, plm, lfe, igraph, multicore\nCRAN Task Views implied by cited packages\nEconometrics, gR, Graphics, Multivariate, NumericalMathematics, Optimization, Spatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-033/",
    "title": "On Sampling from the Multivariate t Distribution",
    "description": "The multivariate normal and the multivariate t distributions belong to the most widely used multivariate distributions in statistics, quantitative risk management, and insurance. In contrast to the multivariate normal distribution, the parameterization of the multivariate t distribution does not correspond to its moments. This, paired with a non-standard implementation in the R package mvtnorm, provides traps for working with the multivariate t distribution. In this paper, common traps are clarified and corresponding recent changes to mvtnorm are presented.",
    "author": [
      {
        "name": "Marius Hofert",
        "url": {}
      }
    ],
    "date": "2013-11-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmvtnorm, MASS, evir, mnormt, QRM\nCRAN Task Views implied by cited packages\nDistributions, Multivariate, Environmetrics, ExtremeValue, Econometrics, Finance, NumericalMathematics, Pharmacokinetics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-027/",
    "title": "rlme: An R Package for Rank-Based Estimation and Prediction in Random Effects Nested Models",
    "description": "There is a lack of robust statistical analyses for random effects linear models. In practice, statistical analyses, including estimation, prediction and inference, are not reliable when data are unbalanced, of small size, contain outliers, or not normally distributed. It is fortunate that rank-based regression analysis is a robust nonparametric alternative to likelihood and least squares analysis. We propose an R package that calculates rank-based statistical analyses for twoand three-level random effects nested designs. In this package, a new algorithm which recursively obtains robust predictions for both scale and random effects is used, along with three rank-based fitting methods.",
    "author": [
      {
        "name": "Yusuf K. Bilgic",
        "url": {}
      },
      {
        "name": "Herbert Susmann",
        "url": {}
      }
    ],
    "date": "2013-10-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\naa, Rfit, rlme, lme4\nCRAN Task Views implied by cited packages\nBayesian, Econometrics, Environmetrics, OfficialStatistics, Psychometrics, SocialSciences, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-023/",
    "title": "RNetCDF - A Package for Reading and Writing NetCDF Datasets",
    "description": "This paper describes the RNetCDF package (version 1.6), an interface for reading and writing files in Unidata NetCDF format, and gives an introduction to the NetCDF file format. NetCDF is a machine independent binary file format which allows storage of different types of array based data, along with short metadata descriptions. The package presented here allows access to the most important functions of the NetCDF C-interface for reading, writing, and modifying NetCDF datasets. In this paper, we present a short overview on the NetCDF file format and show usage examples of the package.",
    "author": [
      {
        "name": "Pavel Michna",
        "url": {}
      },
      {
        "name": "Milton Woods",
        "url": {}
      }
    ],
    "date": "2013-10-14",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRNetCDF, ncdf, ncdf4\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-022/",
    "title": "spMC: Modelling Spatial Random Fields with Continuous Lag Markov Chains",
    "description": "Currently, a part of the R statistical software is developed in order to deal with spatial models. More specifically, some available packages allow the user to analyse categorical spatial random patterns. However, only the spMC package considers a viewpoint based on transition probabilities between locations. Through the use of this package it is possible to analyse the spatial variability of data, make inference, predict and simulate the categorical classes in unobserved sites. An example is presented by analysing the well-known Swiss Jura data set.",
    "author": [
      {
        "name": "Luca Sartore",
        "url": {}
      }
    ],
    "date": "2013-09-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspMC, gstat, geoRglm, RandomFields\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Bayesian\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-035/",
    "title": "Changes to grid for R 3.0.0",
    "description": "From R 3.0.0, there is a new recommended way to develop new grob classes in grid. In a nutshell, two new “hook” functions, makeContext() and makeContent() have been added to grid to provide an alternative to the existing hook functions preDrawDetails(), drawDetails(), and postDrawDetails(). There is also a new function called grid.force(). This article discusses why these changes have been made, provides a simple demonstration of the use of the new functions, and discusses some of the implications for packages that build on grid.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2013-09-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlattice, ggplot2, gtable, gridSVG, grImport, gridGraphviz, gridExtra\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, Pharmacokinetics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-025/",
    "title": "Performance Attribution for Equity Portfolios",
    "description": "The pa package provides tools for conducting performance attribution for long-only, single currency equity portfolios. The package uses two methods: the Brinson-Hood-Beebower model (hereafter referred to as the Brinson model) and a regression-based analysis. The Brinson model takes an ANOVA-type approach and decomposes the active return of any portfolio into asset allocation, stock selection, and interaction effect. The regression-based analysis utilizes estimated coefficients, based on a regression model, to attribute active return to different factors.",
    "author": [
      {
        "name": "Yang Lu",
        "url": {}
      },
      {
        "name": "David Kane",
        "url": {}
      }
    ],
    "date": "2013-09-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npa, portfolio, PerformanceAnalytics, portfolio\nCRAN Task Views implied by cited packages\nFinance\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-024/",
    "title": "Surface Melting Curve Analysis with R",
    "description": "Nucleic acid Melting Curve Analysis is a powerful method to investigate the interaction of double stranded nucleic acids. Many researchers rely on closed source software which is not ubiquitously available, and gives only little control over the computation and data presentation. R in contrast, is open source, highly adaptable and provides numerous utilities for data import, sophisticated statistical analysis and presentation in publication quality. This article covers methods, implemented in the MBmca package, for DNA Melting Curve Analysis on microbead surfaces. Particularly, the use of the second derivative melting peaks is suggested as an additional parameter to characterize the melting behavior of DNA duplexes. Examples of microbead surface Melting Curve Analysis on fragments of human genes are presented.",
    "author": [
      {
        "name": "Stefan Rödiger",
        "url": {}
      },
      {
        "name": "Alexander Böhm",
        "url": {}
      },
      {
        "name": "Ingolf Schimke",
        "url": {}
      }
    ],
    "date": "2013-08-26",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nqpcR, MBmca, robustbase, stats, signal, zoo, delftfews, Hmisc, base, fda\nCRAN Task Views implied by cited packages\nEconometrics, Multivariate, Bayesian, ClinicalTrials, Environmetrics, Finance, NumericalMathematics, OfficialStatistics, ReproducibleResearch, Robust, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-028/",
    "title": "Temporal Disaggregation of Time Series",
    "description": "Temporal disaggregation methods are used to disaggregate low frequency time series to higher frequency series, where either the sum, the average, the first or the last value of the resulting high frequency series is consistent with the low frequency series. Temporal disaggregation can be performed with or without one or more high frequency indicator series. The package tempdisagg is a collection of several methods for temporal disaggregation.",
    "author": [
      {
        "name": "Christoph Sax",
        "url": {}
      },
      {
        "name": "Peter Steiner",
        "url": {}
      }
    ],
    "date": "2013-08-26",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntempdisagg\nCRAN Task Views implied by cited packages\nTimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-026/",
    "title": "ExactCIdiff: An R Package for Computing Exact Confidence Intervals for the Difference of Two Proportions",
    "description": "Comparing two proportions through the difference is a basic problem in statistics and has applications in many fields. More than twenty confidence intervals (Newcombe, 1998a,b) have been proposed. Most of them are approximate intervals with an asymptotic infimum coverage probability much less than the nominal level. In addition, large sample may be costly in practice. So exact optimal confidence intervals become critical for drawing valid statistical inference with accuracy and precision. Recently, Wang (2010, 2012) derived the exact smallest (optimal) one-sided 1 − α confidence intervals for the difference of two paired or independent proportions. His intervals, however, are computer-intensive by nature. In this article, we provide an R package ExactCIdiff to implement the intervals when the sample size is not large. This would be the first available package in R to calculate the exact confidence intervals for the difference of proportions. Exact two-sided 1 − α interval can be easily obtained by taking the intersection of the lower and upper one-sided 1 − α/2 intervals. Readers may jump to Examples 1 and 2 to obtain these intervals.",
    "author": [
      {
        "name": "Guogen Shan",
        "url": {}
      },
      {
        "name": "Weizhen Wang",
        "url": {}
      }
    ],
    "date": "2013-08-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nExactCIdiff, Epi, PropCIs, exactci\nCRAN Task Views implied by cited packages\nSurvival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-020/",
    "title": "Translating Probability Density Functions: From R to BUGS and Back Again",
    "description": "The ability to implement statistical models in the BUGS language facilitates Bayesian in ference by automating MCMC algorithms. Software packages that interpret the BUGS language include OpenBUGS, WinBUGS, and JAGS. R packages that link BUGS software to the R environment, including rjags and R2WinBUGS, are widely used in Bayesian analysis. Indeed, many packages in the Bayesian task view on CRAN (http://cran.r-project.org/web/views/Bayesian.html) depend on this integration. However, the R and BUGS languages use different representations of common probability density functions, creating a potential for errors to occur in the implementation or interpre tation of analyses that use both languages. Here we review different parameterizations used by the R and BUGS languages, describe how to translate between the languages, and provide an R function, r2bugs.distributions, that transforms parameterizations from R to BUGS and back again.",
    "author": [
      {
        "name": "David S. LeBauer",
        "url": {}
      },
      {
        "name": "Michael C. Dietze",
        "url": {}
      },
      {
        "name": "Benjamin M. Bolker",
        "url": {}
      }
    ],
    "date": "2013-06-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrjags, R2WinBUGS\nCRAN Task Views implied by cited packages\nBayesian, gR, Cluster\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-008/",
    "title": "PIN: Measuring Asymmetric Information in Financial Markets with R",
    "description": "The package PIN computes a measure of asymmetric information in financial markets, the so-called probability of informed trading. This is obtained from a sequential trade model and is used to study the determinants of an asset price. Since the probability of informed trading depends on the number of buyand sell-initiated trades during a trading day, this paper discusses the entire modelling cycle, from data handling to the computation of the probability of informed trading and the estimation of parameters for the underlying theoretical model.",
    "author": [
      {
        "name": "Paolo Zagaglia",
        "url": {}
      }
    ],
    "date": "2013-06-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPIN, highfrequency, IBrokers, orderbook\nCRAN Task Views implied by cited packages\nFinance\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-001/",
    "title": "RTextTools: A Supervised Learning Package for Text Classification",
    "description": "Social scientists have long hand-labeled texts to create datasets useful for studying topics from congressional policymaking to media reporting. Many social scientists have begun to incorporate machine learning into their toolkits. RTextTools was designed to make machine learning accessible by providing a start-to-finish product in less than 10 steps. After installing RTextTools, the initial step is to generate a document term matrix. Second, a container object is created, which holds all the objects needed for further analysis. Third, users can use up to nine algorithms to train their data. Fourth, the data are classified. Fifth, the classification is summarized. Sixth, functions are available for performance evaluation. Seventh, ensemble agreement is conducted. Eighth, users can cross-validate their data. Finally, users write their data to a spreadsheet, allowing for further manual coding if required.",
    "author": [
      {
        "name": "Timothy P. Jurka",
        "url": {}
      },
      {
        "name": "Loren Collingwood",
        "url": {}
      },
      {
        "name": "Amber E. Boydstun",
        "url": {}
      },
      {
        "name": "Emiliano Grossman",
        "url": {}
      },
      {
        "name": "Wouter van            Atteveldt",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRTextTools, glmnet, maxent, e1071, tm, ipred, caTools, randomForest, nnet, tree\nCRAN Task Views implied by cited packages\nMachineLearning, Environmetrics, NaturalLanguageProcessing, Survival, Cluster, Distributions, Econometrics, HighPerformanceComputing, Multivariate, Psychometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-002/",
    "title": "Generalized Simulated Annealing for Global Optimization: The GenSA Package",
    "description": "Many problems in statistics, finance, biology, pharmacology, physics, mathematics, eco nomics, and chemistry involve determination of the global minimum of multidimensional functions. R packages for different stochastic methods such as genetic algorithms and differential evolution have been developed and successfully used in the R community. Based on Tsallis statistics, the R package GenSA was developed for generalized simulated annealing to process complicated non-linear objective functions with a large number of local minima. In this paper we provide a brief introduction to the R package and demonstrate its utility by solving a non-convex portfolio optimization problem in finance and the Thomson problem in physics. GenSA is useful and can serve as a complementary tool to, rather than a replacement for, other widely used R packages for optimization.",
    "author": [
      {
        "name": "Yang Xiang",
        "url": {}
      },
      {
        "name": "Sylvain Gubian",
        "url": {}
      },
      {
        "name": "Brian Suomela",
        "url": {}
      },
      {
        "name": "Julia Hoeng",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nDEoptim, rgenoud, likelihood, dclone, subselect, GenSA\nCRAN Task Views implied by cited packages\nOptimization, HighPerformanceComputing, Bayesian, ChemPhys, gR, MachineLearning\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-003/",
    "title": "Multiple Factor Analysis for Contingency Tables in the FactoMineR Package",
    "description": "We present multiple factor analysis for contingency tables (MFACT) and its implementation in the FactoMineR package. This method, through an option of the MFA function, allows us to deal with multiple contingency or frequency tables, in addition to the categorical and quantitative multiple tables already considered in previous versions of the package. Thanks to this revised function, either a multiple contingency table or a mixed multiple table integrating quantitative, categorical and frequency data can be tackled. The FactoMineR package (Lê et al., 2008; Husson et al., 2011) offers the most commonly used principal component methods: principal component analysis (PCA), correspondence analysis (CA; Benzécri, 1973), multiple correspondence analysis (MCA; Lebart et al., 2006) and multiple factor analysis (MFA; Escofier and Pagès, 2008). Detailed presentations of these methods enriched by numerous examples can be consulted at the website http://factominer.free.fr/. An extension of the MFA function that considers contingency or frequency tables as proposed by Bécue-Bertaut and Pagès (2004, 2008) is detailed in this article. First, an example is presented in order to motivate the approach. Next, the mortality data used to illustrate the method are introduced. Then we briefly describe multiple factor analysis (MFA) and present the principles of its extension to contingency tables. A real example on mortality data illustrates the handling of the MFA function to analyse these multiple tables and, finally, conclusions are presented.",
    "author": [
      {
        "name": "Belchin Kostov",
        "url": {}
      },
      {
        "name": "Mónica Bécue-Bertaut",
        "url": {}
      },
      {
        "name": "François Husson",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nFactoMineR\nCRAN Task Views implied by cited packages\nMultivariate, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-004/",
    "title": "Hypothesis Tests for Multivariate Linear Models Using the car Package",
    "description": "The multivariate linear model is Y = X B + E (n×m) (n× p)( p×m) (n×m) The multivariate linear model can be fit with the lm function in R, where the left-hand side of the model comprises a matrix of response variables, and the right-hand side is specified exactly as for a univariate linear model (i.e., with a single response variable). This paper explains how to use the Anova and linearHypothesis functions in the car package to perform convenient hypothesis tests for parameters in multivariate linear models, including models for repeated-measures data.",
    "author": [
      {
        "name": "John Fox",
        "url": {}
      },
      {
        "name": "Michael Friendly",
        "url": {}
      },
      {
        "name": "Sanford Weisberg",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncar, lme4, nlme, survival, nnet, MASS, survey, heplots\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, Environmetrics, OfficialStatistics, Psychometrics, Finance, Multivariate, Pharmacokinetics, SpatioTemporal, Survival, Bayesian, ChemPhys, ClinicalTrials, Distributions, MachineLearning, NumericalMathematics, Robust, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-005/",
    "title": "osmar: OpenStreetMap and R",
    "description": "OpenStreetMap provides freely accessible and editable geographic data. The osmar package smoothly integrates the OpenStreetMap project into the R ecosystem. The osmar package provides infrastructure to access OpenStreetMap data from different sources, to enable working with the OSM data in the familiar R idiom, and to convert the data into objects based on classes provided by existing R packages. This paper explains the package’s concept and shows how to use it. As an application we present a simple navigation device.",
    "author": [
      {
        "name": "Manuel J. A. Eugster",
        "url": {}
      },
      {
        "name": "Thomas Schlesinger",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nosmar, osmar, OpenStreetMap, RgoogleMaps, ggmap, sp, igraph, geosphere, Rcpp\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, gR, Graphics, HighPerformanceComputing, NumericalMathematics, Optimization, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-006/",
    "title": "ftsa: An R Package for Analyzing Functional Time Series",
    "description": "Recent advances in computer recording and storing technology have tremendously increased the presence of functional data, whose graphical representation can be infinite-dimensional curve, image, or shape. When the same functional object is observed over a period of time, such data are known as functional time series. This article makes first attempt to describe several techniques (centered around functional principal component analysis) for modeling and forecasting functional time series from a computational aspect, using a readily-available R addon package. These methods are demonstrated using age-specific Australian fertility rate data from 1921 to 2006, and monthly sea surface temperature data from January 1950 to December 2011.",
    "author": [
      {
        "name": "Han Lin Shang",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nftsa\nCRAN Task Views implied by cited packages\nFunctionalData, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-007/",
    "title": "Statistical Software from a Blind Person's Perspective",
    "description": "Blind people have experienced access issues to many software applications since the advent of the Windows operating system; statistical software has proven to follow the rule and not be an exception. The ability to use R within minutes of download with next to no adaptation has opened doors for accessible production of statistical analyses for this author (himself blind) and blind students around the world. This article shows how little is required to make R the most accessible statistical software available today. There is any number of ramifications that this opportunity creates for blind students, especially in terms of their future research and employment prospects. There is potential for making R even better for blind users. The extensibility of R makes this possible through added functionality being made available in an add-on package called BrailleR. Functions in this package are intended to make graphical information available in text form.",
    "author": [
      {
        "name": "A. Jonathan R. Godfrey",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRcmdr, TeachingDemos, BrailleR, R2HTML\nCRAN Task Views implied by cited packages\nFinance, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-009/",
    "title": "QCA: A Package for Qualitative Comparative Analysis",
    "description": "We present QCA, a package for performing Qualitative Comparative Analysis (QCA). QCA is becoming increasingly popular with social scientists, but none of the existing software alternatives covers the full range of core procedures. This gap is now filled by QCA. After a mapping of the method’s diffusion, we introduce some of the package’s main capabilities, including the calibration of crisp and fuzzy sets, the analysis of necessity relations, the construction of truth tables and the derivation of complex, parsimonious and intermediate solutions.",
    "author": [
      {
        "name": "Alrik Thiem",
        "url": {}
      },
      {
        "name": "Adrian Duşa",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nQCA, QCA3, VennDiagram\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-010/",
    "title": "An Introduction to the EcoTroph R Package: Analyzing Aquatic Ecosystem Trophic Networks",
    "description": "Recent advances in aquatic ecosystem modelling have particularly focused on trophic network analysis through trophodynamic models. We present here a R package devoted to a recently developed model, EcoTroph. This model enables the analysis of aquatic ecological networks and the related impacts of fisheries. It was available through a plug-in in the well-known Ecopath with Ecosim software or through implementations in Excel sheets. The R package we developed simplifies the access to the EcoTroph model and offers a new interfacing between two widely used software, Ecopath and R.",
    "author": [
      {
        "name": "Mathieu Colléter",
        "url": {}
      },
      {
        "name": "Jérôme Guitton",
        "url": {}
      },
      {
        "name": "Didier Gascuel",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nEcoTroph, XML\nCRAN Task Views implied by cited packages\nWebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-011/",
    "title": "stellaR: A Package to Manage Stellar Evolution Tracks and Isochrones",
    "description": "We present the R package stellaR, which is designed to access and manipulate publicly available stellar evolutionary tracks and isochrones from the Pisa low-mass database. The procedures for extracting important stages in the evolution of a star from the database, for constructing isochrones from stellar tracks and for interpolating among tracks are discussed and demonstrated. Due to the advance in the instrumentation, nowadays astronomers can deal with a huge amount of high-quality observational data. In the last decade impressive improvements of spectroscopic and photometric observational capabilities made available data which stimulated the research in the glob ular clusters field. The theoretical effort of recovering the evolutionary history of the clusters benefits from the computation of extensive databases of stellar tracks and isochrones, such as Pietrinferni et al. (2006); Dotter et al. (2008); Bertelli et al. (2008). We recently computed a large data set of stellar tracks and isochrones, “The Pisa low-mass database” (Dell’Omodarme et al., 2012), with up to date physical and chemical inputs, and made available all the calculations to the astrophysical community at the Centre de Données astronomiques de Strasbourg (CDS)1 , a data center dedicated to the collection and worldwide distribution of astronomical data. In most databases, the management of the information and the extraction of the relevant evolu tionary properties from libraries of tracks and/or isochrones is the responsibility of the end users. Due to its extensive capabilities of data manipulation and analysis, however, R is an ideal choice for these tasks. Nevertheless R is not yet well known in astrophysics; up to December 2012 only seven astronomical or astrophysical-oriented packages have been published on CRAN (see the CRAN Task View Chemometrics and Computational Physics). The package stellaR (Dell’Omodarme and Valle, 2012) is an effort to make available to the astro physical community a basic tool set with the following capabilities: retrieve the required calculations from CDS; plot the information in a suitable form; construct by interpolation tracks or isochrones of compositions different to the ones available in the database; construct isochrones for age not included in the database; extract relevant evolutionary points from tracks or isochrones.",
    "author": [
      {
        "name": "Matteo Dell’Omodarme",
        "url": {}
      },
      {
        "name": "Giada Valle",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nstellaR\nCRAN Task Views implied by cited packages\nChemPhys\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-012/",
    "title": "Let Graphics Tell the Story - Datasets in R",
    "description": "Graphics are good for showing the information in datasets and for complementing modelling. Sometimes graphics show information models miss, sometimes graphics help to make model results more understandable, and sometimes models show whether information from graphics has statistical support or not. It is the interplay of the two approaches that is valuable. Graphics could be used a lot more in R examples and we explore this idea with some datasets available in R packages.",
    "author": [
      {
        "name": "Antony Unwin",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMASS, granova, ggplot2, vcd, knitr, HH\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, SocialSciences, ClinicalTrials, Distributions, Econometrics, Environmetrics, ExperimentalDesign, NumericalMathematics, Pharmacokinetics, Phylogenetics, Psychometrics, ReproducibleResearch, Robust\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-013/",
    "title": "Estimating Spatial Probit Models in R",
    "description": "In this article we present the Bayesian estimation of spatial probit models in R and provide an implementation in the package spatialprobit. We show that large probit models can be estimated with sparse matrix representations and Gibbs sampling of a truncated multivariate normal distribution with the precision matrix. We present three examples and point to ways to achieve further performance gains through parallelization of the Markov Chain Monte Carlo approach.",
    "author": [
      {
        "name": "Stefan Wilhelm",
        "url": {}
      },
      {
        "name": "Miguel Godinho de Matos",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspBayes, spatial, geoR, sgeostat, spdep, sphet, sna, network, Matrix, sparseM, spatialprobit, McSpatial, LearnBayes, tmvtnorm, mvtnorm, igraph\nCRAN Task Views implied by cited packages\nSpatial, Bayesian, Distributions, Econometrics, SocialSciences, gR, Multivariate, Optimization, SpatioTemporal, Finance, Graphics, NumericalMathematics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-014/",
    "title": "ggmap: Spatial Visualization with ggplot2",
    "description": "In spatial statistics the ability to visualize data and models superimposed with their basic social landmarks and geographic context is invaluable. ggmap is a new tool which enables such visualization by combining the spatial information of static maps from Google Maps, OpenStreetMap, Stamen Maps or CloudMade Maps with the layered grammar of graphics implementation of ggplot2. In addition, several new utility functions are introduced which allow the user to access the Google Geocoding, Distance Matrix, and Directions APIs. The result is an easy, consistent and modular framework for spatial graphics with several convenient tools for spatial data analysis.",
    "author": [
      {
        "name": "David Kahle",
        "url": {}
      },
      {
        "name": "Hadley Wickham",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsp, RgoogleMaps, ggplot2, ggmap, maps, maptools, DeducerSpatial, plyr, rjson, osmar\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, Graphics, Phylogenetics, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-015/",
    "title": "mpoly: Multivariate Polynomials in R",
    "description": "The mpoly package is a general purpose collection of tools for symbolic computing with multivariate polynomials in R. In addition to basic arithmetic, mpoly can take derivatives of polyno mials, compute Gröbner bases of collections of polynomials, and convert polynomials into a functional form to be evaluated. Among other things, it is hoped that mpoly will provide an R-based foundation for the computational needs of algebraic statisticians.",
    "author": [
      {
        "name": "David Kahle",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmpoly, multipol, polynom, PolynomF, rSymPy\nCRAN Task Views implied by cited packages\nNumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-016/",
    "title": "beadarrayFilter: An R Package to Filter Beads",
    "description": "Microarrays enable the expression levels of thousands of genes to be measured simultane ously. However, only a small fraction of these genes are expected to be expressed under different experimental conditions. Nowadays, filtering has been introduced as a step in the microarray pre processing pipeline. Gene filtering aims at reducing the dimensionality of data by filtering redundant features prior to the actual statistical analysis. Previous filtering methods focus on the Affymetrix platform and can not be easily ported to the Illumina platform. As such, we developed a filtering method for Illumina bead arrays. We developed an R package, beadarrayFilter, to implement the latter method. In this paper, the main functions in the package are highlighted and using many examples, we illustrate how beadarrayFilter can be used to filter bead arrays.",
    "author": [
      {
        "name": "Anyiawung Chiara Forcheh",
        "url": {}
      },
      {
        "name": "Geert Verbeke",
        "url": {}
      },
      {
        "name": "Adetayo Kasim",
        "url": {}
      },
      {
        "name": "Dan Lin",
        "url": {}
      },
      {
        "name": "Ziv Shkedy",
        "url": {}
      },
      {
        "name": "Willem Talloen",
        "url": {}
      },
      {
        "name": "           Hinrich W.H. Göhlmann",
        "url": {}
      },
      {
        "name": "Lieven Clement",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\ntitle: ‘beadarrayFilter: An R Package to Filter Beads’\ndescription: Microarrays enable the expression levels of thousands of genes to be\nmeasured simultane ously. However, only a small fraction of these genes are expected\nto be expressed under different experimental conditions. Nowadays, filtering has\nbeen introduced as a step in the microarray pre processing pipeline. Gene filtering\naims at reducing the dimensionality of data by filtering redundant features prior\nto the actual statistical analysis. Previous filtering methods focus on the Affymetrix\nplatform and can not be easily ported to the Illumina platform. As such, we developed\na filtering method for Illumina bead arrays. We developed an R package, beadarrayFilter,\nto implement the latter method. In this paper, the main functions in the package\nare highlighted and using many examples, we illustrate how beadarrayFilter can be\nused to filter bead arrays.\nauthor:\n- Anyiawung Chiara Forcheh\n- Geert Verbeke\n- Adetayo Kasim\n- Dan Lin\n- Ziv Shkedy\n- Willem Talloen\n- ’ Hinrich W.H. Göhlmann’\n- Lieven Clement\ndate: ‘2013-06-03’\ndate_received: ‘2012-07-06’\njournal:\ntitle: The R Journal\nissn: 2073-4859\nfirstpage: 171\nlastpage: 180\nvolume: 5\nissue: 1\nslug: RJ-2013-016\npackages:\ncran: beadarrayFilter\nbioc: ~\npreview: preview.png\nCTV: []\noutput:\ndistill::distill_article:\nself_contained: no\ntoc: no\nlegacy_pdf: yes\npdf_url: RJ-2013-016.pdf\ncitation_url: https://doi.org/10.32614/RJ-2013-016\ndoi: 10.32614/RJ-2013-016\ncreative_commons: CC BY\ncsl: /home/mitchell/R/x86_64-pc-linux-gnu-library/4.1/rjtools/rjournal.csl\n\n\n\nCRAN packages used\nbeadarrayFilter\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-017/",
    "title": "Fast Pure R Implementation of GEE: Application of the Matrix Package",
    "description": "Generalized estimating equation solvers in R only allow for a few pre-determined options for the link and variance functions. We provide a package, geeM, which is implemented entirely in R and allows for user specified link and variance functions. The sparse matrix representations provided in the Matrix package enable a fast implementation. To gain speed, we make use of analytic inverses of the working correlation when possible and a trick to find quick numeric inverses when an analytic inverse is not available. Through three examples, we demonstrate the speed of geeM, which is not much worse than C implementations like geepack and gee on small data sets and faster on large data sets.",
    "author": [
      {
        "name": "Lee S. McDaniel",
        "url": {}
      },
      {
        "name": "Nicholas C. Henderson",
        "url": {}
      },
      {
        "name": "Paul J. Rathouz",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngeepack, gee, geeM, Matrix\nCRAN Task Views implied by cited packages\nEconometrics, SocialSciences, Multivariate, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-018/",
    "title": "RcmdrPlugin.temis, a Graphical Integrated Text Mining Solution in R",
    "description": "We present the package RcmdrPlugin.temis, a graphical user interface for user-friendly text mining in R. Built as a plug-in to the R Commander provided by the Rcmdr package, it brings together several existing packages and provides new features streamlining the process of importing, managing and analyzing a corpus, in addition to saving results and plots to a report file. Beyond common file formats, automated import of corpora from the Dow Jones Factiva content provider and Twitter is supported. Featured analyses include vocabulary and dissimilarity tables, terms frequencies, terms specific of levels of a variable, term co-occurrences, time series, correspondence analysis and hierarchical clustering.",
    "author": [
      {
        "name": "Milan Bouchet-Valat",
        "url": {}
      },
      {
        "name": "Gilles Bastin",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntm, RcmdrPlugin.temis, Rcmdr, RODBC, tm.plugin.factiva, twitteR, SnowballC, zoo, lattice, ca, R2HTML\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, Finance, Multivariate, Econometrics, Environmetrics, Graphics, HighPerformanceComputing, Pharmacokinetics, Psychometrics, ReproducibleResearch, TimeSeries, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-019/",
    "title": "Possible Directions for Improving Dependency Versioning in R",
    "description": "One of the most powerful features of R is its infrastructure for contributed code. The built-in package manager and complementary repositories provide a great system for development and exchange of code, and have played an important role in the growth of the platform towards the de-facto standard in statistical computing that it is today. However, the number of packages on CRAN and other repositories has increased beyond what might have been foreseen, and is revealing some limitations of the current design. One such problem is the general lack of dependency versioning in the infrastructure. This paper explores this problem in greater detail, and suggests approaches taken by other open source communities that might work for R as well. Three use cases are defined that exemplify the issue, and illustrate how improving this aspect of package management could increase reliability while supporting further growth of the R community.",
    "author": [
      {
        "name": "Jeroen Ooms",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCRAN\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-010/",
    "title": "frailtyHL: A Package for Fitting Frailty Models with H-likelihood",
    "description": "We present the frailtyHL package for fitting semi-parametric frailty models using h likelihood. This package allows lognormal or gamma frailties for random-effect distribution, and it fits shared or multilevel frailty models for correlated survival data. Functions are provided to format and summarize the frailtyHL results. The estimates of fixed effects and frailty parameters and their standard errors are calculated. We illustrate the use of our package with three well-known data sets and compare our results with various alternative R-procedures.",
    "author": [
      {
        "name": "Il Do Ha",
        "url": {}
      },
      {
        "name": "Maengseok Noh",
        "url": {}
      },
      {
        "name": "Youngjo Lee",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfrailtyHL, survival, coxme, phmm, frailtypack, hglm, HGLMMM, dhglm\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-011/",
    "title": "influence.ME: Tools for Detecting Influential Data in Mixed Effects Models",
    "description": "influence.ME provides tools for detecting influential data in mixed effects models. The application of these models has become common practice, but the development of diagnostic tools has lagged behind. influence.ME calculates standardized measures of influential data for the point estimates of generalized mixed effects models, such as DFBETAS, Cook’s distance, as well as percentile change and a test for changing levels of significance. influence.ME calculates these measures of influence while accounting for the nesting structure of the data. The package and measures of influential data are introduced, a practical example is given, and strategies for dealing with influential data are suggested. The application of mixed effects regression models has become common practice in the field of social sciences. As used in the social sciences, mixed effects regression models take into account that observations on individual respondents are nested within higher-level groups such as schools, classrooms, states, and countries (Snijders and Bosker, 1999), and are often referred to as multilevel regression models. Despite these models’ increasing popularity, diagnostic tools to evaluate fitted models lag behind. We introduce influence.ME (Nieuwenhuis, Pelzer, and te Grotenhuis, 2012), an R-package that provides tools for detecting influential cases in mixed effects regression models estimated with lme4 (Bates and Maechler, 2010). It is commonly accepted that tests for influential data should be performed on regression models, especially when estimates are based on a relatively small number of cases. However, most existing procedures do not account for the nesting structure of the data. As a result, these existing procedures fail to detect that higher-level cases may be influential on estimates of variables measured at specifically that level. In this paper, we outline the basic rationale on detecting influential data, describe standardized measures of influence, provide a practical example of the analysis of students in 23 schools, and discuss strategies for dealing with influential cases. Testing for influential cases in mixed effects regression models is important, because influential data negatively influence the statistical fit and generalizability of the model. In social science applications of mixed models the testing for influential data is especially important, since these models are frequently based on large numbers of observations at the individual level while the number of higher level groups is relatively small. For instance, Van der Meer, te Grotenhuis, and Pelzer (2010) were unable to find any country-level comparative studies involving more than 54 countries. With such a relatively low number of countries, a single country can easily be overly influential on the parameter estimates of one or more of the country-level variables.",
    "author": [
      {
        "name": "Rense Nieuwenhuis",
        "url": {}
      },
      {
        "name": "Manfred te Grotenhuis",
        "url": {}
      },
      {
        "name": "Ben Pelzer",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-012/",
    "title": "The crs Package: Nonparametric Regression Splines for Continuous and Categorical Predictors",
    "description": "A new package crs is introduced for computing nonparametric regression (and quantile) splines in the presence of both continuous and categorical predictors. B-splines are employed in the regression model for the continuous predictors and kernel weighting is employed for the categorical predictors. We also develop a simple R interface to NOMAD, which is a mixed integer optimization solver used to compute optimal regression spline solutions.",
    "author": [
      {
        "name": "Zhenghua Nie",
        "url": {}
      },
      {
        "name": "Jeffrey S. Racine",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncrs, SemiPar, mgcv, gss, gam, MASS, rgl\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, Environmetrics, Multivariate, Bayesian, Distributions, Graphics, NumericalMathematics, Optimization, Pharmacokinetics, Psychometrics, Robust, SpatioTemporal, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-013/",
    "title": "Debugging grid Graphics",
    "description": "A graphical scene that has been produced using the grid graphics package consists of grobs (graphical objects) and viewports. This article describes functions that allow the exploration and inspection of the grobs and viewports in a grid scene, including several functions that are available in a new package called gridDebug. The ability to explore the grobs and viewports in a grid scene is useful for adding more drawing to a scene that was produced using grid and for understanding and debugging the grid code that produced a scene.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      },
      {
        "name": "Velvet Ly",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nggplot2, gridDebug, graph, Rgraphviz, gridGraphviz, gridSVG, playwith\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-014/",
    "title": "Rfit: Rank-based Estimation for Linear Models",
    "description": "In the nineteen seventies, Jurečková and Jaeckel proposed rank estimation for linear models. Since that time, several authors have developed inference and diagnostic methods for these estimators. These rank-based estimators and their associated inference are highly efficient and are robust to outliers in response space. The methods include estimation of standard errors, tests of general linear hypotheses, confidence intervals, diagnostic procedures including studentized residuals, and measures of influential cases. We have developed an R package, Rfit, for computing of these robust procedures. In this paper we highlight the main features of the package. The package uses standard linear model syntax and includes many of the main inference and diagnostic functions.",
    "author": [
      {
        "name": "John D. Kloke",
        "url": {}
      },
      {
        "name": "Joseph W. McKean",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRfit, Rfit, MASS, quantreg\nCRAN Task Views implied by cited packages\nEconometrics, Environmetrics, Robust, SocialSciences, Distributions, Multivariate, NumericalMathematics, Optimization, Pharmacokinetics, Psychometrics, ReproducibleResearch, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-015/",
    "title": "Graphical Markov Models with Mixed Graphs in R",
    "description": "In this paper we provide a short tutorial illustrating the new functions in the package ggm that deal with ancestral, summary and ribbonless graphs. These are mixed graphs (containing three types of edges) that are important because they capture the modified independence structure after marginalisation over, and conditioning on, nodes of directed acyclic graphs. We provide functions to verify whether a mixed graph implies that A is independent of B given C for any disjoint sets of nodes and to generate maximal graphs inducing the same independence structure of non-maximal graphs. Finally, we provide functions to decide on the Markov equivalence of two graphs with the same node set but different types of edges.",
    "author": [
      {
        "name": "Kayvan Sadeghi",
        "url": {}
      },
      {
        "name": "Giovanni M. Marchetti",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngRain, ggm, ggm, igraph, gRbase\nCRAN Task Views implied by cited packages\ngR, Graphics, Optimization, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-016/",
    "title": "What's in a Name?",
    "description": "Any shape that is drawn using the grid graphics package can have a name associated with it. If a name is provided, it is possible to access, query, and modify the shape after it has been drawn. These facilities allow for very detailed customisations of plots and also for very general transformations of plots that are drawn by packages based on grid.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-017/",
    "title": "It's Not What You Draw,It's What You Don't Draw",
    "description": "The R graphics engine has new support for drawing complex paths via the functions polypath() and grid.path(). This article explains what is meant by a complex path and demonstrates the usefulness of complex paths in drawing non-trivial shapes, logos, customised data symbols, and maps. One of the design goals of the R graphics system is to allow fine control over the small details of plots. One way that the R graphics system does this is by providing access to low-level generic graphics facilities, such as the ability to draw basic shapes and the ability to control apparently esoteric, but still useful, features of those shapes, such as the line end style used for drawing lines. In R version 2.12.0, another low-level graphics facility was added to R: the ability to draw complex paths (not just polygons). This article describes this new facility and presents some examples that show how complex paths might be useful.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngrImport, maptools, maps\nCRAN Task Views implied by cited packages\nSpatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-018/",
    "title": "The State of Naming Conventions in R",
    "description": "Most programming language communities have naming conventions that are generally agreed upon, that is, a set of rules that governs how functions and variables are named. This is not the case with R, and a review of unofficial style guides and naming convention usage on CRAN shows that a number of different naming conventions are currently in use. Some naming conventions are, however, more popular than others and as a newcomer to the R community or as a developer of a new package this could be useful to consider when choosing what naming convention to adopt.",
    "author": [
      {
        "name": "Rasmus Bååth",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-001/",
    "title": "Analysing Seasonal Data",
    "description": "Many common diseases, such as the flu and cardiovascular disease, increase markedly in winter and dip in summer. These seasonal patterns have been part of life for millennia and were first noted in ancient Greece by both Hippocrates and Herodotus. Recent interest has focused on climate change, and the concern that seasons will become more extreme with harsher winter and summer weather. We describe a set of R functions designed to model seasonal patterns in disease. We illustrate some simple descriptive and graphical methods, a more complex method that is able to model non-stationary patterns, and the case-crossover to control for seasonal confounding. In this paper we illustrate some of the functions of the season package (Barnett et al., 2012), which contains a range of functions for analysing seasonal health data. We were motivated by the great interest in seasonality found in the health literature, and the relatively small number of seasonal tools in R (or other software packages). The existing seasonal tools in R are: • the baysea function of the timsac package and the decompose and stl functions of the stats package for decomposing a time series into a trend and season; • the dynlm function of the dynlm package and the ssm function of the sspir package for fitting dynamic linear models with optional seasonal components; • the arima function of the stats package and the Arima function of the forecast package for fitting seasonal components as part of an autoregressive integrated moving average (ARIMA) model; and • the bfast package for detecting breaks in a seasonal pattern. These tools are all useful, but most concern decomposing equally spaced time series data. Our package includes models that can be applied to seasonal patterns in unequally spaced data. Such data are common in observational studies when the timing of responses cannot be controlled (e.g. for a postal survey). In the health literature much of the analysis of seasonal data uses simple methods such as com paring rates of disease by month or using a cosinor regression model, which assumes a sinusoidal seasonal pattern. We have created functions for these simple, but often very effective analyses, as we describe below. More complex seasonal analyses examine non-stationary seasonal patterns that change over time. Changing seasonal patterns in health are currently of great interest as global warming is predicted to make seasonal changes in the weather more extreme. Hence there is a need for statistical tools that can estimate whether a seasonal pattern has become more extreme over time or whether its phase has changed. Ours is also the first R package that includes the case-crossover, a useful method for controlling for seasonality. This paper illustrates just some of the functions of the season package. We show some descriptive functions that give simple means or plots, and functions whose goal is inference based on generalised linear models. The package was written as a companion to a book on seasonal analysis by Barnett and Dobson (2010), which contains further details on the statistical methods and R code.",
    "author": [
      {
        "name": "Adrian G Barnett",
        "url": {}
      },
      {
        "name": "Peter Baker",
        "url": {}
      },
      {
        "name": "Annette J Dobson",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nseason, timsac, dynlm, sspir, forecast, bfast\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, Econometrics, Environmetrics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-002/",
    "title": "MARSS: Multivariate Autoregressive State-space Models for Analyzing Time-series Data",
    "description": "MARSS is a package for fitting multivariate autoregressive state-space models to time-series data. The MARSS package implements state-space models in a maximum likelihood framework. The core functionality of MARSS is based on likelihood maximization using the Kalman filter/smoother, combined with an EM algorithm. To make comparisons with other packages available, parameter estimation is also permitted via direct search routines available in ’optim’. The MARSS package allows data to contain missing values and allows a wide variety of model structures and constraints to be specified (such as fixed or shared parameters). In addition to model-fitting, the package provides bootstrap routines for simulating data and generating confidence intervals, and multiple options for calculating model selection criteria (such as AIC).",
    "author": [
      {
        "name": "Elizabeth E. Holmes",
        "url": {}
      },
      {
        "name": "Eric J. Ward",
        "url": {}
      },
      {
        "name": "Kellie Wills",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMARSS, sspir, dlm, dse, KFAS, FKF\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, Bayesian, Environmetrics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-003/",
    "title": "openair - Data Analysis Tools for the Air Quality Community",
    "description": "The openair package contains data analysis tools for the air quality community. This paper provides an overview of data importers, main functions, and selected utilities and workhorse functions within the package and the function output class, as of package version 0.4-14. It is intended as an explanation of the rationale for the package and a technical description for those wishing to work more interactively with the main functions or develop additional functions to support ‘higher level’ use of openair and R. Large volumes of air quality data are routinely collected for regulatory purposes, but few of those in local authorities and government bodies tasked with this responsibility have the time, expertise or funds to comprehensively analyse this potential resource (Chow and Watson, 2008). Furthermore, few of these institutions can routinely access the more powerful statistical methods typically required to make the most effective use of such data without a suite of often expensive and niche-application proprietary software products. This in turn places large cost and time burdens on both these institutions and others (e.g. academic or commercial) wishing to contribute to this work. In addition, such collaborative working practices can also become highly restricted and polarised if data analysis undertaken by one partner cannot be validated or replicated by another because they lack access to the same licensed products. Being freely distributed under general licence, R has the obvious potential to act as a common platform for those routinely collecting and archiving data and the wider air quality community. This potential has already been proven in several other research areas, and commonly cited ex amples include the Bioconductor project (Gentleman et al, 2004) and the Epitools collaboration (http://www.medepi.com/epitools). However, what is perhaps most inspiring is the degree of trans parency that has been demonstrated by the recent public analysis of climate change data in R and as sociated open debate (http://chartsgraphs.wordpress.com/category/r-climate-data-analysis tool/). Anyone affected by a policy decision, could potentially have unlimited access to scrutinise both the tools and data used to shape that decision.",
    "author": [
      {
        "name": "Karl Ropkins",
        "url": {}
      },
      {
        "name": "David C. Carslaw",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nopenair, openair, lattice, latticeExtra, hexbin, grDevices, mgcv, stats, grDevices, RColorBrewer\nCRAN Task Views implied by cited packages\nGraphics, Environmetrics, SpatioTemporal, Bayesian, Econometrics, Multivariate, Pharmacokinetics, SocialSciences, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-004/",
    "title": "Foreign Library Interface",
    "description": "We present an improved Foreign Function Interface (FFI) for R to call arbitary native functions without the need for C wrapper code. Further we discuss a dynamic linkage framework for binding standard C libraries to R across platforms using a universal type information format. The package rdyncall comprises the framework and an initial repository of cross-platform bindings for standard libraries such as (legacy and modern) OpenGL, the family of SDL libraries and Expat. The package enables system-level programming using the R language; sample applications are given in the article. We outline the underlying automation tool-chain that extracts cross-platform bindings from C headers, making the repository extendable and open for library developers.",
    "author": [
      {
        "name": "Daniel Adler",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrdyncall, Rffi\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-005/",
    "title": "Vdgraph: A Package for Creating Variance Dispersion Graphs",
    "description": "This article introduces the package Vdgraph that is used for making variance dispersion graphs of response surface designs. The package includes functions that make the variance dispersion graph of one design or compare variance dispersion graphs of two designs, which are stored in data frames or matrices. The package also contains several minimum run response surface designs (stored as matrices) that are not available in other R packages.",
    "author": [
      {
        "name": "John Lawson",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nVdgraph, rsm\nCRAN Task Views implied by cited packages\nExperimentalDesign\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-006/",
    "title": "xgrid and R: Parallel Distributed Processing Using Heterogeneous Groups of Apple Computers",
    "description": "The Apple Xgrid system provides access to groups (or grids) of computers that can be used to facilitate parallel processing. We describe the xgrid package which facilitates access to this system to undertake independent simulations or other long-running jobs that can be divided into replicate runs within R. Detailed examples are provided to demonstrate the interface, along with results from a simulation study of the performance gains using a variety of grids. Use of the grid for “embarassingly parallel” independent jobs has the potential for major speedups in time to completion. Appendices provide guidance on setting up the workflow, utilizing add-on packages, and constructing grids using existing machines.",
    "author": [
      {
        "name": "Sarah C. Anoke",
        "url": {}
      },
      {
        "name": "Yuting Zhao",
        "url": {}
      },
      {
        "name": "Rafael Jaeger",
        "url": {}
      },
      {
        "name": "Nicholas J. Horton",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nGridR, Rmpi, snow, multicore, xgrid, runjags, poLCA\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Bayesian, Cluster, Multivariate, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-007/",
    "title": "maxent: An R Package for Low-memory Multinomial Logistic Regression with Support for Semi-automated Text Classification",
    "description": "maxent is a package with tools for data classification using multinomial logistic regression, also known as maximum entropy. The focus of this maximum entropy classifier is to minimize memory consumption on very large datasets, particularly sparse document-term matrices represented by the tm text mining package.",
    "author": [
      {
        "name": "Timothy P. Jurka",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nnnet, mlogit, maxent, Rcpp, tm, Matrix, slam, SparseM\nCRAN Task Views implied by cited packages\nEconometrics, NumericalMathematics, HighPerformanceComputing, Multivariate, NaturalLanguageProcessing, SocialSciences, MachineLearning\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-008/",
    "title": "Sumo: An Authenticating Web Application with an Embedded R Session",
    "description": "Sumo is a web application intended as a template for developers. It is distributed as a Java ‘war’ file that deploys automatically when placed in a Servlet container’s ‘webapps’ directory. If a user supplies proper credentials, Sumo creates a session-specific Secure Shell connection to the host and a user-specific R session over that connection. Developers may write dynamic server pages that make use of the persistent R session and user-specific file space. The supplied example plots a data set conditional on preferences indicated by the user; it also displays some static text. A companion server page allows the user to interact directly with the R session. Sumo’s novel feature set complements previous efforts to supply R functionality over the internet.",
    "author": [
      {
        "name": "Timothy T. Bergsma",
        "url": {}
      },
      {
        "name": "Michael S. Smith",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRpad, brew, R.rsp, Rook, Rserve, R2HTML\nCRAN Task Views implied by cited packages\nReproducibleResearch, WebTechnologies, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-009/",
    "title": "Who Did What? The Roles of R Package Authors and How to Refer to Them",
    "description": "Computational infrastructure for representing persons and citations has been available in R for several years, but has been restructured through enhanced classes \"person\" and \"bibentry\" in recent versions of R. The new features include support for the specification of the roles of package authors (e.g. maintainer, author, contributor, translator, etc.) and more flexible formatting/printing tools among various other improvements. Here, we introduce the new classes and their methods and indicate how this functionality is employed in the management of R packages. Specifically, we show how the authors of R packages can be specified along with their roles in package ‘DESCRIPTION’ and/or ‘CITATION’ files and the citations produced from it. R packages are the result of scholarly activity and as such constitute scholarly resources which must be clearly identifiable for the respective scientific communities and, more generally, today’s information society. In particular, packages published by standard repositories can be regarded as reliable sources which can and should be referenced (i.e. cited) by scientific works such as articles or other packages. This requires conceptual frameworks and computational infrastructure for describing bibliographic resources, general enough to encompass the needs of communities with an interest in R. These needs include support for exporting bibliographic metadata in standardized formats such as BIBTEX (Berry and Patashnik, 2010), but also facilitating bibliometric analyses and investigations of the social fabric underlying the creation of scholarly knowledge. The latter requires a richer vocabulary than commonly employed by reference management software such as BIBTEX, identifying persons and their roles in relation to bibliographic resources. For example, a thesis typically has an author and advisors. Software can have an (original) author and a translator to another language (such as from S to R). The maintainer of an R package is not necessarily an author. In this paper, we introduce the base R infrastructure (as completely available in R since version 2.14.0) for representing and manipulating such scholarly data: objects of class \"person\" (hereafter, per son objects) hold information about persons, possibly including their roles; objects of class \"bibentry\" (hereafter, bibentry objects) hold bibliographic information in enhanced BIBTEX style, ideally using person objects when referring to persons (such as authors or editors). Furthermore, we indicate how this functionality is employed in the management of R packages, in particular in their ‘CITATION’ and ‘DESCRIPTION’ files.",
    "author": [
      {
        "name": "Kurt Hornik",
        "url": {}
      },
      {
        "name": "Duncan Murdoch",
        "url": {}
      },
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nboot, bibtex, XML\nCRAN Task Views implied by cited packages\nEconometrics, Optimization, ReproducibleResearch, SocialSciences, Survival, TimeSeries, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-011/",
    "title": "Creating and Deploying an Application with (R)Excel and R",
    "description": "We present some ways of using R in Excel and build an example application using the package rpart. Starting with simple interactive use of rpart in Excel, we eventually package the code into an Excel-based application, hiding all details (including R itself) from the end user. In the end, our application implements a service-oriented architecture (SOA) with a clean separation of presentation and computation layer.",
    "author": [
      {
        "name": "Thomas Baier",
        "url": {}
      },
      {
        "name": "Erich Neuwirth",
        "url": {}
      },
      {
        "name": "Michele De Meo",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrpart\nCRAN Task Views implied by cited packages\nEnvironmetrics, MachineLearning, Multivariate, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-012/",
    "title": "glm2: Fitting Generalized Linear Models with Convergence Problems",
    "description": "The R function glm uses step-halving to deal with certain types of convergence problems when using iteratively reweighted least squares to fit a generalized linear model. This works well in some circumstances but non-convergence remains a possibility, particularly with a non-standard link function. In some cases this is because step-halving is never invoked, despite a lack of convergence. In other cases step-halving is invoked but is unable to induce convergence. One remedy is to impose a stricter form of step-halving than is currently available in glm, so that the deviance is forced to decrease in every iteration. This has been implemented in the glm2 function available in the glm2 package. Aside from a modified computational algorithm, glm2 operates in exactly the same way as glm and provides improved convergence properties. These improvements are illustrated here with an identity link Poisson model, but are also relevant in other contexts.",
    "author": [
      {
        "name": "Ian C. Marschner",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nglm2\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-013/",
    "title": "Implementing the Compendium Concept with Sweave and DOCSTRIP",
    "description": "This article suggests an implementation of the compendium concept by combining Sweave and the LATEX literate programming environment DOCSTRIP.",
    "author": [
      {
        "name": "Michael Lundholm",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-014/",
    "title": "Watch Your Spelling!",
    "description": "We discuss the facilities in base R for spell checking via Aspell, Hunspell or Ispell, which are useful in particular for conveniently checking the spelling of natural language texts in package Rd files and vignettes. Spell checking performance is illustrated using the Rd files in package stats. This example clearly indicates the need for a domain-specific statistical dictionary. We analyze the results of spell checking all Rd files in all CRAN packages and show how these can be employed for building such a dictionary.",
    "author": [
      {
        "name": "Kurt Hornik",
        "url": {}
      },
      {
        "name": "Duncan Murdoch",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-015/",
    "title": "Ckmeans.1d.dp: Optimal k-means Clustering in One Dimension by Dynamic Programming",
    "description": "The heuristic k-means algorithm, widely used for cluster analysis, does not guarantee optimality. We developed a dynamic programming algorithm for optimal one-dimensional clustering. The algorithm is implemented as an R package called Ckmeans.1d.dp. We demonstrate its advantage in optimality and runtime over the standard iterative k-means algorithm.",
    "author": [
      {
        "name": "Haizhou Wang",
        "url": {}
      },
      {
        "name": "Mingzhou Song",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCkmeans.1d.dp\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-016/",
    "title": "Nonparametric Goodness-of-Fit Tests for Discrete Null Distributions",
    "description": "Methodology extending nonparametric goodness-of-fit tests to discrete null distributions has existed for several decades. However, modern statistical software has generally failed to provide this methodology to users. We offer a revision of R’s ks.test() function and a new cvm.test() function that fill this need in the R language for two of the most popular nonparametric goodness-of-fit tests. This paper describes these contributions and provides examples of their usage. Particular attention is given to various numerical issues that arise in their implementation.",
    "author": [
      {
        "name": "Taylor B. Arnold",
        "url": {}
      },
      {
        "name": "John W. Emerson",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndgof, nortest, ADGofTest, CvM2SL1Test, CvM2SL2Test, cramer\nCRAN Task Views implied by cited packages\nMultivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-017/",
    "title": "Using the Google Visualisation API with R",
    "description": "The googleVis package provides an interface between R and the Google Visualisation API to create interactive charts which can be embedded into web pages. The best known of these charts is probably the Motion Chart, popularised by Hans Rosling in his TED talks. With the googleVis package users can easily create web pages with interactive charts based on R data frames and display them either via the local R HTTP help server or within their own sites.",
    "author": [
      {
        "name": "Markus Gesmann",
        "url": {}
      },
      {
        "name": "Diego de Castillo",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrsp, googleVis, rjsonio, brew\nCRAN Task Views implied by cited packages\nReproducibleResearch, SpatioTemporal, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-018/",
    "title": "GrapheR: a Multiplatform GUI for Drawing Customizable Graphs in R",
    "description": "This article presents GrapheR, a Graphical User Interface allowing the user to draw customiz able and high-quality graphs without knowing any R commands. Six kinds of graph are available: histograms, box-and-whisker plots, bar plots, pie charts, curves and scatter plots. The complete process is described with the examples of a bar plot and a scatter plot illustrating the legendary puzzle of African and European swallows’ migrations.",
    "author": [
      {
        "name": "Maxime Hervé",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nJGR, playwith, GrapheR\nCRAN Task Views implied by cited packages\nGraphics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-019/",
    "title": "rainbow: An R Package for Visualizing Functional Time Series",
    "description": "Recent advances in computer technology have tremendously increased the use of functional data, whose graphical representation can be infinite-dimensional curves, images or shapes. This article describes four methods for visualizing functional time series using an R add-on package. These methods are demonstrated using age-specific Australian fertility data from 1921 to 2006 and monthly sea surface temperatures from January 1950 to December 2006.",
    "author": [
      {
        "name": "Han Lin Shang",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-020/",
    "title": "Portable C++ for R Packages",
    "description": "Package checking errors are more common on Solaris than Linux. In many cases, these errors are due to non-portable C++ code. This article reviews some commonly recurring problems in C++ code found in R packages and suggests solutions.",
    "author": [
      {
        "name": "Martyn Plummer",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRcpp\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-001/",
    "title": "Rmetrics - timeDate Package",
    "description": "The management of time and holidays can prove crucial in applications that rely on historical data. A typical example is the aggregation of a data set recorded in different time zones and under dif ferent daylight saving time rules. Besides the time zone conversion function, which is well supported by default classes in R, one might need functions to handle special days or holidays. In this respect, the package timeDate enhances default date-time classes in R and brings new functionalities to time zone management and the creation of holiday calendars.",
    "author": [
      {
        "name": "Yohan Chalabi",
        "url": {}
      },
      {
        "name": "Martin Mächler",
        "url": {}
      },
      {
        "name": "Diethelm Würtz",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntimeDate, timeDate, timeDate, timeDate, timeDate, timeDate, timeDate, timeDate, timeDate\nCRAN Task Views implied by cited packages\nFinance, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-002/",
    "title": "testthat: Get Started with Testing",
    "description": "Software testing is important, but many of us don’t do it because it is frustrating and boring. testthat is a new testing framework for R that is easy learn and use, and integrates with your existing workflow. This paper shows how, with illustrations from existing packages.",
    "author": [
      {
        "name": "Hadley Wickham",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntestthat, RUnit, svUnit, stringr, lubridate\nCRAN Task Views implied by cited packages\nReproducibleResearch, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-003/",
    "title": "Content-Based Social Network Analysis of Mailing Lists",
    "description": "Social Network Analysis (SNA) provides tools to examine relationships between people. Text Mining (TM) allows capturing the text they produce in Web 2.0 applications, for example, however it neglects their social structure. This paper applies an approach to combine the two methods named “content-based SNA”. Using the R mailing lists, R-help and R-devel, we show how this combination can be used to describe people’s interests and to find out if authors who have similar interests actually communicate. We find that the expected positive relationship between sharing interests and communicating gets stronger as the centrality scores of authors in the communication networks increase.",
    "author": [
      {
        "name": "Angela Bohn",
        "url": {}
      },
      {
        "name": "Ingo Feinerer",
        "url": {}
      },
      {
        "name": "Kurt Hornik",
        "url": {}
      },
      {
        "name": "Patrick Mair",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntm.plugin.mail, car, tm, sna, igraph\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, Optimization, SocialSciences, Bayesian, Econometrics, Finance, gR, Graphics, HighPerformanceComputing, Multivariate, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-004/",
    "title": "The digitize Package: Extracting Numerical Data from Scatterplots",
    "description": "I present the small R package digitize, designed to extract data from scatterplots with a simple method and suited to small datasets. I present an application of this method to the extraction of data from a graph whose source is not available.",
    "author": [
      {
        "name": "Timothée Poisot",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndigitize, ReadImages\nCRAN Task Views implied by cited packages\nMetaAnalysis\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-005/",
    "title": "Differential Evolution with DEoptim",
    "description": "The R package DEoptim implements the Differential Evolution algorithm. This algorithm is an evolutionary technique similar to classic genetic algorithms that is useful for the solution of global optimization problems. In this note we provide an introduction to the package and demonstrate its utility for financial applications by solving a non-convex portfolio optimization problem.",
    "author": [
      {
        "name": "David Ardia",
        "url": {}
      },
      {
        "name": "Kris Boudt",
        "url": {}
      },
      {
        "name": "Peter Carl",
        "url": {}
      },
      {
        "name": "Katharine M. Mullen",
        "url": {}
      },
      {
        "name": "Brian G. Peterson",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nDEoptim, PortfolioAnalytics, quantmod, PerformanceAnalytics\nCRAN Task Views implied by cited packages\nFinance, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-006/",
    "title": "rworldmap : a new R package for mapping global data",
    "description": "rworldmap is a new package available on CRAN for mapping and visualisation of global data. The vision is to make the display of global data easier, to facilitate understanding and com munication. The initial concentration is on data referenced by country or grid due to the frequency of use of such data in global assessments. Tools to link data referenced by country (either name or code) to a map, and then to display the map are provided as are functions to map global gridded data. Country and gridded functions accept the same arguments to specify the nature of categories and colour and how legends are formatted. This package builds on the functionality of existing packages, particularly sp, maptools and fields. Example code is provided to produce maps, to link with the packages classInt, RColorBrewer and ncdf, and to plot examples of publicly available country and gridded data.",
    "author": [
      {
        "name": "Andy South",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-007/",
    "title": "Cryptographic Boolean Functions with R",
    "description": "A new package called boolfun is available for R users. The package provides tools to handle Boolean functions, in particular for cryptographic purposes. This document guides the user through some (code) examples and gives a feel of what can be done with the package.",
    "author": [
      {
        "name": "Frédéric Lafitte",
        "url": {}
      },
      {
        "name": "Dirk Van Heule",
        "url": {}
      },
      {
        "name": "Julien Van hamme",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nboolfun, R.oo, multipol\nCRAN Task Views implied by cited packages\nNumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-008/",
    "title": "Raster Images in R Graphics",
    "description": "The R graphics engine has new support for rendering raster images via the functions rasterImage() and grid.raster(). This leads to better scaling of raster images, faster rendering to screen, and smaller graphics files. Several examples of possible applications of these new features are described.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-009/",
    "title": "Probabilistic Weather Forecasting in R",
    "description": "This article describes two R packages for probabilistic weather forecasting, ensembleBMA, which offers ensemble postprocessing via Bayesian model averaging (BMA), and ProbForecastGOP, which implements the geostatistical output perturbation (GOP) method. BMA forecasting models use mixture distributions, in which each component corresponds to an ensemble member, and the form of the component distribution depends on the weather parameter (temperature, quantitative precipitation or wind speed). The model parameters are estimated from training data. The GOP technique uses geostatistical methods to produce probabilistic forecasts of entire weather fields for temperature or pressure, based on a single numerical forecast on a spatial grid. Both packages include functions for evaluating predictive performance, in addition to model fitting and forecasting.",
    "author": [
      {
        "name": "Chris Fraley",
        "url": {}
      },
      {
        "name": "Adrian Raftery",
        "url": {}
      },
      {
        "name": "Tilmann Gneiting",
        "url": {}
      },
      {
        "name": "McLean Sloughter",
        "url": {}
      },
      {
        "name": "Veronica Berrocal",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nensembleBMA, chron, fields, maps, ProbForecastGOP, RandomFields, fields\nCRAN Task Views implied by cited packages\nSpatial, TimeSeries, Bayesian, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-010/",
    "title": "Analyzing an Electronic Limit Order Book",
    "description": "The orderbook package provides facilities for exploring and visualizing the data associated with an order book: the electronic collection of the outstanding limit orders for a financial instrument. This article provides an overview of the orderbook package and examples of its use.",
    "author": [
      {
        "name": "David Kane",
        "url": {}
      },
      {
        "name": "Andrew Liu",
        "url": {}
      },
      {
        "name": "Khanh Nguyen",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-009/",
    "title": "hglm: A Package for Fitting Hierarchical Generalized Linear Models",
    "description": "We present the hglm package for fitting hierarchical generalized linear models. It can be used for linear mixed models and generalized linear mixed models with random effects for a variety of links and a variety of distributions for both the outcomes and the random effects. Fixed effects can also be fitted in the dispersion part of the model.",
    "author": [
      {
        "name": "Lars Rönnegård",
        "url": {}
      },
      {
        "name": "Xia Shen",
        "url": {}
      },
      {
        "name": "Moudud Alam",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhglm, lme4, MASS, dglm, HGLMMM, nlme\nCRAN Task Views implied by cited packages\nEconometrics, Environmetrics, Psychometrics, SocialSciences, OfficialStatistics, Pharmacokinetics, SpatioTemporal, Bayesian, ChemPhys, Distributions, Finance, Multivariate, NumericalMathematics, Robust, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-010/",
    "title": "Source References",
    "description": "Since version 2.10.0, R includes expanded support for source references in R code and ‘.Rd’ files. This paper describes the origin and purposes of source references, and current and future support for them.",
    "author": [
      {
        "name": "Duncan Murdoch",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-011/",
    "title": "dclone: Data Cloning in R",
    "description": "The dclone R package contains low level functions for implementing maximum likelihood estimating procedures for complex models using data cloning and Bayesian Markov Chain Monte Carlo methods with support for JAGS, WinBUGS and OpenBUGS.",
    "author": [
      {
        "name": "Péter Sólymos",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndclone, rjags, coda, R2WinBUGS, BRugs\nCRAN Task Views implied by cited packages\ngR, Bayesian, Cluster, HighPerformanceComputing, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-012/",
    "title": "stringr: modern, consistent string processing",
    "description": "String processing is not glamorous, but it is frequently used in data cleaning and preparation. The existing string functions in R are powerful, but not friendly. To remedy this, the stringr package provides string functions that are simpler and more consistent, and also fixes some functionality that R is missing compared to other programming languages.",
    "author": [
      {
        "name": "Hadley Wickham",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-013/",
    "title": "Solving Differential Equations in R",
    "description": "Although R is still predominantly applied for statistical analysis and graphical representation, it is rapidly becoming more suitable for mathematical computing. One of the fields where considerable progress has been made recently is the solution of differential equations. Here we give a brief overview of differential equations that can now be solved by R.",
    "author": [
      {
        "name": "Karline Soetaert",
        "url": {}
      },
      {
        "name": "Thomas Petzoldt",
        "url": {}
      },
      {
        "name": "R. Woodrow Setzer",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlimSolve, rootSolve, deSolve, bvpSolve, ReacTran, PBSddesolve, sde, pomp, bvpSolve, ReacTran, deSolve, deSolve, ReacTran, deSolve, odesolve, odesolve, nlmeODE, FME, ccems, ReacTran\nCRAN Task Views implied by cited packages\nDifferentialEquations, Pharmacokinetics, TimeSeries, Bayesian, Finance, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-014/",
    "title": "Bayesian Estimation of the GARCH(1,1) Model with Student-t Innovations",
    "description": "This note presents the R package bayesGARCH which provides functions for the Bayesian estimation of the parsimonious and effective GARCH(1,1) model with Student-t innovations. The estimation procedure is fully automatic and thus avoids the tedious task of tuning an MCMC sampling algorithm. The usage of the package is shown in an empirical application to exchange rate log-returns.",
    "author": [
      {
        "name": "David Ardia",
        "url": {}
      },
      {
        "name": "Lennart F. Hoogerheide",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfGarch, rgarch, tseries, bayesGARCH, coda, foreach\nCRAN Task Views implied by cited packages\nFinance, Bayesian, TimeSeries, Econometrics, Environmetrics, gR, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-015/",
    "title": "cudaBayesreg: Bayesian Computation in CUDA",
    "description": "Graphical processing units are rapidly gaining maturity as powerful general parallel comput ing devices. The package cudaBayesreg uses GPU–oriented procedures to improve the performance of Bayesian computations. The paper motivates the need for devising high-performance computing strategies in the context of fMRI data analysis. Some features of the package for Bayesian analysis of brain fMRI data are illustrated. Comparative computing performance figures between sequential and parallel implementations are presented as well.",
    "author": [
      {
        "name": "Adelino Ferreira da Silva",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncudaBayesreg, bayesm, cudaBayesregData, oro.nifti\nCRAN Task Views implied by cited packages\nMedicalImaging, Bayesian, HighPerformanceComputing, Cluster, Distributions, Econometrics, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-016/",
    "title": "binGroup: A Package for Group Testing",
    "description": "When the prevalence of a disease or of some other binary characteristic is small, group testing (also known as pooled testing) is frequently used to estimate the prevalence and/or to identify individuals as positive or negative. We have developed the binGroup package as the first package designed to address the estimation problem in group testing. We present functions to estimate an overall prevalence for a homogeneous population. Also, for this setting, we have functions to aid in the very important choice of the group size. When individuals come from a heterogeneous population, our group testing regression functions can be used to estimate an individual probability of disease positivity by using the group observations only. We illustrate our functions with data from a multiple vector transfer design experiment and a human infectious disease prevalence study.",
    "author": [
      {
        "name": "Christopher R. Bilder",
        "url": {}
      },
      {
        "name": "Boan Zhang",
        "url": {}
      },
      {
        "name": "Frank Schaarschmidt",
        "url": {}
      },
      {
        "name": "Joshua M. Tebbs",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbinGroup, binom\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-017/",
    "title": "The RecordLinkage Package: Detecting Errors in Data",
    "description": "Record linkage deals with detecting homonyms and mainly synonyms in data. The package RecordLinkage provides means to perform and evaluate different record linkage methods. A stochas tic framework is implemented which calculates weights through an EM algorithm. The determination of the necessary thresholds in this model can be achieved by tools of extreme value theory. Further more, machine learning methods are utilized, including decision trees (rpart), bootstrap aggregating (bagging), ada boost (ada), neural nets (nnet) and support vector machines (svm). The generation of record pairs and comparison patterns from single data items are provided as well. Comparison patterns can be chosen to be binary or based on some string metrics. In order to reduce computation time and memory usage, blocking can be used. Future development will concentrate on additional and refined methods, performance improvements and input/output facilities needed for real-world application.",
    "author": [
      {
        "name": "Murat Sariyar",
        "url": {}
      },
      {
        "name": "Andreas Borg",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-018/",
    "title": "spikeslab: Prediction and Variable Selection Using Spike and Slab Regression",
    "description": "Weighted generalized ridge regression offers unique advantages in correlated high-dimensional problems. Such estimators can be efficiently computed using Bayesian spike and slab models and are effective for prediction. For sparse variable selection, a generalization of the elastic net can be used in tandem with these Bayesian estimates. In this article, we describe the R-software package spikeslab for implementing this new spike and slab prediction and variable selection methodology.",
    "author": [
      {
        "name": "Hemant Ishwaran",
        "url": {}
      },
      {
        "name": "Udaya B. Kogalur",
        "url": {}
      },
      {
        "name": "J. Sunil Rao",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlars, snow\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MachineLearning\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-001/",
    "title": "IsoGene: An R Package for Analyzing Dose-response Studies in Microarray Experiments",
    "description": "IsoGene is an R package for the analysis of dose-response microarray experiments to identify gene or subsets of genes with a monotone relationship between the gene expression and the doses. Several testing procedures (i.e., the likelihood ratio test, Williams, Marcus, the M, and Modified M), that take into account the order restriction of the means with respect to the increasing doses are implemented in the package. The inference is based on resampling methods, both permutations and the Significance Analysis of Microarrays (SAM).",
    "author": [
      {
        "name": "Setia Pramana",
        "url": {}
      },
      {
        "name": "Dan Lin",
        "url": {}
      },
      {
        "name": "Philippe Haldermans",
        "url": {}
      },
      {
        "name": "Ziv Shkedy",
        "url": {}
      },
      {
        "name": "Tobias Verbeke",
        "url": {}
      },
      {
        "name": "Hinrich Göhlmann",
        "url": {}
      },
      {
        "name": "An De Bondt",
        "url": {}
      },
      {
        "name": "Willem Talloen",
        "url": {}
      },
      {
        "name": "Luc Bijnens.",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-002/",
    "title": "Online Reproducible Research: An Application to Multivariate Analysis of Bacterial DNA Fingerprint Data",
    "description": "This paper presents an example of online reproducible multivariate data analysis. This example is based on a web page providing an online computing facility on a server. HTML forms contain editable R code snippets that can be executed in any web browser thanks to the Rweb software. The example is based on the multivariate analysis of DNA fingerprints of the internal bacterial flora of the poultry red mite Dermanyssus gallinae. Several multivariate data analysis methods from the ade4 package are used to compare the fingerprints of mite pools coming from various poultry farms. All the computations and graphical displays can be redone interactively and further explored online, using only a web browser. Statistical methods are detailed in the duality diagram framework, and a discussion about online reproducibility is initiated.",
    "author": [
      {
        "name": "Jean Thioulouse",
        "url": {}
      },
      {
        "name": "Claire Valiente-Moro",
        "url": {}
      },
      {
        "name": "Lionel Zenner",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nade4, seqinr, ade4, vegan, CGIwithR, R2HTML\nCRAN Task Views implied by cited packages\nEnvironmetrics, Multivariate, Psychometrics, Spatial, Graphics, Genetics, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-003/",
    "title": "MCMC for Generalized Linear Mixed Models with glmmBUGS",
    "description": "The glmmBUGS package is a bridging tool between Generalized Linear Mixed Models (GLMMs) in R and the BUGS language. It provides a simple way of performing Bayesian inference using Markov Chain Monte Carlo (MCMC) methods, taking a model formula and data frame in R and writing a BUGS model file, data file, and initial values files. Functions are provided to reformat and summarize the BUGS results. A key aim of the package is to provide files and objects that can be modified prior to calling BUGS, giving users a platform for customizing and extending the models to accommodate a wide variety of analyses.",
    "author": [
      {
        "name": "Patrick Brown",
        "url": {}
      },
      {
        "name": "Lutong Zhou",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-004/",
    "title": "Mapping and Measuring Country Shapes",
    "description": "The article introduces the cshapes R package, which includes our CShapes dataset of contemporary and historical country boundaries, as well as computational tools for computing geographical measures from these maps. We provide an overview of the need for considering spatial dependence in comparative research, how this requires appropriate historical maps, and detail how the cshapes associated R package cshapes can contribute to these ends. We illustrate the use of the package for drawing maps, computing spatial variables for countries, and generating weights matrices for spatial statistics.",
    "author": [
      {
        "name": "Nils B. Weidmann",
        "url": {}
      },
      {
        "name": "Kristian Skrede Gleditsch",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-005/",
    "title": "tmvtnorm: A Package for the Truncated Multivariate Normal Distribution",
    "description": "In this article we present tmvtnorm, an R package implementation for the truncated mul tivariate normal distribution. We consider random number generation with rejection and Gibbs sampling, computation of marginal densities as well as computation of the mean and covariance of the truncated variables. This contribution brings together latest research in this field and provides useful methods for both scholars and practitioners when working with truncated normal variables.",
    "author": [
      {
        "name": "Stefan Wilhelm",
        "url": {}
      },
      {
        "name": "B. G. Manjunath",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-006/",
    "title": "neuralnet: Training of Neural Networks",
    "description": "Artificial neural networks are applied in many situations. neuralnet is built to train multilayer perceptrons in the context of regression analyses, i.e. to approximate functional relationships between covariates and response variables. Thus, neural networks are used as extensions of generalized linear models. neuralnet is a very flexible package. The backpropagation algorithm and three versions of resilient backpropagation are implemented and it provides a custom-choice of activation and error function. An arbitrary number of covariates and response variables as well as of hidden layers can theoretically be included. The paper gives a brief introduction to multi-layer perceptrons and resilient backpropagation and demonstrates the application of neuralnet using the data set infert, which is contained in the R distribution.",
    "author": [
      {
        "name": "Frauke Günther",
        "url": {}
      },
      {
        "name": "Stefan Fritsch",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-007/",
    "title": "glmperm: A Permutation of Regressor Residuals Test for Inference in Generalized Linear Models",
    "description": "We introduce a new R package called glmperm for inference in generalized linear models especially for small and moderate-sized data sets. The inference is based on the permutation of regressor residuals test introduced by Potter (2005). The implementation of glmperm outperforms currently available permutation test software as glmperm can be applied in situations where more than one covariate is involved.",
    "author": [
      {
        "name": "Wiebke Werft",
        "url": {}
      },
      {
        "name": "Axel Benner",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-008/",
    "title": "Two-sided Exact Tests and Matching Confidence Intervals for Discrete Data",
    "description": "There is an inherent relationship between two-sided hypothesis tests and confidence intervals. A series of two-sided hypothesis tests may be inverted to obtain the matching 100(1-α)% confidence interval defined as the smallest interval that contains all point null parameter values that would not be rejected at the α level. Unfortunately, for discrete data there are several different ways of defining two-sided exact tests and the most commonly used two-sided exact tests are defined one way, while the most commonly used exact confidence intervals are inversions of tests defined another way. This can lead to inconsistencies where the exact test rejects but the exact confidence interval contains the null parameter value. The packages exactci and exact2x2 provide several exact tests with the matching confidence intervals avoiding these inconsistencies as much as possible. Examples are given for binomial and Poisson parameters and both paired and unpaired 2 × 2 tables. Applied statisticians are increasingly being encouraged to report confidence intervals (CI) and parameter estimates along with p-values from hypothesis tests. The htest class of the stats package is ideally suited to these kinds of analyses, because all the related statistics may be presented when the results are printed. For exact two-sided tests applied to discrete data, a test-CI inconsistency may occur: the p-value may indicate a significant result at level α while the associated 100(1-α)% confidence interval may cover the null value of the parameter. Ideally, we would like to present a unified report (Hirji, 2006), whereby the p-value and the confidence interval match as much as possible.",
    "author": [
      {
        "name": "Michael P. Fay",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nexactci, exact2x2, exactci, exact2x2, exactci, exact2x2, PropCIs, rateratio.test, coin, perm\nCRAN Task Views implied by cited packages\nClinicalTrials, Survival\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-011/",
    "title": "ConvergenceConcepts: An R Package to Investigate Various Modes of Convergence",
    "description": "ConvergenceConcepts is an R package, built upon the tkrplot, tcltk and lattice packages, designed to investigate the convergence of simulated sequences of random variables. Four classical modes of convergence may be studied, namely: almost sure convergence (a.s.), convergence in probability (P), convergence in law (L) and convergence in r-th mean (r). This investigation is performed through accurate graphical representations. This package may be used as a pedagogical tool. It may give students a better understanding of these notions and help them to visualize these difficult theoretical concepts. Moreover, some scholars could gain some insight into the behaviour of some random sequences they are interested in.",
    "author": [
      {
        "name": "Pierre Lafaye de Micheaux",
        "url": {}
      },
      {
        "name": "Benoit Liquet",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-012/",
    "title": "copas: An R package for Fitting the Copas Selection Model",
    "description": "This article describes the R package copas which is an add-on package to the R package meta. The R package copas can be used to fit the Copas selection model to adjust for bias in meta-analysis. A clinical example is used to illustrate fitting and interpreting the Copas selection model.",
    "author": [
      {
        "name": "J. Carpenter",
        "url": {}
      },
      {
        "name": "G. Rücker",
        "url": {}
      },
      {
        "name": "G. Schwarzer",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncopas, meta\nCRAN Task Views implied by cited packages\nClinicalTrials, MetaAnalysis\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-013/",
    "title": "Party on!",
    "description": "Recursive partitioning methods are amongst the most popular and widely used statistical learning tools for nonparametric regression and classification. Especially random forests, that can deal with large numbers of predictor variables even in the presence of complex interactions, are being applied successfully in many scientific fields (see, e.g., ??, and the references therein for applications in genetics and social sciences). Thus, it is not surprising that there is a variety of recursive partitioning tools available in R (see http://CRAN.R-project.org/view=MachineLearning for an overview).",
    "author": [
      {
        "name": "Carolin Strobl",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      },
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-014/",
    "title": "Aspects of the Social Organization and Trajectory of the R Project",
    "description": "Based partly on interviews with members of the R Core team, this paper considers the development of the R Project in the context of open-source software development and, more generally, voluntary activities. The paper describes aspects of the social organization of the R Project, including the organization of the R Core team; describes the trajectory of the R Project; seeks to identify factors crucial to the success of R; and speculates about the prospects for R.",
    "author": [
      {
        "name": "John Fox",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-015/",
    "title": "asympTest: A Simple R Package for Classical Parametric Statistical Tests and Confidence Intervals in Large Samples",
    "description": "asympTest is an R package implementing large sample tests and confidence intervals. One and two sample mean and variance tests (differences and ratios) are considered. The test statistics are all expressed in the same form as the Student t-test, which facilitates their presentation in the classroom. This contribution also fills the gap of a robust (to non-normality) alternative to the chi square single variance test for large samples, since no such procedure is implemented in standard statistical software.",
    "author": [
      {
        "name": "J.-F. Coeurjolly",
        "url": {}
      },
      {
        "name": "R. Drouilhet",
        "url": {}
      },
      {
        "name": "P. Lafaye de Micheaux",
        "url": {}
      },
      {
        "name": "J.-F. Robineau",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nasympTest, asympTest, asympTest\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-016/",
    "title": "Rattle: A Data Mining GUI for R",
    "description": "Data mining delivers insights, patterns, and descriptive and predictive models from the large amounts of data available today in many organisations. The data miner draws heavily on methodologies, techniques and algorithms from statistics, machine learning, and computer science. R increasingly provides a powerful platform for data mining. However, scripting and programming is sometimes a challenge for data analysts moving into data mining. The Rattle package provides a graphical user interface specifically for data mining using R. It also provides a stepping stone toward using R as a programming language for data analysis.",
    "author": [
      {
        "name": "Graham J Williams",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\narules, RGtk2, RGtk2, rattle, rattle, rattle, rattle, Hmisc, fBasics, mice, rggobi, rggobi, latticist, playwith, lattice, reshape, randomForest, Amelia, rpart, party, rpart, randomForest, ROCR, pmml, rattle, pmml, RGtk2\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, Graphics, Environmetrics, OfficialStatistics, SocialSciences, Survival, Bayesian, ClinicalTrials, Distributions, Econometrics, Finance, Pharmacokinetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-017/",
    "title": "sos: Searching Help Pages of R Packages",
    "description": "The sos package provides a means to quickly and flexibly search the help pages of contributed packages, finding functions and datasets in seconds or minutes that could not be found in hours or days by any other means we know. Its findFn function accesses Jonathan Baron’s R Site Search database and returns the matches in a data frame of class \"findFn\", which can be further manipulated by other sos functions to produce, for example, an Excel file that starts with a summary sheet that makes it relatively easy to prioritize alternative packages for further study. As such, it provides a very powerful way to do a literature search for functions and packages relevant to a particular topic of interest and could become virtually mandatory for authors of new packages or papers in publications such as The R Journal and the Journal of Statistical Software.",
    "author": [
      {
        "name": "Spencer Graves",
        "url": {}
      },
      {
        "name": "Sundar Dorai-Raj",
        "url": {}
      },
      {
        "name": "Romain François",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsos, sos, sos, sos, WriteXLS, RODBC, sos, fda, deSolve, PKfit, sos, sos, sos, sos\nCRAN Task Views implied by cited packages\nDifferentialEquations, Pharmacokinetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-018/",
    "title": "Transitioning to R: Replicating SAS, Stata, and SUDAAN Analysis Techniques in Health Policy Data",
    "description": "Statistical, data manipulation, and presentation tools make R an ideal integrated package for research in the fields of health policy and healthcare management and evaluation. However, the technical documentation accompanying most data sets used by researchers in these fields does not include syntax examples for analysts to make the transition from another statistical package to R. This paper describes the steps required to import health policy data into R, to prepare that data for analysis using the two most common complex survey variance calculation techniques, and to produce the principal set of statistical estimates sought by health policy researchers. Using data from the Medical Expenditure Panel Survey Household Component (MEPS-HC), this paper outlines complex survey data analysis techniques in R, with side-by-side comparisons to the SAS, Stata, and SUDAAN statistical software packages.",
    "author": [
      {
        "name": "Anthony Damico",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-001/",
    "title": "New Numerical Algorithm for Multivariate Normal Probabilities in Package mvtnorm",
    "description": "? proposed a numerical algorithm for evaluating multivariate normal probabilities. Starting with version 0.9-0 of the mvtnorm package (??), this algorithm is available to the R community. We give a brief introduction to Miwa’s procedure and compare it to a quasi-randomized Monte-Carlo procedure proposed by ?, which has been available through mvtnorm for some years now, both with respect to computing time and accuracy.",
    "author": [
      {
        "name": "Xuefei Mi",
        "url": {}
      },
      {
        "name": "Tetsuhisa Miwa",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-002/",
    "title": "EMD: A Package for Empirical Mode Decomposition and Hilbert Spectrum",
    "description": "The concept of empirical mode decomposition (EMD) and the Hilbert spectrum (HS) has been developed rapidly in many disciplines of science and engineering since Huang et al. (1998) invented EMD. The key feature of EMD is to decompose a signal into so-called intrinsic mode function (IMF). Furthermore, the Hilbert spectral analysis of intrinsic mode functions provides frequency information evolving with time and quantifies the amount of variation due to oscillation at different time scales and time locations. In this article, we introduce an R package called EMD (Kim and Oh, 2008) that performs oneand twodimensional EMD and HS.",
    "author": [
      {
        "name": "Donghoh Kim",
        "url": {}
      },
      {
        "name": "Hee-Seok Oh",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-003/",
    "title": "AdMit",
    "description": "A package for constructing and using an adaptive mixture of Student-t distributions as a flexible candidate distribution for efficient simulation.",
    "author": [
      {
        "name": "David Ardia",
        "url": {}
      },
      {
        "name": "Lennart F. Hoogerheide",
        "url": {}
      },
      {
        "name": "Herman K. van Dijk",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-004/",
    "title": "Easier parallel computing in R with snowfall and sfCluster",
    "description": "Many statistical analysis task in areas such as bioinformatics are computationally very intensive, while lots of them rely on embarrasingly parallel computations (Ananth Grama, 2003). Multiple computers or even multiple processor cores on standard desktop computers, which are widespread available nowadays, can easily contribute to faster analyses.",
    "author": [
      {
        "name": "Jochen Knaus",
        "url": {}
      },
      {
        "name": "Christine Porzelius",
        "url": {}
      },
      {
        "name": "Harald Binder",
        "url": {}
      },
      {
        "name": "Guido Schwarzer",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-005/",
    "title": "expert: Modeling Without Data Using Expert Opinion",
    "description": "The expert package provides tools to create and manipulate empirical statistical models using expert opinion (or judgment). Here, the latter expression refers to a specific body of techniques to elicit the distribution of a random variable when data is scarce or unavailable. Opinions on the quantiles of the distribution are sought from experts in the field and aggregated into a final estimate. The package supports aggregation by means of the Cooke, Mendel–Sheridan and predefined weights models.",
    "author": [
      {
        "name": "Vincent Goulet",
        "url": {}
      },
      {
        "name": "Michel Jacques",
        "url": {}
      },
      {
        "name": "Mathieu Pigeon",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-006/",
    "title": "Drawing Diagrams with R",
    "description": "R provides a number of well-known high-level facilities for producing sophisticated statistical plots, including the “traditional” plots in the graphics package (R Development Core Team, 2008), the Trellis-style plots provided by lattice (Sarkar, 2008), and the grammar-of-graphics-inspired approach of ggplot2 (Wickham, 2009).",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngraphics, lattice, ggplot2\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, Pharmacokinetics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-007/",
    "title": "Collaborative Software Development Using R-Forge",
    "description": "Open source software (OSS) is typically created in a decentralized self-organizing process by a community of developers having the same or similar interests (see the famous essay by ?). A key factor for the success of OSS over the last two decades is the Internet: Developers who rarely meet face-to-face can employ new means of communication, both for rapidly writing and deploying software (in the spirit of Linus Torvald’s “release early, release often paradigm”). Therefore, many tools emerged that assist a collaborative software development process, including in particular tools for source code management (SCM) and version control.",
    "author": [
      {
        "name": "Stefan Theußl",
        "url": {}
      },
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-008/",
    "title": "Facets of R",
    "description": "We are seeing today a widespread, and welcome, tendency for non-computer-specialists among statisticians and others to write collections of R functions that organize and communicate their work. Along with the flood of software sometimes comes an attitude that one need only learn, or teach, a sort of basic how-to-write-the-function level of R programming, beyond which most of the detail is unimportant or can be absorbed without much discussion. As delusions go, this one is not very objectionable if it encourages participation. Nevertheless, a delusion it is. In fact, functions are only one of a variety of important facets that R has acquired by intent or circumstance during the three-plus decades of the history of the software and of its predecessor S. To create valuable and trustworthy software using R often requires an understanding of some of these facets and their interrelations. This paper identifies six facets, discussing where they came from, how they support or conflict with each other, and what implications they have for the future of programming with R.",
    "author": [
      {
        "name": "John M. Chambers",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-009/",
    "title": "The hwriter package: Composing HTML documents with R objects",
    "description": "HTML documents are structured documents made of diverse elements such as paragraphs, sections, columns, figures and tables organized in a hierarchical layout. Combination of HTML documents and hyperlinking is useful to report analysis results; for example, in the package arrayQualityMetrics, estimating the quality of microarray data sets and cellHTS2, performing the analysis of cell-based screens.",
    "author": [
      {
        "name": "Gregoire Pau",
        "url": {}
      },
      {
        "name": "Wolfgang Huber",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-010/",
    "title": "PMML: An Open Standard for Sharing Models",
    "description": "The PMML package exports a variety of predictive and descriptive models from R to the Predictive Model Markup Language (Data Mining Group, 2008). PMML is an XML-based language and has become the de-facto standard to represent not only predictive and descriptive models, but also data preand post-processing. In so doing, it allows for the interchange of models among different tools and environments, mostly avoiding proprietary issues and incompatibilities.",
    "author": [
      {
        "name": "Alex Guazzelli",
        "url": {}
      },
      {
        "name": "Michael Zeller",
        "url": {}
      },
      {
        "name": "Wen-Ching Lin",
        "url": {}
      },
      {
        "name": "Graham Williams",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:50+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-019/",
    "title": "Sample Size Estimation while Controlling False Discovery Rate for Microarray Experiments Using the ssize.fdr Package",
    "description": "Microarray experiments are becoming more and more popular and critical in many biological disciplines. As in any statistical experiment, appropriate experimental design is essential for reliable statistical inference, and sample size has a crucial role in experimental design. Because microarray experiments are rather costly, it is important to have an adequate sample size that will achieve a desired power without wasting resources.",
    "author": [
      {
        "name": "Megan Orr",
        "url": {}
      },
      {
        "name": "Peng Liu",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:09:51+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-009/",
    "title": "An introduction to rggobi",
    "description": "\"An introduction to rggobi\" published in R News.",
    "author": [
      {
        "name": "Hadley Wickham",
        "url": {}
      },
      {
        "name": "Michael Lawrence",
        "url": {}
      },
      {
        "name": "Duncan Temple Lang",
        "url": {}
      },
      {
        "name": "Deborah F. Swayne",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2008-009/preview.png",
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {},
    "preview_width": 370,
    "preview_height": 370
  },
  {
    "path": "articles/RN-2008-010/",
    "title": "Introducing the bipartite package: Analysing ecological networks",
    "description": "\"Introducing the bipartite package: Analysing ecological networks\" published in R News.",
    "author": [
      {
        "name": "Carsten F. Dormann",
        "url": {}
      },
      {
        "name": "Bernd Gruber",
        "url": {}
      },
      {
        "name": "Jochen Fründ",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-011/",
    "title": "The profileModel R package: Profiling objectives for models with linear predictors",
    "description": "\"The profileModel R package: Profiling objectives for models with linear predictors\" published in R News.",
    "author": [
      {
        "name": "Ioannis Kosmidis",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-012/",
    "title": "An introduction to text mining in R",
    "description": "\"An introduction to text mining in R\" published in R News.",
    "author": [
      {
        "name": "Ingo Feinerer",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-013/",
    "title": "Animation: A package for statistical animations",
    "description": "\"Animation: A package for statistical animations\" published in R News.",
    "author": [
      {
        "name": "Yihui Xie",
        "url": {}
      },
      {
        "name": "Xiaoyue Cheng",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-014/",
    "title": "The VGAM package",
    "description": "\"The VGAM package\" published in R News.",
    "author": [
      {
        "name": "Thomas W. Yee",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-015/",
    "title": "Comparing non-identical objects",
    "description": "\"Comparing non-identical objects\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-016/",
    "title": "Mvna: An R package for the Nelson-Aalen estimator in multistate models",
    "description": "\"Mvna: An R package for the Nelson-Aalen estimator in multistate models\" published in R News.",
    "author": [
      {
        "name": "A. Allignol",
        "url": {}
      },
      {
        "name": "J. Beyersmann",
        "url": {}
      },
      {
        "name": "M. Schumacher",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-017/",
    "title": "Programmers’ Niche: The Y of R",
    "description": "\"Programmers’ Niche: The Y of R\" published in R News.",
    "author": [
      {
        "name": "Vince Carey",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-001/",
    "title": "Using Sweave with LyX",
    "description": "\"Using Sweave with LyX\" published in R News.",
    "author": [
      {
        "name": "Gregor Gorjanc",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-002/",
    "title": "Trade costs",
    "description": "\"Trade costs\" published in R News.",
    "author": [
      {
        "name": "Jeff Enos",
        "url": {}
      },
      {
        "name": "David Kane",
        "url": {}
      },
      {
        "name": "Arjun Ravi Narayan",
        "url": {}
      },
      {
        "name": "Aaron Schwartz",
        "url": {}
      },
      {
        "name": "Daniel Suo",
        "url": {}
      },
      {
        "name": "Luyi Zhao",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-003/",
    "title": "Survival analysis for cohorts with missing covariate information",
    "description": "\"Survival analysis for cohorts with missing covariate information\" published in R News.",
    "author": [
      {
        "name": "Hormuzd A. Katki",
        "url": {}
      },
      {
        "name": "Steven D. Mark",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-004/",
    "title": "Segmented: An R package to fit regression models with broken-line relationships",
    "description": "\"Segmented: An R package to fit regression models with broken-line relationships\" published in R News.",
    "author": [
      {
        "name": "Vito M. R. Muggeo",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-005/",
    "title": "Bayesian estimation for parsimonious threshold autoregressive models in R",
    "description": "\"Bayesian estimation for parsimonious threshold autoregressive models in R\" published in R News.",
    "author": [
      {
        "name": "Cathy W. S. Chen",
        "url": {}
      },
      {
        "name": "Edward M. H. Lin",
        "url": {}
      },
      {
        "name": "F. C. Liu",
        "url": {}
      },
      {
        "name": "Richard Gerlach",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2008-005/preview.png",
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {},
    "preview_width": 1198,
    "preview_height": 1195
  },
  {
    "path": "articles/RN-2008-006/",
    "title": "Statistical modeling of loss distributions using actuar",
    "description": "\"Statistical modeling of loss distributions using actuar\" published in R News.",
    "author": [
      {
        "name": "Vincent Goulet",
        "url": {}
      },
      {
        "name": "Mathieu Pigeon",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-007/",
    "title": "Programmers’ Niche: Multivariate polynomials in r",
    "description": "\"Programmers’ Niche: Multivariate polynomials in r\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-008/",
    "title": "R Help Desk: How can I avoid this loop or make it faster?",
    "description": "\"R Help Desk: How can I avoid this loop or make it faster?\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      },
      {
        "name": "John Fox",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-021/",
    "title": "SpherWave: An R package for analyzing scattered spherical data by spherical wavelets",
    "description": "\"SpherWave: An R package for analyzing scattered spherical data by spherical wavelets\" published in R News.",
    "author": [
      {
        "name": "Hee-Seok Oh",
        "url": {}
      },
      {
        "name": "Donghoh Kim",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-021/preview.png",
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {},
    "preview_width": 688,
    "preview_height": 352
  },
  {
    "path": "articles/RN-2007-022/",
    "title": "Diving behaviour analysis in R",
    "description": "\"Diving behaviour analysis in R\" published in R News.",
    "author": [
      {
        "name": "Sebastián P. Luque",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-022/preview.png",
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {},
    "preview_width": 480,
    "preview_height": 480
  },
  {
    "path": "articles/RN-2007-023/",
    "title": "Very large numbers in R: Introducing package Brobdingnag",
    "description": "\"Very large numbers in R: Introducing package Brobdingnag\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-024/",
    "title": "Applied bayesian non- and semi-parametric inference using DPpackage",
    "description": "\"Applied bayesian non- and semi-parametric inference using DPpackage\" published in R News.",
    "author": [
      {
        "name": "Alejandro Jara",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-025/",
    "title": "An introduction to gWidgets",
    "description": "\"An introduction to gWidgets\" published in R News.",
    "author": [
      {
        "name": "John Verzani",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-025/preview.png",
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {},
    "preview_width": 216,
    "preview_height": 147
  },
  {
    "path": "articles/RN-2007-026/",
    "title": "Financial journalism with R",
    "description": "\"Financial journalism with R\" published in R News.",
    "author": [
      {
        "name": "Bill Alpert",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-027/",
    "title": "Need a hint?",
    "description": "\"Need a hint?\" published in R News.",
    "author": [
      {
        "name": "Sanford Weisberg",
        "url": {}
      },
      {
        "name": "Hadley Wickham",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-028/",
    "title": "Psychometrics task view",
    "description": "\"Psychometrics task view\" published in R News.",
    "author": [
      {
        "name": "Patrick Mair",
        "url": {}
      },
      {
        "name": "Reinhold Hatzinger",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-029/",
    "title": "Meta: An R package for meta-analysis",
    "description": "\"Meta: An R package for meta-analysis\" published in R News.",
    "author": [
      {
        "name": "Guido Schwarzer",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-030/",
    "title": "Extending the R Commander by “plug-in” packages",
    "description": "\"Extending the R Commander by “plug-in” packages\" published in R News.",
    "author": [
      {
        "name": "John Fox",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-030/preview.png",
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {},
    "preview_width": 691,
    "preview_height": 745
  },
  {
    "path": "articles/RN-2007-031/",
    "title": "Improvements to the multiple testing package multtest",
    "description": "\"Improvements to the multiple testing package multtest\" published in R News.",
    "author": [
      {
        "name": "Sandra L. Taylor",
        "url": {}
      },
      {
        "name": "Duncan Temple Lang",
        "url": {}
      },
      {
        "name": "Katherine S. Pollard",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-011/",
    "title": "New functions for multivariate analysis",
    "description": "\"New functions for multivariate analysis\" published in R News.",
    "author": [
      {
        "name": "Peter Dalgaard",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-012/",
    "title": "Gnm: A package for generalized nonlinear models",
    "description": "\"Gnm: A package for generalized nonlinear models\" published in R News.",
    "author": [
      {
        "name": "Heather Turner",
        "url": {}
      },
      {
        "name": "David Firth",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-013/",
    "title": "Fmri: A package for analyzing fmri data",
    "description": "\"Fmri: A package for analyzing fmri data\" published in R News.",
    "author": [
      {
        "name": "Jörg Polzehl",
        "url": {}
      },
      {
        "name": "Karten Tabelow",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-013/preview.png",
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {},
    "preview_width": 530,
    "preview_height": 427
  },
  {
    "path": "articles/RN-2007-014/",
    "title": "Optmatch: Flexible, optimal matching for observational studies",
    "description": "\"Optmatch: Flexible, optimal matching for observational studies\" published in R News.",
    "author": [
      {
        "name": "Ben B. Hansen",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-015/",
    "title": "Random survival forests for R",
    "description": "\"Random survival forests for R\" published in R News.",
    "author": [
      {
        "name": "Hemant Ishwaran",
        "url": {}
      },
      {
        "name": "Udaya B. Kogalur",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-016/",
    "title": "Rwui: A web application to create user friendly web interfaces for R scripts",
    "description": "\"Rwui: A web application to create user friendly web interfaces for R scripts\" published in R News.",
    "author": [
      {
        "name": "Richard Newton",
        "url": {}
      },
      {
        "name": "Lorenz Wernisch",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-016/preview.png",
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {},
    "preview_width": 969,
    "preview_height": 1001
  },
  {
    "path": "articles/RN-2007-017/",
    "title": "The np package: Kernel methods for categorical and continuous data",
    "description": "\"The np package: Kernel methods for categorical and continuous data\" published in R News.",
    "author": [
      {
        "name": "Tristen Hayfield",
        "url": {}
      },
      {
        "name": "Jeffrey S. Racine",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-018/",
    "title": "eiPack: {R} \\times {C} ecological inferences and higher-dimension data management",
    "description": "\"eiPack: {R} \\times {C} ecological inferences and higher-dimension data management\" published in R News.",
    "author": [
      {
        "name": "Olivia Lau",
        "url": {}
      },
      {
        "name": "Ryan T. Moore",
        "url": {}
      },
      {
        "name": "Michael Kellermann",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-019/",
    "title": "The ade4 package–II: Two-table and {K}-table methods",
    "description": "\"The ade4 package–II: Two-table and {K}-table methods\" published in R News.",
    "author": [
      {
        "name": "Stéphane Dray",
        "url": {}
      },
      {
        "name": "Anne B. Dufour",
        "url": {}
      },
      {
        "name": "Daniel Chessel",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-020/",
    "title": "Review of “The R Book”",
    "description": "\"Review of “The R Book”\" published in R News.",
    "author": [
      {
        "name": "Friedrich Leisch",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-001/",
    "title": "Viewing binary files with the hexView package",
    "description": "\"Viewing binary files with the hexView package\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-002/",
    "title": "FlexMix: An R package for finite mixture modelling",
    "description": "\"FlexMix: An R package for finite mixture modelling\" published in R News.",
    "author": [
      {
        "name": "Bettina Grün",
        "url": {}
      },
      {
        "name": "Friedrich Leisch",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-003/",
    "title": "Using R to perform the AMMI analysis on agriculture variety trials",
    "description": "\"Using R to perform the AMMI analysis on agriculture variety trials\" published in R News.",
    "author": [
      {
        "name": "Andrea Onofri",
        "url": {}
      },
      {
        "name": "Egidio Ciriciofolo",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-004/",
    "title": "Inferences for ratios of normal means",
    "description": "\"Inferences for ratios of normal means\" published in R News.",
    "author": [
      {
        "name": "Gemechis Dilba",
        "url": {}
      },
      {
        "name": "Frank Schaarschmidt",
        "url": {}
      },
      {
        "name": "Ludwig A. Hothorn",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-005/",
    "title": "Working with unknown values",
    "description": "\"Working with unknown values\" published in R News.",
    "author": [
      {
        "name": "Gregor Gorjanc",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-006/",
    "title": "A new package for fitting random effect models",
    "description": "\"A new package for fitting random effect models\" published in R News.",
    "author": [
      {
        "name": "Jochen Einbeck",
        "url": {}
      },
      {
        "name": "John Hinde",
        "url": {}
      },
      {
        "name": "Ross Darnell",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-007/",
    "title": "Augmenting R with Unix tools",
    "description": "\"Augmenting R with Unix tools\" published in R News.",
    "author": [
      {
        "name": "Andrew Robinson",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-008/",
    "title": "POT: Modelling peaks over a threshold",
    "description": "\"POT: Modelling peaks over a threshold\" published in R News.",
    "author": [
      {
        "name": "Mathieu Ribatet",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-009/",
    "title": "Backtests",
    "description": "\"Backtests\" published in R News.",
    "author": [
      {
        "name": "Kyle Campbell",
        "url": {}
      },
      {
        "name": "Jeff Enos",
        "url": {}
      },
      {
        "name": "Daniel Gerlanc",
        "url": {}
      },
      {
        "name": "David Kane",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-010/",
    "title": "Review of John Verzani’s book: Using R for Introductory Statistics",
    "description": "\"Review of John Verzani’s book: Using R for Introductory Statistics\" published in R News.",
    "author": [
      {
        "name": "Andy Liaw",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-036/",
    "title": "Graphs and networks: Tools in Bioconductor",
    "description": "\"Graphs and networks: Tools in Bioconductor\" published in R News.",
    "author": [
      {
        "name": "Li Long",
        "url": {}
      },
      {
        "name": "Vince Carey",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-037/",
    "title": "Modeling package dependencies using graphs",
    "description": "\"Modeling package dependencies using graphs\" published in R News.",
    "author": [
      {
        "name": "Seth Falcon",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-038/",
    "title": "Image analysis for microscopy screens",
    "description": "\"Image analysis for microscopy screens\" published in R News.",
    "author": [
      {
        "name": "Oleg Sklyar",
        "url": {}
      },
      {
        "name": "Wolfgang Huber",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-038/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 1220,
    "preview_height": 401
  },
  {
    "path": "articles/RN-2006-039/",
    "title": "Beadarray: An R package to analyse Illumina BeadArrays",
    "description": "\"Beadarray: An R package to analyse Illumina BeadArrays\" published in R News.",
    "author": [
      {
        "name": "Mark Dunning",
        "url": {}
      },
      {
        "name": "Mike Smith",
        "url": {}
      },
      {
        "name": "Natalie Thorne",
        "url": {}
      },
      {
        "name": "Simon Tavaré",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-040/",
    "title": "Transcript mapping with high-density tiling arrays",
    "description": "\"Transcript mapping with high-density tiling arrays\" published in R News.",
    "author": [
      {
        "name": "Matthew Ritchie",
        "url": {}
      },
      {
        "name": "Wolfgang Huber",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-041/",
    "title": "Analyzing flow cytometry data with Bioconductor",
    "description": "\"Analyzing flow cytometry data with Bioconductor\" published in R News.",
    "author": [
      {
        "name": "Nolwenn Le Meur",
        "url": {}
      },
      {
        "name": "Florian Hahne",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-042/",
    "title": "Protein complex membership estimation using apComplex",
    "description": "\"Protein complex membership estimation using apComplex\" published in R News.",
    "author": [
      {
        "name": "Denise Scholtens",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-043/",
    "title": "SNP metadata access and use with Bioconductor",
    "description": "\"SNP metadata access and use with Bioconductor\" published in R News.",
    "author": [
      {
        "name": "Vince Carey",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-044/",
    "title": "Integrating biological data resources into R with biomaRt",
    "description": "\"Integrating biological data resources into R with biomaRt\" published in R News.",
    "author": [
      {
        "name": "Steffen Durinck",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-045/",
    "title": "Identifying interesting genes with siggenes",
    "description": "\"Identifying interesting genes with siggenes\" published in R News.",
    "author": [
      {
        "name": "Holger Schwender",
        "url": {}
      },
      {
        "name": "Andreas Krause",
        "url": {}
      },
      {
        "name": "Katja Ickstadt",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-046/",
    "title": "Reverse engineering genetic networks using the GeneNet package",
    "description": "\"Reverse engineering genetic networks using the GeneNet package\" published in R News.",
    "author": [
      {
        "name": "Juliane Schäfer",
        "url": {}
      },
      {
        "name": "Rainer Opgen-Rhein",
        "url": {}
      },
      {
        "name": "Korbinian Strimmer",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-047/",
    "title": "A multivariate approach to integrating datasets using made4 and ade4",
    "description": "\"A multivariate approach to integrating datasets using made4 and ade4\" published in R News.",
    "author": [
      {
        "name": "Aedín C. Culhane",
        "url": {}
      },
      {
        "name": "Jean Thioulouse",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-048/",
    "title": "Using amap and ctc packages for huge clustering",
    "description": "\"Using amap and ctc packages for huge clustering\" published in R News.",
    "author": [
      {
        "name": "Antoine Lucas",
        "url": {}
      },
      {
        "name": "Sylvain Jasson",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-049/",
    "title": "Model-based microarray image analysis",
    "description": "\"Model-based microarray image analysis\" published in R News.",
    "author": [
      {
        "name": "Chris Fraley",
        "url": {}
      },
      {
        "name": "Adrian E. Raftery",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-049/preview.png",
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {},
    "preview_width": 652,
    "preview_height": 652
  },
  {
    "path": "articles/RN-2006-050/",
    "title": "Sample size estimation for microarray experiments using the ssize package",
    "description": "\"Sample size estimation for microarray experiments using the ssize package\" published in R News.",
    "author": [
      {
        "name": "Gregory R. Warnes",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:02+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-025/",
    "title": "Sweave and the open document format – the odfWeave package",
    "description": "\"Sweave and the open document format – the odfWeave package\" published in R News.",
    "author": [
      {
        "name": "Max Kuhn",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-025/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 480,
    "preview_height": 480
  },
  {
    "path": "articles/RN-2006-026/",
    "title": "Plotrix",
    "description": "\"Plotrix\" published in R News.",
    "author": [
      {
        "name": "Jim Lemon",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-027/",
    "title": "Rpanel: Making graphs move with tcltk",
    "description": "\"Rpanel: Making graphs move with tcltk\" published in R News.",
    "author": [
      {
        "name": "Adrian Bowman",
        "url": {}
      },
      {
        "name": "Ewan Crawford",
        "url": {}
      },
      {
        "name": "Richard Bowman",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-027/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 114,
    "preview_height": 75
  },
  {
    "path": "articles/RN-2006-028/",
    "title": "R’s role in the climate change debate.",
    "description": "\"R’s role in the climate change debate.\" published in R News.",
    "author": [
      {
        "name": "Matthew Pocernich",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-029/",
    "title": "Interacting with data using the filehash package",
    "description": "\"Interacting with data using the filehash package\" published in R News.",
    "author": [
      {
        "name": "Roger D. Peng",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-030/",
    "title": "Special functions in R: Introducing the gsl package",
    "description": "\"Special functions in R: Introducing the gsl package\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-031/",
    "title": "A short introduction to the SIMEX and MCSIMEX",
    "description": "\"A short introduction to the SIMEX and MCSIMEX\" published in R News.",
    "author": [
      {
        "name": "Wolfgang Lederer",
        "url": {}
      },
      {
        "name": "Helmut Küchenhoff",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-032/",
    "title": "Parametric links for binary response",
    "description": "\"Parametric links for binary response\" published in R News.",
    "author": [
      {
        "name": "Roger Koenker",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-033/",
    "title": "A new package for the Birnbaum-Saunders distribution",
    "description": "\"A new package for the Birnbaum-Saunders distribution\" published in R News.",
    "author": [
      {
        "name": "Víctor Leiva",
        "url": {}
      },
      {
        "name": "Hugo Hernández",
        "url": {}
      },
      {
        "name": "Marco Riquelme",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-034/",
    "title": "Review of Fionn Murtagh’s book:\ncorrespondence analysis and data coding with Java and R",
    "description": "\"Review of Fionn Murtagh’s book:\ncorrespondence analysis and data coding with Java and R\" published in R News.",
    "author": [
      {
        "name": "Susan Holmes",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-035/",
    "title": "R Help Desk: Accessing the sources",
    "description": "\"R Help Desk: Accessing the sources\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-021/",
    "title": "Fitting dose-response curves from bioassays and toxicity testing",
    "description": "\"Fitting dose-response curves from bioassays and toxicity testing\" published in R News.",
    "author": [
      {
        "name": "Johannes Ranke",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2003-021/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 10,
    "preview_height": 13
  },
  {
    "path": "articles/RN-2006-020/",
    "title": "Non-linear regression for optimising the separation of carboxylic acids",
    "description": "\"Non-linear regression for optimising the separation of carboxylic acids\" published in R News.",
    "author": [
      {
        "name": "Peter Watkins",
        "url": {}
      },
      {
        "name": "Bill Venables",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-021/",
    "title": "The pls package",
    "description": "\"The pls package\" published in R News.",
    "author": [
      {
        "name": "Bjørn-Helge Mevik",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-022/",
    "title": "Some applications of model-based clustering in chemistry",
    "description": "\"Some applications of model-based clustering in chemistry\" published in R News.",
    "author": [
      {
        "name": "Chris Fraley",
        "url": {}
      },
      {
        "name": "Adrian E. Raftery",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-023/",
    "title": "Mapping databases of X-ray powder patterns",
    "description": "\"Mapping databases of X-ray powder patterns\" published in R News.",
    "author": [
      {
        "name": "Ron Wehrens",
        "url": {}
      },
      {
        "name": "Egon Willighagen",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-024/",
    "title": "Generating, using and visualizing molecular information in R",
    "description": "\"Generating, using and visualizing molecular information in R\" published in R News.",
    "author": [
      {
        "name": "Rajarshi Guha",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-024/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 300,
    "preview_height": 300
  },
  {
    "path": "articles/RN-2006-007/",
    "title": "S4 classes for distributions",
    "description": "\"S4 classes for distributions\" published in R News.",
    "author": [
      {
        "name": "Peter Ruckdeschel",
        "url": {}
      },
      {
        "name": "Matthias Kohl",
        "url": {}
      },
      {
        "name": "Thomas Stabla",
        "url": {}
      },
      {
        "name": "Florian Camphausen",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-008/",
    "title": "The regress function",
    "description": "\"The regress function\" published in R News.",
    "author": [
      {
        "name": "David Clifford",
        "url": {}
      },
      {
        "name": "Peter McCullagh",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-009/",
    "title": "Processing data for outliers",
    "description": "\"Processing data for outliers\" published in R News.",
    "author": [
      {
        "name": "Lukasz Komsta",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-010/",
    "title": "Analysing equity portfolios in R",
    "description": "\"Analysing equity portfolios in R\" published in R News.",
    "author": [
      {
        "name": "David Kane",
        "url": {}
      },
      {
        "name": "Jeff Enos",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-011/",
    "title": "GroupSeq: Designing clinical trials using group sequential designs",
    "description": "\"GroupSeq: Designing clinical trials using group sequential designs\" published in R News.",
    "author": [
      {
        "name": "Roman Pahl",
        "url": {}
      },
      {
        "name": "Andreas Ziegler",
        "url": {}
      },
      {
        "name": "Inke R. König",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-011/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 254,
    "preview_height": 192
  },
  {
    "path": "articles/RN-2006-012/",
    "title": "Using R/Sweave in everyday clinical practice",
    "description": "\"Using R/Sweave in everyday clinical practice\" published in R News.",
    "author": [
      {
        "name": "Sven Garbade",
        "url": {}
      },
      {
        "name": "Peter Burgard",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-012/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 304,
    "preview_height": 278
  },
  {
    "path": "articles/RN-2006-013/",
    "title": "changeLOS: An R-package for change in length of hospital stay based on the Aalen-Johansen estimator",
    "description": "\"changeLOS: An R-package for change in length of hospital stay based on the Aalen-Johansen estimator\" published in R News.",
    "author": [
      {
        "name": "M. Wangler",
        "url": {}
      },
      {
        "name": "J. Beyersmann",
        "url": {}
      },
      {
        "name": "M. Schumacher",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-014/",
    "title": "Balloon plot",
    "description": "\"Balloon plot\" published in R News.",
    "author": [
      {
        "name": "Nitin Jain",
        "url": {}
      },
      {
        "name": "Gregory R. Warnes",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-015/",
    "title": "Drawing pedigree diagrams with R and graphviz",
    "description": "\"Drawing pedigree diagrams with R and graphviz\" published in R News.",
    "author": [
      {
        "name": "Jing Hua Zhao",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-016/",
    "title": "Non-standard fonts in PostScript and PDF graphics",
    "description": "\"Non-standard fonts in PostScript and PDF graphics\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      },
      {
        "name": "Brian Ripley",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-016/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 94,
    "preview_height": 92
  },
  {
    "path": "articles/RN-2006-017/",
    "title": "The doBy package",
    "description": "\"The doBy package\" published in R News.",
    "author": [
      {
        "name": "Søren Højsgaard",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-018/",
    "title": "Normed division algebras with R: Introducing the onion package",
    "description": "\"Normed division algebras with R: Introducing the onion package\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-019/",
    "title": "Electrical properties of resistor networks",
    "description": "\"Electrical properties of resistor networks\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-001/",
    "title": "Applied Bayesian inference in R using MCMCpack",
    "description": "\"Applied Bayesian inference in R using MCMCpack\" published in R News.",
    "author": [
      {
        "name": "Andrew D. Martin",
        "url": {}
      },
      {
        "name": "Kevin M. Quinn",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-002/",
    "title": "CODA: Convergence diagnosis and output analysis for MCMC",
    "description": "\"CODA: Convergence diagnosis and output analysis for MCMC\" published in R News.",
    "author": [
      {
        "name": "Martyn Plummer",
        "url": {}
      },
      {
        "name": "Nicky Best",
        "url": {}
      },
      {
        "name": "Kate Cowles",
        "url": {}
      },
      {
        "name": "Karen Vines",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-003/",
    "title": "Bayesian software validation",
    "description": "\"Bayesian software validation\" published in R News.",
    "author": [
      {
        "name": "Samantha Cook",
        "url": {}
      },
      {
        "name": "Andrew Gelman",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-004/",
    "title": "Making BUGS open",
    "description": "\"Making BUGS open\" published in R News.",
    "author": [
      {
        "name": "Andrew Thomas",
        "url": {}
      },
      {
        "name": "Bob OH́ara",
        "url": {}
      },
      {
        "name": "Uwe Ligges",
        "url": {}
      },
      {
        "name": "Sibylle Sturtz",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-005/",
    "title": "The BUGS language",
    "description": "\"The BUGS language\" published in R News.",
    "author": [
      {
        "name": "Andrew Thomas",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-006/",
    "title": "Bayesian data analysis using R",
    "description": "\"Bayesian data analysis using R\" published in R News.",
    "author": [
      {
        "name": "Jouni Kerman",
        "url": {}
      },
      {
        "name": "Andrew Gelman",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-013/",
    "title": "BMA: An R package for Bayesian model averaging",
    "description": "\"BMA: An R package for Bayesian model averaging\" published in R News.",
    "author": [
      {
        "name": "Adrian E. Raftery",
        "url": {}
      },
      {
        "name": "Ian S. Painter",
        "url": {}
      },
      {
        "name": "Christopher T. Volinsky",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-014/",
    "title": "Classes and methods for spatial data in R",
    "description": "\"Classes and methods for spatial data in R\" published in R News.",
    "author": [
      {
        "name": "Edzer J. Pebesma",
        "url": {}
      },
      {
        "name": "Roger S. Bivand",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-015/",
    "title": "Running long R jobs with Condor DAG",
    "description": "\"Running long R jobs with Condor DAG\" published in R News.",
    "author": [
      {
        "name": "Xianhong Xie",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-016/",
    "title": "Rstream: Streams of random numbers for stochastic simulation",
    "description": "\"Rstream: Streams of random numbers for stochastic simulation\" published in R News.",
    "author": [
      {
        "name": "Pierre L’Ecuyer",
        "url": {}
      },
      {
        "name": "Josef Leydold",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-017/",
    "title": "Mfp: Multivariable fractional polynomials",
    "description": "\"Mfp: Multivariable fractional polynomials\" published in R News.",
    "author": [
      {
        "name": "Axel Benner",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-018/",
    "title": "Crossdes: A package for design and randomization in crossover studies",
    "description": "\"Crossdes: A package for design and randomization in crossover studies\" published in R News.",
    "author": [
      {
        "name": "Oliver Sailer",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-019/",
    "title": "R Help Desk: Make “R CMD” work under Windows – an example",
    "description": "\"R Help Desk: Make “R CMD” work under Windows – an example\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      },
      {
        "name": "Duncan Murdoch",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-001/",
    "title": "Internationalization features of R 2.1.0",
    "description": "\"Internationalization features of R 2.1.0\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2005-001/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 665,
    "preview_height": 356
  },
  {
    "path": "articles/RN-2005-002/",
    "title": "Packages and their management in R 2.1.0",
    "description": "\"Packages and their management in R 2.1.0\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2005-002/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 172,
    "preview_height": 159
  },
  {
    "path": "articles/RN-2005-003/",
    "title": "Recent changes in grid graphics",
    "description": "\"Recent changes in grid graphics\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-004/",
    "title": "Hoa: An R package bundle for higher order likelihood inference",
    "description": "\"Hoa: An R package bundle for higher order likelihood inference\" published in R News.",
    "author": [
      {
        "name": "Alessandra Brazzale",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-005/",
    "title": "Fitting linear mixed models in R",
    "description": "\"Fitting linear mixed models in R\" published in R News.",
    "author": [
      {
        "name": "Douglas Bates",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-006/",
    "title": "Using R for statistical seismology",
    "description": "\"Using R for statistical seismology\" published in R News.",
    "author": [
      {
        "name": "Ray Brownrigg",
        "url": {}
      },
      {
        "name": "David Harte",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-007/",
    "title": "Literate programming for creating and maintaining packages",
    "description": "\"Literate programming for creating and maintaining packages\" published in R News.",
    "author": [
      {
        "name": "Jonathan Rougier",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-008/",
    "title": "CRAN task views",
    "description": "\"CRAN task views\" published in R News.",
    "author": [
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-009/",
    "title": "Using control structures with Sweave",
    "description": "\"Using control structures with Sweave\" published in R News.",
    "author": [
      {
        "name": "Damian Betebenner",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-010/",
    "title": "The value of R for preclinical statisticians",
    "description": "\"The value of R for preclinical statisticians\" published in R News.",
    "author": [
      {
        "name": "Bill Pikounis",
        "url": {}
      },
      {
        "name": "Andy Liaw",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-011/",
    "title": "Recreational mathematics with R: Introducing the “magic” package",
    "description": "\"Recreational mathematics with R: Introducing the “magic” package\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-012/",
    "title": "Programmer’s Niche: How do you spell that number?",
    "description": "\"Programmer’s Niche: How do you spell that number?\" published in R News.",
    "author": [
      {
        "name": "John Fox",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-009/",
    "title": "Lazy loading and packages in R 2.0.0",
    "description": "\"Lazy loading and packages in R 2.0.0\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-010/",
    "title": "Fonts, lines, and transparency in R graphics",
    "description": "\"Fonts, lines, and transparency in R graphics\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-011/",
    "title": "The NMMAPSdata package",
    "description": "\"The NMMAPSdata package\" published in R News.",
    "author": [
      {
        "name": "Roger D. Peng",
        "url": {}
      },
      {
        "name": "Leah J. Welty",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-012/",
    "title": "Laying out pathways with Rgraphviz",
    "description": "\"Laying out pathways with Rgraphviz\" published in R News.",
    "author": [
      {
        "name": "Jeff Gentry",
        "url": {}
      },
      {
        "name": "Vincent Carey",
        "url": {}
      },
      {
        "name": "Emden Gansner",
        "url": {}
      },
      {
        "name": "Robert Gentleman",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-013/",
    "title": "Fusing R and BUGS through Wine",
    "description": "\"Fusing R and BUGS through Wine\" published in R News.",
    "author": [
      {
        "name": "Jun Yan",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-014/",
    "title": "R package maintenance",
    "description": "\"R package maintenance\" published in R News.",
    "author": [
      {
        "name": "Paul Gilbert",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-001/",
    "title": "The decision to use R",
    "description": "\"The decision to use R\" published in R News.",
    "author": [
      {
        "name": "Marc Schwartz",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-002/",
    "title": "The ade4 package — I: One-table methods",
    "description": "\"The ade4 package — I: One-table methods\" published in R News.",
    "author": [
      {
        "name": "Daniel Chessel",
        "url": {}
      },
      {
        "name": "Anne B. Dufour",
        "url": {}
      },
      {
        "name": "Jean Thioulouse",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-003/",
    "title": "Qcc: An R package for quality control charting and statistical process control",
    "description": "\"Qcc: An R package for quality control charting and statistical process control\" published in R News.",
    "author": [
      {
        "name": "Luca Scrucca",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-004/",
    "title": "Least squares calculations in R",
    "description": "\"Least squares calculations in R\" published in R News.",
    "author": [
      {
        "name": "Douglas Bates",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-005/",
    "title": "Tools for interactively exploring R packages",
    "description": "\"Tools for interactively exploring R packages\" published in R News.",
    "author": [
      {
        "name": "Jianhua Zhang",
        "url": {}
      },
      {
        "name": "Robert Gentleman",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2004-005/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 531,
    "preview_height": 377
  },
  {
    "path": "articles/RN-2004-006/",
    "title": "The survival package",
    "description": "\"The survival package\" published in R News.",
    "author": [
      {
        "name": "Thomas Lumley",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-007/",
    "title": "R Help Desk: Date and time classes in R",
    "description": "\"R Help Desk: Date and time classes in R\" published in R News.",
    "author": [
      {
        "name": "Gabor Grothendieck",
        "url": {}
      },
      {
        "name": "Thomas Petzoldt",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-008/",
    "title": "Programmers’ Niche: A simple class, in S3 and S4",
    "description": "\"Programmers’ Niche: A simple class, in S3 and S4\" published in R News.",
    "author": [
      {
        "name": "Thomas Lumley",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-014/",
    "title": "Dimensional reduction for data mapping",
    "description": "\"Dimensional reduction for data mapping\" published in R News.",
    "author": [
      {
        "name": "Jonathan Edwards",
        "url": {}
      },
      {
        "name": "Paul Oman",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-015/",
    "title": "R as a simulation platform in ecological modelling",
    "description": "\"R as a simulation platform in ecological modelling\" published in R News.",
    "author": [
      {
        "name": "Thomas Petzoldt",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2003-015/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 10,
    "preview_height": 13
  },
  {
    "path": "articles/RN-2003-016/",
    "title": "Using R for estimating longitudinal student achievement models",
    "description": "\"Using R for estimating longitudinal student achievement models\" published in R News.",
    "author": [
      {
        "name": "J. R. Lockwood",
        "url": {}
      },
      {
        "name": "Harold Doran",
        "url": {}
      },
      {
        "name": "Daniel F. McCaffrey",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-017/",
    "title": "lmeSplines",
    "description": "\"lmeSplines\" published in R News.",
    "author": [
      {
        "name": "Rod Ball",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-018/",
    "title": "Debugging without (too many) tears",
    "description": "\"Debugging without (too many) tears\" published in R News.",
    "author": [
      {
        "name": "Mark Bravington",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-019/",
    "title": "The R2HTML package",
    "description": "\"The R2HTML package\" published in R News.",
    "author": [
      {
        "name": "Eric Lecoutre",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-020/",
    "title": "R Help Desk: Package management",
    "description": "\"R Help Desk: Package management\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-009/",
    "title": "R Help Desk: An introduction to using R’s base graphics",
    "description": "\"R Help Desk: An introduction to using R’s base graphics\" published in R News.",
    "author": [
      {
        "name": "Marc Schwartz",
        "url": {}
      }
    ],
    "date": "2003-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-010/",
    "title": "Integrating grid graphics output with base graphics output",
    "description": "\"Integrating grid graphics output with base graphics output\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2003-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-011/",
    "title": "A new package for the general error distribution",
    "description": "\"A new package for the general error distribution\" published in R News.",
    "author": [
      {
        "name": "Angelo Mineo",
        "url": {}
      }
    ],
    "date": "2003-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-012/",
    "title": "Web-based microarray analysis using Bioconductor",
    "description": "\"Web-based microarray analysis using Bioconductor\" published in R News.",
    "author": [
      {
        "name": "Colin A. Smith",
        "url": {}
      }
    ],
    "date": "2003-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2003-012/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 448,
    "preview_height": 510
  },
  {
    "path": "articles/RN-2003-013/",
    "title": "Sweave, part II: Package vignettes",
    "description": "\"Sweave, part II: Package vignettes\" published in R News.",
    "author": [
      {
        "name": "Friedrich Leisch",
        "url": {}
      }
    ],
    "date": "2003-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-001/",
    "title": "Name space management for R",
    "description": "\"Name space management for R\" published in R News.",
    "author": [
      {
        "name": "Luke Tierney",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-002/",
    "title": "Converting packages to S4",
    "description": "\"Converting packages to S4\" published in R News.",
    "author": [
      {
        "name": "Douglas Bates",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-003/",
    "title": "The genetics package",
    "description": "\"The genetics package\" published in R News.",
    "author": [
      {
        "name": "Gregory R. Warnes",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-004/",
    "title": "Variance inflation factors",
    "description": "\"Variance inflation factors\" published in R News.",
    "author": [
      {
        "name": "Jürgen Groß",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-005/",
    "title": "Building Microsoft Windows versions of R and R packages under Intel Linux",
    "description": "\"Building Microsoft Windows versions of R and R packages under Intel Linux\" published in R News.",
    "author": [
      {
        "name": "Jun Yan",
        "url": {}
      },
      {
        "name": "A. J. Rossini",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-006/",
    "title": "Analysing survey data in R",
    "description": "\"Analysing survey data in R\" published in R News.",
    "author": [
      {
        "name": "Thomas Lumley",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-007/",
    "title": "Computational gains using RPVM on a Beowulf cluster",
    "description": "\"Computational gains using RPVM on a Beowulf cluster\" published in R News.",
    "author": [
      {
        "name": "Brett Carson",
        "url": {}
      },
      {
        "name": "Robert Murison",
        "url": {}
      },
      {
        "name": "Ian A. Mason",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-008/",
    "title": "R Help Desk: Getting help – R’s help facilities and manuals",
    "description": "\"R Help Desk: Getting help – R’s help facilities and manuals\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-017/",
    "title": "Resampling methods in R: The boot package",
    "description": "\"Resampling methods in R: The boot package\" published in R News.",
    "author": [
      {
        "name": "Angelo J. Canty",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2002-017/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 85,
    "preview_height": 102
  },
  {
    "path": "articles/RN-2002-018/",
    "title": "Diagnostic checking in regression relationships",
    "description": "\"Diagnostic checking in regression relationships\" published in R News.",
    "author": [
      {
        "name": "Achim Zeileis",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-019/",
    "title": "Delayed data packages",
    "description": "\"Delayed data packages\" published in R News.",
    "author": [
      {
        "name": "David E. Brahm",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-020/",
    "title": "Geepack: Yet another package for generalized estimating equations",
    "description": "\"Geepack: Yet another package for generalized estimating equations\" published in R News.",
    "author": [
      {
        "name": "Jun Yan",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-021/",
    "title": "On multiple comparisons in R",
    "description": "\"On multiple comparisons in R\" published in R News.",
    "author": [
      {
        "name": "Frank Bretz",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      },
      {
        "name": "Peter Westfall",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-022/",
    "title": "Classification and regression by randomForest",
    "description": "\"Classification and regression by randomForest\" published in R News.",
    "author": [
      {
        "name": "Andy Liaw",
        "url": {}
      },
      {
        "name": "Matthew Wiener",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-023/",
    "title": "Some strategies for dealing with genomic data",
    "description": "\"Some strategies for dealing with genomic data\" published in R News.",
    "author": [
      {
        "name": "Robert Gentleman",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-024/",
    "title": "Changes to the R-Tcl/Tk package",
    "description": "\"Changes to the R-Tcl/Tk package\" published in R News.",
    "author": [
      {
        "name": "Peter Dalgaard",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-025/",
    "title": "Sweave, part I: Mixing R and LaTeX",
    "description": "\"Sweave, part I: Mixing R and LaTeX\" published in R News.",
    "author": [
      {
        "name": "Friedrich Leisch",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2002-025/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 1,
    "preview_height": 1
  },
  {
    "path": "articles/RN-2002-026/",
    "title": "R Help Desk: Automation of mathematical annotation in plots",
    "description": "\"R Help Desk: Automation of mathematical annotation in plots\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-007/",
    "title": "Time series in R 1.5.0",
    "description": "\"Time series in R 1.5.0\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-008/",
    "title": "Naive time series forecasting methods",
    "description": "\"Naive time series forecasting methods\" published in R News.",
    "author": [
      {
        "name": "David Meyer",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-009/",
    "title": "Rmpi: Parallel statistical computing in R",
    "description": "\"Rmpi: Parallel statistical computing in R\" published in R News.",
    "author": [
      {
        "name": "Hao Yu",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-010/",
    "title": "The grid graphics package",
    "description": "\"The grid graphics package\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-011/",
    "title": "Lattice",
    "description": "\"Lattice\" published in R News.",
    "author": [
      {
        "name": "Deepayan Sarkar",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-012/",
    "title": "Programmer’s Niche",
    "description": "\"Programmer’s Niche\" published in R News.",
    "author": [
      {
        "name": "Bill Venables",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-013/",
    "title": "geoRglm: A package for generalised linear spatial models",
    "description": "\"geoRglm: A package for generalised linear spatial models\" published in R News.",
    "author": [
      {
        "name": "Ole F. Christensen",
        "url": {}
      },
      {
        "name": "Paulo J. Ribeiro",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-014/",
    "title": "Querying PubMed",
    "description": "\"Querying PubMed\" published in R News.",
    "author": [
      {
        "name": "Robert Gentleman",
        "url": {}
      },
      {
        "name": "Jeff Gentry",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-015/",
    "title": "Evd: Extreme value distributions",
    "description": "\"Evd: Extreme value distributions\" published in R News.",
    "author": [
      {
        "name": "Alec Stephenson",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-016/",
    "title": "Ipred: Improved predictors",
    "description": "\"Ipred: Improved predictors\" published in R News.",
    "author": [
      {
        "name": "Andrea Peters",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      },
      {
        "name": "Berthold Lausen",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-001/",
    "title": "Reading foreign files",
    "description": "\"Reading foreign files\" published in R News.",
    "author": [
      {
        "name": "Duncan Murdoch",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-002/",
    "title": "Maximally selected rank statistics in R",
    "description": "\"Maximally selected rank statistics in R\" published in R News.",
    "author": [
      {
        "name": "Torsten Hothorn",
        "url": {}
      },
      {
        "name": "Berthold Lausen",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-003/",
    "title": "Quality control and early diagnostics for cDNA microarrays",
    "description": "\"Quality control and early diagnostics for cDNA microarrays\" published in R News.",
    "author": [
      {
        "name": "Günther Sawitzki",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-004/",
    "title": "Bioconductor",
    "description": "\"Bioconductor\" published in R News.",
    "author": [
      {
        "name": "Robert Gentleman",
        "url": {}
      },
      {
        "name": "Vincent Carey",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-005/",
    "title": "AnalyzeFMRI: An R package for the exploration and analysis of MRI and fMRI datasets",
    "description": "\"AnalyzeFMRI: An R package for the exploration and analysis of MRI and fMRI datasets\" published in R News.",
    "author": [
      {
        "name": "Jonathan Marchini",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-006/",
    "title": "Using R for the analysis of DNA microarray data",
    "description": "\"Using R for the analysis of DNA microarray data\" published in R News.",
    "author": [
      {
        "name": "Sandrine Dudoit",
        "url": {}
      },
      {
        "name": "Yee Hwa Yang",
        "url": {}
      },
      {
        "name": "Ben Bolstad",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2002-006/preview.png",
    "last_modified": "2022-10-18T09:10:01+00:00",
    "input_file": {},
    "preview_width": 1441,
    "preview_height": 1080
  },
  {
    "path": "articles/RN-2001-018/",
    "title": "Porting R to Darwin/X11 and Mac OS X",
    "description": "\"Porting R to Darwin/X11 and Mac OS X\" published in R News.",
    "author": [
      {
        "name": "Jan Leeuw",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-019/",
    "title": "RPVM: Cluster statistical computing in R",
    "description": "\"RPVM: Cluster statistical computing in R\" published in R News.",
    "author": [
      {
        "name": "Michael Na Li",
        "url": {}
      },
      {
        "name": "A. J. Rossini",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-020/",
    "title": "Strucchange: Testing for structural change in linear regression relationships",
    "description": "\"Strucchange: Testing for structural change in linear regression relationships\" published in R News.",
    "author": [
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-021/",
    "title": "Programmer’s Niche: Macros in R",
    "description": "\"Programmer’s Niche: Macros in R\" published in R News.",
    "author": [
      {
        "name": "Thomas Lumley",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-022/",
    "title": "More on spatial data",
    "description": "\"More on spatial data\" published in R News.",
    "author": [
      {
        "name": "Roger Bivand",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-023/",
    "title": "Object-oriented programming in R",
    "description": "\"Object-oriented programming in R\" published in R News.",
    "author": [
      {
        "name": "John M. Chambers",
        "url": {}
      },
      {
        "name": "Duncan Temple Lang",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-024/",
    "title": "In search of C/C++ & FORTRAN routines",
    "description": "\"In search of C/C++ & FORTRAN routines\" published in R News.",
    "author": [
      {
        "name": "Duncan Temple Lang",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-025/",
    "title": "Support vector machines",
    "description": "\"Support vector machines\" published in R News.",
    "author": [
      {
        "name": "David Meyer",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-026/",
    "title": "A primer on the R-Tcl/Tk package",
    "description": "\"A primer on the R-Tcl/Tk package\" published in R News.",
    "author": [
      {
        "name": "Peter Dalgaard",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2001-026/preview.png",
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {},
    "preview_width": 92,
    "preview_height": 73
  },
  {
    "path": "articles/RN-2001-027/",
    "title": "Wle: A package for robust statistics using weighted likelihood",
    "description": "\"Wle: A package for robust statistics using weighted likelihood\" published in R News.",
    "author": [
      {
        "name": "Claudio Agostinelli",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-011/",
    "title": "Date-time classes",
    "description": "\"Date-time classes\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      },
      {
        "name": "Kurt Hornik",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-012/",
    "title": "Installing R under Windows",
    "description": "\"Installing R under Windows\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2001-012/preview.png",
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {},
    "preview_width": 501,
    "preview_height": 383
  },
  {
    "path": "articles/RN-2001-013/",
    "title": "geoR: A package for geostatistical analysis",
    "description": "\"geoR: A package for geostatistical analysis\" published in R News.",
    "author": [
      {
        "name": "Paulo J. Ribeiro, Jr.",
        "url": {}
      },
      {
        "name": "Peter J. Diggle",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-014/",
    "title": "Simulation and analysis of random fields",
    "description": "\"Simulation and analysis of random fields\" published in R News.",
    "author": [
      {
        "name": "Martin Schlather",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-015/",
    "title": "Mgcv: GAMs and generalized ridge regression for R",
    "description": "\"Mgcv: GAMs and generalized ridge regression for R\" published in R News.",
    "author": [
      {
        "name": "Simon N. Wood",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-016/",
    "title": "What’s the point of “tensor”?",
    "description": "\"What’s the point of “tensor”?\" published in R News.",
    "author": [
      {
        "name": "Jonathan Rougier",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-017/",
    "title": "On multivariate t and Gauß probabilities in R",
    "description": "\"On multivariate t and Gauß probabilities in R\" published in R News.",
    "author": [
      {
        "name": "Torsten Hothorn",
        "url": {}
      },
      {
        "name": "Frank Bretz",
        "url": {}
      },
      {
        "name": "Alan Genz",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-001/",
    "title": "Under new memory management",
    "description": "\"Under new memory management\" published in R News.",
    "author": [
      {
        "name": "Luke Tierney",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-002/",
    "title": "On exact rank tests in R",
    "description": "\"On exact rank tests in R\" published in R News.",
    "author": [
      {
        "name": "Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-003/",
    "title": "Porting R to the Macintosh",
    "description": "\"Porting R to the Macintosh\" published in R News.",
    "author": [
      {
        "name": "Stefano M. Iacus",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-004/",
    "title": "The density of the non-central chi-squared distribution for large values of the noncentrality parameter",
    "description": "\"The density of the non-central chi-squared distribution for large values of the noncentrality parameter\" published in R News.",
    "author": [
      {
        "name": "Peter Dalgaard",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-005/",
    "title": "Connections",
    "description": "\"Connections\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-006/",
    "title": "Using databases with R",
    "description": "\"Using databases with R\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-007/",
    "title": "Rcgi 4: Making web statistics even easier",
    "description": "\"Rcgi 4: Making web statistics even easier\" published in R News.",
    "author": [
      {
        "name": "M. J. Ray",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-008/",
    "title": "Omegahat packages for R",
    "description": "\"Omegahat packages for R\" published in R News.",
    "author": [
      {
        "name": "John M. Chambers",
        "url": {}
      },
      {
        "name": "Duncan Temple Lang",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-009/",
    "title": "Using XML for statistics: The XML package",
    "description": "\"Using XML for statistics: The XML package\" published in R News.",
    "author": [
      {
        "name": "Duncan Temple Lang",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-010/",
    "title": "Programmer’s Niche",
    "description": "\"Programmer’s Niche\" published in R News.",
    "author": [
      {
        "name": "Bill Venables",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-10-18T09:10:00+00:00",
    "input_file": {}
  }
]
